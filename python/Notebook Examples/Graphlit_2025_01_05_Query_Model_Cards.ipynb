{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyNCW2WxIBbHbEO6A/+FzovP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2025_01_05_Query_Model_Cards.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to query the available LLMs, embedding models and reranking models via Graphlit API."
      ],
      "metadata": {
        "id": "M7pOtiP1OaKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882373f3-9ff7-41be-c212-65c38df7a100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphlit-client in /usr/local/lib/python3.10/dist-packages (1.0.20250105001)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.3)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Graphlit"
      ],
      "metadata": {
        "id": "abV1114jL-bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from graphlit_api import QueryModelsModelsResults\n",
        "\n",
        "async def query_models():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.query_models()\n",
        "\n",
        "        return response.models.results if response.models is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_model(card: QueryModelsModelsResults) -> str:\n",
        "    # Safely unpack features lists (fall back to empty list if None)\n",
        "    features = card.features\n",
        "    metadata = card.metadata\n",
        "\n",
        "    key_features = features.key_features if features is not None and features.key_features is not None else []\n",
        "    strengths = features.strengths if features is not None and features.strengths is not None else []\n",
        "    potential_use_cases = features.use_cases if features is not None and features.use_cases is not None else []\n",
        "\n",
        "    # Determine maximum rows needed for the Features table\n",
        "    max_feature_rows = max(len(key_features), len(strengths), len(potential_use_cases))\n",
        "\n",
        "    # Build the Features table row by row\n",
        "    feature_rows = []\n",
        "    for i in range(max_feature_rows):\n",
        "        key_col = f\"- {key_features[i]}\" if i < len(key_features) else \"\"\n",
        "        strengths_col = f\"- {strengths[i]}\" if i < len(strengths) else \"\"\n",
        "        potential_col = f\"- {potential_use_cases[i]}\" if i < len(potential_use_cases) else \"\"\n",
        "        feature_rows.append(f\"| {key_col} | {strengths_col} | {potential_col} |\")\n",
        "\n",
        "    if len(feature_rows) > 0:\n",
        "        # Join all rows into a single string with the header\n",
        "        features_table = (\n",
        "            \"| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\\n\"\n",
        "            \"|---------------------------|----------------------------|--------------------------------|\\n\"\n",
        "            + \"\\n\".join(feature_rows)\n",
        "        )\n",
        "    else:\n",
        "        features_table = \"n/a\"\n",
        "\n",
        "    # Prepare data for the Metadata table\n",
        "    metadata_table_rows = [\n",
        "        f\"| **Multilingual**            | {'Yes' if metadata and metadata.multilingual else 'No'} |\",\n",
        "        f\"| **Multimodal**              | {'Yes' if metadata and metadata.multimodal else 'No'} |\",\n",
        "    ]\n",
        "\n",
        "    if metadata is not None:\n",
        "        if metadata.knowledge_cutoff:\n",
        "            metadata_table_rows.append(f\"| **Knowledge Cutoff**        | {metadata.knowledge_cutoff} |\")\n",
        "        if metadata.prompt_cost_per_million:\n",
        "            metadata_table_rows.append(f\"| **Prompt Cost per Million** | `${metadata.prompt_cost_per_million:,.2f}` |\")\n",
        "        if metadata.completion_cost_per_million:\n",
        "            metadata_table_rows.append(f\"| **Completion Cost per Million** | `${metadata.completion_cost_per_million:,.2f}` |\")\n",
        "        if metadata.embeddings_cost_per_million:\n",
        "            metadata_table_rows.append(f\"| **Embedding Cost per Million** | `${metadata.embeddings_cost_per_million:,.2f}` |\")\n",
        "        if metadata.reranking_cost_per_million:\n",
        "            metadata_table_rows.append(f\"| **Reranking Cost per Million** | `${metadata.reranking_cost_per_million:,.2f}` |\")\n",
        "        if metadata.context_window_tokens is not None:\n",
        "            metadata_table_rows.append(f\"| **Context Window Tokens**   | {metadata.context_window_tokens:,} |\")\n",
        "        if metadata.max_output_tokens is not None:\n",
        "            metadata_table_rows.append(f\"| **Max Output Tokens**       | {metadata.max_output_tokens:,} |\")\n",
        "\n",
        "    metadata_table = (\n",
        "        \"| **Property**               | **Value**                    |\\n\"\n",
        "        \"|----------------------------|------------------------------|\\n\"\n",
        "        + \"\\n\".join(metadata_table_rows)\n",
        "    )\n",
        "\n",
        "    # Construct the final Markdown\n",
        "    markdown = f\"\"\"# {card.name}\n",
        "\n",
        "### Model Type: {card.type}\n",
        "**Model:** {card.model_type}.{card.model}\n",
        "\n",
        "**Description:** {card.description}\n",
        "\n",
        "**Model Card URI:** {card.uri}\n",
        "\n",
        "**Available On:** {', '.join(card.available_on) if card.available_on else \"N/A\"}\n",
        "\n",
        "---\n",
        "\n",
        "## Features\n",
        "{features_table}\n",
        "\n",
        "---\n",
        "\n",
        "## Metadata\n",
        "{metadata_table}\n",
        "\n",
        "---\n",
        "\n",
        "> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration.\n",
        "\"\"\"\n",
        "\n",
        "    return markdown.strip()\n"
      ],
      "metadata": {
        "id": "-nj-8VEblX60"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute Graphlit example"
      ],
      "metadata": {
        "id": "srzhQt4COLVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "models = await query_models()\n",
        "\n",
        "if models is not None:\n",
        "    for model in models:\n",
        "        if model is not None:\n",
        "            #print(pretty_print_model(model))\n",
        "            display(Markdown(pretty_print_model(model)))\n",
        "\n",
        "            print(\"---\")\n",
        "            print()"
      ],
      "metadata": {
        "id": "-5FVNpl1C9o6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "042a9445-2c61-4903-f1a2-a1e187d0cca5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# jina-clip-v2\n\n### Model Type: MULTIMODAL_EMBEDDING\n**Model:** JinaModels.CLIP_Image\n\n**Description:** The jina-clip-v2 model is a state-of-the-art CLIP-style model designed to handle both text and image data, offering multilingual support for 89 languages. It excels in high-resolution image processing with a resolution of 512x512 and employs Matryoshka representation learning for efficient truncated embeddings. This model is ideal for applications requiring robust multimodal capabilities, such as search and retrieval tasks across diverse languages and formats.\n\n**Model Card URI:** https://huggingface.co/jinaai/jina-clip-v2\n\n**Available On:** AWS SageMaker, Microsoft Azure, Google Cloud\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n| - Multilingual support for 89 languages | - Handles both text and image data | - Search and retrieval tasks |\n| - High image resolution at 512x512 | - Supports high-resolution image processing | - Multimodal applications |\n| - Matryoshka representation learning | - Efficient truncated embeddings | - Cross-lingual tasks |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | Yes |\n| **Multimodal**              | Yes |\n| **Context Window Tokens**   | 8,191 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# jina-embeddings-v3\n\n### Model Type: TEXT_EMBEDDING\n**Model:** JinaModels.Embed_3_0\n\n**Description:** The jina-embeddings-v3 model is a cutting-edge multilingual text embedding solution, boasting 570 million parameters and an impressive 8192 token-length capacity. It surpasses leading proprietary models from OpenAI and Cohere in performance on the MTEB benchmark, making it an excellent choice for applications requiring extensive text analysis and high accuracy in multilingual contexts.\n\n**Model Card URI:** https://huggingface.co/jinaai/jina-embeddings-v3\n\n**Available On:** AWS SageMaker, Microsoft Azure, Google Cloud\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n| - 570M parameters | - High performance in multilingual contexts | - Multilingual text analysis |\n| - 8192 token-length capacity | - Extensive text analysis capabilities | - High-accuracy applications |\n| - Outperforms OpenAI and Cohere on MTEB |  | - Large-scale text processing |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | Yes |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 8,191 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# voyage-3-large\n\n### Model Type: TEXT_EMBEDDING\n**Model:** VoyageModels.Voyage_3_0_Large\n\n**Description:** The Voyage-3-Large model is a robust text embedding solution designed to handle large-scale text data efficiently. It offers a generous free tier of 200 million tokens, making it an economical choice for businesses looking to leverage text embeddings without incurring high costs. With a competitive pricing structure of $0.18 per million tokens, it is ideal for applications requiring extensive text processing capabilities.\n\n**Model Card URI:** None\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 32,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# voyage-3\n\n### Model Type: TEXT_EMBEDDING\n**Model:** VoyageModels.Voyage_3_0\n\n**Description:** Voyage-3 is a versatile text embedding model that provides efficient processing of text data. It offers a substantial free tier of 200 million tokens, making it accessible for various applications. Priced at $0.06 per million tokens, it is suitable for projects that require cost-effective text embedding solutions.\n\n**Model Card URI:** https://blog.voyageai.com/2024/09/18/voyage-3/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 32,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# voyage-3-lite\n\n### Model Type: TEXT_EMBEDDING\n**Model:** VoyageModels.Voyage_Lite_3_0\n\n**Description:** Voyage-3-Lite is an economical text embedding model designed for lightweight applications. It offers a generous free tier of 200 million tokens and is priced at just $0.02 per million tokens, making it an excellent choice for budget-conscious projects that still require reliable text embedding capabilities.\n\n**Model Card URI:** https://blog.voyageai.com/2024/09/18/voyage-3/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 32,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# voyage-code-3\n\n### Model Type: TEXT_EMBEDDING\n**Model:** VoyageModels.Voyage_Code_3_0\n\n**Description:** Voyage-Code-3 is a specialized text embedding model tailored for code and programming-related text. It provides a substantial free tier of 200 million tokens and is priced at $0.18 per million tokens, making it a valuable tool for developers and organizations working with large volumes of code data.\n\n**Model Card URI:** https://blog.voyageai.com/2024/12/04/voyage-code-3/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 32,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# voyage-finance-2\n\n### Model Type: TEXT_EMBEDDING\n**Model:** VoyageModels.Voyage_Finance_2_0\n\n**Description:** Voyage-Finance-2 is a text embedding model optimized for financial data. It offers a free tier of 50 million tokens and is priced at $0.12 per million tokens, making it suitable for financial institutions and analysts who need to process large amounts of financial text data efficiently.\n\n**Model Card URI:** https://blog.voyageai.com/2024/06/03/domain-specific-embeddings-finance-edition-voyage-finance-2/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 32,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# voyage-law-2\n\n### Model Type: TEXT_EMBEDDING\n**Model:** VoyageModels.Voyage_Law_2_0\n\n**Description:** Voyage-Law-2 is a text embedding model designed for legal text processing. It provides a free tier of 50 million tokens and is priced at $0.12 per million tokens, making it an ideal choice for legal professionals and firms that require efficient text embedding solutions for legal documents.\n\n**Model Card URI:** https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 32,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# voyage-code-2\n\n### Model Type: TEXT_EMBEDDING\n**Model:** VoyageModels.Voyage_Code_2_0\n\n**Description:** Voyage-Code-2 is a text embedding model focused on code and programming text. It offers a free tier of 50 million tokens and is priced at $0.12 per million tokens, making it a cost-effective solution for developers and tech companies dealing with code data.\n\n**Model Card URI:** https://blog.voyageai.com/2024/01/23/voyage-code-2-elevate-your-code-retrieval/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 32,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# mistral-large-latest\n\n### Model Type: COMPLETION\n**Model:** MistralModels.Mistral_Large\n\n**Description:** Mistral Large is a premier model designed for high-complexity reasoning tasks. Released in November 2024, it represents the pinnacle of Mistral's model offerings, providing exceptional performance for demanding applications. With a maximum token capacity of 128k, it is well-suited for tasks requiring extensive context and detailed analysis.\n\n**Model Card URI:** https://mistral.ai/news/mistral-large/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$2.00` |\n| **Completion Cost per Million** | `$6.00` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# pixtral-large-latest\n\n### Model Type: COMPLETION\n**Model:** MistralModels.Pixtral_Large\n\n**Description:** Pixtral Large is a cutting-edge multimodal model from Mistral, released in November 2024. It is designed to handle both text and image inputs, making it ideal for applications that require a comprehensive understanding of diverse data types. With a large context window of 128k tokens, it is capable of processing complex multimodal tasks efficiently.\n\n**Model Card URI:** https://mistral.ai/news/pixtral-large/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | Yes |\n| **Prompt Cost per Million** | `$2.00` |\n| **Completion Cost per Million** | `$6.00` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# mistral-small-latest\n\n### Model Type: COMPLETION\n**Model:** MistralModels.Mistral_Small\n\n**Description:** Mistral Small is an enterprise-grade model designed for tasks that require a compact yet powerful solution. Released in September 2024, it offers a maximum token capacity of 32k, making it suitable for applications that need efficient processing without compromising on performance. This model is ideal for businesses looking for a reliable and scalable solution.\n\n**Model Card URI:** https://mistral.ai/news/september-24-release/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.20` |\n| **Completion Cost per Million** | `$0.60` |\n| **Context Window Tokens**   | 32,000 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# mistral-embed\n\n### Model Type: TEXT_EMBEDDING\n**Model:** MistralModels.Mistral_Embed\n\n**Description:** Mistral Embed is a leading text embedding model designed to extract semantic representations from text. It is ideal for applications that require high-quality text embeddings, such as search and information retrieval. With a context window of 8k tokens, it provides efficient and accurate text representation capabilities.\n\n**Model Card URI:** None\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 8,191 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# pixtral-12b-2409\n\n### Model Type: COMPLETION\n**Model:** MistralModels.Pixtral_12b_2409\n\n**Description:** Pixtral is a versatile 12B model that excels in both text and image understanding. It is designed for applications that require a comprehensive approach to multimodal data processing. With a large context window of 128k tokens, Pixtral is capable of handling complex tasks that involve both textual and visual data.\n\n**Model Card URI:** https://mistral.ai/news/pixtral-12b/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | Yes |\n| **Prompt Cost per Million** | `$0.15` |\n| **Completion Cost per Million** | `$0.15` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# open-mistral-nemo\n\n### Model Type: COMPLETION\n**Model:** MistralModels.Mistral_Nemo\n\n**Description:** Mistral Nemo is a premier multilingual model released in July 2024, designed to handle a wide range of languages with high proficiency. It is ideal for applications that require robust multilingual capabilities, offering a maximum token capacity of 128k. This model is perfect for global applications that need to process diverse linguistic data efficiently.\n\n**Model Card URI:** https://mistral.ai/news/mistral-nemo/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | Yes |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.15` |\n| **Completion Cost per Million** | `$0.15` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# open-mixtral-8x7b\n\n### Model Type: COMPLETION\n**Model:** MistralModels.Mixtral_8x7b_Instruct\n\n**Description:** Mixtral 8x7B is an advanced sparse mixture-of-experts model from Mistral, released in December 2023. It is designed to provide high efficiency and performance for complex tasks. With a maximum token capacity of 32k, it is ideal for applications that require a sophisticated approach to data processing.\n\n**Model Card URI:** https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.70` |\n| **Completion Cost per Million** | `$0.70` |\n| **Context Window Tokens**   | 32,000 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# llama3.1-8b\n\n### Model Type: COMPLETION\n**Model:** CerebrasModels.Llama_3_1_8b\n\n**Description:** Llama 3.1 8B is a powerful AI model developed by Cerebras, leveraging the capabilities of the Cerebras Wafer-Scale Engines and CS-3 systems. This model is instruction-tuned, making it ideal for conversational applications. With 8 billion parameters and a context length of 8192 tokens, it provides developers with a robust tool for creating responsive and intelligent chatbots. The model's training on over 15 trillion tokens ensures a comprehensive understanding of language, making it suitable for a wide range of conversational tasks.\n\n**Model Card URI:** https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md\n\n**Available On:** N/A\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n|  |  | - Conversational applications |\n|  |  | - Chatbots |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Knowledge Cutoff**        | 2025-03-23 |\n| **Prompt Cost per Million** | `$0.10` |\n| **Completion Cost per Million** | `$0.10` |\n| **Context Window Tokens**   | 8,192 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# llama-3.3-70b\n\n### Model Type: COMPLETION\n**Model:** CerebrasModels.Llama_3_3_70b\n\n**Description:** Llama 3.3 70B is an advanced AI model provided by Cerebras, utilizing the high-speed capabilities of the Cerebras Wafer-Scale Engines and CS-3 systems. This model is instruction-tuned for optimal performance in conversational applications. With a massive 70 billion parameters and a context length of 8192 tokens, it is designed to handle complex conversational tasks with ease. The extensive training on over 15 trillion tokens ensures a deep understanding of language, making it a valuable asset for developers looking to build sophisticated chatbots and conversational agents.\n\n**Model Card URI:** https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md\n\n**Available On:** N/A\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n|  |  | - Conversational applications |\n|  |  | - Chatbots |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Knowledge Cutoff**        | 2025-12-23 |\n| **Prompt Cost per Million** | `$0.60` |\n| **Completion Cost per Million** | `$0.60` |\n| **Context Window Tokens**   | 8,192 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# command-r-plus\n\n### Model Type: COMPLETION\n**Model:** CohereModels.Command_R_Plus\n\n**Description:** Command R+ is a state-of-the-art large language model designed to handle complex enterprise use cases with high scalability and performance. It is optimized for real-world applications, making it ideal for businesses looking to integrate advanced AI capabilities into their operations. With its robust architecture, Command R+ can efficiently process large volumes of data, providing accurate and insightful outputs that enhance decision-making and operational efficiency.\n\n**Model Card URI:** https://docs.cohere.com/v2/docs/responsible-use\n\n**Available On:** N/A\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n|  |  | - Enterprise applications |\n|  |  | - Scalable AI solutions |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$2.50` |\n| **Completion Cost per Million** | `$10.00` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# command-r\n\n### Model Type: COMPLETION\n**Model:** CohereModels.Command_R\n\n**Description:** Command R is a versatile generative model tailored for tasks requiring long context understanding, such as retrieval-augmented generation and integration with external APIs. It is designed to handle complex queries and provide coherent and contextually relevant responses, making it a valuable tool for developers and businesses seeking to enhance their AI-driven applications.\n\n**Model Card URI:** https://docs.cohere.com/v2/docs/responsible-use\n\n**Available On:** N/A\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n|  |  | - Long context tasks |\n|  |  | - Retrieval-augmented generation |\n|  |  | - API integration |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$1.50` |\n| **Completion Cost per Million** | `$6.00` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# command-r7b-12-2024\n\n### Model Type: COMPLETION\n**Model:** CohereModels.Command_R7B_202412\n\n**Description:** Command R7B is a compact yet powerful generative model that excels in speed and efficiency, making it ideal for developers looking to build high-performance AI applications. Despite its smaller size, it delivers quality outputs, ensuring that applications run smoothly and effectively without compromising on performance.\n\n**Model Card URI:** https://docs.cohere.com/v2/docs/responsible-use\n\n**Available On:** N/A\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n|  |  | - High-speed AI applications |\n|  |  | - Efficient AI solutions |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.38` |\n| **Completion Cost per Million** | `$1.50` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 4,096 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# embed-english-v3.0\n\n### Model Type: MULTIMODAL_EMBEDDING\n**Model:** CohereModels.Embed_English_3_0\n\n**Description:** Embed 3 is a premier multimodal embedding model designed to serve as an intelligent retrieval engine for semantic search and retrieval-augmented generation systems. It supports both text and image embeddings, making it a versatile tool for developers looking to enhance their applications with advanced search and retrieval capabilities. With its ability to handle multiple modalities, Embed 3 provides comprehensive solutions for complex data environments.\n\n**Model Card URI:** https://docs.cohere.com/v2/docs/cohere-embed\n\n**Available On:** N/A\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n|  |  | - Semantic search |\n|  |  | - Retrieval-augmented generation |\n|  |  | - Multimodal applications |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | Yes |\n| **Context Window Tokens**   | 512 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# gemini-2.0-flash-exp\n\n### Model Type: COMPLETION\n**Model:** GoogleModels.Gemini_2_0_Flash_Experimental\n\n**Description:** Gemini 2.0 Flash is Google's latest experimental multimodal model, designed to deliver next-generation features and capabilities. It supports a wide range of inputs including audio, images, video, and text, and provides text outputs, with plans to support image and audio outputs in the future. The model is optimized for speed and multimodal generation, making it suitable for a diverse variety of tasks. With a massive 1 million token context window, it is ideal for applications requiring extensive data processing. However, as an experimental model, it is recommended for exploratory testing and not for production use.\n\n**Model Card URI:** https://ai.google.dev/gemini-api/docs/models/gemini-v2\n\n**Available On:** Google AI Studio\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n| - Supports input from audio, images, video, and text | - Superior speed | - Exploratory testing |\n| - Outputs text, with image and audio outputs coming soon | - Multimodal generation capabilities | - Prototyping applications requiring multimodal input |\n| - 1 million token context window |  |  |\n| - Optimized for speed and multimodal generation |  |  |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | Yes |\n| **Multimodal**              | Yes |\n| **Knowledge Cutoff**        | 2025-08-24 |\n| **Context Window Tokens**   | 1,048,576 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# gemini-1.5-flash\n\n### Model Type: COMPLETION\n**Model:** GoogleModels.Gemini_1_5_Flash\n\n**Description:** Gemini 1.5 Flash is a highly versatile multimodal model from Google, optimized for fast performance across a wide range of tasks. It accepts inputs from audio, images, video, and text, and provides text outputs, making it suitable for applications that require diverse data processing capabilities. With a large context window and support for various system instructions and JSON modes, it is ideal for developers looking to balance performance and cost in their applications.\n\n**Model Card URI:** https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf\n\n**Available On:** Google AI Studio\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n| - Supports input from audio, images, video, and text | - Fast performance | - Applications requiring diverse data processing |\n| - Outputs text | - Versatile task handling | - Balancing performance and cost |\n| - Large context window |  |  |\n| - Versatile performance across diverse tasks |  |  |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | Yes |\n| **Multimodal**              | Yes |\n| **Knowledge Cutoff**        | 2025-05-24 |\n| **Context Window Tokens**   | 1,048,576 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# gemini-1.5-flash-8b\n\n### Model Type: COMPLETION\n**Model:** GoogleModels.Gemini_1_5_Flash_8b\n\n**Description:** Gemini 1.5 Flash-8B is a smaller variant of the Gemini 1.5 series, tailored for handling high volume tasks that require less computational intelligence. It supports a wide range of inputs including audio, images, video, and text, and provides text outputs. This model is ideal for applications that need to process large amounts of data quickly and efficiently, without the need for complex reasoning capabilities.\n\n**Model Card URI:** https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf\n\n**Available On:** Google AI Studio\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n| - Supports input from audio, images, video, and text | - Efficient data processing | - Applications with high data volume |\n| - Outputs text | - Handles high volume tasks | - Tasks requiring less computational intelligence |\n| - Optimized for high volume tasks |  |  |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | Yes |\n| **Multimodal**              | Yes |\n| **Knowledge Cutoff**        | 2025-05-24 |\n| **Prompt Cost per Million** | `$0.04` |\n| **Completion Cost per Million** | `$0.15` |\n| **Context Window Tokens**   | 1,048,576 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# gemini-1.5-pro\n\n### Model Type: COMPLETION\n**Model:** GoogleModels.Gemini_1_5_Pro\n\n**Description:** Gemini 1.5 Pro is a robust multimodal model from Google, designed to handle complex reasoning tasks across various data types. It accepts inputs from audio, images, video, and text, and provides text outputs, making it suitable for applications that require in-depth data analysis and processing. With a larger context window and support for extensive data inputs, it is ideal for developers looking to implement sophisticated reasoning capabilities in their applications.\n\n**Model Card URI:** https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf\n\n**Available On:** Google AI Studio\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n| - Supports input from audio, images, video, and text | - Handles complex reasoning | - Applications requiring sophisticated reasoning |\n| - Outputs text | - Supports extensive data inputs | - In-depth data analysis |\n| - Optimized for complex reasoning tasks |  |  |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | Yes |\n| **Multimodal**              | Yes |\n| **Knowledge Cutoff**        | 2025-05-24 |\n| **Prompt Cost per Million** | `$1.25` |\n| **Completion Cost per Million** | `$5.00` |\n| **Context Window Tokens**   | 2,097,152 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# text-embedding-004\n\n### Model Type: TEXT_EMBEDDING\n**Model:** GoogleModels.Embedding_004\n\n**Description:** The Text Embedding model from Google is designed to measure the relatedness of text strings, making it an essential tool for applications that require semantic understanding and text similarity analysis. With its optimization for creating embeddings with 768 dimensions, it offers superior retrieval performance, making it ideal for AI applications that rely on text embeddings for enhanced data processing and analysis.\n\n**Model Card URI:** https://ai.google.dev/gemini-api/docs/embeddings\n\n**Available On:** Google AI Studio\n\n---\n\n## Features\n| **Key Features**          | **Strengths**              | **Potential Use Cases**       |\n|---------------------------|----------------------------|--------------------------------|\n| - Measures relatedness of text strings | - Superior retrieval performance | - Semantic understanding applications |\n| - Optimized for 768-dimensional embeddings |  | - Text similarity analysis |\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | Yes |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 2,048 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# claude-3.5-sonnet\n\n### Model Type: COMPLETION\n**Model:** AnthropicModels.Claude_3_5_Sonnet\n\n**Description:** Claude 3.5 Sonnet is Anthropic's most advanced model, designed to handle complex tasks with a large 200K context window. It offers flexible pricing options, including a 50% discount with the Batches API, making it suitable for a wide range of applications. This model is ideal for users who require high intelligence and extensive context handling capabilities.\n\n**Model Card URI:** https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Knowledge Cutoff**        | 2025-04-24 |\n| **Prompt Cost per Million** | `$3.00` |\n| **Completion Cost per Million** | `$15.00` |\n| **Context Window Tokens**   | 200,000 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# claude-3.5-haiku\n\n### Model Type: COMPLETION\n**Model:** AnthropicModels.Claude_3_5_Haiku\n\n**Description:** Claude 3.5 Haiku is designed for speed and cost-effectiveness, offering a 200K context window and optimized latency for faster inference. It is particularly suitable for applications where quick response times and cost efficiency are critical. The model is available with a 50% discount through the Batches API, making it an attractive option for budget-conscious users.\n\n**Model Card URI:** https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf\n\n**Available On:** Amazon Bedrock\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Knowledge Cutoff**        | 2025-07-24 |\n| **Prompt Cost per Million** | `$1.00` |\n| **Completion Cost per Million** | `$5.00` |\n| **Context Window Tokens**   | 200,000 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# claude-3-opus\n\n### Model Type: COMPLETION\n**Model:** AnthropicModels.Claude_3_Opus\n\n**Description:** Claude 3 Opus is a robust model tailored for handling complex tasks, featuring a substantial 200K context window. It is designed for users who need a powerful tool to manage intricate and demanding applications. The model's pricing structure includes options for input, prompt caching, and output, providing flexibility for various use cases.\n\n**Model Card URI:** https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Knowledge Cutoff**        | 2025-08-23 |\n| **Prompt Cost per Million** | `$15.00` |\n| **Completion Cost per Million** | `$75.00` |\n| **Context Window Tokens**   | 200,000 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# claude-3-haiku\n\n### Model Type: COMPLETION\n**Model:** AnthropicModels.Claude_3_Haiku\n\n**Description:** Claude 3 Haiku is optimized for speed and cost-effectiveness, featuring a 200K context window. It is ideal for applications that require rapid processing and budget-friendly solutions. The model offers a 50% discount with the Batches API, making it a practical choice for users looking to maximize efficiency and minimize costs.\n\n**Model Card URI:** https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Knowledge Cutoff**        | 2025-08-23 |\n| **Prompt Cost per Million** | `$0.25` |\n| **Completion Cost per Million** | `$1.25` |\n| **Context Window Tokens**   | 200,000 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# claude-3-sonnet\n\n### Model Type: COMPLETION\n**Model:** AnthropicModels.Claude_3_Sonnet\n\n**Description:** Claude 3 Sonnet strikes a balance between speed, cost, and performance, offering a 200K context window. It is suitable for users who need a well-rounded model that can handle a variety of tasks efficiently. The model's pricing is structured to provide value while maintaining high performance standards.\n\n**Model Card URI:** https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Knowledge Cutoff**        | 2025-08-23 |\n| **Prompt Cost per Million** | `$3.00` |\n| **Completion Cost per Million** | `$15.00` |\n| **Context Window Tokens**   | 200,000 |\n| **Max Output Tokens**       | 8,193 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# gpt-4o\n\n### Model Type: COMPLETION\n**Model:** OpenAIModels.GPT4o_128k\n\n**Description:** GPT-4o is a cutting-edge multimodal model from OpenAI, designed to be faster and more cost-effective than its predecessor, GPT-4 Turbo. It boasts enhanced vision capabilities, making it ideal for applications requiring advanced image and text processing. With a substantial context window of 128K tokens and a knowledge cutoff in October 2023, GPT-4o is well-suited for complex tasks that demand a deep understanding of both visual and textual data. This model is perfect for developers looking to integrate sophisticated AI capabilities into their applications, offering a balance of performance and cost-efficiency.\n\n**Model Card URI:** https://openai.com/index/gpt-4o-system-card/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | Yes |\n| **Knowledge Cutoff**        | 2025-10-23 |\n| **Prompt Cost per Million** | `$2.50` |\n| **Completion Cost per Million** | `$10.00` |\n| **Context Window Tokens**   | 128,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# gpt-4o-mini\n\n### Model Type: COMPLETION\n**Model:** OpenAIModels.GPT4o_Mini_128k\n\n**Description:** GPT-4o Mini is a compact yet powerful model from OpenAI, designed to offer cost-effective AI solutions with enhanced vision capabilities. It surpasses the performance of GPT-3.5 Turbo while maintaining a lower cost, making it an excellent choice for developers seeking efficient AI models for applications that require both text and image processing. This model is ideal for projects with budget constraints but still demand high-quality AI performance.\n\n**Model Card URI:** https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | Yes |\n| **Knowledge Cutoff**        | 2025-10-23 |\n| **Prompt Cost per Million** | `$0.15` |\n| **Completion Cost per Million** | `$0.60` |\n| **Context Window Tokens**   | 128,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# o1\n\n### Model Type: COMPLETION\n**Model:** OpenAIModels.O1_200k\n\n**Description:** OpenAI o1 is a highly advanced reasoning model, offering robust support for tools, structured outputs, and vision capabilities. With a large context window of 200K tokens and a knowledge cutoff in October 2023, it is designed for complex reasoning tasks that require deep analytical capabilities. This model is perfect for developers looking to implement sophisticated AI solutions that can handle intricate problem-solving and data analysis tasks.\n\n**Model Card URI:** https://openai.com/index/openai-o1-system-card/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | Yes |\n| **Knowledge Cutoff**        | 2025-10-23 |\n| **Prompt Cost per Million** | `$15.00` |\n| **Completion Cost per Million** | `$60.00` |\n| **Context Window Tokens**   | 200,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# o1-mini\n\n### Model Type: COMPLETION\n**Model:** OpenAIModels.O1_Mini_128k\n\n**Description:** OpenAI o1 Mini is a streamlined version of the o1 model, optimized for speed and efficiency in coding and mathematical tasks. It offers a faster processing capability while maintaining the robust reasoning features of its larger counterpart. This model is ideal for developers who need a quick and efficient AI solution for technical applications, particularly in the fields of software development and mathematical computations.\n\n**Model Card URI:** https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Knowledge Cutoff**        | 2025-10-23 |\n| **Prompt Cost per Million** | `$3.00` |\n| **Completion Cost per Million** | `$12.00` |\n| **Context Window Tokens**   | 128,000 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# text-embedding-3-small\n\n### Model Type: TEXT_EMBEDDING\n**Model:** OpenAIModels.Embedding_3_Small\n\n**Description:** Text Embedding 3 Small is a specialized model from OpenAI designed to enhance search, clustering, topic modeling, and classification tasks. It provides a cost-effective solution for embedding needs, making it suitable for applications that require efficient text processing and analysis. This model is perfect for developers looking to implement advanced text analytics in their applications without incurring high costs.\n\n**Model Card URI:** https://platform.openai.com/docs/models#embeddings\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 8,191 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# text-embedding-3-large\n\n### Model Type: TEXT_EMBEDDING\n**Model:** OpenAIModels.Embedding_3_Large\n\n**Description:** Text Embedding 3 Large is a robust model from OpenAI, tailored for advanced text embedding tasks such as search, clustering, topic modeling, and classification. It offers enhanced capabilities for handling large-scale text data, making it ideal for applications that require comprehensive text analysis and processing. This model is suitable for developers who need powerful embedding solutions for complex text analytics.\n\n**Model Card URI:** https://platform.openai.com/docs/models#embeddings\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 8,191 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# text-embedding-ada-002\n\n### Model Type: TEXT_EMBEDDING\n**Model:** OpenAIModels.Ada_002\n\n**Description:** Ada v2 is an advanced text embedding model from OpenAI, designed to facilitate sophisticated search, clustering, topic modeling, and classification tasks. It provides a balanced solution for embedding needs, offering both performance and cost-efficiency. This model is ideal for developers seeking to integrate advanced text analytics into their applications, ensuring high-quality results without excessive costs.\n\n**Model Card URI:** https://platform.openai.com/docs/models#embeddings\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Context Window Tokens**   | 8,191 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# mixtral-8x7b-32768\n\n### Model Type: COMPLETION\n**Model:** GroqModels.Mixtral_8x7b_Instruct\n\n**Description:** The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks we tested.\n\n**Model Card URI:** https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.24` |\n| **Completion Cost per Million** | `$0.24` |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# llama-3.3-70b-versatile\n\n### Model Type: COMPLETION\n**Model:** GroqModels.Llama_3_3_70b\n\n**Description:** The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Card URI:** https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.59` |\n| **Completion Cost per Million** | `$0.79` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 32,768 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# llama-3.2-90b-vision-preview\n\n### Model Type: COMPLETION\n**Model:** GroqModels.Llama_3_2_90b_Vision_Preview\n\n**Description:** The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Card URI:** https://huggingface.co/meta-llama/Llama-3.2-1B\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.90` |\n| **Completion Cost per Million** | `$0.90` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# llama-3.2-11b-vision-preview\n\n### Model Type: COMPLETION\n**Model:** GroqModels.Llama_3_2_11b_Vision_Preview\n\n**Description:** The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Card URI:** https://huggingface.co/meta-llama/Llama-3.2-1B\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.18` |\n| **Completion Cost per Million** | `$0.18` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# llama-3.2-3b-preview\n\n### Model Type: COMPLETION\n**Model:** GroqModels.Llama_3_2_3b_Preview\n\n**Description:** The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Card URI:** https://huggingface.co/meta-llama/Llama-3.2-1B\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.06` |\n| **Completion Cost per Million** | `$0.06` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# llama-3.2-1b-preview\n\n### Model Type: COMPLETION\n**Model:** GroqModels.Llama_3_2_1b_Preview\n\n**Description:** The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Card URI:** https://huggingface.co/meta-llama/Llama-3.2-1B\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.04` |\n| **Completion Cost per Million** | `$0.04` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# llama-3.1-8b-instant\n\n### Model Type: COMPLETION\n**Model:** GroqModels.Llama_3_1_8b\n\n**Description:** The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Card URI:** https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.05` |\n| **Completion Cost per Million** | `$0.08` |\n| **Context Window Tokens**   | 128,000 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# llama3-70b-8192\n\n### Model Type: COMPLETION\n**Model:** GroqModels.Llama_3_70b\n\n**Description:** Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. Llama 3 uses a tokenizer with a vocabulary of 128K tokens, and was trained on on sequences of 8,192 tokens.\n\n**Model Card URI:** https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.59` |\n| **Completion Cost per Million** | `$0.79` |\n| **Context Window Tokens**   | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# llama3-8b-8192\n\n### Model Type: COMPLETION\n**Model:** GroqModels.Llama_3_8b\n\n**Description:** Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. Llama 3 uses a tokenizer with a vocabulary of 128K tokens, and was trained on on sequences of 8,192 tokens.\n\n**Model Card URI:** https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.05` |\n| **Completion Cost per Million** | `$0.08` |\n| **Context Window Tokens**   | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# deepseek-chat\n\n### Model Type: COMPLETION\n**Model:** DeepseekModels.Chat\n\n**Description:** We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.\n\n**Model Card URI:** https://huggingface.co/deepseek-ai/DeepSeek-V3\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.27` |\n| **Completion Cost per Million** | `$1.10` |\n| **Context Window Tokens**   | 65,536 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# deepseek-coder\n\n### Model Type: COMPLETION\n**Model:** DeepseekModels.Coder\n\n**Description:** We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens.\n\n**Model Card URI:** https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct\n\n**Available On:** N/A\n\n---\n\n## Features\nn/a\n\n---\n\n## Metadata\n| **Property**               | **Value**                    |\n|----------------------------|------------------------------|\n| **Multilingual**            | No |\n| **Multimodal**              | No |\n| **Prompt Cost per Million** | `$0.05` |\n| **Completion Cost per Million** | `$0.08` |\n| **Context Window Tokens**   | 65,536 |\n| **Max Output Tokens**       | 8,192 |\n\n---\n\n> **Note**: All costs are in USD. Token limits and costs may vary depending on platform and configuration."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "\n"
          ]
        }
      ]
    }
  ]
}