{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_12_08_CrewAI_Product_Data_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTfW0Ru4KcSQ"
      },
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to integrate with CrewAI and the Graphlit Agent Tools to extract structured data from web pages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyFFDO85KYH_"
      },
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "For CrewAI, assign this property as Colab secret: OPENAI_API_KEY.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr5QOHkIKvzZ"
      },
      "source": [
        "Install LangChain OpenAI client for CrewAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apRpazbc1XXE"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHNf-V7CKscy"
      },
      "source": [
        "Install Graphlit Python agent tools SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECeVkTLW0F8f"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade graphlit-tools[crewai]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60hNiiKmsggh"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade isodate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0W-Ms81K1XQ"
      },
      "source": [
        "Initialize OpenAI for CrewAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9naHYiDPIgI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk568BDYK05o"
      },
      "source": [
        "Initialize Graphlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0rVPd8N0xsB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyYjyo9KK48d"
      },
      "source": [
        "Define Graphlit helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwJiaC-x7XeR"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import isodate\n",
        "\n",
        "async def lookup_usage(correlation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.lookup_usage(correlation_id)\n",
        "\n",
        "        return response.lookup_usage if response.lookup_usage is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def lookup_credits(correlation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.lookup_credits(correlation_id)\n",
        "\n",
        "        return response.lookup_credits if response.lookup_credits is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "def dump_usage_record(record):\n",
        "    print(f\"{record.date}: {record.name}\")\n",
        "\n",
        "    if record.duration is not None:\n",
        "        duration = isodate.parse_duration(record.duration)\n",
        "\n",
        "        if record.workflow:\n",
        "            print(f\"- Workflow [{record.workflow}] took {duration}, used credits [{record.credits:.8f}]\")\n",
        "        else:\n",
        "            print(f\"- Operation took {duration}, used credits [{record.credits:.8f}]\")\n",
        "    else:\n",
        "        if record.workflow:\n",
        "            print(f\"- Workflow [{record.workflow}] used credits [{record.credits:.8f}]\")\n",
        "        else:\n",
        "            print(f\"- Operation used credits [{record.credits:.8f}]\")\n",
        "\n",
        "    if record.entity_id:\n",
        "        if record.entity_type:\n",
        "            if record.entity_type == enums.EntityTypes.CONTENT and record.content_type:\n",
        "                print(f\"- {record.entity_type} [{record.entity_id}]: Content type [{record.content_type}], file type [{record.file_type}]\")\n",
        "            else:\n",
        "                print(f\"- {record.entity_type} [{record.entity_id}]\")\n",
        "        else:\n",
        "            print(f\"- Entity [{record.entity_id}]\")\n",
        "\n",
        "    if record.model_service:\n",
        "        print(f\"- Model service [{record.model_service}], model name [{record.model_name}]\")\n",
        "\n",
        "    if record.processor_name:\n",
        "        if record.processor_name in [\"Deepgram Audio Transcription\", \"Assembly.AI Audio Transcription\"]:\n",
        "            length = timedelta(milliseconds=record.count or 0)\n",
        "\n",
        "            if record.model_name:\n",
        "                print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}], length [{length}]\")\n",
        "            else:\n",
        "                print(f\"- Processor name [{record.processor_name}], length [{length}]\")\n",
        "        else:\n",
        "            if record.count:\n",
        "                if record.model_name:\n",
        "                    print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}], units [{record.count}]\")\n",
        "                else:\n",
        "                    print(f\"- Processor name [{record.processor_name}], units [{record.count}]\")\n",
        "            else:\n",
        "                if record.model_name:\n",
        "                    print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}]\")\n",
        "                else:\n",
        "                    print(f\"- Processor name [{record.processor_name}]\")\n",
        "\n",
        "    if record.uri:\n",
        "        print(f\"- URI [{record.uri}]\")\n",
        "\n",
        "    if record.name == \"Prompt completion\":\n",
        "        if record.prompt:\n",
        "            print(f\"- Prompt [{record.prompt_tokens} tokens (includes RAG context tokens)]:\")\n",
        "            print(record.prompt)\n",
        "\n",
        "        if record.completion:\n",
        "            print(f\"- Completion [{record.completion_tokens} tokens (includes JSON guardrails tokens)], throughput: {record.throughput:.3f} tokens/sec:\")\n",
        "            print(record.completion)\n",
        "\n",
        "    elif record.name == \"Text embedding\":\n",
        "        if record.prompt_tokens is not None:\n",
        "            print(f\"- Text embedding [{record.prompt_tokens} tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"Document preparation\":\n",
        "        if record.prompt_tokens is not None and record.completion_tokens is not None:\n",
        "            print(f\"- Document preparation [{record.prompt_tokens} input tokens, {record.completion_tokens} output tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"Data extraction\":\n",
        "        if record.prompt_tokens is not None and record.completion_tokens is not None:\n",
        "            print(f\"- Data extraction [{record.prompt_tokens} input tokens, {record.completion_tokens} output tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"GraphQL\":\n",
        "        if record.request:\n",
        "            print(f\"- Request:\")\n",
        "            print(record.request)\n",
        "\n",
        "        if record.variables:\n",
        "            print(f\"- Variables:\")\n",
        "            print(record.variables)\n",
        "\n",
        "        if record.response:\n",
        "            print(f\"- Response:\")\n",
        "            print(record.response)\n",
        "\n",
        "    if record.name.startswith(\"Upload\"):\n",
        "        print(f\"- File upload [{record.count} bytes], throughput: {record.throughput:.3f} bytes/sec\")\n",
        "\n",
        "    print()\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4oOtUeKK8WU"
      },
      "source": [
        "Execute Graphlit example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3xvbvic7PTr"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Remove any existing contents; only needed for notebook example\n",
        "await delete_all_contents()\n",
        "\n",
        "print('Deleted all contents.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjFUpjVS7BaS"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "class Product(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    features: List[str]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKcY8qJAK9LW"
      },
      "source": [
        "Define and kickoff CrewAI crew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wS5I6wiOAAq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "from crewai import Agent, Crew, Process, Task\n",
        "from langchain_openai import ChatOpenAI\n",
        "from graphlit_tools import WebSearchTool, WebMapTool, ExtractWebPageTool, CrewAIConverter\n",
        "from datetime import datetime\n",
        "\n",
        "company_name = input('Enter the automaker company name to be analyzed: ')\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# NOTE: create a unique cost correlation ID\n",
        "correlation_id = datetime.now().isoformat()\n",
        "\n",
        "# NOTE: need to convert Graphlit tools to CrewAI tool schema\n",
        "web_search_tool = CrewAIConverter.from_tool(WebSearchTool(graphlit, correlation_id=correlation_id))\n",
        "web_map_tool = CrewAIConverter.from_tool(WebMapTool(graphlit, correlation_id=correlation_id))\n",
        "extract_web_page_tool = CrewAIConverter.from_tool(ExtractWebPageTool(graphlit, correlation_id=correlation_id))\n",
        "\n",
        "web_search_agent = Agent(\n",
        "    role=\"Web Researcher\",\n",
        "    goal=\"Find the {company} website.\",\n",
        "    backstory=\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    tools=[web_search_tool],\n",
        ")\n",
        "\n",
        "web_map_agent = Agent(\n",
        "    role=\"Web Mapping Agent\",\n",
        "    goal=\"Enumerate all the web page URLs for the provided web site.\",\n",
        "    backstory=\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    tools=[web_map_tool],\n",
        ")\n",
        "\n",
        "web_page_analyst_agent = Agent(\n",
        "    role=\"Web Analyst Agent\",\n",
        "    goal=\"Extract structured data from the {company} web pages.\",\n",
        "    backstory=\"\"\"You work for a major automotive manufacturer, and are doing competitive analysis on other automakers websites.\n",
        "    We are looking to gather structured product information about automotive models.\n",
        "    \"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    tools=[extract_web_page_tool],\n",
        ")\n",
        "\n",
        "writer_agent = Agent(\n",
        "    role=\"Data Writer Agent\",\n",
        "    goal=\"Summarize all of the structured product data into CSV format.\",\n",
        "    backstory=\"You work for a major automotive manufacturer, and are doing competitive analysis on other automakers websites.\",\n",
        "    verbose=True,\n",
        "    llm=llm,\n",
        "    allow_delegation=False\n",
        ")\n",
        "\n",
        "search_web_task = Task(\n",
        "    description=(\n",
        "        \"\"\"Given company named {company}, search the web to find their home page.\n",
        "        Return the root path for URLs, not individual web pages.\n",
        "        For example return https://www.example.com, not https://www.example.com/index.html\"\"\"\n",
        "    ),\n",
        "    expected_output=\"A single URL for the {company} home page, prefer the US targeted home page\",\n",
        "    agent=web_search_agent,\n",
        ")\n",
        "\n",
        "fetch_web_pages_task = Task(\n",
        "    description=(\n",
        "        \"\"\"Fetch the URLs at or beneath the given home page for further analysis.\n",
        "        Filter the resulting URLs to locate pages which appear to be about automobile models and specifications.\n",
        "        Select one most relevant page per automobile model.\n",
        "        \"\"\"\n",
        "    ),\n",
        "    expected_output=\"A list of web page URLs, maximum 10\",\n",
        "    agent=web_map_agent,\n",
        "    context=[search_web_task],\n",
        ")\n",
        "\n",
        "extract_web_page_task = Task(\n",
        "    description=(\n",
        "        \"\"\"Extract structured data from the provided web pages from the {company} website.\n",
        "\n",
        "        Execute task once for each provided web page.\n",
        "        \"\"\"\n",
        "    ),\n",
        "    expected_output=\"A list of Product Pydantic data models\",\n",
        "    output_pydantic=Product,\n",
        "    agent=web_page_analyst_agent,\n",
        "    context=[fetch_web_pages_task],\n",
        ")\n",
        "\n",
        "writer_task = Task(\n",
        "    description=(\n",
        "        \"\"\"Collate the extracted structured data from the provided {company} web pages.\n",
        "        \"\"\"\n",
        "    ),\n",
        "    expected_output=\"All the extracted automotive models in structured form, collated into CSV format.\",\n",
        "    agent=writer_agent,\n",
        "    context=[extract_web_page_task],\n",
        ")\n",
        "\n",
        "crew = Crew(\n",
        "    agents=[web_search_agent, web_map_agent, web_page_analyst_agent, writer_agent],\n",
        "    tasks=[search_web_task, fetch_web_pages_task, extract_web_page_task, writer_task],\n",
        "    manager_llm=llm,\n",
        "    function_calling_llm=llm,\n",
        "    planning_llm=llm,\n",
        "    process=Process.sequential,\n",
        "    planning=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Kickoff the process and print the result\n",
        "result = await crew.kickoff_async(inputs={\"company\": company_name})\n",
        "print(\"Website Summary Process Completed:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ctrFgvLKOL6"
      },
      "source": [
        "Calculate Graphlit credits & usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3vjh8bjIEj6"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import display, HTML, JSON, Markdown\n",
        "\n",
        "time.sleep(10) # NOTE: give some time to consume all billing events\n",
        "\n",
        "credits = await lookup_credits(correlation_id)\n",
        "\n",
        "if credits is not None:\n",
        "    display(Markdown(f\"### Credits used: {credits.credits:.6f}\"))\n",
        "    print(f\"- storage [{credits.storage_ratio:.2f}%], compute [{credits.compute_ratio:.2f}%]\")\n",
        "    print(f\"- embedding [{credits.embedding_ratio:.2f}%], completion [{credits.completion_ratio:.2f}%]\")\n",
        "    print(f\"- ingestion [{credits.ingestion_ratio:.2f}%], indexing [{credits.indexing_ratio:.2f}%], preparation [{credits.preparation_ratio:.2f}%], extraction [{credits.extraction_ratio:.2f}%], enrichment [{credits.enrichment_ratio:.2f}%], publishing [{credits.publishing_ratio:.2f}%]\")\n",
        "    print(f\"- search [{credits.search_ratio:.2f}%], conversation [{credits.conversation_ratio:.2f}%]\")\n",
        "    print()\n",
        "\n",
        "usage = await lookup_usage(correlation_id)\n",
        "\n",
        "if usage is not None:\n",
        "    display(Markdown(f\"### Usage records [{len(usage)}]:\"))\n",
        "\n",
        "    for record in usage:\n",
        "        dump_usage_record(record)\n",
        "\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFLxfPCIjVqJ4cAfIf3bl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}