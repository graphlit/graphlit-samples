{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyMvf5PO6L+A0CGAz+zIbpSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_12_27_Publish_Audio_Summary_of_Year_in_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to ingest Graphlit changelog, use OpenAI O1 to write a comprehensive year-in-review, and published using an [ElevenLabs](https://elevenlabs.io/) voice."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfed778b-22cc-4014-edc2-7971c22b349f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting graphlit-client\n",
            "  Downloading graphlit_client-1.0.20241224002-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.3)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n",
            "Downloading graphlit_client-1.0.20241224002-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: graphlit-client\n",
            "Successfully installed graphlit-client-1.0.20241224002\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "async def create_specification(model: enums.OpenAIModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"OpenAI {model}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.OPEN_AI,\n",
        "        openAI=input_types.OpenAIModelPropertiesInput(\n",
        "            model=model,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_web_feed(uri: str, limit: Optional[int] = None):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.FeedInput(\n",
        "        name=uri,\n",
        "        type=enums.FeedTypes.WEB,\n",
        "        web=input_types.WebFeedPropertiesInput(\n",
        "            uri=uri,\n",
        "            readLimit=limit if limit is not None else 100\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_feed(input)\n",
        "\n",
        "        return response.create_feed.id if response.create_feed is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def is_feed_done(feed_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    response = await graphlit.client.is_feed_done(feed_id)\n",
        "\n",
        "    return response.is_feed_done.result if response.is_feed_done is not None else None\n",
        "\n",
        "async def get_content(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    response = await graphlit.client.get_content(content_id)\n",
        "\n",
        "    return response.content\n",
        "\n",
        "async def query_contents(feed_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.query_contents(\n",
        "            filter=input_types.ContentFilter(\n",
        "                feeds=[\n",
        "                    input_types.EntityReferenceFilter(\n",
        "                        id=feed_id\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return response.contents.results if response.contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def publish_contents(feed_id: str, summary_specification_id: str, publish_specification_id: str, summary_prompt: str, publish_prompt: str, voice_id: Optional[str] = None):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.publish_contents(\n",
        "            name=\"Published Summary\",\n",
        "            connector=input_types.ContentPublishingConnectorInput(\n",
        "               type=enums.ContentPublishingServiceTypes.ELEVEN_LABS_AUDIO,\n",
        "               format=enums.ContentPublishingFormats.MP3,\n",
        "               elevenLabs=input_types.ElevenLabsPublishingPropertiesInput(\n",
        "                   model=enums.ElevenLabsModels.TURBO_V2_5,\n",
        "                   voice=voice_id if voice_id is not None else \"ZF6FPAbjXT4488VcRRnw\" # ElevenLabs Amelia voice\n",
        "               )\n",
        "            ),\n",
        "            summary_prompt=summary_prompt,\n",
        "            summary_specification=input_types.EntityReferenceInput(\n",
        "                id=summary_specification_id\n",
        "            ),\n",
        "            publish_prompt = publish_prompt,\n",
        "            publish_specification=input_types.EntityReferenceInput(\n",
        "                id=publish_specification_id\n",
        "            ),\n",
        "            filter=input_types.ContentFilter(\n",
        "                feeds=[input_types.EntityReferenceFilter(id=feed_id)]\n",
        "            ),\n",
        "            is_synchronous=True\n",
        "        )\n",
        "\n",
        "        return response.publish_contents.id if response.publish_contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_all_specifications():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_specifications(is_synchronous=True)\n",
        "\n",
        "async def delete_all_feeds():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_feeds(is_synchronous=True)\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, HTML\n",
        "import time\n",
        "\n",
        "# Remove any existing feeds, contents and specifications; only needed for notebook example\n",
        "await delete_all_feeds()\n",
        "await delete_all_specifications()\n",
        "await delete_all_contents()\n",
        "\n",
        "print('Deleted all feeds, contents and specifications.')\n",
        "\n",
        "uri = \"https://changelog.graphlit.dev\"\n",
        "limit = 100 # maximum number of web pages to ingest\n",
        "\n",
        "feed_id = await create_web_feed(uri, limit)\n",
        "\n",
        "if feed_id is not None:\n",
        "    print(f'Created feed [{feed_id}]: {uri}')\n",
        "\n",
        "    # Wait for feed to complete, since ingestion happens asychronously\n",
        "    done = False\n",
        "    time.sleep(5)\n",
        "    while not done:\n",
        "        done = await is_feed_done(feed_id)\n",
        "\n",
        "        if not done:\n",
        "            time.sleep(10)\n",
        "\n",
        "    print(f'Completed feed [{feed_id}].')\n",
        "\n",
        "    # Query contents by feed\n",
        "    contents = await query_contents(feed_id)\n",
        "\n",
        "    if contents is not None:\n",
        "        print(f'Found {len(contents)} contents in feed [{feed_id}].')\n",
        "        print()\n",
        "\n",
        "        for content in contents:\n",
        "            if content is not None:\n",
        "\n",
        "                display(Markdown(f'# Ingested content [{content.id}]'))\n",
        "\n",
        "                print(f'Text Mezzanine: {content.text_uri}')\n",
        "\n",
        "                print(content.markdown)"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "44062c39-8cd4-46e4-b27f-e2d3c351b60b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all feeds, contents and specifications.\n",
            "Created feed [63fbcd95-170d-4213-ae9b-0f72d78019c6]: https://changelog.graphlit.dev\n",
            "Completed feed [63fbcd95-170d-4213-ae9b-0f72d78019c6].\n",
            "Found 47 contents in feed [63fbcd95-170d-4213-ae9b-0f72d78019c6].\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f36a7915-29cf-4930-9f84-79b652ed74a2]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/f36a7915-29cf-4930-9f84-79b652ed74a2/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# September 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the [Cerebras](https://cerebras.ai/) model service which offers the\n",
            "\n",
            "```\n",
            "LLAMA_3_1_70B\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "LLAMA_3_1_8B\n",
            "```\n",
            "\n",
            "models.\n",
            "\n",
            "üí° Graphlit now supports the [Google AI](https://ai.google.dev/) model service which offers the\n",
            "\n",
            "```\n",
            "GEMINI_1_5_PRO\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "GEMINI_1_5_FLASH\n",
            "```\n",
            "\n",
            "models.\n",
            "\n",
            "We have added support for the latest Groq Llama 3.2 preview models, including\n",
            "\n",
            "```\n",
            "LLAMA_3_2_1B_PREVIEW\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "LLAMA_3_2_3B_PREVIEW\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "LLAMA_3_2_11B_TEXT_PREVIEW\n",
            "```\n",
            "\n",
            ", and\n",
            "\n",
            "```\n",
            "LLAMA_3_2_90B_TEXT_PREVIEW\n",
            "```\n",
            "\n",
            ".  We have also added support for the Llama 3.2 multimodal model\n",
            "\n",
            "```\n",
            "LLAMA_3_2_11B_VISION_PREVIEW.\n",
            "```\n",
            "\n",
            "We have added a new\n",
            "\n",
            "```\n",
            "specification\n",
            "```\n",
            "\n",
            "parameter to the\n",
            "\n",
            "```\n",
            "promptConversation\n",
            "```\n",
            "\n",
            "mutation. Now you can specify your initial specification for a new conversation, or update an existing conversation, without requiring additional API calls.\n",
            "\n",
            "‚ö° We have changed the retrieval behavior of the\n",
            "\n",
            "```\n",
            "promptConversation\n",
            "```\n",
            "\n",
            "mutation. Now, if no relevant content was found via vector-based semantic search (given the user prompt), we will fallback to any relevant content from the message in the conversation. If there was no content from the conversation to fallback to, we will fallback to the last ingested content in the project. This solves an issue where a first prompt like 'Summarize this' would find no relevant content.  Now it will fallback to retrieve the last ingested content.\n",
            "\n",
            "‚ö° We have renamed the Groq model enum from\n",
            "\n",
            "```\n",
            "LLAVA_1_5_7B\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "LLAVA_1_5_7B_PREVIEW.\n",
            "```\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3083: Not sending custom instructions/guidance with extraction prompt\n",
            "\n",
            "GPLA-3146: Filtering Persons by email not working\n",
            "\n",
            "GPLA-3171: Not failing on deprecated OpenAI model\n",
            "\n",
            "GPLA-3158: Summarization not using revision strategy\n",
            "\n",
            "Last updated 3 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [3b23a0a9-32b2-4de8-a994-13bc1aacaa77]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/3b23a0a9-32b2-4de8-a994-13bc1aacaa77/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "\n",
            "### New Features\n",
            "\n",
            "üî• Graphlit now supports paid [Hobby, Starter and Growth](https://www.graphlit.com/#pricing) tiers for projects, in addition to the existing Free tier.  Starting at $49/mo, plus $0.10/credit for usage, we now support higher quota based on your subscribed tier.   By providing a payment method for your organization in the Developer Portal, you can upgrade each project individually to the tier that fits your application's needs.\n",
            "\n",
            "üí° Added GraphQL mutations for the creation, update and deletion of observed entities (i.e. Person, Organization, Place, Product, Event, Label, Category).\n",
            "\n",
            "üí° Added new observed entity types to knowledge graph: Repo (i.e. Git repo), Software.\n",
            "\n",
            "üí° Added\n",
            "\n",
            "```\n",
            "searchType\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "numberSimilar\n",
            "```\n",
            "\n",
            "fields to Specification object for configuring semantic search in conversations.   In situations where the user prompt is limited in length,\n",
            "\n",
            "```\n",
            "HYBRID\n",
            "```\n",
            "\n",
            "search type can provide better semantic search results for the prompt context.\n",
            "\n",
            "üí° Added support for the Azure OpenAI GPT-4 model.\n",
            "\n",
            "Added support for project\n",
            "\n",
            "```\n",
            "quota\n",
            "```\n",
            "\n",
            "field.  Project quotas are based on the subscribed pricing tier.   Quota limits are now applied as content is ingested, and as feeds and conversations are created.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "contentLimit\n",
            "```\n",
            "\n",
            "to conversation strategy object to limit the number of semantic search content results which are formatted into prompt context.\n",
            "\n",
            "Better relevance ranking on semantic search results when formatting prompt context in conversations.\n",
            "\n",
            "‚ÑπÔ∏è Free tier has updated quota: 1GB storage, 100 contents, 3 feeds and 10 conversations.\n",
            "\n",
            "‚ö° Now using the [Deepgram Nova-2 audio transcription model,](https://deepgram.com/learn/nova-2-speech-to-text-api?utm_source=twitter&amp;utm_campaign=0923+Nova-2&amp;utm_medium=organic+social) which is 18% more accurate, and 5-40x faster.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-1373: Failed to extract multiple text pages from DOCX without page breaks.  Now we support token-aware page chunking.\n",
            "\n",
            "GPLA-1377: Failed during semantic search with no content results, when prompting conversation.\n",
            "\n",
            "GPLA-1415: Failed when user prompt couldn't generate text embeddings.\n",
            "\n",
            "Last updated 11 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [4073bfe0-5a5b-4987-bca7-0af9f2ed2c4e]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/4073bfe0-5a5b-4987-bca7-0af9f2ed2c4e/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# October 9: Support for GitHub repository feeds, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports GitHub feeds, by providing the repository owner and name similar to GitHub Issues feeds, and will ingest code files from any GitHub repository.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3262: Missing row separator in table markdown formatting\n",
            "\n",
            "Last updated 2 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [2937ad7e-22d4-4b89-a169-0bb51b772e3f]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/2937ad7e-22d4-4b89-a169-0bb51b772e3f/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# September 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the [Azure AI Model Inference API](https://learn.microsoft.com/en-us/azure/ai-studio/reference/reference-model-inference-api?tabs=python) (aka Models as a Service) model service which offers serverless hosting to many models such as Meta Llama 3.2, Cohere Command-R, and many more.  For Azure AI, all models are 'custom', and you will need to provide the serverless endpoint, API key and number of tokens accepted in context window, after provisioning the model of your choice.\n",
            "\n",
            "We have added support for the multimodal Mistral Pixtral model, under the model enum\n",
            "\n",
            "```\n",
            "PIXTRAL_12B_2409\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "We have added versioned model enums for Google Gemini, so you can access\n",
            "\n",
            "```\n",
            "GEMINI_1_5_FLASH_001\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GEMINI_1_5_FLASH_002\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GEMINI_1_5_PRO_001\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "GEMINI_1_5_PRO_002\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "Last updated 2 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [d6ee417f-364b-4cae-96ac-fea9c6c766fc]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/d6ee417f-364b-4cae-96ac-fea9c6c766fc/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# September 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "\n",
            "### New Features\n",
            "\n",
            "üî• Added [Workflow entity](https://docs.graphlit.dev/api-reference/graphql-data-model/workflows) to data model for configuring stages of content workflow; can be assigned to Feed or with\n",
            "\n",
            "```\n",
            "ingestPage\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "ingestFile\n",
            "```\n",
            "\n",
            ", or\n",
            "\n",
            "```\n",
            "ingestText\n",
            "```\n",
            "\n",
            "mutations to control how content is ingested, prepared, extracted and enriched into the knowledge graph.\n",
            "\n",
            "üí° Added support for [Notion feeds](https://docs.graphlit.dev/api-reference/graphql-data-model/feeds/create-notion-feed): now can create feed to ingest files from Notion pages or databases (i.e. wikis).\n",
            "\n",
            "üí° Added support for API-created Observation entities, which allow for custom observations of observable entities (i.e. Person, Label) on Content.\n",
            "\n",
            "üí° Added support for [Azure AI Document Intelligence](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-3.1.0) as an optional method for preparing PDF files, using OCR and advanced layout analysis.\n",
            "\n",
            "üí° Added summarization strategies, where content can be summarized into paragraphs, bullet points or headline.\n",
            "\n",
            "Added ability to assign [default Workflow and Specification to project](https://docs.graphlit.dev/api-reference/graphql-data-model/projects).\n",
            "\n",
            "Added more well-known link types, during link crawling, such as Discord, Airtable and TypeForm.\n",
            "\n",
            "‚ÑπÔ∏è Free/Hobby plan now has 5GB storage quota; any content ingested past that limit will be auto-deleted.\n",
            "\n",
            "‚ö° Actions have been moved into Workflow entity.\n",
            "\n",
            "‚ö° Link enrichment for Feeds has been moved into the Workflow enrichment stage, now called link crawling.\n",
            "\n",
            "```\n",
            "ExcludeContentDomain\n",
            "```\n",
            "\n",
            "property has been reversed and is now called\n",
            "\n",
            "```\n",
            "IncludeContentDomain\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-1204: Failed to ingest content with backslash in name.\n",
            "\n",
            "GPLA-1276: Failed to ingest RSS posts which contained enclosure URI, but no post URI.\n",
            "\n",
            "Last updated 1 year ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [0a7c8c6e-6c40-4e6a-9aa1-814644f7c735]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/0a7c8c6e-6c40-4e6a-9aa1-814644f7c735/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# September 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üî• Graphlit now supports [YouTube feeds](https://docs.graphlit.dev/api-reference/graphql-data-model/feeds/create-youtube-feed), where you can ingest a set of YouTube videos, or an entire YouTube playlist or channel.   Note, we currently support only the ingestion of audio from YouTube videos, which gets transcribed and added to your conversational knowledge graph.\n",
            "\n",
            "### New Documentation\n",
            "\n",
            "Added documentation for observable entities mutations and queries ([Label](https://docs.graphlit.dev/api-reference/graphql-data-model/observations/labels), [Category](https://docs.graphlit.dev/api-reference/graphql-data-model/observations/categories), [Person](https://docs.graphlit.dev/api-reference/graphql-data-model/observations/persons), [Organization](https://docs.graphlit.dev/api-reference/graphql-data-model/observations/organizations), [Place](https://docs.graphlit.dev/api-reference/graphql-data-model/observations/places), [Event](https://docs.graphlit.dev/api-reference/graphql-data-model/observations/events), [Product](https://docs.graphlit.dev/api-reference/graphql-data-model/observations/products), [Repo](https://docs.graphlit.dev/api-reference/graphql-data-model/observations/repos), [Software](https://docs.graphlit.dev/api-reference/graphql-data-model/observations/software)).\n",
            "\n",
            "Added documentation for using custom [Azure OpenAI](https://docs.graphlit.dev/api-reference/graphql-data-model/specifications/azure-openai) and [OpenAI ](https://docs.graphlit.dev/api-reference/graphql-data-model/specifications/openai)models with Specifications\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-1459: LLM prompt formatting was exceeding the token budget with long user prompts.\n",
            "\n",
            "GPLA-1445: Failed to ingest PDF from URL where filename in\n",
            "\n",
            "```\n",
            "Content-Disposition\n",
            "```\n",
            "\n",
            "header contained a backslash.\n",
            "\n",
            "Last updated 7 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [2ed25786-2464-4416-93ca-6d1e1bdf0e78]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/2ed25786-2464-4416-93ca-6d1e1bdf0e78/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# September 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports entity enrichment from [Fast Healthcare Interoperability Resources (FHIR)](https://en.wikipedia.org/wiki/Fast_Healthcare_Interoperability_Resources) servers. You can provide the\n",
            "\n",
            "```\n",
            "endpoint\n",
            "```\n",
            "\n",
            "for a FHIR server, and Graphlit will enrich medical-related entities from the data found in the FHIR server.\n",
            "\n",
            "Added support for latest Cohere models (\n",
            "\n",
            "```\n",
            "COMMAND_R_202408\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "COMMAND_R_PLUS_202408)\n",
            "```\n",
            "\n",
            "and added datestamped model enums for the previous versions (\n",
            "\n",
            "```\n",
            "COMMAND_R_202403\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "COMMAND_R_PLUS_202404\n",
            "```\n",
            "\n",
            ").  The latest model enums (\n",
            "\n",
            "```\n",
            "COMMAND_R\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "COMMAND_R_PLUS\n",
            "```\n",
            "\n",
            ") currently point to the models (\n",
            "\n",
            "```\n",
            "COMMAND_R_202403\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "COMMAND_R_PLUS_202404\n",
            "```\n",
            "\n",
            ") as specified by the Cohere API.\n",
            "\n",
            "Added support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31), now used by default.\n",
            "\n",
            "‚ö° We have changed the name of the\n",
            "\n",
            "```\n",
            "LinkReferenceType\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "LinkReference\n",
            "```\n",
            "\n",
            "to follow the existing data model standard.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3120: LLM is adding source tags to end of completed messages\n",
            "\n",
            "GPLA-3133: Failed to load sitemap on child page of website.\n",
            "\n",
            "Last updated 3 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [13a0431d-fd6d-4015-83b2-f580f7015925]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/13a0431d-fd6d-4015-83b2-f580f7015925/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# September 3: Support for web search feeds, model deprecations\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports web search feeds, using the [Tavily](https://tavily.com/) and [Exa.AI](https://exa.ai/) web search APIs. You can choose the\n",
            "\n",
            "```\n",
            "SEARCH\n",
            "```\n",
            "\n",
            "feed type, and assign your search\n",
            "\n",
            "```\n",
            "text\n",
            "```\n",
            "\n",
            "property, and we will ingest the referenced web pages from the search results.  Optionally, you can select the search service via the\n",
            "\n",
            "```\n",
            "serviceType\n",
            "```\n",
            "\n",
            "property under\n",
            "\n",
            "```\n",
            "search\n",
            "```\n",
            "\n",
            "feed properties.  By default, Graphlit will use the Tavily API.\n",
            "\n",
            "‚ö° We have deprecated these OpenAI models, according to the future support OpenAI is providing to these legacy models:\n",
            "\n",
            "```\n",
            "GPT35_TURBO\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GPT35_TURBO_0613\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GPT35_TURBO_16K\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GPT35_TURBO_16K_0125\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GPT35_TURBO_16K_0613\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GPT35_TURBO_16K_1106\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GPT4\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GPT4_0613\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GPT4_32K\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GPT4_32K_0613\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "GPT4_TURBO_VISION_128K\n",
            "```\n",
            "\n",
            ", and\n",
            "\n",
            "```\n",
            "GPT4_TURBO_VISION_128K_1106\n",
            "```\n",
            "\n",
            ".  We suggest using GPT-4o or GPT-4o Mini instead.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2523: Can't ingest from same feed URI multiple times and wait on isFeedDone\n",
            "\n",
            "Last updated 3 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [6d2ef5d6-5caa-4403-82b0-261e44572980]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/6d2ef5d6-5caa-4403-82b0-261e44572980/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the\n",
            "\n",
            "```\n",
            "ingestBatch\n",
            "```\n",
            "\n",
            "mutation, which accepts an array of URIs to files or web pages, and will asynchronously ingest these into content objects.\n",
            "\n",
            "üí° Graphlit now supports the\n",
            "\n",
            "```\n",
            "continueConversation\n",
            "```\n",
            "\n",
            "mutation, which accepts an array of called tool responses. Also,\n",
            "\n",
            "```\n",
            "promptConversation\n",
            "```\n",
            "\n",
            "now accepts an array of tool definitions. When tools are called by the LLM, the assistant message returned from\n",
            "\n",
            "```\n",
            "promptConversation\n",
            "```\n",
            "\n",
            "will have a list of\n",
            "\n",
            "```\n",
            "toolCalls\n",
            "```\n",
            "\n",
            "which need to responded to from your calling code.  These responses are to be provided back to the LLM via the\n",
            "\n",
            "```\n",
            "continueConversation\n",
            "```\n",
            "\n",
            "mutation.\n",
            "\n",
            "üí° Graphlit now supports tool calling with OpenAI, Mistral, Deepseek, Groq, and Cerebras model services.  Anthropic, Google Gemini and Cohere support will come later.\n",
            "\n",
            "Added support for prefilled user and assistant messages with\n",
            "\n",
            "```\n",
            "createConversation\n",
            "```\n",
            "\n",
            "mutation. Now you can send an array of messages when creating a new conversation, which will bootstrap the conversation with the LLM.  These must be provided in user/assistant pairs.\n",
            "\n",
            "Added support for [Google Gemini Flash 1.5 8b](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/) model.\n",
            "\n",
            "‚ö° We have deprecated the\n",
            "\n",
            "```\n",
            "tools\n",
            "```\n",
            "\n",
            "property in the Specification object. These will be removed at a later date.  Tools are now to be sent directly to the\n",
            "\n",
            "```\n",
            "extractContents\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "promptConversation\n",
            "```\n",
            "\n",
            "mutations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3207: Models shouldn't be required on update specification call\n",
            "\n",
            "GPLA-3220: Don't send system prompt with OpenAI o1 models\n",
            "\n",
            "Last updated 2 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [683f3613-23c3-4e34-9cbf-19909c22b655]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/683f3613-23c3-4e34-9cbf-19909c22b655/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# October 31: Support for simulated tool calling, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "Graphlit now supports simulated tool calling for LLMs which don't natively support it, such as OpenAI o1-preview and o1-mini.  Tool schema will be formatted into the LLM prompt context, and tool responses are parsed out of the JSON formatted response.\n",
            "\n",
            "‚ö° Given customer feedback, we have lowered the vector and hybrid thresholds used by the semantic search.  Previously, some content at a low relevance was being excluded from the semantic search results.  Now, more low-relevance content will be included in the results, used by the RAG pipeline.  Reranking can be used to sort the search results for relevance.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3357: Not extracting all images from PDF, and should filter out single-color images.\n",
            "\n",
            "Last updated 1 month ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [6709c2f0-340c-48d5-bd2d-96cd26cef63e]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/6709c2f0-340c-48d5-bd2d-96cd26cef63e/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "\n",
            "### New Features\n",
            "\n",
            "Graphlit now supports the latest Anthropic Sonnet 3.5 model (released 10/22/2024).  We have added date-versions model enums for the Anthropic models:\n",
            "\n",
            "```\n",
            "CLAUDE_3_5_SONNET_20240620\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "CLAUDE_3_5_SONNET_20241022\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "CLAUDE_3_HAIKU_20240307\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "CLAUDE_3_OPUS_20240229\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "CLAUDE_3_SONNET_20240229\n",
            "```\n",
            "\n",
            ". The existing model enums will target the latest released models, as specified by Anthropic.\n",
            "\n",
            "Graphlit now supports image embeddings using the [Cohere Embed 3.0 models](https://docs.cohere.com/changelog/embed-v3-is-multimodal).\n",
            "\n",
            "Last updated 2 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [68d44e6e-4e02-4b9b-9960-57dd3db4a0cf]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/68d44e6e-4e02-4b9b-9960-57dd3db4a0cf/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# October 7: Support for Anthropic and Gemini tool calling\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports tool calling with Anthropic and Google Gemini models.\n",
            "\n",
            "‚ö° We have removed the\n",
            "\n",
            "```\n",
            "uri\n",
            "```\n",
            "\n",
            "property for tools from\n",
            "\n",
            "```\n",
            "ToolDefinitionInput\n",
            "```\n",
            "\n",
            ", such that inline webhook tools are no longer supported.  Now you can define any external tools to be called, and those can support sync or async data access to fulfill the tool call.\n",
            "\n",
            "Last updated 2 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [ae2c9f2c-27a2-4330-ab1f-a55ba5bab0e0]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/ae2c9f2c-27a2-4330-ab1f-a55ba5bab0e0/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# October 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the configuration of image and text embedding models, at the Project level.  You can create an embedding specification for a text or image embedding model, and then assign that to the Project, and all further embedding requests will use that embedding model.  See this [Colab notebook](https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_10_21_Configure_Embedding_Model.ipynb) for an example of how to configure the project.\n",
            "\n",
            "üí° Graphlit now supports the OpenAI Embedding-3-Small and Embedding-3-Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0 and 3.0 text embedding models.  Graphlit also now supports Jina CLIP image embeddings, which are used by default for image search.\n",
            "\n",
            "Graphlit now supports the\n",
            "\n",
            "```\n",
            "chunkTokenLimit\n",
            "```\n",
            "\n",
            "property in Specifications, which specifies the number of tokens for each embedded text chunk.  If this is not configured, Graphlit uses 600 tokens for each embedded text chunk.\n",
            "\n",
            "Graphlit now supports the Voyage reranking model.\n",
            "\n",
            "Graphlit now supports the\n",
            "\n",
            "```\n",
            "ingestTextBatch\n",
            "```\n",
            "\n",
            "mutation, which accepts an array of text and name pairs, and will asynchronously ingest these into content objects.\n",
            "\n",
            "‚ö° We have moved the\n",
            "\n",
            "```\n",
            "chunkTokenLimit\n",
            "```\n",
            "\n",
            "property from the Workflow storage embeddings strategy to the Specification object.  The Workflow\n",
            "\n",
            "```\n",
            "storage\n",
            "```\n",
            "\n",
            "property has now been deprecated.\n",
            "\n",
            "‚ö° We have deprecated the\n",
            "\n",
            "```\n",
            "openAIImage\n",
            "```\n",
            "\n",
            "property from Workflow entity extraction properties. Use the\n",
            "\n",
            "```\n",
            "modelImage\n",
            "```\n",
            "\n",
            "property instead.\n",
            "\n",
            "Once a text embedding model has been updated at the project level, any content, conversations or observed entities will no longer be semantically searchable.\n",
            "\n",
            "Text embeddings are not compatible across models, so you will need to delete and reingest any content, or recreate conversations or knowledge graph entities, with the new embedding model to become searchable.\n",
            "\n",
            "Last updated 2 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [938f4f8c-3b22-41fb-8c98-6a43f7078b50]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/938f4f8c-3b22-41fb-8c98-6a43f7078b50/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# October 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports 'aliases' of observable names, as the\n",
            "\n",
            "```\n",
            "alternateNames\n",
            "```\n",
            "\n",
            "property.  When an observed entity, such as Organization, is enriched, we store the original name and the enriched name as an alias.  For example, \"OpenAI\" may be enriched to \"OpenAI, Inc.\", and we store \"OpenAI\" as an alias, and update the name to \"OpenAI, Inc.\".\n",
            "\n",
            "üí° Added\n",
            "\n",
            "```\n",
            "workflows\n",
            "```\n",
            "\n",
            "filter to ContentCriteriaInput type, for filtering content by workflow(s) when creating conversation.\n",
            "\n",
            "Optimized formatting of content sources into prompt context, for more accurate conversation responses.\n",
            "\n",
            "Optimized formatting of extracted text from Slack messages, for better knowledge retrieval.\n",
            "\n",
            "Updated text tokenizer for more accurate token counting.\n",
            "\n",
            "Upgraded Azure Text Analytics to latest preview API version.\n",
            "\n",
            "Authors found in RSS feeds are now stored as observations of Person entities.\n",
            "\n",
            "Added rate limiting for Reddit feeds.\n",
            "\n",
            "Added rate limiting for Wikipedia enrichment.\n",
            "\n",
            "Added support for reading Reddit post comments when reading Reddit feed.\n",
            "\n",
            "‚ö°\n",
            "\n",
            "```\n",
            "EmbedFacets\n",
            "```\n",
            "\n",
            "has been renamed to\n",
            "\n",
            "```\n",
            "EnableFacets\n",
            "```\n",
            "\n",
            "in the conversation strategy.\n",
            "\n",
            "‚ö° Removed extra\n",
            "\n",
            "```\n",
            "content\n",
            "```\n",
            "\n",
            "level in IngestionWorkflowStage type.  Now, the\n",
            "\n",
            "```\n",
            "if\n",
            "```\n",
            "\n",
            "property is of type IngestionContentFilter.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-1556: Better handling of very long user prompts.\n",
            "\n",
            "GPLA-1627: Optimized token budget for more accurate prompt completion.\n",
            "\n",
            "GPLA-1585: More accurate entity matching in Wikipedia entity enrichment.\n",
            "\n",
            "Last updated 1 year ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [465d59f2-d78c-43a8-b2c8-f1643a00b437]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/465d59f2-d78c-43a8-b2c8-f1643a00b437/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# November 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports web search with the\n",
            "\n",
            "```\n",
            "searchWeb\n",
            "```\n",
            "\n",
            "mutation.  You can select the search service, either Tavily or Exa.AI, and provide the search query and number of search results to be returned.  This is different than the web search feed, in that\n",
            "\n",
            "```\n",
            "searchWeb\n",
            "```\n",
            "\n",
            "returns the relevant text from the web page and the web page URL from each search hit, but does not ingest each of the web pages. This new mutation is optimized to be used from within an LLM tool.\n",
            "\n",
            "üí° Graphlit now supports multi-turn summarization of content with the\n",
            "\n",
            "```\n",
            "reviseContent\n",
            "```\n",
            "\n",
            "mutation.  You can provide an LLM prompt and a content reference, along with an optional specification.  This can be used for summarizing any content (documents, web pages, audio transcripts, etc.), and having a multi-turn conversation with the LLM to revise the output from the LLM.  Internally, this creates a conversation locked to a single piece of content.  This works especially well with the OpenAI o1-preview and o1-mini models, because they provide a longer LLM output from each turn.\n",
            "\n",
            "Graphlit now supports the configuration of the Deepgram transcription\n",
            "\n",
            "```\n",
            "language\n",
            "```\n",
            "\n",
            ", and whether\n",
            "\n",
            "```\n",
            "detectLanguage\n",
            "```\n",
            "\n",
            "is enabled in\n",
            "\n",
            "```\n",
            "DeepgramAudioPreparationPropertiesInput\n",
            "```\n",
            "\n",
            ".  Language detection is now enabled by default, and can be disabled by setting\n",
            "\n",
            "```\n",
            "detectLanguage\n",
            "```\n",
            "\n",
            "to false.\n",
            "\n",
            "‚ö° We have added a\n",
            "\n",
            "```\n",
            "requireTool\n",
            "```\n",
            "\n",
            "option to\n",
            "\n",
            "```\n",
            "promptConversation\n",
            "```\n",
            "\n",
            "mutation, so you can control whether the LLM must call one of the provided tool, or if tool calling is optional.\n",
            "\n",
            "‚ö° For accounts created after Nov 8, 2024, we have lowered the credits quota on the Free tier from 1000 credits to 100 credits, and now offer unlimited feeds on the Hobby Tier.\n",
            "\n",
            "‚ö° The Graphlit Data API will now return HTTP 402 (Payment Required) when you have exceeded the credits quota on the free tier.  You must upgrade to the Hobby Tier (or higher) to continue using the API, once the credits quota has been reached.\n",
            "\n",
            "Last updated 1 month ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [910d16b7-4e1b-4284-8136-002032429e24]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/910d16b7-4e1b-4284-8136-002032429e24/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "\n",
            "### New Features\n",
            "\n",
            "üî• Graphlit now supports Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "\n",
            "üî• Graphlit now supports Slack feeds, and will ingest Slack messages and linked file attachments from a Slack channel.  Note, this requires the creation of a Slack bot which has been added to the appropriate Slack channel.\n",
            "\n",
            "üí° Added support for entity enrichment to workflow object, which offers Diffbot, Wikipedia and Crunchbase enrichment of observed entities, such as Person, Organization and Place.\n",
            "\n",
            "üí° Added support for text extraction from images.  When using Azure Image Analytics for entity extraction, Graphlit will extract and store any identified text which then becomes searchable.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "embedFacets\n",
            "```\n",
            "\n",
            "property to conversation strategy in specification object.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "embedCitations\n",
            "```\n",
            "\n",
            "property to conversation strategy in specification object.  This makes content citations optional with the completed conversation message.\n",
            "\n",
            "Added GraphQL mutations for multi-delete of entities, such as\n",
            "\n",
            "```\n",
            "deleteCollections\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "deleteLabels\n",
            "```\n",
            "\n",
            ", or\n",
            "\n",
            "```\n",
            "deleteConversations\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "Added GraphQL\n",
            "\n",
            "```\n",
            "deleteAllConversations\n",
            "```\n",
            "\n",
            "mutation to delete all conversations.\n",
            "\n",
            "Added support for automatically adding ingested content to one or more collections, via ingestion stage of workflow object.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "specification\n",
            "```\n",
            "\n",
            "property to preparation workflow stage, which will be used to select the LLM for text summarization.\n",
            "\n",
            "Expanded the properties for observed entities, such as Person, Organization or Product.  Now supports a wider range of properties for entity enrichment.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-1520: Unlimited conversation quota not assigned when upgrading project tier\n",
            "\n",
            "GPLA-1285: Entity enrichment not firing event, which can be sent to actions\n",
            "\n",
            "GPLA-1361: Web page left in ingested state, when URL not accessible.\n",
            "\n",
            "Last updated 1 year ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [71b9497d-1494-49ee-94b3-1778254c3bad]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/71b9497d-1494-49ee-94b3-1778254c3bad/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# November 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "Graphlit now supports the latest Anthropic Haiku 3.5 model, with the model enum\n",
            "\n",
            "```\n",
            "CLAUDE_3_5_HAIKU_20241022\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "‚ö° Once a project has hit the free tier quota, we will now automatically disable all feeds.  Once the project has been upgraded to a paid tier, you can use the\n",
            "\n",
            "```\n",
            "enableFeed\n",
            "```\n",
            "\n",
            "mutation to re-enable your existing feeds to continue ingestion.\n",
            "\n",
            "‚ö° We have added the\n",
            "\n",
            "```\n",
            "disableFallback\n",
            "```\n",
            "\n",
            "flag to the\n",
            "\n",
            "```\n",
            "RetrievalStrategyInput\n",
            "```\n",
            "\n",
            "type, so you can disable the default behavior of falling back to the previous conversation's contents, or worst-case, falling back to the most recently uploaded content.  By setting\n",
            "\n",
            "```\n",
            "disableFallback\n",
            "```\n",
            "\n",
            "to true, conversations will only attempt to retrieve contents based on the provided\n",
            "\n",
            "```\n",
            "filter\n",
            "```\n",
            "\n",
            "and/or\n",
            "\n",
            "```\n",
            "augmentedFilter\n",
            "```\n",
            "\n",
            "properties.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3367: Not extracting text from HTML button element\n",
            "\n",
            "Last updated 1 month ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [da4af7ff-2250-42bd-820a-2b49dcb2bb58]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/da4af7ff-2250-42bd-820a-2b49dcb2bb58/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports multi-turn analysis of images with the\n",
            "\n",
            "```\n",
            "reviseImage\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "reviseEncodedImage\n",
            "```\n",
            "\n",
            "mutations.  You can provide an LLM prompt and either a URI or Base-64 encoded image and MIME type, along with an optional LLM specification.  This can be used for analyzing any image and having a multi-turn conversation with the LLM to revise the output from the LLM. ([Colab Notebook Example](https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_11_24_Multi_turn_Analysis_of_Image.ipynb))\n",
            "\n",
            "üí° Graphlit now supports directly prompting an LLM with the\n",
            "\n",
            "```\n",
            "prompt\n",
            "```\n",
            "\n",
            "mutation, bypassing any RAG content retrieval, while providing an optional list of previous conversation messages.  This also accepts an optional LLM specification. ([Colab Notebook Example](https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_11_24_Directly_Prompt_LLM_via_Conversation_Messages.ipynb))\n",
            "\n",
            "We have added support for the new Mistral Pixtral Large model, with\n",
            "\n",
            "```\n",
            "PIXTRAL_LARGE\n",
            "```\n",
            "\n",
            "model enum, which can be used with LLM completion or entity extraction LLM specifications.\n",
            "\n",
            "We have added support for the OpenAI 2024-11-20 version of GPT-4o, with\n",
            "\n",
            "```\n",
            "GPT4O_128K_20241120\n",
            "```\n",
            "\n",
            "model enum.\n",
            "\n",
            "‚ö° We have added Microsoft Entra ID (fka Azure Active Directory)\n",
            "\n",
            "```\n",
            "clientId\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "clientSecret\n",
            "```\n",
            "\n",
            "properties to the\n",
            "\n",
            "```\n",
            "SharePointFeedPropertiesInput\n",
            "```\n",
            "\n",
            "type, which are now required when creating a SharePoint feed using user authentication with\n",
            "\n",
            "```\n",
            "refreshToken\n",
            "```\n",
            "\n",
            "property. ([Colab Notebook Example](https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_11_25_SharePoint_to_RAG.ipynb))\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3438: Not filtering on desktop presentation when scraping web pages\n",
            "\n",
            "GPLA-3340: Failed to parse invalid JSON from extracted PDF page\n",
            "\n",
            "GPLA-3427: Not formatting extracted tables properly from Sonnet 3.5\n",
            "\n",
            "Last updated 1 month ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [8c70fdaf-adfa-41aa-bcb0-7dedab29d862]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/8c70fdaf-adfa-41aa-bcb0-7dedab29d862/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# November 16: Support for image description, multi-turn text summarization\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports multi-turn summarization of text with the\n",
            "\n",
            "```\n",
            "reviseText\n",
            "```\n",
            "\n",
            "mutation.  You can provide an LLM prompt and text string, along with an optional specification.  This can be used for summarizing any raw text and having a multi-turn conversation with the LLM to revise the output from the LLM.  ([Colab Notebook Example](https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_11_11_Multi_turn_Summarization.ipynb))\n",
            "\n",
            "üí° Graphlit now supports image descriptions using vision LLMs, without needing to ingest the image first.  With the new\n",
            "\n",
            "```\n",
            "describeImage\n",
            "```\n",
            "\n",
            "mutation, which takes a URI, and\n",
            "\n",
            "```\n",
            "describeEncodedImage\n",
            "```\n",
            "\n",
            "mutation, which takes a Base-64 encoded image and MIME type, you can use any vision LLM to prompt an image description.  These mutations accept an optional specification, where you can select your vision LLM.  If not provided, OpenAI GPT-4o will be used. ([Colab Notebook Example](https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_11_16_Describe_Image_with_Vision_LLM.ipynb))\n",
            "\n",
            "Last updated 1 month ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [7cf4454e-89b5-4384-a9ce-8de2ebed8bf7]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/7cf4454e-89b5-4384-a9ce-8de2ebed8bf7/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the [Jina reranker](https://jina.ai/reranker/) and [Pongo semantic filtering](https://www.joinpongo.com/) (reranking), in the Specification object.  Now you can choose between\n",
            "\n",
            "```\n",
            "COHERE\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "PONGO\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "JINA\n",
            "```\n",
            "\n",
            "for your reranking\n",
            "\n",
            "```\n",
            "serviceType\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "üí° Graphlit now supports Microsoft Teams feeds for reading messages from Teams channels.\n",
            "\n",
            "Given changes in YouTube video player HTML, we have rewritten the YouTube downloader to support the new page format.\n",
            "\n",
            "Added better handling of HTTP errors when validating URIs.  Previously some websites were returning HTTP 403 (Forbidden) errors when validating their URI, or downloading content.  Now Graphlit is able to scrape these sites, which previously returned errors.\n",
            "\n",
            "Added support for updating content metadata in\n",
            "\n",
            "```\n",
            "updateContent\n",
            "```\n",
            "\n",
            "mutation.  Now the video, audio, document, etc. metadata can be updated after the content workflow has finished.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "query_contents_graph\n",
            "```\n",
            "\n",
            "(and\n",
            "\n",
            "```\n",
            "queryContentsGraph\n",
            "```\n",
            "\n",
            ") functions to SDKs, which can be used to return nodes and edges from knowledge graph for visualization.\n",
            "\n",
            "‚ö° Citation indices have been changed to be one-based from zero-based.  For example, you will now see \"This is a citation. [1]\" as the first citation in the list.\n",
            "\n",
            "‚ö° Added\n",
            "\n",
            "```\n",
            "isSynchronous\n",
            "```\n",
            "\n",
            "flag to deleteAll and multiple delete mutations.  By default, bulk delete operations are now asynchronous (and completed after the mutation returns), unless the isSynchronous flag is set to true.\n",
            "\n",
            "‚ö° Added missing count mutations, such as\n",
            "\n",
            "```\n",
            "countAlerts\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "countFeeds\n",
            "```\n",
            "\n",
            ", etc.\n",
            "\n",
            "‚ö° Renamed\n",
            "\n",
            "```\n",
            "query_content_facets\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "query_contents_facets\n",
            "```\n",
            "\n",
            "in Python SDK\n",
            "\n",
            "‚ö° Renamed\n",
            "\n",
            "```\n",
            "queryContentFacets\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "queryContentsFacets\n",
            "```\n",
            "\n",
            "in Node.js SDK\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2544: Page\n",
            "\n",
            "```\n",
            "relevance\n",
            "```\n",
            "\n",
            "not filled-in in all situations\n",
            "\n",
            "GPLA-2546: Not extracting links from PDF with Azure AI Doc Intelligence\n",
            "\n",
            "GPLA-2557: Sporadically returning HTTP 500 from GraphQL API\n",
            "\n",
            "GPLA-2573: Failed to re-ingest content which was deleted immediately after initial ingestion\n",
            "\n",
            "GPLA-2575: Not validating for empty (non-null) parameters in mutations\n",
            "\n",
            "GPLA-2578: Need to handle invalid JSON from LLMs; improper escaping or formatting\n",
            "\n",
            "GPLA-2585: Failed to ingest encoded file with colon (:) in name\n",
            "\n",
            "Last updated 7 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [13aff7aa-566f-4911-9a0e-2a2a2e89360e]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/13aff7aa-566f-4911-9a0e-2a2a2e89360e/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# March 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports a Command-Line Interface (CLI) for directly accessing the Graphlit Data API without writing code.  See the documentation [here](https://docs.graphlit.dev/cli-reference/graphlit-cli).\n",
            "\n",
            "üí° Graphlit now supports the Groq Platform, and models such as [Mixtral 8x7b](https://console.groq.com/docs/models#mixtral8x7b).\n",
            "\n",
            "üí° Graphlit now supports Claude 3 Opus and Sonnet models.\n",
            "\n",
            "üí° Graphlit now supports [Mistral La Plateforme](https://mistral.ai/technology/), and models such as Mistral Small, Medium, and Large and Mixtral 8x7b.\n",
            "\n",
            "üí° Graphlit now supports the [latest v4 of Azure Document Intelligence](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/whats-new?view=doc-intel-4.0.0&amp;tabs=csharp#february-2024), including their new models such as Credit Card, Marriage Certificate, and Mortgage documents.\n",
            "\n",
            "Added support for detailed usage and credits telemetry via API, with the\n",
            "\n",
            "```\n",
            "usage\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "credits\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "lookupUsage\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "lookupCredits\n",
            "```\n",
            "\n",
            "queries.\n",
            "\n",
            "Added support for correlated telemetry, where an optional\n",
            "\n",
            "```\n",
            "correlationId\n",
            "```\n",
            "\n",
            "can be provided with GraphQL queries and mutations, so credits and usage can be tracked across requests.\n",
            "\n",
            "Added support for project webhook, which will be called when credits have been consumed by the project.\n",
            "\n",
            "Added support for image extraction during DOCX, XLSX, and PPTX document preparation.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "text\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "markdown\n",
            "```\n",
            "\n",
            "properties to Content object, which provide formatted output of extracted text from any content.\n",
            "\n",
            "Added more accurate extraction of tables into mezzanine JSON format, across all content types.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "throughput\n",
            "```\n",
            "\n",
            "property to Conversation messages, which returns the tokens/second throughput of LLM.\n",
            "\n",
            "‚ö° Deprecated\n",
            "\n",
            "```\n",
            "mezzanineUri\n",
            "```\n",
            "\n",
            "property in Content object, which has been replaced by\n",
            "\n",
            "```\n",
            "textUri\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "audioUri\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2281: Not extracting table from PPTX file.\n",
            "\n",
            "GPLA-2282: Not extracting Markdown tables.\n",
            "\n",
            "GPLA-2247: Not extracting relative HTML links properly.\n",
            "\n",
            "GPLA-2241: Failed to post Alert to Slack with Markdown format.\n",
            "\n",
            "Last updated 7 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [8979b5ec-cc44-48e3-abaa-be10e4436188]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/8979b5ec-cc44-48e3-abaa-be10e4436188/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# March 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the Claude 3 Haiku model.\n",
            "\n",
            "Added support for direct ingestion of Base64 encoded files with the\n",
            "\n",
            "```\n",
            "ingestEncodedFile\n",
            "```\n",
            "\n",
            "mutation.  You can pass a Base64 encoded string and MIME type of the file, and it will be ingested into the Graphlit Platform.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "modelService\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "model\n",
            "```\n",
            "\n",
            "properties to\n",
            "\n",
            "```\n",
            "ConversationMessage\n",
            "```\n",
            "\n",
            "type, which return the model service and model which was used for the LLM completion.\n",
            "\n",
            "Last updated 7 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [d66a9ba9-832d-4347-b6c9-d6a129e68b88]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/d66a9ba9-832d-4347-b6c9-d6a129e68b88/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# March 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports [Linear](https://docs.graphlit.dev/graphlit-data-api/api-reference/feeds/issue-feeds/create-linear-feed), [GitHub Issues](https://docs.graphlit.dev/graphlit-data-api/api-reference/feeds/issue-feeds/create-github-issues-feed) and [Atlassian Jira](https://docs.graphlit.dev/graphlit-data-api/api-reference/feeds/issue-feeds/create-jira-feed) feeds.  Graphlit will ingest issues (aka tasks, stories) from these issue-tracking services as individual content items, which will be made searchable and conversational.\n",
            "\n",
            "üí° Added support for\n",
            "\n",
            "```\n",
            "ISSUE\n",
            "```\n",
            "\n",
            "content type, which includes metadata such as title, authors, commenters, status, type, project and team.\n",
            "\n",
            "üí° Added support for default feed read limit.  Now, if you don't specify the\n",
            "\n",
            "```\n",
            "readLimit\n",
            "```\n",
            "\n",
            "property on feeds, it will default to reading 100 content items.  You can override this default by assigning a custom read limit, which has no upper bounds.  However, one-shot feeds much complete within 15 minutes, or they will be stopped automatically.\n",
            "\n",
            "Added support for ingesting files referenced in a Web sitemap.  Previously any files (i.e. PDF, MP3) referenced in a sitemap.xml would be ignored.  Now you can optionally enable\n",
            "\n",
            "```\n",
            "includeFiles\n",
            "```\n",
            "\n",
            "in the\n",
            "\n",
            "```\n",
            "WebFeedPropertiesInput\n",
            "```\n",
            "\n",
            "object to have Graphlit ingest non-HTML pages as part of the Web feed.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2374: Failed to ingest MP4 with large XMP metadata.\n",
            "\n",
            "Last updated 8 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [cf9e692e-73ad-4abe-9b75-1f5692aa7c79]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/cf9e692e-73ad-4abe-9b75-1f5692aa7c79/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports GraphRAG, where the extracted entities in the knowledge graph can be added as additional context to your RAG con,versation.  Also, with GraphRAG, entities can be extracted from the user prompt, and used as additional content filters - or can be used to query related content sources, which are combined with the vector search results.  This can be configured by specifying your\n",
            "\n",
            "```\n",
            "graphStrategy\n",
            "```\n",
            "\n",
            "in the Specification object.\n",
            "\n",
            "üí° Graphlit now supports LLM revisions within RAG conversations, where the LLM can be prompted to revise its initial completion response. From our testing, this has been shown to provide 35% more output tokens with higher quality responses.  This can be configured by specifying your\n",
            "\n",
            "```\n",
            "revisionStrategy\n",
            "```\n",
            "\n",
            ", and you can use our built-in revision prompt, or provide a custom one, and specify how many revisions you want the LLM to make.\n",
            "\n",
            "üí° Graphlit now supports the new OpenAI [GPT-4o ](https://platform.openai.com/docs/models/gpt-4o)model for RAG conversations.\n",
            "\n",
            "‚ö° We have changed the default model for Conversations to be OpenAI GPT-4o, from Azure OpenAI GPT-3.5 16k.  This provides faster performance and better quality output.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "graph\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "promptConversation\n",
            "```\n",
            "\n",
            "response, so you can visualize or leverage the nodes and edges of the knowledge graph, resulting from the content retrieval.  For example, if a Person and Organization were observed in the cited content sources used by the RAG pipeline, you will get back those entities and their relationship (such as Person 'works-for' Organization).\n",
            "\n",
            "Expanded the enriched data from WIkipedia to include the long description of an entity.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "getSharePointLibraries\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "getSharePointFolders\n",
            "```\n",
            "\n",
            ", and\n",
            "\n",
            "```\n",
            "getOneDriveFolders\n",
            "```\n",
            "\n",
            "queries to the API, which can be used to enumerate the storage services.  This makes locating the SharePoint\n",
            "\n",
            "```\n",
            "libraryId\n",
            "```\n",
            "\n",
            "easier, for example.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "getTeams\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "getTeamsChannels\n",
            "```\n",
            "\n",
            "queries to the API for enumerating Microsoft Teams workspaces.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "extractedCount\n",
            "```\n",
            "\n",
            "to the entity extraction connector to limit the number of extracted entities, per entity type.  I.e. if extracted count is 10, it will extract at most ten each of Persons, Organizations, etc.\n",
            "\n",
            "üî•  We have improved performance in several areas: creation of observations after entity extraction, access to cloud storage, rendering the RAG context.\n",
            "\n",
            "üî•  We have optimized the LLM entity extraction process to identify more properties, as well as entity-to-entity relationships.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2652: Not extracting text from HTML in RSS post\n",
            "\n",
            "GPLA-2627: Limit filter only returning half the results\n",
            "\n",
            "GPLA-2613: Not properly extracting structured text from JSON/XML formats\n",
            "\n",
            "Last updated 6 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [1a8ef840-30b3-4d00-9f3d-dadb84538037]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/1a8ef840-30b3-4d00-9f3d-dadb84538037/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports [Deepseek](https://platform.deepseek.com/api-docs/#models) LLMs for prompt completion.  We offer the\n",
            "\n",
            "```\n",
            "deepseek-chat\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "deepseek-coder\n",
            "```\n",
            "\n",
            "models.\n",
            "\n",
            "üí° Graphlit now supports parsing embedded JSON-LD from web pages.  If a web page contains 'script' tags with JSON-LD, we will automatically parse and inject into the knowledge graph.\n",
            "\n",
            "‚ö° We have changed the default model for entity extraction and image completions to be OpenAI GPT-4o.  This provides faster performance and better quality output.\n",
            "\n",
            "‚ö° We have changed the behavior of knowledge graph generation, from a prompted conversation, to be opt-in.  In order to receive the graph's nodes and edges with the response, you will now need to set\n",
            "\n",
            "```\n",
            "generateGraph\n",
            "```\n",
            "\n",
            "to True in the specification's\n",
            "\n",
            "```\n",
            "graphStrategy\n",
            "```\n",
            "\n",
            "object.  This provides improved performance when the graph is not needed for visualization.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "thing\n",
            "```\n",
            "\n",
            "property for observable entities, which returns the JSON-LD metadata associated with the entity.\n",
            "\n",
            "Added regex-based filtering for URI paths during feed ingestion, link crawling, and workflow filtering.  You can assign regex patterns in\n",
            "\n",
            "```\n",
            "allowedPaths\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "excludedPaths\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "observableLimit\n",
            "```\n",
            "\n",
            "to configure the limit of how many observed entities will be added to the GraphRAG context, defaults to 1000.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "prompt\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "suggestConversation\n",
            "```\n",
            "\n",
            "mutation, which allows customization of the followup question generation.\n",
            "\n",
            "Updated\n",
            "\n",
            "```\n",
            "suggestConversation\n",
            "```\n",
            "\n",
            "to incorporate the past conversation message history, in addition to the filtered set of content sources.\n",
            "\n",
            "üî•  We have improved performance in knowledge graph retrieval and generation, via better parallelization and batching.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2748: Optimize the retrieval performance of observed entities during GraphRAG\n",
            "\n",
            "GPLA-2732: Invalid user-provided URI causing parsing exception\n",
            "\n",
            "GPLA-2666: Shouldn't require tenant ID for Microsoft email or Teams\n",
            "\n",
            "GPLA-2772: Not returning labels or categories from graph in API\n",
            "\n",
            "GPLA-2762: Failed to extract spreadsheet images\n",
            "\n",
            "GPLA-2687: Email to/from not getting added as observations on emails\n",
            "\n",
            "GPLA-2738: API is returning 'audio' metadata from podcast HTML document\n",
            "\n",
            "Last updated 6 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f935efb5-7b33-4d08-84eb-54d920cf9331]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/f935efb5-7b33-4d08-84eb-54d920cf9331/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the Anthropic Claude 3.5 Sonnet model, which can be assigned with the\n",
            "\n",
            "```\n",
            "CLAUDE_3_5_SONNET\n",
            "```\n",
            "\n",
            "model enum.\n",
            "\n",
            "üí° Graphlit now supports semantic search of observable entities in the knowledge graph, such as Person, Organization and Place.  These entity types will now have vector embeddings created from their enriched metadata, and support searching by similar text, and searching by similar entities.\n",
            "\n",
            "‚ö° We have changed the Google Drive and Google Email feed properties to require the Google OAuth client ID and client secret, along with the existing refresh token, for proper authentication against Google APIs.\n",
            "\n",
            "‚ö° We have added a credits quota on the Free Tier.  Once 1000 credits have been used on the Free Tier, no more content can be ingested, and an upgrade to a paid tier is required.  Customers will receive an email when the credits, storage or contents quota has been reached.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2837: Failed to ingest LinkedIn page as Web feed\n",
            "\n",
            "GPLA-2831: Zero-byte file was left in Indexed state\n",
            "\n",
            "GPLA-2834: Not reading any files from Azure blob feed with space in prefix\n",
            "\n",
            "GPLA-2828: Better handling for files with unknown (or missing) file extensions\n",
            "\n",
            "Last updated 6 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [ef749999-7457-45fc-a9c1-6cb2a6943547]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/ef749999-7457-45fc-a9c1-6cb2a6943547/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "indexing\n",
            "```\n",
            "\n",
            "workflow stage. This provides for configuration of indexing services, which may infer metadata from the content.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "AZURE_AI_LANGUAGE\n",
            "```\n",
            "\n",
            "content indexing service, which supports inferring the language of extracted text or transcript.\n",
            "\n",
            "Added support for\n",
            "\n",
            "```\n",
            "language\n",
            "```\n",
            "\n",
            "content metadata.  This returns a list of languages in ISO 639-1 format, which may have been inferred from the extracted text or transcript.\n",
            "\n",
            "Added support for\n",
            "\n",
            "```\n",
            "MODEL_IMAGE\n",
            "```\n",
            "\n",
            "extraction service.  This provides integration with vision models beyond those provided by OpenAI.  You can assign a custom specification and bring-your-own API key for image extraction models.\n",
            "\n",
            "‚ö° We have deprecated the\n",
            "\n",
            "```\n",
            "OPENAI_IMAGE\n",
            "```\n",
            "\n",
            "service type, and developers should now use the LLM image service instead.\n",
            "\n",
            "‚ö° We have removed the\n",
            "\n",
            "```\n",
            "language\n",
            "```\n",
            "\n",
            "field from\n",
            "\n",
            "```\n",
            "AudioMetadata\n",
            "```\n",
            "\n",
            "type, which has been replaced by the new\n",
            "\n",
            "```\n",
            "LanguageMetadata\n",
            "```\n",
            "\n",
            "type.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2987: Extracting text with Azure Doc Intelligence does not extract hyperlinks\n",
            "\n",
            "Last updated 5 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [d182ac8a-ccd5-4b0a-92b6-faf5beceec95]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/d182ac8a-ccd5-4b0a-92b6-faf5beceec95/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports webhook Alerts.  In addition to Slack notifications, you can now receive an HTTP POST webhook with the results of the published text (or text and audio URI) from a prompted alert.\n",
            "\n",
            "Updated the Deepseek chat and coder models to support a [128k token context window](https://platform.deepseek.com/api-docs/#models).\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "customSummary\n",
            "```\n",
            "\n",
            "property to\n",
            "\n",
            "```\n",
            "Content\n",
            "```\n",
            "\n",
            "object, which returns the custom summary generated via preparation workflow.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "keywords\n",
            "```\n",
            "\n",
            "summarization type, which is now stored in\n",
            "\n",
            "```\n",
            "keywords\n",
            "```\n",
            "\n",
            "property in\n",
            "\n",
            "```\n",
            "Content\n",
            "```\n",
            "\n",
            "object.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "slackChannels\n",
            "```\n",
            "\n",
            "query, which returns the list of Slack channels from the workspace authenticated by the Slack bot token.\n",
            "\n",
            "‚ö° We have changed the response from the\n",
            "\n",
            "```\n",
            "credits\n",
            "```\n",
            "\n",
            "query to return a single\n",
            "\n",
            "```\n",
            "ProjectCredits\n",
            "```\n",
            "\n",
            "object, rather than the list of correlated objects previously returned.  The\n",
            "\n",
            "```\n",
            "credits\n",
            "```\n",
            "\n",
            "response now covers all credit usage over the time period specified.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2874: Processing entities is taking longer than 30min for 300+ page PDF\n",
            "\n",
            "GPLA-2875: Messages in queue expiring too early\n",
            "\n",
            "GPLA-2881: Feed read count increasing, after hitting read limit\n",
            "\n",
            "GPLA-2884: Need to handle Anthropic 'overloaded' API response\n",
            "\n",
            "GPLA-2906: JIRA issue identifier not assigned to issue metadata\n",
            "\n",
            "Last updated 5 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [7f5a76ab-915f-414c-8cc4-f6fcdfb3f39d]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/7f5a76ab-915f-414c-8cc4-f6fcdfb3f39d/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the Mistral Large 2 and Mistral Nemo models. The existing\n",
            "\n",
            "```\n",
            "MISTRAL_LARGE\n",
            "```\n",
            "\n",
            "model enum now will use Mistral Large 2.\n",
            "\n",
            "üí° Graphlit now supports the Llama 3.1 8b, 70b and 405b models on Groq.  (Note, these are rate-limited according to Groq's platform constraints.)\n",
            "\n",
            "Added support for revision strategy on data extraction specifications.  Now you can prompt the LLM to revise its previous data extraction response, similar to the existing completion revision strategy.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "version\n",
            "```\n",
            "\n",
            "property for\n",
            "\n",
            "```\n",
            "AzureDocumentPreparationProperties\n",
            "```\n",
            "\n",
            "type for assigning the [API version](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-4.0.0) used by Azure AI Document Intelligence.   By default, Graphlit will continue to use the v4.0 (Preview) API version, but you can override this to assign\n",
            "\n",
            "```\n",
            "version\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "V2023_10_31\n",
            "```\n",
            "\n",
            "to use the v3.1 (GA) API version instead.  For some documents, the General Availability (GA) version of the API can provide better results.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2988: Not extracting hyperlinks from Office documents.\n",
            "\n",
            "Last updated 5 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [5201e73c-2b85-4bfa-851b-a14b602167b0]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/5201e73c-2b85-4bfa-851b-a14b602167b0/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# July 15: Support for SharePoint feeds, new Conversation features\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Added support for SharePoint feeds: now can create feed to ingest files from SharePoint document library (and optionally, folder within document library)\n",
            "\n",
            "üí° Added support for PII detection during entity extraction from text documents and audio transcripts: now we will create labels such as\n",
            "\n",
            "```\n",
            "PII: Social Security Number\n",
            "```\n",
            "\n",
            "automatically when PII is detected\n",
            "\n",
            "üí° Added support for developer's own OpenAI API keys and Azure OpenAI deployments in\n",
            "\n",
            "```\n",
            "Specifications\n",
            "```\n",
            "\n",
            "‚ÑπÔ∏è  Changed semantics of\n",
            "\n",
            "```\n",
            "deleteFeed\n",
            "```\n",
            "\n",
            "to delete the contents ingested by the feed; since contents are linked to feeds, now feeds can be disabled, while keeping the lineage to the feed, and if feeds are deleted, they will delete the linked contents, so we never lose the feed-to-content lineage\n",
            "\n",
            "Added GraphQL query for SharePoint consent URI, for registered Graphlit Platform Azure AD application\n",
            "\n",
            "Better handling of web sitemap indexes: now if a sitemap.xml contains a\n",
            "\n",
            "```\n",
            "sitemapindex\n",
            "```\n",
            "\n",
            "element, we will load all linked sitemaps for evaluating web pages to ingest from Web feed\n",
            "\n",
            "Added new GraphQL mutations for\n",
            "\n",
            "```\n",
            "openConversation\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "closeConversation\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "undoConversation\n",
            "```\n",
            "\n",
            "Added timestamps to Conversation messages\n",
            "\n",
            "Added new GraphQL mutations for\n",
            "\n",
            "```\n",
            "openCollection\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "closeCollection\n",
            "```\n",
            "\n",
            "Added more configuration for content search: now can specify\n",
            "\n",
            "```\n",
            "searchType\n",
            "```\n",
            "\n",
            "(KEYWORD, VECTOR, HYBRID) and\n",
            "\n",
            "```\n",
            "queryType\n",
            "```\n",
            "\n",
            "(SIMPLE, FULL - aka Lucene syntax)\n",
            "\n",
            "Better parsing of iTunes podcast metadata\n",
            "\n",
            "‚ö° Renamed\n",
            "\n",
            "```\n",
            "listingLimit\n",
            "```\n",
            "\n",
            "field on feeds to\n",
            "\n",
            "```\n",
            "readLimit\n",
            "```\n",
            "\n",
            "‚ö° Renamed\n",
            "\n",
            "```\n",
            "topK\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "numberSimilar\n",
            "```\n",
            "\n",
            "for content vector search type\n",
            "\n",
            "‚ö° Changed GraphQL feed properties: split out\n",
            "\n",
            "```\n",
            "azure\n",
            "```\n",
            "\n",
            "into\n",
            "\n",
            "```\n",
            "azureBlob\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "azureFile\n",
            "```\n",
            "\n",
            "properties\n",
            "\n",
            "‚ö° Changed GraphQL specification properties: split out\n",
            "\n",
            "```\n",
            "openAI\n",
            "```\n",
            "\n",
            "into\n",
            "\n",
            "```\n",
            "openAI\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "azureOpenAI\n",
            "```\n",
            "\n",
            "properties\n",
            "\n",
            "‚ö° Removed\n",
            "\n",
            "```\n",
            "count\n",
            "```\n",
            "\n",
            "fields on query results, and replaced with explicit\n",
            "\n",
            "```\n",
            "count{Entity}\n",
            "```\n",
            "\n",
            "queries, which support search and filtering.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-1043: Reddit\n",
            "\n",
            "```\n",
            "readLimit\n",
            "```\n",
            "\n",
            "not taking effect: now the specified limit of Reddit posts will be leveraged for Reddit feeds\n",
            "\n",
            "GPLA-1064: Performance on entity extraction and observation creation for large PDFs was under expectations: now able to build knowledge graph from large PDFs much faster (4x speed improvement)\n",
            "\n",
            "GPLA-1053: If rendition generation errored during content workflow, the content was not properly marked as errored\n",
            "\n",
            "GPLA-1102: Large Web sitemaps were slow to load; rewrote sitemap index handling, and now can process sitemaps with 150K+ entries in seconds.\n",
            "\n",
            "Last updated 5 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [e1a1033f-6e02-4f95-aff3-86b78b02b775]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/e1a1033f-6e02-4f95-aff3-86b78b02b775/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the [OpenAI GPT-4o Mini](https://platform.openai.com/docs/models/gpt-4o-mini) model, with 16k output tokens.\n",
            "\n",
            "üí° Graphlit now supports 'bring-your-own-key' for Azure AI Document Intelligence models.  We have added a custom\n",
            "\n",
            "```\n",
            "endpoint\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "key\n",
            "```\n",
            "\n",
            "property, which can be assigned to use your own Azure AI resource.\n",
            "\n",
            "Updated to use [Jina reranker v2](https://jina.ai/reranker/) (jina-reranker-v2-base-multilingual) by default.\n",
            "\n",
            "Updated to assign the\n",
            "\n",
            "```\n",
            "summary\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "bullets\n",
            "```\n",
            "\n",
            ", etc properties when calling\n",
            "\n",
            "```\n",
            "summarizeContents\n",
            "```\n",
            "\n",
            "mutation.  Now when summarizing contents, we will store the resulting summary in the content itself, in addition to returning the summarized results.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "relevance\n",
            "```\n",
            "\n",
            "property to all entity types, which will be assigned when searching for these entities.  Entity results will be sorted (descending) by this search relevance score.\n",
            "\n",
            "Added the ability to manually update\n",
            "\n",
            "```\n",
            "summary\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "bullets\n",
            "```\n",
            "\n",
            ", etc. properties when calling the\n",
            "\n",
            "```\n",
            "updateContent\n",
            "```\n",
            "\n",
            "mutation.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "offset\n",
            "```\n",
            "\n",
            "property to AtlassianJiraFeedProperties, so the timezone offset can be properly assigned for paging of the Jira feed.  (Defaults to zero offset, i.e. UTC.)  Jira does not store dates in UTC format, and the timestamps are based on the server timezone of the hosted Jira instance.  By assigning the timezone offset with the Jira feed, we can reliably page the updated date timestamps from the Jira API.\n",
            "\n",
            "‚ö° We have changed the content similarity search behavior to find similar content by summary, rather than text of the document, when a summary has been previously generated.  For long documents, this will provide a more accurate similarity, rather than comparing on the first few pages of text in a document.\n",
            "\n",
            "‚ö° We have changed the behavior of assigning\n",
            "\n",
            "```\n",
            "offset\n",
            "```\n",
            "\n",
            "in the entity filter objects for paging through entities.  If using vector or hybrid search, this offset will be ignored (i.e. zero offset).  Paging will not be supported through vector or hybrid search results. For keyword search, the offset will continue to be used, along with the\n",
            "\n",
            "```\n",
            "limit\n",
            "```\n",
            "\n",
            "property, to provide paging through the search results.  We have made this change because we have found that index-based paging is not reliable with our vector/hybrid search approach.  We are investigating ways to support this reliably with vector/hybrid search in the future.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2915: Add retry on OpenAI API HTTP 524 error (gateway timeout).\n",
            "\n",
            "GPLA-2908: Not paging through Jira feed correctly.\n",
            "\n",
            "GPLA-2917: Search by similar content is not giving expected results on long documents.\n",
            "\n",
            "GPLA-2244: Keyword search not finding text in latter part of long PDF.\n",
            "\n",
            "Last updated 5 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [2543e33d-4331-4e29-8e3e-31b7c74e801b]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/2543e33d-4331-4e29-8e3e-31b7c74e801b/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports Google and Microsoft email feeds.  Email feeds can be created to ingest past emails, or poll for new emails.  Emails create an\n",
            "\n",
            "```\n",
            "EMAIL\n",
            "```\n",
            "\n",
            "content type. Attachment files can optionally be extracted from emails, and will be linked to their parent email content. If assigning a workflow to the feed, the workflow will be applied both to the email content and the extracted attachment files.\n",
            "\n",
            "üí° Graphlit now supports reingesting content in-place.  The\n",
            "\n",
            "```\n",
            "ingestText\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "ingestPage\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "ingestFile\n",
            "```\n",
            "\n",
            "mutations now take an optional\n",
            "\n",
            "```\n",
            "id\n",
            "```\n",
            "\n",
            "parameter for an existing content object.  If this id is provided, the existing content will be updated from the provided text or URI source, and will restart the assigned workflow.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "restartAllContents\n",
            "```\n",
            "\n",
            "mutation to restart workflow on all partially-ingested contents in project.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "text\n",
            "```\n",
            "\n",
            "field to ConversationCitation type, which returns the relevant text from the content source with the citation.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-1313: Not extracting links from HTML\n",
            "\n",
            "GPLA-2030: No text extracted from shapes in PPTX files\n",
            "\n",
            "Last updated 8 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [5ca8c379-fc75-4652-9b9b-d8a6b8182c0f]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/5ca8c379-fc75-4652-9b9b-d8a6b8182c0f/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports OneDrive and Google Drive feeds.  Files can be ingested from OneDrive or Google Drive, including shared drives where the authenticated user has access.  Both OneDrive and Google Drive support the reading of existing files, and tracking new files added to storage with recurrent feeds.\n",
            "\n",
            "üí° Graphlit now supports email backup files, such as EML or MSG, which will be assigned the\n",
            "\n",
            "```\n",
            "EMAIL\n",
            "```\n",
            "\n",
            "file type.  During email file preparation, we will automatically extract and ingest any file attachments.\n",
            "\n",
            "üí° Graphlit now automatically extracts embedded images in PDF files, ingests them as content objects, and links them as children of the parent PDF.\n",
            "\n",
            "üí° Graphlit now supports recursive Notion feeds.  When the\n",
            "\n",
            "```\n",
            "isRecursive\n",
            "```\n",
            "\n",
            "flag is true in the Notion feed properties, we will crawl child pages and databases, and recursively ingest them in addition to the specified pages and databases.\n",
            "\n",
            "Added support for assigning\n",
            "\n",
            "```\n",
            "collections\n",
            "```\n",
            "\n",
            "to content ingested with the\n",
            "\n",
            "```\n",
            "ingestPage\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "ingestFile\n",
            "```\n",
            "\n",
            "or\n",
            "\n",
            "```\n",
            "ingestText\n",
            "```\n",
            "\n",
            "mutations.  This saves a step where the content will automatically be added to the collection(s) without requiring another mutation call.\n",
            "\n",
            "Added support for the\n",
            "\n",
            "```\n",
            "CODE\n",
            "```\n",
            "\n",
            "file type for a wide variety of source code formats, i.e. Python .py, Javascript .js.  Code files use optimized text splitting for enhanced search and retrieval.\n",
            "\n",
            "Added support for\n",
            "\n",
            "```\n",
            "customGuidance\n",
            "```\n",
            "\n",
            "in Specification object, which can be used for injecting a guidance prompt during the RAG process.  For example, you can instruct the LLM to return a default response string if no content sources are found via semantic search.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "tenants\n",
            "```\n",
            "\n",
            "field to Project object, which returns a list of all tenant IDs which have been used to create an entity in Graphlit.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "email\n",
            "```\n",
            "\n",
            "metadata, separate from\n",
            "\n",
            "```\n",
            "document\n",
            "```\n",
            "\n",
            "metadata.  Now emails will contain indexed metadata such as to, from, or subject.\n",
            "\n",
            "‚ö° The\n",
            "\n",
            "```\n",
            "contents\n",
            "```\n",
            "\n",
            "field for content objects has been replaced with\n",
            "\n",
            "```\n",
            "children\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "parent\n",
            "```\n",
            "\n",
            "fields.  For example, when a ZIP file is unpacked, the unpacked files will be added as children of the ZIP file, and the ZIP file will be the parent of each of the unpacked files.\n",
            "\n",
            "‚ö° Removed\n",
            "\n",
            "```\n",
            "enableImageAnalysis\n",
            "```\n",
            "\n",
            "field from image preparation properties in workflow object.  Now is enabled by default.\n",
            "\n",
            "‚ö° Moved\n",
            "\n",
            "```\n",
            "disableSmartCapture\n",
            "```\n",
            "\n",
            "field to preparation workflow stage from page preparation properties.  This is used to disable the use of headless Chrome browser to capture HTML from web pages.  It is enabled by default, and if disabled, Graphlit will simply download the HTML from the web page rather than rendering on headless Chrome browser.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2099: Failed to ingest ArXiV PDF.  Fixed PDF parsing error.\n",
            "\n",
            "GPLA-2174: LLM response is incorrect with conversation history, but no content sources.\n",
            "\n",
            "GPLA-2199: ZIP package left in Indexed state after content workflow.\n",
            "\n",
            "Last updated 9 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [079f5da2-65ea-42e7-a074-232a3bee1782]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/079f5da2-65ea-42e7-a074-232a3bee1782/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports mapping a website with the\n",
            "\n",
            "```\n",
            "mapWeb\n",
            "```\n",
            "\n",
            "mutation. You can provide a URL to a website, and the query will return a list of URLs based on the sitemap.xml (or sitemap-index.xml) file, at or underneath the provided URL.\n",
            "\n",
            "üí° Graphlit now supports the generation of web page screenshots with the\n",
            "\n",
            "```\n",
            "screenshotPage\n",
            "```\n",
            "\n",
            "mutation. By providing the URL of a web page, and optionally, the maximum desired height of the screenshot, we will screenshot the webpage and ingest it automatically as content.  You can provide an optional workflow, which will be applied to the ingested image content, for operations like generating image descriptions with a vision LLM.\n",
            "\n",
            "üí° Graphlit now supports the direct summarization of text with the\n",
            "\n",
            "```\n",
            "summarizeText\n",
            "```\n",
            "\n",
            "mutation. By providing the desired summarization strategy, we will summarize the text (i.e. bullet points, social media posts) and return the summarization.\n",
            "\n",
            "üí° Graphlit now supports the direct extraction of text with the\n",
            "\n",
            "```\n",
            "extractText\n",
            "```\n",
            "\n",
            "mutation. By providing the LLM tool definitions and an optional LLM specification, we will prompt the desired LLM (or OpenAI GPT-4o, by default) to invoke the provided tools, and return the JSON responses from the LLM tool calling.\n",
            "\n",
            "Graphlit now supports the latest Groq Llama 3.3 model, with the model enum\n",
            "\n",
            "```\n",
            "LLAMA_3_3_70B\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "We have updated Cohere reranking to use the latest Cohere\n",
            "\n",
            "```\n",
            "rerank-v3.5\n",
            "```\n",
            "\n",
            "model by default.\n",
            "\n",
            "‚ö° We have added a new\n",
            "\n",
            "```\n",
            "flattenCitations\n",
            "```\n",
            "\n",
            "field to the\n",
            "\n",
            "```\n",
            "ConversationStrategyInput\n",
            "```\n",
            "\n",
            "type.  By assigning this field to True, when calling\n",
            "\n",
            "```\n",
            "promptConversation,\n",
            "```\n",
            "\n",
            "we will combine multiple citations from the same content into a single citation.\n",
            "\n",
            "‚ö° For Microsoft email, Microsoft Teams and OneDrive feeds, we have added the\n",
            "\n",
            "```\n",
            "clientId\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "clientSecret\n",
            "```\n",
            "\n",
            "fields as required feed properties. These properties must be assigned, in addition to the\n",
            "\n",
            "```\n",
            "refreshToken\n",
            "```\n",
            "\n",
            "field for proper authentication to the Microsoft Graph API used by these feeds. ([Colab Notebook Example](https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_12_09_Locate_Microsoft_Emails_by_Organization.ipynb))\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3492: Not finding sitemap at parent web path\n",
            "\n",
            "GPLA-3500: Failed to handle mismatch of Deepgram model/language\n",
            "\n",
            "Last updated 17 days ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [ba3d0301-8930-4132-bf47-ee8e58a462aa]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/ba3d0301-8930-4132-bf47-ee8e58a462aa/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports content publishing, where documents, audio transcripts and even image descriptions, can be summarized, and repurposed into blog posts, emails or AI-generated podcasts.  With the new\n",
            "\n",
            "```\n",
            "publishContents\n",
            "```\n",
            "\n",
            "mutation, you can configure LLM prompts for summarization and publishing, and assign\n",
            "\n",
            "```\n",
            "specifications\n",
            "```\n",
            "\n",
            "to use different models and/or system prompts for each step in the process.  The published content will be reingested into Graphlit, and can be searched or used for conversations, like any other form of content.\n",
            "\n",
            "üí° Graphlit now supports publishing conversations as content with the new\n",
            "\n",
            "```\n",
            "publishConversation\n",
            "```\n",
            "\n",
            "mutation.  You can generate text or audio transcripts of your conversations, to be reused in other tools.\n",
            "\n",
            "üí° Graphlit now supports bulk summarization of contents with the\n",
            "\n",
            "```\n",
            "summarizeContents\n",
            "```\n",
            "\n",
            "mutation.  You can filter a set of content, by feed, by observable or by similar text, and run a set of summarizations across each content in parallel.\n",
            "\n",
            "üí° Graphlit now supports LLM entity extraction, with the new\n",
            "\n",
            "```\n",
            "MODEL_TEXT\n",
            "```\n",
            "\n",
            "entity extraction service type.  Similar to using Azure Cognitive Service Text Analytics, you can use any OpenAI or Anthropic model for extracting entities from text.  Internally the LLM returns JSON-LD entities, which we convert into Person, Organization, Place, etc. entities and assign\n",
            "\n",
            "```\n",
            "observations\n",
            "```\n",
            "\n",
            "to the extracted content.\n",
            "\n",
            "üí° Graphlit now supports LLM tools (aka function calls) with OpenAI models.  You can define the tools to be used with the LLM in the\n",
            "\n",
            "```\n",
            "specification\n",
            "```\n",
            "\n",
            "object.  With the new\n",
            "\n",
            "```\n",
            "extractContents\n",
            "```\n",
            "\n",
            "mutation, you can execute a prompt against content using a specification with tools defined.  The mutation will return the JSON arguments assigned by the LLM.\n",
            "\n",
            "üí° Graphlit now supports callback webhooks for LLM tools.  If you assign a URI in the\n",
            "\n",
            "```\n",
            "ToolDefinition\n",
            "```\n",
            "\n",
            "object, Graphlit will call your webhook the tool name and JSON arguments.  When you respond to the webhook with JSON, we will add that response to the LLM messages, and ask the LLM to complete the original prompt.\n",
            "\n",
            "üí° Graphlit now supports the selection of the [Deepgram model (such as Meeting, Phonecall or Finance)](https://developers.deepgram.com/docs/model) with the preparation workflow.  Also, you can assign your own Deepgram API key, which will be used for audio transcription using that workflow.\n",
            "\n",
            "Added support for CLIP image embeddings using [Roboflow](https://www.roboflow.com/), which can be used for similar image search.  If you search for contents by similar contents, we will now use the content's text and/or image embeddings to find similar content.\n",
            "\n",
            "Added support for dynamic web page ingestion.  Graphlit now navigates to and automatically scrolls web pages using [Browserless.io](https://www.browserless.io/), so we capture the fully rendered HTML before extracting text.  Also, we now support web page screenshots, if enabled with\n",
            "\n",
            "```\n",
            "enableImageAnalysis\n",
            "```\n",
            "\n",
            "property in preparation workflow.  These screenshots can be analyzed with multimodal modals, such as GPT-4 Vision, or can be used to create image embeddings for similar image search.\n",
            "\n",
            "Added table parsing when preparing documents.  We now store structured (tab-delimited) text in the JSON text mezzanine which is extracted from documents in the preparation workflow.\n",
            "\n",
            "Added reverse geocoding of lat/long locations found in image or other content metadata.  We now store the real-world address with the content metadata, for use in conversations.\n",
            "\n",
            "Added assistant messages to the conversation message history provided to the LLM.  Originally we had included only user messages, but now we are formatting both user and assistant messages into the LLM prompt for conversations.\n",
            "\n",
            "Added new chunking algorithm for text embeddings.  We support semantic chunking at the page or transcript segment level, and now will create embeddings from smaller sized text chunks per page or segment.\n",
            "\n",
            "Added content metadata to text and image embeddings.  To provide better context for the text embeddings, we now include formatted content metadata, which includes fields like title, subject, author, or description.  For emails, we include to, from, cc, and bcc fields.\n",
            "\n",
            "Added helper mutations\n",
            "\n",
            "```\n",
            "isContentDone\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "isFeedDone\n",
            "```\n",
            "\n",
            "which can be used for polling completion of ingested content, or all content ingested by a feed.\n",
            "\n",
            "Added richer image descriptions generated by the GPT-4 Vision model.  Now these provide more useful detail.\n",
            "\n",
            "Added validation of extracted hyperlinks.  Now we test the URIs and remove any inaccessible links during content enrichment.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "deleteContents\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "deleteFeeds\n",
            "```\n",
            "\n",
            ",  and\n",
            "\n",
            "```\n",
            "deleteConversations\n",
            "```\n",
            "\n",
            "mutations for multi-deletion of contents, feeds or conversations.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "deleteAllContents\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "deleteAllFeeds\n",
            "```\n",
            "\n",
            ",  and\n",
            "\n",
            "```\n",
            "deleteAllConversations\n",
            "```\n",
            "\n",
            "mutations for bulk, filtered deletion of entities.  You can delete all your contents, feeds, or conversations in your project, or a filtered subset of those entities.\n",
            "\n",
            "‚ÑπÔ∏è Starter tier now has a higher content limit of 100K content items.\n",
            "\n",
            "‚ö° In the\n",
            "\n",
            "```\n",
            "OpenAIImageExtractionProperties\n",
            "```\n",
            "\n",
            "type, the\n",
            "\n",
            "```\n",
            "detailMode\n",
            "```\n",
            "\n",
            "field was renamed to\n",
            "\n",
            "```\n",
            "detailLevel\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "‚ö° Each\n",
            "\n",
            "```\n",
            "SummarizationStrategy\n",
            "```\n",
            "\n",
            "object now accepts the\n",
            "\n",
            "```\n",
            "specification\n",
            "```\n",
            "\n",
            "which is used by the summarization, rather than being assigned at the preparation workflow stage.\n",
            "\n",
            "‚ö°\n",
            "\n",
            "```\n",
            "addCollectionContents\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "removeCollectionContents\n",
            "```\n",
            "\n",
            "mutations have been deprecated in favor of\n",
            "\n",
            "```\n",
            "addContentsToCollections\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "removeContentsFromCollection\n",
            "```\n",
            "\n",
            "mutations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-1846: Parse Markdown headings into mezzanine JSON\n",
            "\n",
            "GPLA-1779: Not returning SAS token with mezzanine, master URIs\n",
            "\n",
            "GPLA-1348: Summarize text content, not just file content\n",
            "\n",
            "GPLA-1297: Not assigning content error message on preparation workflow failure\n",
            "\n",
            "Last updated 8 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [96198d70-b6f9-4850-9e63-03a623f7f783]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/96198d70-b6f9-4850-9e63-03a623f7f783/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports [Semantic Alerts](https://docs.graphlit.dev/api-reference/graphql-data-model/alerts), which allows for LLM summarization and publishing of content, on a periodic basis.  This is useful for generating daily reports from email, Slack or other time-based feeds.  Alerts support the same publishing options, i.e. audio and text, as the\n",
            "\n",
            "```\n",
            "publishContents\n",
            "```\n",
            "\n",
            "mutation.\n",
            "\n",
            "üí° Graphlit now supports the latest OpenAI 0125 model versions, for GPT-4 and GPT-3.5 Turbo.  We will add support for Azure OpenAI when Microsoft releases support for these.\n",
            "\n",
            "Slack feeds now support a listing\n",
            "\n",
            "```\n",
            "type\n",
            "```\n",
            "\n",
            "field, where you can specify if you want\n",
            "\n",
            "```\n",
            "PAST\n",
            "```\n",
            "\n",
            "or\n",
            "\n",
            "```\n",
            "NEW\n",
            "```\n",
            "\n",
            "Slack messages in the feed.\n",
            "\n",
            "üî• This release provides many performance enhancements, which will speed up the content workflows for ingested content.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2114: Collections not being added to text embedding index documents.\n",
            "\n",
            "GPLA-2063: Not handling hallucinated citations.\n",
            "\n",
            "GPLA-1916: Collections not inherited from project-scope into tenant-scope.\n",
            "\n",
            "GPLA-2105: Should error on add/remove of contents to/from collections if content does not exist.\n",
            "\n",
            "Last updated 10 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [0ac76649-bfac-4347-bc0f-8aa20e6844d1]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/0ac76649-bfac-4347-bc0f-8aa20e6844d1/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# December 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports formatting of LLM-ready prompts with our RAG pipeline, via the new\n",
            "\n",
            "```\n",
            "formatConversation\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "completeConversation\n",
            "```\n",
            "\n",
            "mutations.  This is valuable for supporting LLM streaming by directly calling the LLM from your application, and using Graphlit for RAG retrieval and conversation history. ([Colab Notebook Example](https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_12_01_OpenAI_LLM_Streaming.ipynb))\n",
            "\n",
            "We have added support for inline hyperlinks in extracted text from documents and web pages.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3466: Owner ID should accept any non-whitespace string\n",
            "\n",
            "GPLA-3458: Not getting Person-to-Organization edges from entity extraction\n",
            "\n",
            "Last updated 25 days ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [2dce4142-7fb1-4560-a70c-7ca422a6cb81]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/2dce4142-7fb1-4560-a70c-7ca422a6cb81/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the [OpenAI GPT-4 Turbo 128k](https://openai.com/research/gpt-4v-system-card) model, both in Azure OpenAI and native OpenAI services.  Added new model enum\n",
            "\n",
            "```\n",
            "GPT4_TURBO_VISION_128K\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "üí° Graphlit now supports Llama 2 7b, 13b, 70b models and Mistral 7b model, via [Replicate](https://replicate.com/).  Developers can use their own Replicate API key, or be charged as credits for Graphlit usage.\n",
            "\n",
            "üí° Graphlit now supports the [Anthropic Claude 2.1](https://www.anthropic.com/index/claude-2-1) model. Added new model enum\n",
            "\n",
            "```\n",
            "CLAUDE_2_1\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "üí° Graphlit now supports the [OpenAI GPT-4 Vision](https://openai.com/research/gpt-4v-system-card) model for image descriptions and text extraction.  Added new model enum\n",
            "\n",
            "```\n",
            "GPT4_TURBO_VISION_128K\n",
            "```\n",
            "\n",
            ". See usage example in [\"Multimodal RAG\" blog post](https://www.graphlit.com/blog/multimodal-rag-insurance-insights).\n",
            "\n",
            "Added query by example to\n",
            "\n",
            "```\n",
            "contents\n",
            "```\n",
            "\n",
            "query.  Developers can specify one or more example contents, and query will use vector embeddings to return similar contents.\n",
            "\n",
            "Added query by example to\n",
            "\n",
            "```\n",
            "conversations\n",
            "```\n",
            "\n",
            "query.  Developers can specify one or more example conversations, and query will use vector embeddings to return similar conversations.\n",
            "\n",
            "Added vector search support for\n",
            "\n",
            "```\n",
            "conversations\n",
            "```\n",
            "\n",
            "queries.  Developers can provide search text which will use vector embeddings to return similar conversations.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "promptSpecifications\n",
            "```\n",
            "\n",
            "mutation for directly prompting multiple models.  This can be used to evaluate prompts against multiple models or compare different specification parameters in parallel.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "promptStrategy\n",
            "```\n",
            "\n",
            "field to Specification, which supports multiple strategy types for preprocessing the prompt before being sent to the LLM model.  For example,\n",
            "\n",
            "```\n",
            "REWRITE\n",
            "```\n",
            "\n",
            "prompt strategy will ask LLM to rewrite the incoming user prompt based on the previous conversation messages.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "suggestConversation\n",
            "```\n",
            "\n",
            "mutation, which returns a list of suggested followup questions based on the specified conversation and related contents.  This can be used to auto-suggest questions for chatbot users.\n",
            "\n",
            "Added new summarization types:\n",
            "\n",
            "```\n",
            "CHAPTERS\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "QUESTIONS\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "POSTS\n",
            "```\n",
            "\n",
            ".   See usage examples in the [\"LLMs for Podcasters\" blog post](https://www.graphlit.com/blog/llms-for-podcasters).\n",
            "\n",
            "Added versioned model enums such as\n",
            "\n",
            "```\n",
            "GPT4_0613\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "GPT35_TURBO_16K_1106\n",
            "```\n",
            "\n",
            ".  Without version specified, such as\n",
            "\n",
            "```\n",
            "GPT35_TURBO_16K\n",
            "```\n",
            "\n",
            ", Graphlit will use the latest production model version, as defined by the LLM vendor.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "lookupContents\n",
            "```\n",
            "\n",
            "query to get multiple contents by id in one query.\n",
            "\n",
            "‚ö° In Content type,\n",
            "\n",
            "```\n",
            "headline\n",
            "```\n",
            "\n",
            "field was renamed to\n",
            "\n",
            "```\n",
            "headlines\n",
            "```\n",
            "\n",
            "and now returns an array of strings.\n",
            "\n",
            "‚ö° Entity names are now limited to 1024 characters.  Names will be truncated if they exceed the maximum length.\n",
            "\n",
            "‚ö° In SummarizationTypes enum,\n",
            "\n",
            "```\n",
            "BULLET_POINTS\n",
            "```\n",
            "\n",
            "was renamed to\n",
            "\n",
            "```\n",
            "BULLETS\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "‚ö° In ProjectStorage type,\n",
            "\n",
            "```\n",
            "originalTotalSize\n",
            "```\n",
            "\n",
            "was renamed to\n",
            "\n",
            "```\n",
            "totalSize\n",
            "```\n",
            "\n",
            ", and\n",
            "\n",
            "```\n",
            "totalRenditionSize\n",
            "```\n",
            "\n",
            "field was added.\n",
            "\n",
            "```\n",
            "totalSize\n",
            "```\n",
            "\n",
            "is the sum of the ingested source file sizes, and\n",
            "\n",
            "```\n",
            "totalRenditionSize\n",
            "```\n",
            "\n",
            "is the sum of the source file sizes and any derived rendition sizes.\n",
            "\n",
            "‚ö° In ConversationStrategy type,\n",
            "\n",
            "```\n",
            "strategyType\n",
            "```\n",
            "\n",
            "was renamed to\n",
            "\n",
            "```\n",
            "type\n",
            "```\n",
            "\n",
            "for consistency with rest of data model.\n",
            "\n",
            "‚ö° In Specification type,\n",
            "\n",
            "```\n",
            "optimizeSearchConversation\n",
            "```\n",
            "\n",
            "was removed, and now is handled by\n",
            "\n",
            "```\n",
            "OPTIMIZE_SEARCH\n",
            "```\n",
            "\n",
            "prompt strategy.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-1725: Should ignore RSS.xml from web feed sitemap\n",
            "\n",
            "GPLA-1726: GPT-3.5 Turbo 16k LLM is adding \"Citation #\" to response\n",
            "\n",
            "GPLA-1698: Workflow not applied to link-crawled content\n",
            "\n",
            "GPLA-1692: Mismatched project storage total size, when some content has errored\n",
            "\n",
            "GPLA-1237: Add relevance threshold for semantic search\n",
            "\n",
            "Last updated 11 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [e8b1cbde-39e4-4866-99f9-0092f22d133e]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/e8b1cbde-39e4-4866-99f9-0092f22d133e/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# August 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports LLM-based document preparation, using vision-capable models such as OpenAI GPT-4o and Anthropic Sonnet 3.5.  This is available via the\n",
            "\n",
            "```\n",
            "MODEL_DOCUMENT\n",
            "```\n",
            "\n",
            "preparation service type, and you can assign a custom\n",
            "\n",
            "```\n",
            "specification\n",
            "```\n",
            "\n",
            "object and bring your own LLM keys.\n",
            "\n",
            "üí° Graphlit now provides an open source .NET SDK, supporting .NET 6 and .NET 8 (and above).  SDK package can be found on [Nuget.org](https://www.nuget.org/packages/Graphlit).  Code samples can be found on [GitHub](https://github.com/graphlit/graphlit-samples/tree/main/dotnet).\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "identifier\n",
            "```\n",
            "\n",
            "property to\n",
            "\n",
            "```\n",
            "Content\n",
            "```\n",
            "\n",
            "object for mapping content to external database identifiers.  This is supported for content filtering as well.\n",
            "\n",
            "Added support for Claude 3 vision models for image-based entity extraction, using the\n",
            "\n",
            "```\n",
            "MODEL_IMAGE\n",
            "```\n",
            "\n",
            "entity extraction service.\n",
            "\n",
            "Added context augmentation to conversations, via the\n",
            "\n",
            "```\n",
            "augmentedFilter\n",
            "```\n",
            "\n",
            "property on the\n",
            "\n",
            "```\n",
            "Conversation\n",
            "```\n",
            "\n",
            "object.  Any content which matches this augmented filter will be injected into the LLM prompt content, without needing to be related by vector similarity to the user prompt.  This is useful for specifying domain knowledge which should always be referenced by the RAG pipeline.\n",
            "\n",
            "Added support for the latest snapshot of OpenAI GPT-4o, with the model enum\n",
            "\n",
            "```\n",
            "GPT4O_128K_20240806.\n",
            "```\n",
            "\n",
            "Added reranking of related entities, when preparing the LLM prompt context for GraphRAG.  If reranking is enabled, the metadata from the related entities will be reranked with the same reranker assigned to the conversation specification.\n",
            "\n",
            "‚ö° We have changed the type of the\n",
            "\n",
            "```\n",
            "duration\n",
            "```\n",
            "\n",
            "field in the\n",
            "\n",
            "```\n",
            "AudioMetadata\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "VideoMetadata\n",
            "```\n",
            "\n",
            "types to be\n",
            "\n",
            "```\n",
            "TimeSpan\n",
            "```\n",
            "\n",
            "rather than\n",
            "\n",
            "```\n",
            "string\n",
            "```\n",
            "\n",
            ", as to be more consistent with the rest of the API data model.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2884: Support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "\n",
            "Last updated 4 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [ffe75030-5961-49c6-92b3-621c2606ab9a]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/ffe75030-5961-49c6-92b3-621c2606ab9a/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# August 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports the extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "\n",
            "üí° Graphlit now supports medical-related entities in GraphRAG, and via API for queries and mutations.\n",
            "\n",
            "Added support for [Anthropic prompt caching](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#how-prompt-caching-works). When using Anthropic Sonnet 3.5 or Haiku 3, Anthropic will now cache the entity extraction and LLM document preparation system prompts, which saves on token cost and increases performance.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3104: Should default search type to VECTOR, when performing entity similarity filter.\n",
            "\n",
            "GPLA-3112: Empty PDF fails entity extraction.\n",
            "\n",
            "Last updated 4 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [4fb50b71-ba4a-4b56-8375-d4dac91c8f27]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/4fb50b71-ba4a-4b56-8375-d4dac91c8f27/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Added\n",
            "\n",
            "```\n",
            "ingestText\n",
            "```\n",
            "\n",
            "mutation which supports direct Content ingestion of plain text, Markdown and HTML.  Now, if you have pre-scraped HTML or Markdown text, you can ingest it into Graphlit without reading from a URL.\n",
            "\n",
            "üí° Added Specification\n",
            "\n",
            "```\n",
            "strategy\n",
            "```\n",
            "\n",
            "property, which allows customization of the LLM context when prompting a conversation.\n",
            "\n",
            "```\n",
            "ConversationStrategy\n",
            "```\n",
            "\n",
            "now provides\n",
            "\n",
            "```\n",
            "Windowed\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "Summarized\n",
            "```\n",
            "\n",
            "message histories, as well as configuration of the weight between existing conversation messages and Content text pages (or audio transcript segments) in the LLM context.\n",
            "\n",
            "üí° Added auto-summarization of extracted text and audio transcripts.  There is a new Content\n",
            "\n",
            "```\n",
            "summary\n",
            "```\n",
            "\n",
            "property where a list of summary bullet points can be found.  These summaries can be optionally included in the Conversation prompt context for more accurate LLM responses.\n",
            "\n",
            "‚ÑπÔ∏è Added\n",
            "\n",
            "```\n",
            "AzureOpenAIModels\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "OpenAIModels\n",
            "```\n",
            "\n",
            "types to Specification model properties to make it easier to specify the desired LLM.\n",
            "\n",
            "‚ÑπÔ∏è Renamed ConversationMessage\n",
            "\n",
            "```\n",
            "date\n",
            "```\n",
            "\n",
            "property to\n",
            "\n",
            "```\n",
            "timestamp\n",
            "```\n",
            "\n",
            "‚ú® Refined the internal LLM prompts for providing content as part of Conversation context.  This provides for much clearer and accurate results from the LLM.\n",
            "\n",
            "Last updated 1 year ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [6afee902-48a9-4b2a-b983-c4cf85a5075b]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/6afee902-48a9-4b2a-b983-c4cf85a5075b/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# August 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "\n",
            "### New Features\n",
            "\n",
            "Added support for language-aware summaries when using LLM-based document extraction.  Now the summaries for tables and sections generated by the LLM will follow the language of the source text.\n",
            "\n",
            "Added support for language-aware entity descriptions with using LLM-based entity extraction. Now the entity descriptions generated by the LLM will follow the language of the source text.\n",
            "\n",
            "‚ö° We have changed the default document preparation method to use Azure AI Document Intelligence, rather than our built-in document parsers.  We have found that the fidelity of Azure AI is considerably better for complex PDFs, and provides better support for table extraction, so we have made this the default. Note: this does come with increased credit usage per-page, for PDF, DOCX and PPTX documents, but the quality of the extracted documents are noticeably higher for use in RAG pipelines.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3070: Not getting slide count assigned to metadata for PPTX files.\n",
            "\n",
            "Last updated 4 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f6e0191f-87fb-4600-8571-5e6f2c5999ae]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/f6e0191f-87fb-4600-8571-5e6f2c5999ae/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# August 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "\n",
            "### New Features\n",
            "\n",
            "‚ÑπÔ∏è Behind the scenes, Graphlit is preparing to launch usage-based billing.  This release put in place the infrastructure to track billable events.  Organizations now have a Stripe customer associated with them, and Graphlit projects are auto-subscribed to a Free/Hobby pricing plan.  In a future release, we will provide the ability to upgrade to a paid plan in the Graphlit Developer Portal.  Also, we will provide visualization of usage, on granular basis, in the Portal.\n",
            "\n",
            "üí° Content URIs now have Shared Access Signature (SAS) token appended, so they are accessible after query.  For example, content.transcriptUri will now be able to be downloaded or used directly in an application (until the SAS token expires).\n",
            "\n",
            "üß± Added more robustness for error handling and retries, especially for LLM APIs and audio transcription APIs.\n",
            "\n",
            "Last updated 1 year ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [fe886ac1-9763-4a97-933d-3e7fda6e6091]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/fe886ac1-9763-4a97-933d-3e7fda6e6091/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports Dropbox feeds for ingesting files on the Dropbox cloud service. Dropbox feeds require your\n",
            "\n",
            "```\n",
            "appKey\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "appSecret\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "redirectUri\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "refreshToken\n",
            "```\n",
            "\n",
            "to be assigned. The feed also accepts an optional\n",
            "\n",
            "```\n",
            "path\n",
            "```\n",
            "\n",
            "parameter to read files from a specific Dropbox folder.\n",
            "\n",
            "üí° Graphlit now supports Box feeds for ingesting files on the Box cloud service. Box feeds require your\n",
            "\n",
            "```\n",
            "clientId\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "clientSecret\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "redirectUri\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "refreshToken\n",
            "```\n",
            "\n",
            "to be assigned.\n",
            "\n",
            "üí° Graphlit now supports Intercom feeds for ingesting Intercom Articles and Tickets. We will ingest Intercom Articles as\n",
            "\n",
            "```\n",
            "PAGE\n",
            "```\n",
            "\n",
            "content type, and Tickets as\n",
            "\n",
            "```\n",
            "ISSUE\n",
            "```\n",
            "\n",
            "content type. Intercom feeds require the\n",
            "\n",
            "```\n",
            "accessToken\n",
            "```\n",
            "\n",
            "property to be assigned.\n",
            "\n",
            "üí° Graphlit now supports Zendesk feeds for ingesting Zendesk Articles and Tickets.  We will ingest Zendesk Articles as\n",
            "\n",
            "```\n",
            "PAGE\n",
            "```\n",
            "\n",
            "content type, and Tickets as\n",
            "\n",
            "```\n",
            "ISSUE\n",
            "```\n",
            "\n",
            "content type. Zendesk feeds require the\n",
            "\n",
            "```\n",
            "accessToken\n",
            "```\n",
            "\n",
            "property and your Zendesk subdomain to be assigned.\n",
            "\n",
            "Graphlit now supports the latest OpenAI o1 model, with the model enums\n",
            "\n",
            "```\n",
            "O1_200k\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "O1_200k_20241217\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "Graphlit now supports the latest Gemini Flash 2.0 Experimental model, with the model enum\n",
            "\n",
            "```\n",
            "GEMINI_2_0_FLASH_EXPERIMENTAL\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "Graphlit now supports the latest Cohere R7B model, with the model enum\n",
            "\n",
            "```\n",
            "COMMAND_R7B_202412\n",
            "```\n",
            "\n",
            ".\n",
            "\n",
            "Graphlit now supports returning the low-level details from prompting RAG conversations, by adding the\n",
            "\n",
            "```\n",
            "includeDetails\n",
            "```\n",
            "\n",
            "parameter and setting to True. This includes details on the number of sources, the exact list of messages provided to the LLM, and more.\n",
            "\n",
            "We have added support for filtering of observables, such as Person or Organization, by URI property.\n",
            "\n",
            "We have added the ability to bypass semantic search in content retrieval with conversations. You can assign\n",
            "\n",
            "```\n",
            "NONE\n",
            "```\n",
            "\n",
            "for the conversation search type, and it will ignore the user prompt when retrieving content.  It will inject all contents resulting from the content filter into the RAG prompt context.\n",
            "\n",
            "We have added a new\n",
            "\n",
            "```\n",
            "createdInLast\n",
            "```\n",
            "\n",
            "property to all entity filters, which allows easier filtering of entities created within a recent time period. Also, we have added a new\n",
            "\n",
            "```\n",
            "inLast\n",
            "```\n",
            "\n",
            "property to the content filter, which allows easier filtering of content authored within a recent time period. For example, find all images taken in the last 3 days, or find me all emails I received yesterday.\n",
            "\n",
            "We have added support for the latest Azure AI Document Intelligence models, with enums\n",
            "\n",
            "```\n",
            "US_PAY_STUB\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "US_BANK_STATEMENT\n",
            "```\n",
            "\n",
            ", and\n",
            "\n",
            "```\n",
            "US_BANK_CHECK.\n",
            "```\n",
            "\n",
            "We have added support for Google Drive and OneDrive feeds to ingest specific files by providing a list of file identifiers (\n",
            "\n",
            "```\n",
            "files\n",
            "```\n",
            "\n",
            "), in addition to the folder identifier (\n",
            "\n",
            "```\n",
            "folderId\n",
            "```\n",
            "\n",
            ").  If files identifiers are provided, they take precedence over the folder identifier.\n",
            "\n",
            "‚ö° For projects upgraded to the Starter Tier after Dec 9, 2024, we have removed the content items limit. Now you can store an unlimited number of content items (i.e. files, web pages, Slack messages) on the Starter or Growth Tiers.  If you have an existing project on the Starter Tier, please reach out and we will manually remove that content item limit on the project.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-3529: Can't assign collection to multitenant content\n",
            "\n",
            "GPLA-3579: Should decode HTML characters when parsing HTML email\n",
            "\n",
            "GPLA-3576: Ingesting content in-place doesn't handle isSynchronous properly\n",
            "\n",
            "GPLA-3457: IsFeedDone doesn't return True for finished feed with no contents\n",
            "\n",
            "GPLA-3572: Not handling HTTP 400 error on uploading from URI\n",
            "\n",
            "Last updated 2 days ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [8f77bad2-831d-4880-8d8d-75e359fa3440]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/8f77bad2-831d-4880-8d8d-75e359fa3440/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports Discord feeds.  By connecting to a Discord channel and providing a bot token, you can ingest all Discord messages and file attachments.\n",
            "\n",
            "üí°  Graphlit now supports [Cohere reranking](https://cohere.com/rerank) after content retrieval in RAG pipeline.  You can optionally use the Cohere rerank model to semantically rerank the semantic search results, before providing as context to the LLM.\n",
            "\n",
            "Added support for section-aware text chunking and retrieval.  Now, when using section-aware document preparation, such as Azure AI Document Intelligence, Graphlit will store the extracted text according to the semantic chunks (i.e. sections).  The text for each section will be individually chunked and embedded into the vector index.\n",
            "\n",
            "Added support for\n",
            "\n",
            "```\n",
            "retrievalStrategy\n",
            "```\n",
            "\n",
            "in Specification type. Graphlit now supports\n",
            "\n",
            "```\n",
            "CHUNK\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "SECTION\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "CONTENT\n",
            "```\n",
            "\n",
            "retrieval strategies.  Chunk retrieval will use the search hit chunk, section retrieval will expand the search hit chunk to the containing section (or page, if not using section-aware preparation).  Content retrieval will expand the search hit chunk to the text of the entire document.\n",
            "\n",
            "Added support for\n",
            "\n",
            "```\n",
            "rerankingStrategy\n",
            "```\n",
            "\n",
            "in Specification type. You can now configure the reranking of content sources, using the Cohere reranking model, by assigning\n",
            "\n",
            "```\n",
            "serviceType\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "COHERE\n",
            "```\n",
            "\n",
            ".  More reranking models are planned for the future.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "isSynchronous\n",
            "```\n",
            "\n",
            "flag to content ingestion mutations, such as\n",
            "\n",
            "```\n",
            "ingestUri\n",
            "```\n",
            "\n",
            ", so the mutation will wait for the content to complete the ingestion workflow (or error) before returning.  This is useful for utilizing the API in a Jupyter notebook or Streamlit application, in a synchronous manner without polling.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "includeAttachments\n",
            "```\n",
            "\n",
            "flag to SlackFeedProperties.  When enabled, Graphlit will automatically ingest any attachments within Slack messages.\n",
            "\n",
            "‚ö° Added\n",
            "\n",
            "```\n",
            "ingestUri\n",
            "```\n",
            "\n",
            "mutation to replace the now deprecated\n",
            "\n",
            "```\n",
            "ingestPage\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "ingestFile\n",
            "```\n",
            "\n",
            "mutations.  We had seen confusion on when to use one vs the other, and now for any URI, whether it is a web page or hosted PDF, you can pass it to\n",
            "\n",
            "```\n",
            "ingestUri\n",
            "```\n",
            "\n",
            ", and we will infer the correct content ingestion workflow.\n",
            "\n",
            "‚ö° Removed\n",
            "\n",
            "```\n",
            "includeSummaries\n",
            "```\n",
            "\n",
            "from the ConversationStrategyInput type.  This will re-added in the future as part of the retrieval strategy.\n",
            "\n",
            "‚ö° Deprecated\n",
            "\n",
            "```\n",
            "enableExpandedRetrieval\n",
            "```\n",
            "\n",
            "in ConversationStrategyInput type.  This is now handled by setting\n",
            "\n",
            "```\n",
            "strategyType\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "SECTION\n",
            "```\n",
            "\n",
            "or\n",
            "\n",
            "```\n",
            "CONTENT\n",
            "```\n",
            "\n",
            "in the RetrievalStrategyInput type.\n",
            "\n",
            "‚ö° Moved\n",
            "\n",
            "```\n",
            "contentLimit\n",
            "```\n",
            "\n",
            "from ConversationStrategyInput type to RetrievalStrategyInput type. You can optionally assign the\n",
            "\n",
            "```\n",
            "contentLimit\n",
            "```\n",
            "\n",
            "to\n",
            "\n",
            "```\n",
            "retrievalStrategy\n",
            "```\n",
            "\n",
            "which limits the number of content sources leveraged in the LLM prompt context. (Default is 100.)\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2469: Failed to ingest PDF hosted on GitHub\n",
            "\n",
            "GPLA-2390: Claude 3 Haiku not adhering to JSON schema\n",
            "\n",
            "GPLA-2474: Prompt rewriting should ignore formatting instructions in prompt\n",
            "\n",
            "GPLA-2462: Missing line break after table rows\n",
            "\n",
            "GPLA-2417: Not extracting images from PPTX correctly\n",
            "\n",
            "Last updated 8 months ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [7bd1e01f-0197-4ad0-be04-f80da7238dce]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/7bd1e01f-0197-4ad0-be04-f80da7238dce/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# August 3: New data model for Observations, new Category entity\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Revised data model for Observations, Occurrences and observables (i.e. Person, Organization).  Now after entity extraction, content will have one Observation for each observed entity, and a list of occurrences.  Occurrence now supports text, time and image occurrence types.  (Text: page index, time: start/end timestamp, image: bounding box)  Observations now have ObservableType and Observable fields, which specify the observed entity type and entity reference.\n",
            "\n",
            "üí° Added Category entity to GraphQL data model, which supports [PII](https://en.wikipedia.org/wiki/Personal_data) categories such as Phone Number or Credit Card Number.\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "probability\n",
            "```\n",
            "\n",
            "field to model properties, for the LLM's token probability.  (See [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-top_p) for more detail.)\n",
            "\n",
            "Added\n",
            "\n",
            "```\n",
            "error\n",
            "```\n",
            "\n",
            "field to feeds.  If a feed fails to read from the data source, and is marked as\n",
            "\n",
            "```\n",
            "ERRORED\n",
            "```\n",
            "\n",
            "state, the\n",
            "\n",
            "```\n",
            "error\n",
            "```\n",
            "\n",
            "field will have the error description.\n",
            "\n",
            "Support reingestion of changed files from feeds.  For feeds, such as SharePoint or Web, where we can recognize that a file or page was updated, we will now reingest the content in-place.  Content will keep the same ID, and will restart the content workflow by re-downloading the updated content from the data source.   Existing observations will be deleted, and new observations will be created from the updated content.\n",
            "\n",
            "‚ÑπÔ∏è Ingestion of content is now idempotent, meaning if you ingest content again from the same URI, we will reingest the content in-place, while keeping the same ID.  (If we can recognize the content has not changed, such as by ETag, we will return the existing content object.)\n",
            "\n",
            "‚ÑπÔ∏è Changed GraphQL data type of SharePoint\n",
            "\n",
            "```\n",
            "tenantId\n",
            "```\n",
            "\n",
            ",\n",
            "\n",
            "```\n",
            "libraryId\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "siteId\n",
            "```\n",
            "\n",
            "to ID rather than String.\n",
            "\n",
            "‚ú® Performance optimization of entity extraction, and the creation of observations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-1130: Only was extracting text from first column of PDF tables.\n",
            "\n",
            "GPLA-1140: Text from DOCX tables was not extracted properly.\n",
            "\n",
            "GPLA-1154: Audio content ingested from RSS feed was not deleted when feed was deleted.\n",
            "\n",
            "Last updated 1 year ago\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [6abaac7e-030f-4003-9cb1-cc0781bf4a95]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/6abaac7e-030f-4003-9cb1-cc0781bf4a95/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\n",
            "# April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "\n",
            "### New Features\n",
            "\n",
            "üí° Graphlit now supports a native Python SDK, using Pydantic types. The Python SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest PyPi package [here](https://pypi.org/project/graphlit-client/).  The Streamlit [sample applications](https://github.com/graphlit/graphlit-samples/tree/main/python) have been updated to use the new Python SDK.\n",
            "\n",
            "üí° Graphlit now supports a native Node.js SDK, using TypeScript types. The Node.js SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest NPM package [here](https://www.npmjs.com/package/graphlit-client).\n",
            "\n",
            "üí° Graphlit now supports the 2024-04-09 models in the OpenAI model service.\n",
            "\n",
            "```\n",
            "GPT4_TURBO-128K\n",
            "```\n",
            "\n",
            "will give the latest OpenAI GPT-4 model, following this model [list](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4).  We have added the\n",
            "\n",
            "```\n",
            "GPT4_TURBO_128K_2024_04_09\n",
            "```\n",
            "\n",
            "enum to specify the new model.\n",
            "\n",
            "üí° Graphlit now supports [LLaMA3 70b, LLaMA3 8b and Gemma 7b models](https://console.groq.com/docs/models) in the Groq model service.\n",
            "\n",
            "üí° Graphlit now supports the [Command R and Command-R+](https://cohere.com/command) models in the Cohere model service.\n",
            "\n",
            "Added support for [Jina reranking](https://jina.ai/reranker/), using the\n",
            "\n",
            "```\n",
            "JINA\n",
            "```\n",
            "\n",
            "reranking model service type in the reranking retrieval strategy.\n",
            "\n",
            "Updated the Cohere reranking model to use the latest [v3.0](https://docs.cohere.com/reference/rerank) model.\n",
            "\n",
            "Increased the reliability of parsing LLM responses, in cases where they don't follow the JSON schema.\n",
            "\n",
            "‚ö° Cleaned up nullability of GraphQL parameters, so parameters better reflect if they are required or optional, or allow nulls.\n",
            "\n",
            "‚ö° Added missing\n",
            "\n",
            "```\n",
            "deleteWorkflows\n",
            "```\n",
            "\n",
            "and\n",
            "\n",
            "```\n",
            "deleteAllCollections\n",
            "```\n",
            "\n",
            "mutations.\n",
            "\n",
            "‚ö° Split out reranking model service type as\n",
            "\n",
            "```\n",
            "RetrievalModelServiceTypes\n",
            "```\n",
            "\n",
            "enum.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "GPLA-2114: Adding content to collections not syncing search index\n",
            "\n",
            "GPLA-2511: Failing to render any conversation sources with section retrieval and text content\n",
            "\n",
            "Last updated 8 months ago\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the ElevenLabs voice ID to use\n",
        "voice_id = \"ZF6FPAbjXT4488VcRRnw\" # ElevenLabs Amelia voice\n",
        "\n",
        "# Prompt which gets run on each web page to summarize key points\n",
        "summary_prompt = \"\"\"\n",
        "You are an AI assistant that extracts the most important information from product changelog pages.\n",
        "\n",
        "You are being provided a changelog web page for one of many releases of the Graphlit Platform in 2024.\n",
        "\n",
        "Your task is to produce a concise summary that covers:\n",
        "\n",
        "New Features ‚Äì Briefly list or describe each new capability.\n",
        "Enhancements/Improvements ‚Äì Any notable improvements or changes.\n",
        "Bug Fixes ‚Äì Summaries of what was fixed and why it matters.\n",
        "Other Key Details ‚Äì Any version numbers, feature flags, or breaking changes.\n",
        "Dates - When a feature was released\n",
        "Value - What this offers to developers.\n",
        "Keep it succinct, accurate, and organized. Use short sentences or bullet points so it‚Äôs easy to incorporate into a map/reduce pipeline. Omit any superfluous text.\n",
        "\n",
        "Output:\n",
        "A concise summary in bullet points highlighting the essential updates from the changelog.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt which gets run against all summaries (in map/reduce manner) to generate final script for ElevenLabs audio\n",
        "publish_prompt = \"\"\"\n",
        "You are an enthusiastic host focused on developer marketing, and you work for Graphlit who is creating a 2024 year-in-review of their API-based platform.\n",
        "\n",
        "Don't refer to yourself in the script. Just talk to the audience.\n",
        "\n",
        "Don't add in any podcast-like references like intro music, sound effects, etc.  This will be used with a text-to-speech API to generate an audio recording.\n",
        "\n",
        "Your audience is somewhat technical ‚Äî software engineers, product builders, and tech-savvy product managers ‚Äî so the script should be clear, concise, and sprinkled with a bit of technical depth.\n",
        "\n",
        "Using the provided changelog for the Graphlit Platform, create a podcast-like script that:\n",
        "\n",
        "- Sets the stage with a warm, engaging introduction.\n",
        "- Highlights each new feature, explaining how it helps developers or teams be more productive, efficient, or creative.\n",
        "- Refers to when a feature was released.\n",
        "- Mentions any model updates and why they matter for technical use cases.\n",
        "- Reviews notable bug fixes, providing just enough context to show the improvements without overwhelming detail.\n",
        "- Closes with a quick recap and a call to action, encouraging listeners to try out the new features or learn more.\n",
        "\n",
        "At the very end, mention that the listener can signup for free at graphlit.com and try out all these features.\n",
        "Also, mention that in 2025, Graphlit will be offering exciting new features to accelerate the building of AI agents.\n",
        "\n",
        "The tone should be friendly, positive, and confident‚Äîlike a technology evangelist who‚Äôs genuinely excited about these updates.\n",
        "\n",
        "Keep it interesting and conversational, but maintain enough depth to engage developers who care about how things work under the hood.\n",
        "Use analogies or practical examples to illustrate why certain features are useful.\n",
        "Feel free to add transitions such as ‚ÄúNow, let‚Äôs dive in,‚Äù or ‚ÄúMoving on to our next highlight‚Äù to keep it flowing.\n",
        "\n",
        "Output: A detailed, TTS-ready 10-minute long script that hits all the points above.\n",
        "\"\"\"\n",
        "\n",
        "if feed_id is not None:\n",
        "    summary_specification_id = await create_specification(enums.OpenAIModels.GPT4O_MINI_128K)\n",
        "\n",
        "    if summary_specification_id is not None:\n",
        "        print(f'Created summary specification [{summary_specification_id}]:')\n",
        "\n",
        "        publish_specification_id = await create_specification(enums.OpenAIModels.O1_200K)\n",
        "\n",
        "        if publish_specification_id is not None:\n",
        "            print(f'Created publish specification [{publish_specification_id}]:')\n",
        "\n",
        "            display(Markdown(f'### Publishing Contents...'))\n",
        "\n",
        "            published_content_id = await publish_contents(feed_id, summary_specification_id, publish_specification_id, summary_prompt, publish_prompt, voice_id)\n",
        "\n",
        "            if published_content_id is not None:\n",
        "                print(f'Completed publishing content [{published_content_id}].')\n",
        "\n",
        "                # Need to reload content to get presigned URL to MP3\n",
        "                published_content = await get_content(published_content_id)\n",
        "\n",
        "                if published_content is not None:\n",
        "                    display(Markdown(f'### Published [{published_content.name}]({published_content.audio_uri})'))\n",
        "\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <audio controls>\n",
        "                    <source src=\"{published_content.audio_uri}\" type=\"audio/mp3\">\n",
        "                    Your browser does not support the audio element.\n",
        "                    </audio>\n",
        "                    \"\"\"))\n",
        "\n",
        "                    # After the audio is generated, we ingest the MP3 as a new content object in Graphlit, and it gets auto-transcribed\n",
        "                    display(Markdown('### Transcript'))\n",
        "                    display(Markdown(published_content.markdown))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ieBzAp6Z2Zew",
        "outputId": "00a4edc1-9090-42ce-af6f-d8453a2613a3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created summary specification [2ca45933-f4e1-44f7-b8d6-d09d8039b1d7]:\n",
            "Created publish specification [dd92d97a-dd59-41e8-bdc3-0042d59268d3]:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Publishing Contents..."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed publishing content [d846d398-4b86-45cb-aeed-306f52938c0c].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Published [Published Summary.mp3](https://graphlit20241212dc396403.blob.core.windows.net/files/d846d398-4b86-45cb-aeed-306f52938c0c/Mezzanine/Published%20Summary.mp3?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                    <audio controls>\n",
              "                    <source src=\"https://graphlit20241212dc396403.blob.core.windows.net/files/d846d398-4b86-45cb-aeed-306f52938c0c/Mezzanine/Published%20Summary.mp3?sv=2025-01-05&se=2024-12-28T02%3A44%3A58Z&sr=c&sp=rl&sig=zMFSWzdzpQAa7x9c4vDSUKrflfooVcDYtciGdL2e%2Bsg%3D\" type=\"audio/mp3\">\n",
              "                    Your browser does not support the audio element.\n",
              "                    </audio>\n",
              "                    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Transcript"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[00:00:00] Hello, everyone, and welcome to this 2024\n\n[00:00:03] year in review for the Graphlet API based platform.\n\n[00:00:07] There has been incredible momentum all year with new SDKs,\n\n[00:00:12] model integrations,\n\n[00:00:14] and workflow improvements\n\n[00:00:15] designed to help you build smarter solutions faster.\n\n[00:00:19] Whether you're an engineer,\n\n[00:00:20] a product builder, or a technical\n\n[00:00:23] product manager,\n\n[00:00:25] these highlights should give you a clear picture of what's now possible with Graphlet.\n\n[00:00:30] Let's kick things off with January.\n\n[00:00:33] In mid January, on 18th,\n\n[00:00:36] Graphlet introduced brand new content publishing features.\n\n[00:00:40] These let you publish documents, audio transcripts,\n\n[00:00:43] and image descriptions straight from the platform.\n\n[00:00:46] And bulk summarization\n\n[00:00:48] meant you could quickly get top level takeaways across multiple files.\n\n[00:00:53] There was also new LLM entity extraction.\n\n[00:00:56] So if you needed to automatically identify organizations,\n\n[00:01:00] products, or other key data points, it became easier than ever.\n\n[00:01:05] A few days later, on January 22nd,\n\n[00:01:08] Graphlet added support for Google and Microsoft email feeds.\n\n[00:01:12] So you could ingest both past and new emails,\n\n[00:01:16] including attachments.\n\n[00:01:18] In place reingestion was also introduced,\n\n[00:01:22] letting developers reprocess content without having to create entirely new entries.\n\n[00:01:27] This is especially helpful when you realize there's more data you want to pull from an existing file.\n\n[00:01:33] Important bug fixes around link extraction in HTML\n\n[00:01:37] and shape text in PowerPoint were also tackled.\n\n[00:01:40] Then\n\n[00:01:41] in February, on 2nd, Graphlet rolled out semantic alerts,\n\n[00:01:45] which let you generate daily or routine reports on content changes,\n\n[00:01:50] new conversation summaries, or whatever else you wanted to keep an eye on.\n\n[00:01:54] That same release brought performance enhancements,\n\n[00:01:58] speeding up workflows for ingested content.\n\n[00:02:01] On February 21st, Graphlet expanded feed support further by adding OneDrive and Google Drive options,\n\n[00:02:07] including shared drives, which is a big deal for teams working across multiple cloud services.\n\n[00:02:13] Automatic extraction of embedded images from PDFs landed in the same update,\n\n[00:02:19] enabling richer content analysis for those who rely on PDF based assets.\n\n[00:02:24] A few bug fixes addressed PDF passing errors and conversation history issues,\n\n[00:02:29] ensuring more reliable experiences.\n\n[00:02:33] March was particularly active.\n\n[00:02:35] On March 10th, Graphlet debuts a command line interface,\n\n[00:02:39] letting developers quickly interact with the data API\n\n[00:02:42] from any terminal.\n\n[00:02:44] Claude 3, Mistral, and Grock models were added,\n\n[00:02:47] reflecting Graphlet's commitment to supporting multiple model providers.\n\n[00:02:52] The release also brought usage\n\n[00:02:54] and credits telemetry\n\n[00:02:56] so you could track how many tokens you're using or see which projects were using the most resources.\n\n[00:03:02] Then on March 13th, Graphlet announced support for Claude 3 haiku,\n\n[00:03:07] plus direct ingestion of base64\n\n[00:03:10] encoded files.\n\n[00:03:12] Right after that, on March 23rd,\n\n[00:03:15] came ingestion for popular issue tracking services.\n\n[00:03:19] Developers using linear\n\n[00:03:21] GitHub issues\n\n[00:03:22] or Jira could bring in tickets and sync them as searchable content.\n\n[00:03:28] You could also\n\n[00:03:29] ingest files using a web feed site map,\n\n[00:03:32] making big one shot ingestion tasks simpler.\n\n[00:03:36] Moving along to April, on April 7th, Graphlet added support for Discord feeds.\n\n[00:03:42] This meant you could pull in messages and attachments from Discord channels as easily as you might from Slack or other chat apps.\n\n[00:03:50] Cohere re ranking was also introduced,\n\n[00:03:53] allowing you to refine semantic search results in your retrieval augmented generation pipeline\n\n[00:03:58] for more accurate responses.\n\n[00:04:01] Section aware text chunking was a highlight too,\n\n[00:04:04] helping you create more meaningful chunks for better LLM interactions.\n\n[00:04:08] Then on April 23rd,\n\n[00:04:10] major developer focused updates arrived.\n\n[00:04:13] Native Python and Node. Js SDKs\n\n[00:04:17] with typed classes,\n\n[00:04:19] plus new OpenAI,\n\n[00:04:21] Cohere,\n\n[00:04:21] and Grok model support.\n\n[00:04:24] The GPT 4 surge\n\n[00:04:26] turbo\n\n[00:04:27] 128\n\n[00:04:28] k variant arrived here, offering a big context window for large queries and responses. Bug fixes in that release brand\n\n[00:04:33] new\n\n[00:04:35] features\n\n[00:04:37] included\n\n[00:04:38] Gina\n\n[00:04:40] and\n\n[00:04:42] Pongo\n\n[00:04:44] re rankers.\n\n[00:04:45] Brand new features included Gina and Pongo re rankers,\n\n[00:04:49] plus a Microsoft Teams feed for reading messages\n\n[00:04:52] from Teams channels.\n\n[00:04:54] Graphlet also improved error management\n\n[00:04:57] by handling URI validation failures more gracefully.\n\n[00:05:01] The May 15th release introduced graphRAG,\n\n[00:05:04] letting you pull extracted entities from your knowledge graph for use in LLM conversations.\n\n[00:05:10] This made conversation smarter\n\n[00:05:13] since the system could dynamically revise the AI's answer using structured data.\n\n[00:05:18] That same release offered open AI GPT 4 o for rag conversations\n\n[00:05:24] with better performance than earlier defaults.\n\n[00:05:28] June was a big month for advanced model support.\n\n[00:05:31] On June 9th, Graphlet announced integration\n\n[00:05:34] with deep seek l l m's and automatic JSON LD passing from web pages,\n\n[00:05:40] a handy feature if you're analyzing structured data embedded in HTML.\n\n[00:05:45] This release also improved knowledge,\n\n[00:05:48] right, graph performance\n\n[00:05:50] by parallelizing retrieval tasks.\n\n[00:05:53] Then on June 21st, developers gained access to the anthropic Claude 3.5\n\n[00:05:58] SONNET model and knowledge graph semantic search for entities like person or organization.\n\n[00:06:05] This meant you could do more advanced queries and store richer metadata for accurate entity searching.\n\n[00:06:11] July was jam packed.\n\n[00:06:13] Kicking off on July 4th, Graphlet introduced webhook alerts\n\n[00:06:17] so you could receive HTTP\n\n[00:06:19] post notifications\n\n[00:06:21] anytime certain conditions were met,\n\n[00:06:24] like a newly ingested file\n\n[00:06:26] or a specific entity type appearing.\n\n[00:06:29] DeepSeek's 128 k token context window was also unveiled,\n\n[00:06:34] letting you feed extra large contexts into queries.\n\n[00:06:39] On July 15th,\n\n[00:06:41] SharePoint feed support arrived,\n\n[00:06:44] plus conversation features like time stamps and more flexible search configurations.\n\n[00:06:49] Then on July 19th, we got support for OpenAI's\n\n[00:06:53] GPT 4 o Mini,\n\n[00:06:55] a more lightweight variant with 16 k output tokens.\n\n[00:07:00] That release included a bring your own key feature for Azure AI document intelligence\n\n[00:07:05] so you could manage your own billing.\n\n[00:07:08] On July 25th,\n\n[00:07:10] Graphlet added Mistral Large 2 and Nemo,\n\n[00:07:13] as well as expanded llama 3.1\n\n[00:07:16] models for Grok,\n\n[00:07:17] a great boost for developers in need of large scale language capabilities.\n\n[00:07:23] And on July 28th, an indexing workflow stage was introduced for advanced language detection.\n\n[00:07:29] This made it possible to detect text languages\n\n[00:07:32] automatically\n\n[00:07:33] and tag content accordingly.\n\n[00:07:36] In August, on 3rd, a new data model for observations\n\n[00:07:40] provided more structured ways to store and link text occurrences,\n\n[00:07:45] images,\n\n[00:07:45] or time stamps to your content.\n\n[00:07:48] On August 8th, Graflett released LLM based document preparation,\n\n[00:07:53] an open source dot net SDK,\n\n[00:07:56] and support for Claude 3 Vision Models for image based entity analysis.\n\n[00:08:02] By August 11th, Azure AI Document Intelligence had become the default for document preparation,\n\n[00:08:08] improving table extraction in PDFs,\n\n[00:08:11] Word docs, and more, albeit at a slightly higher credit usage.\n\n[00:08:16] Then on August 20th, Graphlet tackled medical centric use cases.\n\n[00:08:21] Medical related entities like drug names, guidelines,\n\n[00:08:24] or conditions were enriched\n\n[00:08:26] so teams building health care apps or research tools could map content more precisely.\n\n[00:08:32] September opened with a release on the first,\n\n[00:08:35] introducing far enrichment for medical data\n\n[00:08:38] and new Cohere model updates.\n\n[00:08:41] On September 3rd, Graphlet launched web search feeds with Tivili\n\n[00:08:46] or Exa dotai,\n\n[00:08:47] letting you ingest public web data for further analysis.\n\n[00:08:51] September 20th was quite significant as well, marking the official introduction of paid subscriptions,\n\n[00:08:58] hobby, starter, and growth.\n\n[00:09:01] That provided clearer usage limits for different project sizes.\n\n[00:09:06] Users also got better semantic search with improved relevance.\n\n[00:09:11] A few days later, on September 26th, Graphlet announced model expansions for Google AI, Sarah Brass, and the newest Grok Llama 3.2 preview sets, plus updates to help custom instructions flow more smoothly into extraction prompts.\n\n[00:09:27] Finally, on September 30th, Azure AI model inference was introduced,\n\n[00:09:31] meaning you could host models like MetaLama\n\n[00:09:35] 3.2\n\n[00:09:35] on a serverless Azure environment for easier scaling.\n\n[00:09:39] Then came October.\n\n[00:09:41] On October 3rd, Graphlet extended tool calling to multiple model providers,\n\n[00:09:46] including OpenAI,\n\n[00:09:48] Mistral, DeepSeek, Grok, and Cerebras.\n\n[00:09:52] The new ingest batch mutation\n\n[00:09:54] let you send in arrays of URIs\n\n[00:09:57] asynchronously,\n\n[00:09:58] ideal for large scale ingestion projects.\n\n[00:10:02] On October 7th, the platform added official support for Anthropic and Gemini tool calling,\n\n[00:10:07] giving you more ways to connect AI logic with external data or services.\n\n[00:10:13] October 9th introduced GitHub repository feeds so you could ingest code files automatically,\n\n[00:10:20] An excellent feature for developer documentation or release notes.\n\n[00:10:24] Later in the month, on October 21st, Graphlet began supporting a wide range of embedding models from OpenAI,\n\n[00:10:31] Cohere, Gina, Mistral, Voyage, and Google AI.\n\n[00:10:36] On October 22nd,\n\n[00:10:37] the Anthropic Sonnet 3 0.5 model arrived,\n\n[00:10:41] plus Cohere image embeddings,\n\n[00:10:43] letting you handle visual content more efficiently.\n\n[00:10:46] And on October 30, well, Graphlet brought in simulated tool calling for certain LLMs,\n\n[00:10:51] offering a more flexible approach to orchestrating calls without leaving the platform.\n\n[00:10:56] Moving on to November, on November 4th, Graphlet released the anthropic Claude 3.5 haiku model,\n\n[00:11:03] plus a new feature that automatically\n\n[00:11:05] disables feeds after the free tier quota is reached.\n\n[00:11:09] Then on November 10th, multi turn content summarization was unveiled,\n\n[00:11:14] and a web search feature allowed you to search the Internet for additional context.\n\n[00:11:19] November 16th specifically added multi turn text summarization.\n\n[00:11:24] And November 24th introduced multi turn image analysis,\n\n[00:11:29] letting you revise images or image descriptions across steps in a conversation.\n\n[00:11:34] These capabilities\n\n[00:11:36] let teams handle dynamic visual data\n\n[00:11:39] in ways that go beyond a single prompt and response flow.\n\n[00:11:43] Finally, December capped off the year with 3 notable updates.\n\n[00:11:47] On December 1st, Graphlet added support for retrieval only rag workflows.\n\n[00:11:52] This helps those who want to format LLM prompts and retrieve relevant data\n\n[00:11:57] without having the model generate final answers.\n\n[00:12:00] Then on December 9th, the platform introduced website mapping,\n\n[00:12:05] screenshot generation,\n\n[00:12:07] and summarization\n\n[00:12:08] or text extraction in one go,\n\n[00:12:11] plus a new Grop Llama model.\n\n[00:12:14] And wrapping things up on December 22nd,\n\n[00:12:17] Graphlet announced new feeds for Dropbox,\n\n[00:12:20] Box, Intercom, and Zendesk.\n\n[00:12:23] They also integrated the latest OpenAI\n\n[00:12:26] o 1 and Gemini Flash 2 point o experimental models.\n\n[00:12:31] This last release of the year included numerous smaller improvements,\n\n[00:12:35] like flexible filtering on observables\n\n[00:12:38] and bug\n\n[00:12:39] fixes that made ingestion from various cloud and web sources even more robust.\n\n[00:12:45] Throughout 24, bug fixes addressed everything from misconfigured feeds to partial PDF extractions.\n\n[00:12:52] The team worked hard to ensure conversations remain stable,\n\n[00:12:56] especially\n\n[00:12:57] with large or unusual datasets.\n\n[00:13:00] Model updates also kept pace with new releases\n\n[00:13:03] from OpenAI,\n\n[00:13:05] Anthropic,\n\n[00:13:06] Cohere,\n\n[00:13:07] Mistral, Grok,\n\n[00:13:08] Google, and more.\n\n[00:13:10] This means you can always leverage some of the largest context windows on the market\n\n[00:13:15] and unify your data ingestion and retrieval flows\n\n[00:13:19] no matter which language model you choose.\n\n[00:13:22] That concludes our look back at 2024.\n\n[00:13:25] It's been an exciting year of expanded integrations,\n\n[00:13:29] robust feed ingestion,\n\n[00:13:31] upgraded workflows,\n\n[00:13:33] more extensive model support,\n\n[00:13:35] and improved reliability.\n\n[00:13:37] If you haven't explored these features yet, it's a perfect time to jump in and see what Graphlet can do for you.\n\n[00:13:45] Head to graphlet.com\n\n[00:13:46] and sign up for a free account to try out all these capabilities for yourself.\n\n[00:13:52] And here's something else to look forward to.\n\n[00:13:54] In 2025,\n\n[00:13:56] Graphlet will be offering even more features to accelerate the building of AI agents,\n\n[00:14:02] expanding\n\n[00:14:02] how you orchestrate multiple models and data flows in sophisticated ways.\n\n[00:14:07] Thanks for listening, and may all your projects in the coming year be more productive,\n\n[00:14:12] more creative,\n\n[00:14:13] and more impactful\n\n[00:14:14] with Graphlet at your side.\n\n[00:14:16] Happy building.\n\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}