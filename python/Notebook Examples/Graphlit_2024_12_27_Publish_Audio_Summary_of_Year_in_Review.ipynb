{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyNXkRizKON1GyGak0v7BWLU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_12_27_Publish_Audio_Summary_of_Year_in_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to ingest Graphlit changelog, use OpenAI O1 to write a comprehensive year-in-review, and published using an [ElevenLabs](https://elevenlabs.io/) voice."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc90c665-8432-4ab1-986b-f35a5e1183a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphlit-client in /usr/local/lib/python3.10/dist-packages (1.0.20241227001)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.3)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade isodate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqZf2TOgnXsD",
        "outputId": "d357f83e-42c2-4f78-9638-d06cf39f3d3e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: isodate in /usr/local/lib/python3.10/dist-packages (0.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "async def create_specification(model: enums.OpenAIModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"OpenAI {model}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.OPEN_AI,\n",
        "        openAI=input_types.OpenAIModelPropertiesInput(\n",
        "            model=model,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_web_feed(uri: str, correlation_id: Optional[str], limit: Optional[int] = None):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.FeedInput(\n",
        "        name=uri,\n",
        "        type=enums.FeedTypes.WEB,\n",
        "        web=input_types.WebFeedPropertiesInput(\n",
        "            uri=uri,\n",
        "            readLimit=limit if limit is not None else 100\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_feed(input, correlation_id=correlation_id)\n",
        "\n",
        "        return response.create_feed.id if response.create_feed is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def is_feed_done(feed_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    response = await graphlit.client.is_feed_done(feed_id)\n",
        "\n",
        "    return response.is_feed_done.result if response.is_feed_done is not None else None\n",
        "\n",
        "\n",
        "async def lookup_usage(correlation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.lookup_usage(correlation_id)\n",
        "\n",
        "        return response.lookup_usage if response.lookup_usage is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def lookup_credits(correlation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.lookup_credits(correlation_id)\n",
        "\n",
        "        return response.lookup_credits if response.lookup_credits is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "\n",
        "def dump_usage_record(record):\n",
        "    print(f\"{record.date}: {record.name}\")\n",
        "\n",
        "    duration = isodate.parse_duration(record.duration)\n",
        "\n",
        "    if record.workflow:\n",
        "        print(f\"- Workflow [{record.workflow}] took {duration}, used credits [{record.credits:.8f}]\")\n",
        "    else:\n",
        "        print(f\"- Operation took {duration}, used credits [{record.credits:.8f}]\")\n",
        "\n",
        "    if record.entity_id:\n",
        "        if record.entity_type:\n",
        "            if record.entity_type == enums.EntityTypes.CONTENT and record.content_type:\n",
        "                print(f\"- {record.entity_type} [{record.entity_id}]: Content type [{record.content_type}], file type [{record.file_type}]\")\n",
        "            else:\n",
        "                print(f\"- {record.entity_type} [{record.entity_id}]\")\n",
        "        else:\n",
        "            print(f\"- Entity [{record.entity_id}]\")\n",
        "\n",
        "    if record.model_service:\n",
        "        print(f\"- Model service [{record.model_service}], model name [{record.model_name}]\")\n",
        "\n",
        "    if record.processor_name:\n",
        "        if record.processor_name in [\"Deepgram Audio Transcription\", \"Assembly.AI Audio Transcription\"]:\n",
        "            length = timedelta(milliseconds=record.count or 0)\n",
        "\n",
        "            if record.model_name:\n",
        "                print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}], length [{length}]\")\n",
        "            else:\n",
        "                print(f\"- Processor name [{record.processor_name}], length [{length}]\")\n",
        "        else:\n",
        "            if record.count:\n",
        "                if record.model_name:\n",
        "                    print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}], units [{record.count}]\")\n",
        "                else:\n",
        "                    print(f\"- Processor name [{record.processor_name}], units [{record.count}]\")\n",
        "            else:\n",
        "                if record.model_name:\n",
        "                    print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}]\")\n",
        "                else:\n",
        "                    print(f\"- Processor name [{record.processor_name}]\")\n",
        "\n",
        "    if record.uri:\n",
        "        print(f\"- URI [{record.uri}]\")\n",
        "\n",
        "    if record.name == \"Prompt completion\":\n",
        "        if record.prompt:\n",
        "            print(f\"- Prompt [{record.prompt_tokens} tokens (includes RAG context tokens)]:\")\n",
        "            print(record.prompt)\n",
        "\n",
        "        if record.completion:\n",
        "            print(f\"- Completion [{record.completion_tokens} tokens (includes JSON guardrails tokens)], throughput: {record.throughput:.3f} tokens/sec:\")\n",
        "            print(record.completion)\n",
        "\n",
        "    elif record.name == \"Text embedding\":\n",
        "        if record.prompt_tokens is not None:\n",
        "            print(f\"- Text embedding [{record.prompt_tokens} tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"Document preparation\":\n",
        "        if record.prompt_tokens is not None and record.completion_tokens is not None:\n",
        "            print(f\"- Document preparation [{record.prompt_tokens} input tokens, {record.completion_tokens} output tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"Data extraction\":\n",
        "        if record.prompt_tokens is not None and record.completion_tokens is not None:\n",
        "            print(f\"- Data extraction [{record.prompt_tokens} input tokens, {record.completion_tokens} output tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"GraphQL\":\n",
        "        if record.request:\n",
        "            print(f\"- Request:\")\n",
        "            print(record.request)\n",
        "\n",
        "        if record.variables:\n",
        "            print(f\"- Variables:\")\n",
        "            print(record.variables)\n",
        "\n",
        "        if record.response:\n",
        "            print(f\"- Response:\")\n",
        "            print(record.response)\n",
        "\n",
        "    if record.name.startswith(\"Upload\"):\n",
        "        print(f\"- File upload [{record.count} bytes], throughput: {record.throughput:.3f} bytes/sec\")\n",
        "\n",
        "    print()\n",
        "\n",
        "async def get_content(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    response = await graphlit.client.get_content(content_id)\n",
        "\n",
        "    return response.content\n",
        "\n",
        "async def query_contents(feed_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.query_contents(\n",
        "            filter=input_types.ContentFilter(\n",
        "                feeds=[\n",
        "                    input_types.EntityReferenceFilter(\n",
        "                        id=feed_id\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return response.contents.results if response.contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def publish_contents(feed_id: str, summary_specification_id: str, publish_specification_id: str, summary_prompt: str, publish_prompt: str, correlation_id: str, voice_id: Optional[str] = None):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.publish_contents(\n",
        "            name=\"Published Summary\",\n",
        "            connector=input_types.ContentPublishingConnectorInput(\n",
        "               type=enums.ContentPublishingServiceTypes.ELEVEN_LABS_AUDIO,\n",
        "               format=enums.ContentPublishingFormats.MP3,\n",
        "               elevenLabs=input_types.ElevenLabsPublishingPropertiesInput(\n",
        "                   model=enums.ElevenLabsModels.TURBO_V2_5,\n",
        "                   voice=voice_id if voice_id is not None else \"ZF6FPAbjXT4488VcRRnw\" # ElevenLabs Amelia voice\n",
        "               )\n",
        "            ),\n",
        "            summary_prompt=summary_prompt,\n",
        "            summary_specification=input_types.EntityReferenceInput(\n",
        "                id=summary_specification_id\n",
        "            ),\n",
        "            publish_prompt = publish_prompt,\n",
        "            publish_specification=input_types.EntityReferenceInput(\n",
        "                id=publish_specification_id\n",
        "            ),\n",
        "            filter=input_types.ContentFilter(\n",
        "                feeds=[input_types.EntityReferenceFilter(id=feed_id)]\n",
        "            ),\n",
        "            is_synchronous=True,\n",
        "            correlation_id=correlation_id\n",
        "        )\n",
        "\n",
        "        return response.publish_contents.id if response.publish_contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_all_specifications():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_specifications(is_synchronous=True)\n",
        "\n",
        "async def delete_all_feeds():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_feeds(is_synchronous=True)\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import isodate\n",
        "from IPython.display import display, Markdown, HTML\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Remove any existing feeds, contents and specifications; only needed for notebook example\n",
        "await delete_all_feeds()\n",
        "await delete_all_specifications()\n",
        "await delete_all_contents()\n",
        "\n",
        "print('Deleted all feeds, contents and specifications.')\n",
        "\n",
        "# NOTE: create a unique cost correlation ID\n",
        "ingestion_correlation_id = datetime.now().isoformat()\n",
        "publish_correlation_id = datetime.now().isoformat()\n",
        "\n",
        "uri = \"https://changelog.graphlit.dev\"\n",
        "limit = 100 # maximum number of web pages to ingest\n",
        "\n",
        "feed_id = await create_web_feed(uri, ingestion_correlation_id, limit)\n",
        "\n",
        "if feed_id is not None:\n",
        "    print(f'Created feed [{feed_id}]: {uri}')\n",
        "\n",
        "    # Wait for feed to complete, since ingestion happens asychronously\n",
        "    done = False\n",
        "    time.sleep(5)\n",
        "    while not done:\n",
        "        done = await is_feed_done(feed_id)\n",
        "\n",
        "        if not done:\n",
        "            time.sleep(10)\n",
        "\n",
        "    print(f'Completed feed [{feed_id}].')\n",
        "\n",
        "    # Query contents by feed\n",
        "    contents = await query_contents(feed_id)\n",
        "\n",
        "    if contents is not None:\n",
        "        print(f'Found {len(contents)} contents in feed [{feed_id}].')\n",
        "        print()\n",
        "\n",
        "        for content in contents:\n",
        "            if content is not None:\n",
        "\n",
        "                display(Markdown(f'# Ingested content [{content.id}]'))\n",
        "\n",
        "                print(f'Text Mezzanine: {content.text_uri}')\n",
        "\n",
        "                print(content.markdown)"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c3a31be-6411-4b05-ee93-7b181ad4983b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all feeds, contents and specifications.\n",
            "Created feed [ff940e5a-1374-4664-9701-108390a9a6f3]: https://changelog.graphlit.dev\n",
            "Completed feed [ff940e5a-1374-4664-9701-108390a9a6f3].\n",
            "Found 47 contents in feed [ff940e5a-1374-4664-9701-108390a9a6f3].\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [44c810e9-ce3d-44f0-aff5-8f091d30acda]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/44c810e9-ce3d-44f0-aff5-8f091d30acda/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎒\tSeptember 2024\n",
            "\n",
            "# September 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Azure AI Model Inference API (aka Models as a Service) model service which offers serverless hosting to many models such as Meta Llama 3.2, Cohere Command-R, and many more. For Azure AI, all models are 'custom', and you will need to provide the serverless endpoint, API key and number of tokens accepted in context window, after provisioning the model of your choice.\n",
            "- We have added support for the multimodal Mistral Pixtral model, under the model enum PIXTRAL_12B_2409.\n",
            "- We have added versioned model enums for Google Gemini, so you can access GEMINI_1_5_FLASH_001, GEMINI_1_5_FLASH_002, GEMINI_1_5_PRO_001 and GEMINI_1_5_PRO_002.\n",
            "\n",
            "PreviousOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "NextSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [801f60cf-bbaf-4167-a6f5-6123c5109f8a]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/801f60cf-bbaf-4167-a6f5-6123c5109f8a/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎒\tSeptember 2024\n",
            "\n",
            "# September 3: Support for web search feeds, model deprecations\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports web search feeds, using the Tavily and Exa.AI web search APIs. You can choose the SEARCH feed type, and assign your search text property, and we will ingest the referenced web pages from the search results. Optionally, you can select the search service via the serviceType property under search feed properties. By default, Graphlit will use the Tavily API.\n",
            "- ⚡ We have deprecated these OpenAI models, according to the future support OpenAI is providing to these legacy models: GPT35_TURBO, GPT35_TURBO_0613, GPT35_TURBO_16K, GPT35_TURBO_16K_0125, GPT35_TURBO_16K_0613, GPT35_TURBO_16K_1106, GPT4, GPT4_0613, GPT4_32K, GPT4_32K_0613, GPT4_TURBO_VISION_128K, and GPT4_TURBO_VISION_128K_1106. We suggest using GPT-4o or GPT-4o Mini instead.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2523: Can't ingest from same feed URI multiple times and wait on isFeedDone\n",
            "\n",
            "PreviousSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "NextSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "Last updated3 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [2967b74e-cd6c-40bf-82e4-b8bdfe04545d]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/2967b74e-cd6c-40bf-82e4-b8bdfe04545d/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎒\tSeptember 2024\n",
            "\n",
            "# September 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Cerebras model service which offers the LLAMA_3_1_70B and LLAMA_3_1_8B models.\n",
            "- 💡 Graphlit now supports the Google AI model service which offers the GEMINI_1_5_PRO and GEMINI_1_5_FLASH models.\n",
            "- We have added support for the latest Groq Llama 3.2 preview models, including LLAMA_3_2_1B_PREVIEW, LLAMA_3_2_3B_PREVIEW, LLAMA_3_2_11B_TEXT_PREVIEW, and LLAMA_3_2_90B_TEXT_PREVIEW. We have also added support for the Llama 3.2 multimodal model LLAMA_3_2_11B_VISION_PREVIEW.\n",
            "- We have added a new specification parameter to the promptConversation mutation. Now you can specify your initial specification for a new conversation, or update an existing conversation, without requiring additional API calls.\n",
            "- ⚡ We have changed the retrieval behavior of the promptConversation mutation. Now, if no relevant content was found via vector-based semantic search (given the user prompt), we will fallback to any relevant content from the message in the conversation. If there was no content from the conversation to fallback to, we will fallback to the last ingested content in the project. This solves an issue where a first prompt like 'Summarize this' would find no relevant content. Now it will fallback to retrieve the last ingested content.\n",
            "- ⚡ We have renamed the Groq model enum from LLAVA_1_5_7B to LLAVA_1_5_7B_PREVIEW.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3083: Not sending custom instructions/guidance with extraction prompt\n",
            "- GPLA-3146: Filtering Persons by email not working\n",
            "- GPLA-3171: Not failing on deprecated OpenAI model\n",
            "- GPLA-3158: Summarization not using revision strategy\n",
            "\n",
            "PreviousSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "NextSeptember 3: Support for web search feeds, model deprecations\n",
            "Last updated3 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [9529be6c-cfd7-49db-b604-c45749004809]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/9529be6c-cfd7-49db-b604-c45749004809/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎒\tSeptember 2024\n",
            "\n",
            "# September 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports entity enrichment from Fast Healthcare Interoperability Resources (FHIR) servers. You can provide the endpoint for a FHIR server, and Graphlit will enrich medical-related entities from the data found in the FHIR server.\n",
            "- Added support for latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408) and added datestamped model enums for the previous versions (COMMAND_R_202403, COMMAND_R_PLUS_202404). The latest model enums (COMMAND_R and COMMAND_R_PLUS) currently point to the models (COMMAND_R_202403 and COMMAND_R_PLUS_202404) as specified by the Cohere API.\n",
            "- Added support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31), now used by default.\n",
            "- ⚡ We have changed the name of the LinkReferenceType to LinkReference to follow the existing data model standard.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3120: LLM is adding source tags to end of completed messages\n",
            "- GPLA-3133: Failed to load sitemap on child page of website.\n",
            "\n",
            "PreviousSeptember 3: Support for web search feeds, model deprecations\n",
            "NextAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "Last updated3 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [3a5fcfa3-9f3e-4856-a4b9-afe16c9a4813]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/3a5fcfa3-9f3e-4856-a4b9-afe16c9a4813/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🛠️\tSeptember 2023\n",
            "\n",
            "# September 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "### New Features\n",
            "\n",
            "- 🔥 Added Workflow entity to data model for configuring stages of content workflow; can be assigned to Feed or with ingestPage, ingestFile, or ingestText mutations to control how content is ingested, prepared, extracted and enriched into the knowledge graph.\n",
            "- 💡 Added support for Notion feeds: now can create feed to ingest files from Notion pages or databases (i.e. wikis).\n",
            "- 💡 Added support for API-created Observation entities, which allow for custom observations of observable entities (i.e. Person, Label) on Content.\n",
            "- 💡 Added support for Azure AI Document Intelligence as an optional method for preparing PDF files, using OCR and advanced layout analysis.\n",
            "- 💡 Added summarization strategies, where content can be summarized into paragraphs, bullet points or headline.\n",
            "- Added ability to assign default Workflow and Specification to project.\n",
            "- Added more well-known link types, during link crawling, such as Discord, Airtable and TypeForm.\n",
            "- ℹ️ Free/Hobby plan now has 5GB storage quota; any content ingested past that limit will be auto-deleted.\n",
            "- ⚡ Actions have been moved into Workflow entity.\n",
            "- ⚡ Link enrichment for Feeds has been moved into the Workflow enrichment stage, now called link crawling. ExcludeContentDomain property has been reversed and is now called IncludeContentDomain.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1204: Failed to ingest content with backslash in name.\n",
            "- GPLA-1276: Failed to ingest RSS posts which contained enclosure URI, but no post URI.\n",
            "\n",
            "PreviousSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "NextAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [9c5622e0-48d5-44b2-9999-e9697b31c84e]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/9c5622e0-48d5-44b2-9999-e9697b31c84e/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🛠️\tSeptember 2023\n",
            "\n",
            "# September 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "### New Features\n",
            "\n",
            "- 🔥 Graphlit now supports YouTube feeds, where you can ingest a set of YouTube videos, or an entire YouTube playlist or channel. Note, we currently support only the ingestion of audio from YouTube videos, which gets transcribed and added to your conversational knowledge graph.\n",
            "\n",
            "### New Documentation\n",
            "\n",
            "- Added documentation for observable entities mutations and queries (Label, Category, Person, Organization, Place, Event, Product, Repo, Software).\n",
            "- Added documentation for using custom Azure OpenAI and OpenAI models with Specifications\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1459: LLM prompt formatting was exceeding the token budget with long user prompts.\n",
            "- GPLA-1445: Failed to ingest PDF from URL where filename in Content-Disposition header contained a backslash.\n",
            "\n",
            "PreviousOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "NextSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "Last updated7 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [a943fa59-2f9c-4731-bcc2-a52b9219215b]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/a943fa59-2f9c-4731-bcc2-a52b9219215b/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🛠️\tSeptember 2023\n",
            "\n",
            "# September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "### New Features\n",
            "\n",
            "- 🔥 Graphlit now supports paid Hobby, Starter and Growth tiers for projects, in addition to the existing Free tier. Starting at $49/mo, plus $0.10/credit for usage, we now support higher quota based on your subscribed tier. By providing a payment method for your organization in the Developer Portal, you can upgrade each project individually to the tier that fits your application's needs.\n",
            "- 💡 Added GraphQL mutations for the creation, update and deletion of observed entities (i.e. Person, Organization, Place, Product, Event, Label, Category).\n",
            "- 💡 Added new observed entity types to knowledge graph: Repo (i.e. Git repo), Software.\n",
            "- 💡 Added searchType and numberSimilar fields to Specification object for configuring semantic search in conversations. In situations where the user prompt is limited in length, HYBRID search type can provide better semantic search results for the prompt context.\n",
            "- 💡 Added support for the Azure OpenAI GPT-4 model.\n",
            "- Added support for project quota field. Project quotas are based on the subscribed pricing tier. Quota limits are now applied as content is ingested, and as feeds and conversations are created.\n",
            "- Added contentLimit to conversation strategy object to limit the number of semantic search content results which are formatted into prompt context.\n",
            "- Better relevance ranking on semantic search results when formatting prompt context in conversations.\n",
            "- ℹ️ Free tier has updated quota: 1GB storage, 100 contents, 3 feeds and 10 conversations.\n",
            "- ⚡ Now using the Deepgram Nova-2 audio transcription model, which is 18% more accurate, and 5-40x faster.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1373: Failed to extract multiple text pages from DOCX without page breaks. Now we support token-aware page chunking.\n",
            "- GPLA-1377: Failed during semantic search with no content results, when prompting conversation.\n",
            "- GPLA-1415: Failed when user prompt couldn't generate text embeddings.\n",
            "\n",
            "PreviousSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "NextSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "Last updated11 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [aa3d4b80-1e4e-4a6f-afd0-09e6fd0217a4]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/aa3d4b80-1e4e-4a6f-afd0-09e6fd0217a4/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 9: Support for GitHub repository feeds, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports GitHub feeds, by providing the repository owner and name similar to GitHub Issues feeds, and will ingest code files from any GitHub repository.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3262: Missing row separator in table markdown formatting\n",
            "\n",
            "PreviousOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "NextOctober 7: Support for Anthropic and Gemini tool calling\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [10d4a01f-d93e-4e60-aba9-cad12418f6d7]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/10d4a01f-d93e-4e60-aba9-cad12418f6d7/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 7: Support for Anthropic and Gemini tool calling\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports tool calling with Anthropic and Google Gemini models.\n",
            "- ⚡ We have removed the uri property for tools from ToolDefinitionInput, such that inline webhook tools are no longer supported. Now you can define any external tools to be called, and those can support sync or async data access to fulfill the tool call.\n",
            "\n",
            "PreviousOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "NextOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [0ce3818c-7562-43b3-a554-38b014cb36b3]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/0ce3818c-7562-43b3-a554-38b014cb36b3/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 31: Support for simulated tool calling, bug fixes\n",
            "### New Features\n",
            "\n",
            "- Graphlit now supports simulated tool calling for LLMs which don't natively support it, such as OpenAI o1-preview and o1-mini. Tool schema will be formatted into the LLM prompt context, and tool responses are parsed out of the JSON formatted response.\n",
            "- ⚡ Given customer feedback, we have lowered the vector and hybrid thresholds used by the semantic search. Previously, some content at a low relevance was being excluded from the semantic search results. Now, more low-relevance content will be included in the results, used by the RAG pipeline. Reranking can be used to sort the search results for relevance.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3357: Not extracting all images from PDF, and should filter out single-color images.\n",
            "\n",
            "PreviousNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "NextOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [52d4fb3e-db27-4bd2-bc42-97410cb7216c]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/52d4fb3e-db27-4bd2-bc42-97410cb7216c/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the ingestBatch mutation, which accepts an array of URIs to files or web pages, and will asynchronously ingest these into content objects.\n",
            "- 💡 Graphlit now supports the continueConversation mutation, which accepts an array of called tool responses. Also, promptConversation now accepts an array of tool definitions. When tools are called by the LLM, the assistant message returned from promptConversation will have a list of toolCalls which need to responded to from your calling code. These responses are to be provided back to the LLM via the continueConversation mutation.\n",
            "- 💡 Graphlit now supports tool calling with OpenAI, Mistral, Deepseek, Groq, and Cerebras model services. Anthropic, Google Gemini and Cohere support will come later.\n",
            "- Added support for prefilled user and assistant messages with createConversation mutation. Now you can send an array of messages when creating a new conversation, which will bootstrap the conversation with the LLM. These must be provided in user/assistant pairs.\n",
            "- Added support for Google Gemini Flash 1.5 8b model.\n",
            "- ⚡ We have deprecated the tools property in the Specification object. These will be removed at a later date. Tools are now to be sent directly to the extractContents and promptConversation mutations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3207: Models shouldn't be required on update specification call\n",
            "- GPLA-3220: Don't send system prompt with OpenAI o1 models\n",
            "\n",
            "PreviousOctober 7: Support for Anthropic and Gemini tool calling\n",
            "NextSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [0f863f8a-abac-4e44-8787-c8d28bf7f323]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/0f863f8a-abac-4e44-8787-c8d28bf7f323/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "### New Features\n",
            "\n",
            "- Graphlit now supports the latest Anthropic Sonnet 3.5 model (released 10/22/2024). We have added date-versions model enums for the Anthropic models: CLAUDE_3_5_SONNET_20240620, CLAUDE_3_5_SONNET_20241022, CLAUDE_3_HAIKU_20240307, CLAUDE_3_OPUS_20240229, CLAUDE_3_SONNET_20240229. The existing model enums will target the latest released models, as specified by Anthropic.\n",
            "- Graphlit now supports image embeddings using the Cohere Embed 3.0 models.\n",
            "\n",
            "PreviousOctober 31: Support for simulated tool calling, bug fixes\n",
            "NextOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [71f330b4-23a2-40ec-bd77-544a11cf764b]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/71f330b4-23a2-40ec-bd77-544a11cf764b/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the configuration of image and text embedding models, at the Project level. You can create an embedding specification for a text or image embedding model, and then assign that to the Project, and all further embedding requests will use that embedding model. See this Colab notebook for an example of how to configure the project.\n",
            "- 💡 Graphlit now supports the OpenAI Embedding-3-Small and Embedding-3-Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0 and 3.0 text embedding models. Graphlit also now supports Jina CLIP image embeddings, which are used by default for image search.\n",
            "- Graphlit now supports the chunkTokenLimit property in Specifications, which specifies the number of tokens for each embedded text chunk. If this is not configured, Graphlit uses 600 tokens for each embedded text chunk.\n",
            "- Graphlit now supports the Voyage reranking model.\n",
            "- Graphlit now supports the ingestTextBatch mutation, which accepts an array of text and name pairs, and will asynchronously ingest these into content objects.\n",
            "- ⚡ We have moved the chunkTokenLimit property from the Workflow storage embeddings strategy to the Specification object. The Workflow storage property has now been deprecated.\n",
            "- ⚡ We have deprecated the openAIImage property from Workflow entity extraction properties. Use the modelImage property instead.\n",
            "\n",
            "Once a text embedding model has been updated at the project level, any content, conversations or observed entities will no longer be semantically searchable.\n",
            "Text embeddings are not compatible across models, so you will need to delete and reingest any content, or recreate conversations or knowledge graph entities, with the new embedding model to become searchable.\n",
            "PreviousOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "NextOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [65a42e3c-ac9a-4c93-bf43-fa296df6a1f7]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/65a42e3c-ac9a-4c93-bf43-fa296df6a1f7/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎃\tOctober 2023\n",
            "\n",
            "# October 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports 'aliases' of observable names, as the alternateNames property. When an observed entity, such as Organization, is enriched, we store the original name and the enriched name as an alias. For example, \"OpenAI\" may be enriched to \"OpenAI, Inc.\", and we store \"OpenAI\" as an alias, and update the name to \"OpenAI, Inc.\".\n",
            "- 💡 Added workflows filter to ContentCriteriaInput type, for filtering content by workflow(s) when creating conversation.\n",
            "- Optimized formatting of content sources into prompt context, for more accurate conversation responses.\n",
            "- Optimized formatting of extracted text from Slack messages, for better knowledge retrieval.\n",
            "- Updated text tokenizer for more accurate token counting.\n",
            "- Upgraded Azure Text Analytics to latest preview API version.\n",
            "- Authors found in RSS feeds are now stored as observations of Person entities.\n",
            "- Added rate limiting for Reddit feeds.\n",
            "- Added rate limiting for Wikipedia enrichment.\n",
            "- Added support for reading Reddit post comments when reading Reddit feed.\n",
            "- ⚡ EmbedFacets has been renamed to EnableFacets in the conversation strategy.\n",
            "- ⚡ Removed extra content level in IngestionWorkflowStage type. Now, the if property is of type IngestionContentFilter.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1556: Better handling of very long user prompts.\n",
            "- GPLA-1627: Optimized token budget for more accurate prompt completion.\n",
            "- GPLA-1585: More accurate entity matching in Wikipedia entity enrichment.\n",
            "\n",
            "PreviousDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "NextOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [27269714-fa48-438c-b75e-d486a030b697]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/27269714-fa48-438c-b75e-d486a030b697/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎃\tOctober 2023\n",
            "\n",
            "# October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "### New Features\n",
            "\n",
            "- 🔥 Graphlit now supports Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "- 🔥 Graphlit now supports Slack feeds, and will ingest Slack messages and linked file attachments from a Slack channel. Note, this requires the creation of a Slack bot which has been added to the appropriate Slack channel.\n",
            "- 💡 Added support for entity enrichment to workflow object, which offers Diffbot, Wikipedia and Crunchbase enrichment of observed entities, such as Person, Organization and Place.\n",
            "- 💡 Added support for text extraction from images. When using Azure Image Analytics for entity extraction, Graphlit will extract and store any identified text which then becomes searchable.\n",
            "- Added embedFacets property to conversation strategy in specification object.\n",
            "- Added embedCitations property to conversation strategy in specification object. This makes content citations optional with the completed conversation message.\n",
            "- Added GraphQL mutations for multi-delete of entities, such as deleteCollections, deleteLabels, or deleteConversations.\n",
            "- Added GraphQL deleteAllConversations mutation to delete all conversations.\n",
            "- Added support for automatically adding ingested content to one or more collections, via ingestion stage of workflow object.\n",
            "- Added specification property to preparation workflow stage, which will be used to select the LLM for text summarization.\n",
            "- Expanded the properties for observed entities, such as Person, Organization or Product. Now supports a wider range of properties for entity enrichment.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1520: Unlimited conversation quota not assigned when upgrading project tier\n",
            "- GPLA-1285: Entity enrichment not firing event, which can be sent to actions\n",
            "- GPLA-1361: Web page left in ingested state, when URL not accessible.\n",
            "\n",
            "PreviousOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "NextSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f0afc1f7-b938-40f0-927d-f6a274243532]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/f0afc1f7-b938-40f0-927d-f6a274243532/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🦃\tNovember 2024\n",
            "\n",
            "# November 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "### New Features\n",
            "\n",
            "- Graphlit now supports the latest Anthropic Haiku 3.5 model, with the model enum CLAUDE_3_5_HAIKU_20241022.\n",
            "- ⚡ Once a project has hit the free tier quota, we will now automatically disable all feeds. Once the project has been upgraded to a paid tier, you can use the enableFeed mutation to re-enable your existing feeds to continue ingestion.\n",
            "- ⚡ We have added the disableFallback flag to the RetrievalStrategyInput type, so you can disable the default behavior of falling back to the previous conversation's contents, or worst-case, falling back to the most recently uploaded content. By setting disableFallback to true, conversations will only attempt to retrieve contents based on the provided filter and/or augmentedFilter properties.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3367: Not extracting text from HTML button element\n",
            "\n",
            "PreviousNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "NextOctober 31: Support for simulated tool calling, bug fixes\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [dd3d6052-821d-4398-8cc5-37fd5feefed7]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/dd3d6052-821d-4398-8cc5-37fd5feefed7/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🦃\tNovember 2024\n",
            "\n",
            "# November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports multi-turn analysis of images with the reviseImage and reviseEncodedImage mutations. You can provide an LLM prompt and either a URI or Base-64 encoded image and MIME type, along with an optional LLM specification. This can be used for analyzing any image and having a multi-turn conversation with the LLM to revise the output from the LLM. (Colab Notebook Example)\n",
            "- 💡 Graphlit now supports directly prompting an LLM with the prompt mutation, bypassing any RAG content retrieval, while providing an optional list of previous conversation messages. This also accepts an optional LLM specification. (Colab Notebook Example)\n",
            "- We have added support for the new Mistral Pixtral Large model, with PIXTRAL_LARGE model enum, which can be used with LLM completion or entity extraction LLM specifications.\n",
            "- We have added support for the OpenAI 2024-11-20 version of GPT-4o, with GPT4O_128K_20241120 model enum.\n",
            "- ⚡ We have added Microsoft Entra ID (fka Azure Active Directory) clientId and clientSecret properties to the SharePointFeedPropertiesInput type, which are now required when creating a SharePoint feed using user authentication with refreshToken property. (Colab Notebook Example)\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3438: Not filtering on desktop presentation when scraping web pages\n",
            "- GPLA-3340: Failed to parse invalid JSON from extracted PDF page\n",
            "- GPLA-3427: Not formatting extracted tables properly from Sonnet 3.5\n",
            "\n",
            "PreviousDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "NextNovember 16: Support for image description, multi-turn text summarization\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [ca79d129-fe97-4afb-aa98-43fd1b5386cb]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/ca79d129-fe97-4afb-aa98-43fd1b5386cb/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🦃\tNovember 2024\n",
            "\n",
            "# November 16: Support for image description, multi-turn text summarization\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports multi-turn summarization of text with the reviseText mutation. You can provide an LLM prompt and text string, along with an optional specification. This can be used for summarizing any raw text and having a multi-turn conversation with the LLM to revise the output from the LLM. (Colab Notebook Example)\n",
            "- 💡 Graphlit now supports image descriptions using vision LLMs, without needing to ingest the image first. With the new describeImage mutation, which takes a URI, and describeEncodedImage mutation, which takes a Base-64 encoded image and MIME type, you can use any vision LLM to prompt an image description. These mutations accept an optional specification, where you can select your vision LLM. If not provided, OpenAI GPT-4o will be used. (Colab Notebook Example)\n",
            "\n",
            "PreviousNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "NextNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [b9b9ea9e-9e6d-4d19-a851-ca0115eba74f]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/b9b9ea9e-9e6d-4d19-a851-ca0115eba74f/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🦃\tNovember 2024\n",
            "\n",
            "# November 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports web search with the searchWeb mutation. You can select the search service, either Tavily or Exa.AI, and provide the search query and number of search results to be returned. This is different than the web search feed, in that searchWeb returns the relevant text from the web page and the web page URL from each search hit, but does not ingest each of the web pages. This new mutation is optimized to be used from within an LLM tool.\n",
            "- 💡 Graphlit now supports multi-turn summarization of content with the reviseContent mutation. You can provide an LLM prompt and a content reference, along with an optional specification. This can be used for summarizing any content (documents, web pages, audio transcripts, etc.), and having a multi-turn conversation with the LLM to revise the output from the LLM. Internally, this creates a conversation locked to a single piece of content. This works especially well with the OpenAI o1-preview and o1-mini models, because they provide a longer LLM output from each turn.\n",
            "- Graphlit now supports the configuration of the Deepgram transcription language, and whether detectLanguage is enabled in DeepgramAudioPreparationPropertiesInput. Language detection is now enabled by default, and can be disabled by setting detectLanguage to false.\n",
            "- ⚡ We have added a requireTool option to promptConversation mutation, so you can control whether the LLM must call one of the provided tool, or if tool calling is optional.\n",
            "- ⚡ For accounts created after Nov 8, 2024, we have lowered the credits quota on the Free tier from 1000 credits to 100 credits, and now offer unlimited feeds on the Hobby Tier.\n",
            "- ⚡ The Graphlit Data API will now return HTTP 402 (Payment Required) when you have exceeded the credits quota on the free tier. You must upgrade to the Hobby Tier (or higher) to continue using the API, once the credits quota has been reached.\n",
            "\n",
            "PreviousNovember 16: Support for image description, multi-turn text summarization\n",
            "NextNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/e74c8a01-189a-4f47-87e6-2c2e33b3da7a/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "💐\tMay 2024\n",
            "\n",
            "# May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Jina reranker and Pongo semantic filtering (reranking), in the Specification object. Now you can choose between COHERE, PONGO and JINA for your reranking serviceType.\n",
            "- 💡 Graphlit now supports Microsoft Teams feeds for reading messages from Teams channels.\n",
            "- Given changes in YouTube video player HTML, we have rewritten the YouTube downloader to support the new page format.\n",
            "- Added better handling of HTTP errors when validating URIs. Previously some websites were returning HTTP 403 (Forbidden) errors when validating their URI, or downloading content. Now Graphlit is able to scrape these sites, which previously returned errors.\n",
            "- Added support for updating content metadata in updateContent mutation. Now the video, audio, document, etc. metadata can be updated after the content workflow has finished.\n",
            "- Added query_contents_graph (and queryContentsGraph) functions to SDKs, which can be used to return nodes and edges from knowledge graph for visualization.\n",
            "- ⚡ Citation indices have been changed to be one-based from zero-based. For example, you will now see \"This is a citation. [1]\" as the first citation in the list.\n",
            "- ⚡ Added isSynchronous flag to deleteAll and multiple delete mutations. By default, bulk delete operations are now asynchronous (and completed after the mutation returns), unless the isSynchronous flag is set to true.\n",
            "- ⚡ Added missing count mutations, such as countAlerts, countFeeds, etc.\n",
            "- ⚡ Renamed query_content_facets to query_contents_facets in Python SDK\n",
            "- ⚡ Renamed queryContentFacets to queryContentsFacets in Node.js SDK\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2544: Page relevance not filled-in in all situations\n",
            "- GPLA-2546: Not extracting links from PDF with Azure AI Doc Intelligence\n",
            "- GPLA-2557: Sporadically returning HTTP 500 from GraphQL API\n",
            "- GPLA-2573: Failed to re-ingest content which was deleted immediately after initial ingestion\n",
            "- GPLA-2575: Not validating for empty (non-null) parameters in mutations\n",
            "- GPLA-2578: Need to handle invalid JSON from LLMs; improper escaping or formatting\n",
            "- GPLA-2585: Failed to ingest encoded file with colon (:) in name\n",
            "\n",
            "PreviousMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "NextApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "Last updated7 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [5c81e515-f157-4e72-bff3-fbf7de488625]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/5c81e515-f157-4e72-bff3-fbf7de488625/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "💐\tMay 2024\n",
            "\n",
            "# May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports GraphRAG, where the extracted entities in the knowledge graph can be added as additional context to your RAG con,versation. Also, with GraphRAG, entities can be extracted from the user prompt, and used as additional content filters - or can be used to query related content sources, which are combined with the vector search results. This can be configured by specifying your graphStrategy in the Specification object.\n",
            "- 💡 Graphlit now supports LLM revisions within RAG conversations, where the LLM can be prompted to revise its initial completion response. From our testing, this has been shown to provide 35% more output tokens with higher quality responses. This can be configured by specifying your revisionStrategy, and you can use our built-in revision prompt, or provide a custom one, and specify how many revisions you want the LLM to make.\n",
            "- 💡 Graphlit now supports the new OpenAI GPT-4o model for RAG conversations.\n",
            "- ⚡ We have changed the default model for Conversations to be OpenAI GPT-4o, from Azure OpenAI GPT-3.5 16k. This provides faster performance and better quality output.\n",
            "- Added graph to promptConversation response, so you can visualize or leverage the nodes and edges of the knowledge graph, resulting from the content retrieval. For example, if a Person and Organization were observed in the cited content sources used by the RAG pipeline, you will get back those entities and their relationship (such as Person 'works-for' Organization).\n",
            "- Expanded the enriched data from WIkipedia to include the long description of an entity.\n",
            "- Added getSharePointLibraries, getSharePointFolders, and getOneDriveFolders queries to the API, which can be used to enumerate the storage services. This makes locating the SharePoint libraryId easier, for example.\n",
            "- Added getTeams and getTeamsChannels queries to the API for enumerating Microsoft Teams workspaces.\n",
            "- Added extractedCount to the entity extraction connector to limit the number of extracted entities, per entity type. I.e. if extracted count is 10, it will extract at most ten each of Persons, Organizations, etc.\n",
            "- 🔥 We have improved performance in several areas: creation of observations after entity extraction, access to cloud storage, rendering the RAG context.\n",
            "- 🔥 We have optimized the LLM entity extraction process to identify more properties, as well as entity-to-entity relationships.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2652: Not extracting text from HTML in RSS post\n",
            "- GPLA-2627: Limit filter only returning half the results\n",
            "- GPLA-2613: Not properly extracting structured text from JSON/XML formats\n",
            "\n",
            "PreviousJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "NextMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "Last updated6 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🍀\tMarch 2024\n",
            "\n",
            "# March 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Linear, GitHub Issues and Atlassian Jira feeds. Graphlit will ingest issues (aka tasks, stories) from these issue-tracking services as individual content items, which will be made searchable and conversational.\n",
            "- 💡 Added support for ISSUEcontent type, which includes metadata such as title, authors, commenters, status, type, project and team.\n",
            "- 💡 Added support for default feed read limit. Now, if you don't specify the readLimit property on feeds, it will default to reading 100 content items. You can override this default by assigning a custom read limit, which has no upper bounds. However, one-shot feeds much complete within 15 minutes, or they will be stopped automatically.\n",
            "- Added support for ingesting files referenced in a Web sitemap. Previously any files (i.e. PDF, MP3) referenced in a sitemap.xml would be ignored. Now you can optionally enable includeFiles in the WebFeedPropertiesInput object to have Graphlit ingest non-HTML pages as part of the Web feed.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2374: Failed to ingest MP4 with large XMP metadata.\n",
            "\n",
            "PreviousApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "NextMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [a7ae398c-7c08-4742-b509-ff8341ad0bb0]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/a7ae398c-7c08-4742-b509-ff8341ad0bb0/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎇\tJuly 2023\n",
            "\n",
            "# July 15: Support for SharePoint feeds, new Conversation features\n",
            "### New Features\n",
            "\n",
            "- 💡 Added support for SharePoint feeds: now can create feed to ingest files from SharePoint document library (and optionally, folder within document library)\n",
            "- 💡 Added support for PII detection during entity extraction from text documents and audio transcripts: now we will create labels such as PII: Social Security Number automatically when PII is detected\n",
            "- 💡 Added support for developer's own OpenAI API keys and Azure OpenAI deployments in Specifications\n",
            "- ℹ️ Changed semantics of deleteFeed to delete the contents ingested by the feed; since contents are linked to feeds, now feeds can be disabled, while keeping the lineage to the feed, and if feeds are deleted, they will delete the linked contents, so we never lose the feed-to-content lineage\n",
            "- Added GraphQL query for SharePoint consent URI, for registered Graphlit Platform Azure AD application\n",
            "- Better handling of web sitemap indexes: now if a sitemap.xml contains a sitemapindex element, we will load all linked sitemaps for evaluating web pages to ingest from Web feed\n",
            "- Added new GraphQL mutations for openConversation, closeConversation and undoConversation\n",
            "- Added timestamps to Conversation messages\n",
            "- Added new GraphQL mutations for openCollection and closeCollection\n",
            "- Added more configuration for content search: now can specify searchType (KEYWORD, VECTOR, HYBRID) and queryType (SIMPLE, FULL - aka Lucene syntax)\n",
            "- Better parsing of iTunes podcast metadata\n",
            "- ⚡ Renamed listingLimit field on feeds to readLimit\n",
            "- ⚡ Renamed topK to numberSimilar for content vector search type\n",
            "- ⚡ Changed GraphQL feed properties: split out azure into azureBlob and azureFile properties\n",
            "- ⚡ Changed GraphQL specification properties: split out openAI into openAI and azureOpenAI properties\n",
            "- ⚡ Removed count fields on query results, and replaced with explicit count{Entity} queries, which support search and filtering.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1043: Reddit readLimit not taking effect: now the specified limit of Reddit posts will be leveraged for Reddit feeds\n",
            "- GPLA-1064: Performance on entity extraction and observation creation for large PDFs was under expectations: now able to build knowledge graph from large PDFs much faster (4x speed improvement)\n",
            "- GPLA-1053: If rendition generation errored during content workflow, the content was not properly marked as errored\n",
            "- GPLA-1102: Large Web sitemaps were slow to load; rewrote sitemap index handling, and now can process sitemaps with 150K+ entries in seconds.\n",
            "\n",
            "PreviousAugust 3: New data model for Observations, new Category entity\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [1f279e50-9b05-4d17-a2af-22177d8f091a]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/1f279e50-9b05-4d17-a2af-22177d8f091a/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🍀\tMarch 2024\n",
            "\n",
            "# March 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Claude 3 Haiku model.\n",
            "- Added support for direct ingestion of Base64 encoded files with the ingestEncodedFile mutation. You can pass a Base64 encoded string and MIME type of the file, and it will be ingested into the Graphlit Platform.\n",
            "- Added modelService and model properties to ConversationMessage type, which return the model service and model which was used for the LLM completion.\n",
            "\n",
            "PreviousMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "NextMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "Last updated7 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [3fcc7bb7-b065-417b-9638-7e746a206271]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/3fcc7bb7-b065-417b-9638-7e746a206271/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎆\tJanuary 2024\n",
            "\n",
            "# January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Google and Microsoft email feeds. Email feeds can be created to ingest past emails, or poll for new emails. Emails create an EMAIL content type. Attachment files can optionally be extracted from emails, and will be linked to their parent email content. If assigning a workflow to the feed, the workflow will be applied both to the email content and the extracted attachment files.\n",
            "- 💡 Graphlit now supports reingesting content in-place. The ingestText, ingestPage and ingestFile mutations now take an optional id parameter for an existing content object. If this id is provided, the existing content will be updated from the provided text or URI source, and will restart the assigned workflow.\n",
            "- Added restartAllContents mutation to restart workflow on all partially-ingested contents in project.\n",
            "- Added text field to ConversationCitation type, which returns the relevant text from the content source with the citation.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1313: Not extracting links from HTML\n",
            "- GPLA-2030: No text extracted from shapes in PPTX files\n",
            "\n",
            "PreviousFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "NextJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [88f0827d-ef2c-440c-b135-12baae1564cd]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/88f0827d-ef2c-440c-b135-12baae1564cd/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎆\tJanuary 2024\n",
            "\n",
            "# January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports content publishing, where documents, audio transcripts and even image descriptions, can be summarized, and repurposed into blog posts, emails or AI-generated podcasts. With the new publishContents mutation, you can configure LLM prompts for summarization and publishing, and assign specifications to use different models and/or system prompts for each step in the process. The published content will be reingested into Graphlit, and can be searched or used for conversations, like any other form of content.\n",
            "- 💡 Graphlit now supports publishing conversations as content with the new publishConversation mutation. You can generate text or audio transcripts of your conversations, to be reused in other tools.\n",
            "- 💡 Graphlit now supports bulk summarization of contents with the summarizeContents mutation. You can filter a set of content, by feed, by observable or by similar text, and run a set of summarizations across each content in parallel.\n",
            "- 💡 Graphlit now supports LLM entity extraction, with the new MODEL_TEXT entity extraction service type. Similar to using Azure Cognitive Service Text Analytics, you can use any OpenAI or Anthropic model for extracting entities from text. Internally the LLM returns JSON-LD entities, which we convert into Person, Organization, Place, etc. entities and assign observations to the extracted content.\n",
            "- 💡 Graphlit now supports LLM tools (aka function calls) with OpenAI models. You can define the tools to be used with the LLM in the specification object. With the new extractContents mutation, you can execute a prompt against content using a specification with tools defined. The mutation will return the JSON arguments assigned by the LLM.\n",
            "- 💡 Graphlit now supports callback webhooks for LLM tools. If you assign a URI in the ToolDefinition object, Graphlit will call your webhook the tool name and JSON arguments. When you respond to the webhook with JSON, we will add that response to the LLM messages, and ask the LLM to complete the original prompt.\n",
            "- 💡 Graphlit now supports the selection of the Deepgram model (such as Meeting, Phonecall or Finance) with the preparation workflow. Also, you can assign your own Deepgram API key, which will be used for audio transcription using that workflow.\n",
            "- Added support for CLIP image embeddings using Roboflow, which can be used for similar image search. If you search for contents by similar contents, we will now use the content's text and/or image embeddings to find similar content.\n",
            "- Added support for dynamic web page ingestion. Graphlit now navigates to and automatically scrolls web pages using Browserless.io, so we capture the fully rendered HTML before extracting text. Also, we now support web page screenshots, if enabled with enableImageAnalysis property in preparation workflow. These screenshots can be analyzed with multimodal modals, such as GPT-4 Vision, or can be used to create image embeddings for similar image search.\n",
            "- Added table parsing when preparing documents. We now store structured (tab-delimited) text in the JSON text mezzanine which is extracted from documents in the preparation workflow.\n",
            "- Added reverse geocoding of lat/long locations found in image or other content metadata. We now store the real-world address with the content metadata, for use in conversations.\n",
            "- Added assistant messages to the conversation message history provided to the LLM. Originally we had included only user messages, but now we are formatting both user and assistant messages into the LLM prompt for conversations.\n",
            "- Added new chunking algorithm for text embeddings. We support semantic chunking at the page or transcript segment level, and now will create embeddings from smaller sized text chunks per page or segment.\n",
            "- Added content metadata to text and image embeddings. To provide better context for the text embeddings, we now include formatted content metadata, which includes fields like title, subject, author, or description. For emails, we include to, from, cc, and bcc fields.\n",
            "- Added helper mutations isContentDone and isFeedDone which can be used for polling completion of ingested content, or all content ingested by a feed.\n",
            "- Added richer image descriptions generated by the GPT-4 Vision model. Now these provide more useful detail.\n",
            "- Added validation of extracted hyperlinks. Now we test the URIs and remove any inaccessible links during content enrichment.\n",
            "- Added deleteContents, deleteFeeds, and deleteConversations mutations for multi-deletion of contents, feeds or conversations.\n",
            "- Added deleteAllContents, deleteAllFeeds, and deleteAllConversations mutations for bulk, filtered deletion of entities. You can delete all your contents, feeds, or conversations in your project, or a filtered subset of those entities.\n",
            "- ℹ️ Starter tier now has a higher content limit of 100K content items.\n",
            "- ⚡ In the OpenAIImageExtractionProperties type, the detailMode field was renamed to detailLevel.\n",
            "- ⚡ Each SummarizationStrategy object now accepts the specification which is used by the summarization, rather than being assigned at the preparation workflow stage.\n",
            "- ⚡ addCollectionContents and removeCollectionContents mutations have been deprecated in favor of addContentsToCollections and removeContentsFromCollection mutations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1846: Parse Markdown headings into mezzanine JSON\n",
            "- GPLA-1779: Not returning SAS token with mezzanine, master URIs\n",
            "- GPLA-1348: Summarize text content, not just file content\n",
            "- GPLA-1297: Not assigning content error message on preparation workflow failure\n",
            "\n",
            "PreviousJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "NextDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "\n",
            "---\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [0490de5f-5c5d-4636-b76a-2b91ba640fe8]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/0490de5f-5c5d-4636-b76a-2b91ba640fe8/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🍀\tMarch 2024\n",
            "\n",
            "# March 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports a Command-Line Interface (CLI) for directly accessing the Graphlit Data API without writing code. See the documentation here.\n",
            "- 💡 Graphlit now supports the Groq Platform, and models such as Mixtral 8x7b.\n",
            "- 💡 Graphlit now supports Claude 3 Opus and Sonnet models.\n",
            "- 💡 Graphlit now supports Mistral La Plateforme, and models such as Mistral Small, Medium, and Large and Mixtral 8x7b.\n",
            "- 💡 Graphlit now supports the latest v4 of Azure Document Intelligence, including their new models such as Credit Card, Marriage Certificate, and Mortgage documents.\n",
            "- Added support for detailed usage and credits telemetry via API, with the usage, credits, lookupUsage and lookupCredits queries.\n",
            "- Added support for correlated telemetry, where an optional correlationId can be provided with GraphQL queries and mutations, so credits and usage can be tracked across requests.\n",
            "- Added support for project webhook, which will be called when credits have been consumed by the project.\n",
            "- Added support for image extraction during DOCX, XLSX, and PPTX document preparation.\n",
            "- Added text and markdown properties to Content object, which provide formatted output of extracted text from any content.\n",
            "- Added more accurate extraction of tables into mezzanine JSON format, across all content types.\n",
            "- Added throughput property to Conversation messages, which returns the tokens/second throughput of LLM.\n",
            "- ⚡ Deprecated mezzanineUri property in Content object, which has been replaced by textUri and audioUri.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2281: Not extracting table from PPTX file.\n",
            "- GPLA-2282: Not extracting Markdown tables.\n",
            "- GPLA-2247: Not extracting relative HTML links properly.\n",
            "- GPLA-2241: Failed to post Alert to Slack with Markdown format.\n",
            "\n",
            "PreviousMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "NextFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "Last updated7 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [259e0a23-5ac8-49b8-b180-593fec210fdc]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/259e0a23-5ac8-49b8-b180-593fec210fdc/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🌧️\tFebruary 2024\n",
            "\n",
            "# February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports OneDrive and Google Drive feeds. Files can be ingested from OneDrive or Google Drive, including shared drives where the authenticated user has access. Both OneDrive and Google Drive support the reading of existing files, and tracking new files added to storage with recurrent feeds.\n",
            "- 💡 Graphlit now supports email backup files, such as EML or MSG, which will be assigned the EMAIL file type. During email file preparation, we will automatically extract and ingest any file attachments.\n",
            "- 💡 Graphlit now automatically extracts embedded images in PDF files, ingests them as content objects, and links them as children of the parent PDF.\n",
            "- 💡 Graphlit now supports recursive Notion feeds. When the isRecursive flag is true in the Notion feed properties, we will crawl child pages and databases, and recursively ingest them in addition to the specified pages and databases.\n",
            "- Added support for assigning collections to content ingested with the ingestPage, ingestFile or ingestText mutations. This saves a step where the content will automatically be added to the collection(s) without requiring another mutation call.\n",
            "- Added support for the CODE file type for a wide variety of source code formats, i.e. Python .py, Javascript .js. Code files use optimized text splitting for enhanced search and retrieval.\n",
            "- Added support for customGuidance in Specification object, which can be used for injecting a guidance prompt during the RAG process. For example, you can instruct the LLM to return a default response string if no content sources are found via semantic search.\n",
            "- Added tenants field to Project object, which returns a list of all tenant IDs which have been used to create an entity in Graphlit.\n",
            "- Added email metadata, separate from document metadata. Now emails will contain indexed metadata such as to, from, or subject.\n",
            "- ⚡ The contents field for content objects has been replaced with children and parent fields. For example, when a ZIP file is unpacked, the unpacked files will be added as children of the ZIP file, and the ZIP file will be the parent of each of the unpacked files.\n",
            "- ⚡ Removed enableImageAnalysis field from image preparation properties in workflow object. Now is enabled by default.\n",
            "- ⚡ Moved disableSmartCapture field to preparation workflow stage from page preparation properties. This is used to disable the use of headless Chrome browser to capture HTML from web pages. It is enabled by default, and if disabled, Graphlit will simply download the HTML from the web page rather than rendering on headless Chrome browser.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2099: Failed to ingest ArXiV PDF. Fixed PDF parsing error.\n",
            "- GPLA-2174: LLM response is incorrect with conversation history, but no content sources.\n",
            "- GPLA-2199: ZIP package left in Indexed state after content workflow.\n",
            "\n",
            "PreviousMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "NextFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "Last updated9 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [14614eae-5e42-4912-aa53-2ce05a1ce2d2]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/14614eae-5e42-4912-aa53-2ce05a1ce2d2/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎓\tJune 2024\n",
            "\n",
            "# June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Deepseek LLMs for prompt completion. We offer the deepseek-chat and deepseek-coder models.\n",
            "- 💡 Graphlit now supports parsing embedded JSON-LD from web pages. If a web page contains 'script' tags with JSON-LD, we will automatically parse and inject into the knowledge graph.\n",
            "- ⚡ We have changed the default model for entity extraction and image completions to be OpenAI GPT-4o. This provides faster performance and better quality output.\n",
            "- ⚡ We have changed the behavior of knowledge graph generation, from a prompted conversation, to be opt-in. In order to receive the graph's nodes and edges with the response, you will now need to set generateGraph to True in the specification's graphStrategy object. This provides improved performance when the graph is not needed for visualization.\n",
            "- Added thing property for observable entities, which returns the JSON-LD metadata associated with the entity.\n",
            "- Added regex-based filtering for URI paths during feed ingestion, link crawling, and workflow filtering. You can assign regex patterns in allowedPaths and excludedPaths.\n",
            "- Added observableLimit to configure the limit of how many observed entities will be added to the GraphRAG context, defaults to 1000.\n",
            "- Added prompt to suggestConversation mutation, which allows customization of the followup question generation.\n",
            "- Updated suggestConversation to incorporate the past conversation message history, in addition to the filtered set of content sources.\n",
            "- 🔥 We have improved performance in knowledge graph retrieval and generation, via better parallelization and batching.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2748: Optimize the retrieval performance of observed entities during GraphRAG\n",
            "- GPLA-2732: Invalid user-provided URI causing parsing exception\n",
            "- GPLA-2666: Shouldn't require tenant ID for Microsoft email or Teams\n",
            "- GPLA-2772: Not returning labels or categories from graph in API\n",
            "- GPLA-2762: Failed to extract spreadsheet images\n",
            "- GPLA-2687: Email to/from not getting added as observations on emails\n",
            "- GPLA-2738: API is returning 'audio' metadata from podcast HTML document\n",
            "\n",
            "PreviousJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "NextMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "Last updated6 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/cf01faf7-c155-4d70-8dcc-44c97cbf7c8b/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🌧️\tFebruary 2024\n",
            "\n",
            "# February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Semantic Alerts, which allows for LLM summarization and publishing of content, on a periodic basis. This is useful for generating daily reports from email, Slack or other time-based feeds. Alerts support the same publishing options, i.e. audio and text, as the publishContents mutation.\n",
            "- 💡 Graphlit now supports the latest OpenAI 0125 model versions, for GPT-4 and GPT-3.5 Turbo. We will add support for Azure OpenAI when Microsoft releases support for these.\n",
            "- Slack feeds now support a listing type field, where you can specify if you want PAST or NEW Slack messages in the feed.\n",
            "- 🔥 This release provides many performance enhancements, which will speed up the content workflows for ingested content.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2114: Collections not being added to text embedding index documents.\n",
            "- GPLA-2063: Not handling hallucinated citations.\n",
            "- GPLA-1916: Collections not inherited from project-scope into tenant-scope.\n",
            "- GPLA-2105: Should error on add/remove of contents to/from collections if content does not exist.\n",
            "\n",
            "PreviousFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "NextJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "Last updated10 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [6da2b811-9ec9-484c-a102-a4a7b4a89199]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/6da2b811-9ec9-484c-a102-a4a7b4a89199/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎓\tJune 2024\n",
            "\n",
            "# June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Anthropic Claude 3.5 Sonnet model, which can be assigned with the CLAUDE_3_5_SONNET model enum.\n",
            "- 💡 Graphlit now supports semantic search of observable entities in the knowledge graph, such as Person, Organization and Place. These entity types will now have vector embeddings created from their enriched metadata, and support searching by similar text, and searching by similar entities.\n",
            "- ⚡ We have changed the Google Drive and Google Email feed properties to require the Google OAuth client ID and client secret, along with the existing refresh token, for proper authentication against Google APIs.\n",
            "- ⚡ We have added a credits quota on the Free Tier. Once 1000 credits have been used on the Free Tier, no more content can be ingested, and an upgrade to a paid tier is required. Customers will receive an email when the credits, storage or contents quota has been reached.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2837: Failed to ingest LinkedIn page as Web feed\n",
            "- GPLA-2831: Zero-byte file was left in Indexed state\n",
            "- GPLA-2834: Not reading any files from Azure blob feed with space in prefix\n",
            "- GPLA-2828: Better handling for files with unknown (or missing) file extensions\n",
            "\n",
            "PreviousJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "NextJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "Last updated6 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [0457d83e-b449-40c8-9544-c268a114e122]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/0457d83e-b449-40c8-9544-c268a114e122/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "☀️\tJuly 2024\n",
            "\n",
            "# July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports webhook Alerts. In addition to Slack notifications, you can now receive an HTTP POST webhook with the results of the published text (or text and audio URI) from a prompted alert.\n",
            "- Updated the Deepseek chat and coder models to support a 128k token context window.\n",
            "- Added customSummary property to Content object, which returns the custom summary generated via preparation workflow.\n",
            "- Added keywords summarization type, which is now stored in keywords property in Content object.\n",
            "- Added slackChannels query, which returns the list of Slack channels from the workspace authenticated by the Slack bot token.\n",
            "- ⚡ We have changed the response from the credits query to return a single ProjectCredits object, rather than the list of correlated objects previously returned. The credits response now covers all credit usage over the time period specified.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2874: Processing entities is taking longer than 30min for 300+ page PDF\n",
            "- GPLA-2875: Messages in queue expiring too early\n",
            "- GPLA-2881: Feed read count increasing, after hitting read limit\n",
            "- GPLA-2884: Need to handle Anthropic 'overloaded' API response\n",
            "- GPLA-2906: JIRA issue identifier not assigned to issue metadata\n",
            "\n",
            "PreviousJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "NextJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [4830e2a8-ea48-4f1e-878c-3bf37909c275]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/4830e2a8-ea48-4f1e-878c-3bf37909c275/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎄\tDecember 2024\n",
            "\n",
            "# December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports mapping a website with the mapWebmutation. You can provide a URL to a website, and the query will return a list of URLs based on the sitemap.xml (or sitemap-index.xml) file, at or underneath the provided URL.\n",
            "- 💡 Graphlit now supports the generation of web page screenshots with the screenshotPagemutation. By providing the URL of a web page, and optionally, the maximum desired height of the screenshot, we will screenshot the webpage and ingest it automatically as content. You can provide an optional workflow, which will be applied to the ingested image content, for operations like generating image descriptions with a vision LLM.\n",
            "- 💡 Graphlit now supports the direct summarization of text with the summarizeTextmutation. By providing the desired summarization strategy, we will summarize the text (i.e. bullet points, social media posts) and return the summarization.\n",
            "- 💡 Graphlit now supports the direct extraction of text with the extractTextmutation. By providing the LLM tool definitions and an optional LLM specification, we will prompt the desired LLM (or OpenAI GPT-4o, by default) to invoke the provided tools, and return the JSON responses from the LLM tool calling.\n",
            "- Graphlit now supports the latest Groq Llama 3.3 model, with the model enum LLAMA_3_3_70B.\n",
            "- We have updated Cohere reranking to use the latest Cohere rerank-v3.5model by default.\n",
            "- ⚡ We have added a new flattenCitationsfield to the ConversationStrategyInputtype. By assigning this field to True, when calling promptConversation,we will combine multiple citations from the same content into a single citation.\n",
            "- ⚡ For Microsoft email, Microsoft Teams and OneDrive feeds, we have added the clientIdand clientSecretfields as required feed properties. These properties must be assigned, in addition to the refreshTokenfield for proper authentication to the Microsoft Graph API used by these feeds. (Colab Notebook Example)\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3492: Not finding sitemap at parent web path\n",
            "- GPLA-3500: Failed to handle mismatch of Deepgram model/language\n",
            "\n",
            "PreviousDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "NextDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "Last updated17 days ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [a69b7ff1-2c57-4b20-894b-0c924bebad57]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/a69b7ff1-2c57-4b20-894b-0c924bebad57/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "☀️\tJuly 2024\n",
            "\n",
            "# July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "### New Features\n",
            "\n",
            "- Added indexing workflow stage. This provides for configuration of indexing services, which may infer metadata from the content.\n",
            "- Added AZURE_AI_LANGUAGE content indexing service, which supports inferring the language of extracted text or transcript.\n",
            "- Added support for language content metadata. This returns a list of languages in ISO 639-1 format, which may have been inferred from the extracted text or transcript.\n",
            "- Added support for MODEL_IMAGE extraction service. This provides integration with vision models beyond those provided by OpenAI. You can assign a custom specification and bring-your-own API key for image extraction models.\n",
            "- ⚡ We have deprecated the OPENAI_IMAGE service type, and developers should now use the LLM image service instead.\n",
            "- ⚡ We have removed the language field from AudioMetadata type, which has been replaced by the new LanguageMetadata type.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2987: Extracting text with Azure Doc Intelligence does not extract hyperlinks\n",
            "\n",
            "PreviousAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "NextJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [7694d101-376d-4cbe-a773-3b64002727bf]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/7694d101-376d-4cbe-a773-3b64002727bf/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎄\tDecember 2024\n",
            "\n",
            "# December 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports formatting of LLM-ready prompts with our RAG pipeline, via the new formatConversation and completeConversation mutations. This is valuable for supporting LLM streaming by directly calling the LLM from your application, and using Graphlit for RAG retrieval and conversation history. (Colab Notebook Example)\n",
            "- We have added support for inline hyperlinks in extracted text from documents and web pages.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3466: Owner ID should accept any non-whitespace string\n",
            "- GPLA-3458: Not getting Person-to-Organization edges from entity extraction\n",
            "\n",
            "PreviousDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "NextNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "Last updated26 days ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [53a80fc0-7526-4cb8-a921-b66f0c941380]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/53a80fc0-7526-4cb8-a921-b66f0c941380/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎄\tDecember 2023\n",
            "\n",
            "# December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the OpenAI GPT-4 Turbo 128k model, both in Azure OpenAI and native OpenAI services. Added new model enum GPT4_TURBO_VISION_128K.\n",
            "- 💡 Graphlit now supports Llama 2 7b, 13b, 70b models and Mistral 7b model, via Replicate. Developers can use their own Replicate API key, or be charged as credits for Graphlit usage.\n",
            "- 💡 Graphlit now supports the Anthropic Claude 2.1 model. Added new model enum CLAUDE_2_1.\n",
            "- 💡 Graphlit now supports the OpenAI GPT-4 Vision model for image descriptions and text extraction. Added new model enum GPT4_TURBO_VISION_128K. See usage example in \"Multimodal RAG\" blog post.\n",
            "- Added query by example to contents query. Developers can specify one or more example contents, and query will use vector embeddings to return similar contents.\n",
            "- Added query by example to conversations query. Developers can specify one or more example conversations, and query will use vector embeddings to return similar conversations.\n",
            "- Added vector search support for conversations queries. Developers can provide search text which will use vector embeddings to return similar conversations.\n",
            "- Added promptSpecifications mutation for directly prompting multiple models. This can be used to evaluate prompts against multiple models or compare different specification parameters in parallel.\n",
            "- Added promptStrategy field to Specification, which supports multiple strategy types for preprocessing the prompt before being sent to the LLM model. For example, REWRITE prompt strategy will ask LLM to rewrite the incoming user prompt based on the previous conversation messages.\n",
            "- Added suggestConversation mutation, which returns a list of suggested followup questions based on the specified conversation and related contents. This can be used to auto-suggest questions for chatbot users.\n",
            "- Added new summarization types: CHAPTERS, QUESTIONS and POSTS. See usage examples in the \"LLMs for Podcasters\" blog post.\n",
            "- Added versioned model enums such as GPT4_0613 and GPT35_TURBO_16K_1106. Without version specified, such as GPT35_TURBO_16K, Graphlit will use the latest production model version, as defined by the LLM vendor.\n",
            "- Added lookupContents query to get multiple contents by id in one query.\n",
            "- ⚡ In Content type, headline field was renamed to headlines and now returns an array of strings.\n",
            "- ⚡ Entity names are now limited to 1024 characters. Names will be truncated if they exceed the maximum length.\n",
            "- ⚡ In SummarizationTypes enum, BULLET_POINTS was renamed to BULLETS.\n",
            "- ⚡ In ProjectStorage type, originalTotalSize was renamed to totalSize, and totalRenditionSize field was added. totalSize is the sum of the ingested source file sizes, and totalRenditionSize is the sum of the source file sizes and any derived rendition sizes.\n",
            "- ⚡ In ConversationStrategy type, strategyType was renamed to type for consistency with rest of data model.\n",
            "- ⚡ In Specification type, optimizeSearchConversation was removed, and now is handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1725: Should ignore RSS.xml from web feed sitemap\n",
            "- GPLA-1726: GPT-3.5 Turbo 16k LLM is adding \"Citation #\" to response\n",
            "- GPLA-1698: Workflow not applied to link-crawled content\n",
            "- GPLA-1692: Mismatched project storage total size, when some content has errored\n",
            "- GPLA-1237: Add relevance threshold for semantic search\n",
            "\n",
            "PreviousJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "NextOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "Last updated11 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "☀️\tJuly 2024\n",
            "\n",
            "# July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Mistral Large 2 and Mistral Nemo models. The existing MISTRAL_LARGE model enum now will use Mistral Large 2.\n",
            "- 💡 Graphlit now supports the Llama 3.1 8b, 70b and 405b models on Groq. (Note, these are rate-limited according to Groq's platform constraints.)\n",
            "- Added support for revision strategy on data extraction specifications. Now you can prompt the LLM to revise its previous data extraction response, similar to the existing completion revision strategy.\n",
            "- Added version property for AzureDocumentPreparationProperties type for assigning the API version used by Azure AI Document Intelligence. By default, Graphlit will continue to use the v4.0 (Preview) API version, but you can override this to assign version to V2023_10_31 to use the v3.1 (GA) API version instead. For some documents, the General Availability (GA) version of the API can provide better results.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2988: Not extracting hyperlinks from Office documents.\n",
            "\n",
            "PreviousJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "NextJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [ede9b530-8fd4-46c0-9dd1-05ce0514323d]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/ede9b530-8fd4-46c0-9dd1-05ce0514323d/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎂\tAugust 2024\n",
            "\n",
            "# August 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "- 💡 Graphlit now supports medical-related entities in GraphRAG, and via API for queries and mutations.\n",
            "- Added support for Anthropic prompt caching. When using Anthropic Sonnet 3.5 or Haiku 3, Anthropic will now cache the entity extraction and LLM document preparation system prompts, which saves on token cost and increases performance.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3104: Should default search type to VECTOR, when performing entity similarity filter.\n",
            "- GPLA-3112: Empty PDF fails entity extraction.\n",
            "\n",
            "PreviousSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "NextAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "Last updated4 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [17831936-40db-4f70-becd-30a9499bd571]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/17831936-40db-4f70-becd-30a9499bd571/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "☀️\tJuly 2024\n",
            "\n",
            "# July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the OpenAI GPT-4o Mini model, with 16k output tokens.\n",
            "- 💡 Graphlit now supports 'bring-your-own-key' for Azure AI Document Intelligence models. We have added a custom endpoint and key property, which can be assigned to use your own Azure AI resource.\n",
            "- Updated to use Jina reranker v2 (jina-reranker-v2-base-multilingual) by default.\n",
            "- Updated to assign the summary, bullets, etc properties when calling summarizeContents mutation. Now when summarizing contents, we will store the resulting summary in the content itself, in addition to returning the summarized results.\n",
            "- Added relevance property to all entity types, which will be assigned when searching for these entities. Entity results will be sorted (descending) by this search relevance score.\n",
            "- Added the ability to manually update summary, bullets, etc. properties when calling the updateContent mutation.\n",
            "- Added offset property to AtlassianJiraFeedProperties, so the timezone offset can be properly assigned for paging of the Jira feed. (Defaults to zero offset, i.e. UTC.) Jira does not store dates in UTC format, and the timestamps are based on the server timezone of the hosted Jira instance. By assigning the timezone offset with the Jira feed, we can reliably page the updated date timestamps from the Jira API.\n",
            "- ⚡ We have changed the content similarity search behavior to find similar content by summary, rather than text of the document, when a summary has been previously generated. For long documents, this will provide a more accurate similarity, rather than comparing on the first few pages of text in a document.\n",
            "- ⚡ We have changed the behavior of assigning offset in the entity filter objects for paging through entities. If using vector or hybrid search, this offset will be ignored (i.e. zero offset). Paging will not be supported through vector or hybrid search results. For keyword search, the offset will continue to be used, along with the limit property, to provide paging through the search results. We have made this change because we have found that index-based paging is not reliable with our vector/hybrid search approach. We are investigating ways to support this reliably with vector/hybrid search in the future.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2915: Add retry on OpenAI API HTTP 524 error (gateway timeout).\n",
            "- GPLA-2908: Not paging through Jira feed correctly.\n",
            "- GPLA-2917: Search by similar content is not giving expected results on long documents.\n",
            "- GPLA-2244: Keyword search not finding text in latter part of long PDF.\n",
            "\n",
            "PreviousJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "NextJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [7ce5f9ae-f13c-4832-8c74-55c8bac42caf]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/7ce5f9ae-f13c-4832-8c74-55c8bac42caf/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎂\tAugust 2024\n",
            "\n",
            "# August 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports LLM-based document preparation, using vision-capable models such as OpenAI GPT-4o and Anthropic Sonnet 3.5. This is available via the MODEL_DOCUMENT preparation service type, and you can assign a customspecification object and bring your own LLM keys.\n",
            "- 💡 Graphlit now provides an open source .NET SDK, supporting .NET 6 and .NET 8 (and above). SDK package can be found on Nuget.org. Code samples can be found on GitHub.\n",
            "- Added identifier property to Content object for mapping content to external database identifiers. This is supported for content filtering as well.\n",
            "- Added support for Claude 3 vision models for image-based entity extraction, using the MODEL_IMAGE entity extraction service.\n",
            "- Added context augmentation to conversations, via the augmentedFilter property on the Conversation object. Any content which matches this augmented filter will be injected into the LLM prompt content, without needing to be related by vector similarity to the user prompt. This is useful for specifying domain knowledge which should always be referenced by the RAG pipeline.\n",
            "- Added support for the latest snapshot of OpenAI GPT-4o, with the model enum GPT4O_128K_20240806.\n",
            "- Added reranking of related entities, when preparing the LLM prompt context for GraphRAG. If reranking is enabled, the metadata from the related entities will be reranked with the same reranker assigned to the conversation specification.\n",
            "- ⚡ We have changed the type of the duration field in the AudioMetadata and VideoMetadata types to be TimeSpan rather than string, as to be more consistent with the rest of the API data model.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2884: Support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "\n",
            "PreviousAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "NextJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "Last updated4 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f4865d73-f760-47d7-b918-23295e6ad2af]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/f4865d73-f760-47d7-b918-23295e6ad2af/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎂\tAugust 2024\n",
            "\n",
            "# August 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "### New Features\n",
            "\n",
            "- Added support for language-aware summaries when using LLM-based document extraction. Now the summaries for tables and sections generated by the LLM will follow the language of the source text.\n",
            "- Added support for language-aware entity descriptions with using LLM-based entity extraction. Now the entity descriptions generated by the LLM will follow the language of the source text.\n",
            "- ⚡ We have changed the default document preparation method to use Azure AI Document Intelligence, rather than our built-in document parsers. We have found that the fidelity of Azure AI is considerably better for complex PDFs, and provides better support for table extraction, so we have made this the default. Note: this does come with increased credit usage per-page, for PDF, DOCX and PPTX documents, but the quality of the extracted documents are noticeably higher for use in RAG pipelines.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3070: Not getting slide count assigned to metadata for PPTX files.\n",
            "\n",
            "PreviousAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "NextAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "Last updated4 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [eec9e87f-091c-4ba9-b7d9-b8b21a41b458]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/eec9e87f-091c-4ba9-b7d9-b8b21a41b458/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎂\tAugust 2023\n",
            "\n",
            "# August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "### New Features\n",
            "\n",
            "- 💡 Added ingestText mutation which supports direct Content ingestion of plain text, Markdown and HTML. Now, if you have pre-scraped HTML or Markdown text, you can ingest it into Graphlit without reading from a URL.\n",
            "- 💡 Added Specification strategy property, which allows customization of the LLM context when prompting a conversation. ConversationStrategy now provides Windowed and Summarized message histories, as well as configuration of the weight between existing conversation messages and Content text pages (or audio transcript segments) in the LLM context.\n",
            "- 💡 Added auto-summarization of extracted text and audio transcripts. There is a new Content summary property where a list of summary bullet points can be found. These summaries can be optionally included in the Conversation prompt context for more accurate LLM responses.\n",
            "- ℹ️ Added AzureOpenAIModels and OpenAIModels types to Specification model properties to make it easier to specify the desired LLM.\n",
            "- ℹ️ Renamed ConversationMessage date property to timestamp\n",
            "- ✨ Refined the internal LLM prompts for providing content as part of Conversation context. This provides for much clearer and accurate results from the LLM.\n",
            "\n",
            "PreviousAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "NextAugust 3: New data model for Observations, new Category entity\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [7013d1b5-eaa6-4f63-b9f5-973fb33abad4]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/7013d1b5-eaa6-4f63-b9f5-973fb33abad4/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🐇\tApril 2024\n",
            "\n",
            "# April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports a native Python SDK, using Pydantic types. The Python SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest PyPi package here. The Streamlit sample applications have been updated to use the new Python SDK.\n",
            "- 💡 Graphlit now supports a native Node.js SDK, using TypeScript types. The Node.js SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest NPM package here.\n",
            "- 💡 Graphlit now supports the 2024-04-09 models in the OpenAI model service. GPT4_TURBO-128K will give the latest OpenAI GPT-4 model, following this model list. We have added the GPT4_TURBO_128K_2024_04_09 enum to specify the new model.\n",
            "- 💡 Graphlit now supports LLaMA3 70b, LLaMA3 8b and Gemma 7b models in the Groq model service.\n",
            "- 💡 Graphlit now supports the Command R and Command-R+ models in the Cohere model service.\n",
            "- Added support for Jina reranking, using the JINA reranking model service type in the reranking retrieval strategy.\n",
            "- Updated the Cohere reranking model to use the latest v3.0 model.\n",
            "- Increased the reliability of parsing LLM responses, in cases where they don't follow the JSON schema.\n",
            "- ⚡ Cleaned up nullability of GraphQL parameters, so parameters better reflect if they are required or optional, or allow nulls.\n",
            "- ⚡ Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "- ⚡ Split out reranking model service type as RetrievalModelServiceTypes enum.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2114: Adding content to collections not syncing search index\n",
            "- GPLA-2511: Failing to render any conversation sources with section retrieval and text content\n",
            "\n",
            "PreviousMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "NextApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [bdad98c7-3cd1-4a67-af1c-334852ad7ce2]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/bdad98c7-3cd1-4a67-af1c-334852ad7ce2/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🐇\tApril 2024\n",
            "\n",
            "# April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Discord feeds. By connecting to a Discord channel and providing a bot token, you can ingest all Discord messages and file attachments.\n",
            "- 💡 Graphlit now supports Cohere reranking after content retrieval in RAG pipeline. You can optionally use the Cohere rerank model to semantically rerank the semantic search results, before providing as context to the LLM.\n",
            "- Added support for section-aware text chunking and retrieval. Now, when using section-aware document preparation, such as Azure AI Document Intelligence, Graphlit will store the extracted text according to the semantic chunks (i.e. sections). The text for each section will be individually chunked and embedded into the vector index.\n",
            "- Added support for retrievalStrategy in Specification type. Graphlit now supports CHUNK, SECTION and CONTENT retrieval strategies. Chunk retrieval will use the search hit chunk, section retrieval will expand the search hit chunk to the containing section (or page, if not using section-aware preparation). Content retrieval will expand the search hit chunk to the text of the entire document.\n",
            "- Added support for rerankingStrategy in Specification type. You can now configure the reranking of content sources, using the Cohere reranking model, by assigning serviceType to COHERE. More reranking models are planned for the future.\n",
            "- Added isSynchronous flag to content ingestion mutations, such as ingestUri, so the mutation will wait for the content to complete the ingestion workflow (or error) before returning. This is useful for utilizing the API in a Jupyter notebook or Streamlit application, in a synchronous manner without polling.\n",
            "- Added includeAttachments flag to SlackFeedProperties. When enabled, Graphlit will automatically ingest any attachments within Slack messages.\n",
            "- ⚡ Added ingestUri mutation to replace the now deprecated ingestPage and ingestFile mutations. We had seen confusion on when to use one vs the other, and now for any URI, whether it is a web page or hosted PDF, you can pass it to ingestUri, and we will infer the correct content ingestion workflow.\n",
            "- ⚡ Removed includeSummaries from the ConversationStrategyInput type. This will re-added in the future as part of the retrieval strategy.\n",
            "- ⚡ Deprecated enableExpandedRetrieval in ConversationStrategyInput type. This is now handled by setting strategyType to SECTION or CONTENT in the RetrievalStrategyInput type.\n",
            "- ⚡ Moved contentLimit from ConversationStrategyInput type to RetrievalStrategyInput type. You can optionally assign the contentLimit to retrievalStrategy which limits the number of content sources leveraged in the LLM prompt context. (Default is 100.)\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2469: Failed to ingest PDF hosted on GitHub\n",
            "- GPLA-2390: Claude 3 Haiku not adhering to JSON schema\n",
            "- GPLA-2474: Prompt rewriting should ignore formatting instructions in prompt\n",
            "- GPLA-2462: Missing line break after table rows\n",
            "- GPLA-2417: Not extracting images from PPTX correctly\n",
            "\n",
            "PreviousApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "NextMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [a797f749-b15b-4d03-9543-92e26c2bc922]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/a797f749-b15b-4d03-9543-92e26c2bc922/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎂\tAugust 2023\n",
            "\n",
            "# August 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "### New Features\n",
            "\n",
            "- ℹ️ Behind the scenes, Graphlit is preparing to launch usage-based billing. This release put in place the infrastructure to track billable events. Organizations now have a Stripe customer associated with them, and Graphlit projects are auto-subscribed to a Free/Hobby pricing plan. In a future release, we will provide the ability to upgrade to a paid plan in the Graphlit Developer Portal. Also, we will provide visualization of usage, on granular basis, in the Portal.\n",
            "- 💡 Content URIs now have Shared Access Signature (SAS) token appended, so they are accessible after query. For example, content.transcriptUri will now be able to be downloaded or used directly in an application (until the SAS token expires).\n",
            "- 🧱 Added more robustness for error handling and retries, especially for LLM APIs and audio transcription APIs.\n",
            "\n",
            "PreviousSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "NextAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f575eec5-7ccd-47dd-a130-489b47cce8c4]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/f575eec5-7ccd-47dd-a130-489b47cce8c4/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎂\tAugust 2023\n",
            "\n",
            "# August 3: New data model for Observations, new Category entity\n",
            "### New Features\n",
            "\n",
            "- 💡 Revised data model for Observations, Occurrences and observables (i.e. Person, Organization). Now after entity extraction, content will have one Observation for each observed entity, and a list of occurrences. Occurrence now supports text, time and image occurrence types. (Text: page index, time: start/end timestamp, image: bounding box) Observations now have ObservableType and Observable fields, which specify the observed entity type and entity reference.\n",
            "- 💡 Added Category entity to GraphQL data model, which supports PII categories such as Phone Number or Credit Card Number.\n",
            "- Added probability field to model properties, for the LLM's token probability. (See OpenAI documentation for more detail.)\n",
            "- Added error field to feeds. If a feed fails to read from the data source, and is marked as ERRORED state, the error field will have the error description.\n",
            "- Support reingestion of changed files from feeds. For feeds, such as SharePoint or Web, where we can recognize that a file or page was updated, we will now reingest the content in-place. Content will keep the same ID, and will restart the content workflow by re-downloading the updated content from the data source. Existing observations will be deleted, and new observations will be created from the updated content.\n",
            "- ℹ️ Ingestion of content is now idempotent, meaning if you ingest content again from the same URI, we will reingest the content in-place, while keeping the same ID. (If we can recognize the content has not changed, such as by ETag, we will return the existing content object.)\n",
            "- ℹ️ Changed GraphQL data type of SharePoint tenantId, libraryId and siteId to ID rather than String.\n",
            "- ✨ Performance optimization of entity extraction, and the creation of observations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1130: Only was extracting text from first column of PDF tables.\n",
            "- GPLA-1140: Text from DOCX tables was not extracted properly.\n",
            "- GPLA-1154: Audio content ingested from RSS feed was not deleted when feed was deleted.\n",
            "\n",
            "PreviousAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "NextJuly 15: Support for SharePoint feeds, new Conversation features\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [1433f70c-c943-4ea6-849a-0c9fd1eaeb10]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/1433f70c-c943-4ea6-849a-0c9fd1eaeb10/Mezzanine/page.json?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\n",
            "🎄\tDecember 2024\n",
            "\n",
            "# December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Dropbox feeds for ingesting files on the Dropbox cloud service. Dropbox feeds require your appKey, appSecret, redirectUriand refreshTokento be assigned. The feed also accepts an optional pathparameter to read files from a specific Dropbox folder.\n",
            "- 💡 Graphlit now supports Box feeds for ingesting files on the Box cloud service. Box feeds require your clientId, clientSecret, redirectUriand refreshTokento be assigned.\n",
            "- 💡 Graphlit now supports Intercom feeds for ingesting Intercom Articles and Tickets. We will ingest Intercom Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Intercom feeds require the accessTokenproperty to be assigned.\n",
            "- 💡 Graphlit now supports Zendesk feeds for ingesting Zendesk Articles and Tickets. We will ingest Zendesk Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Zendesk feeds require the accessTokenproperty and your Zendesk subdomain to be assigned.\n",
            "- Graphlit now supports the latest OpenAI o1 model, with the model enums O1_200kand O1_200k_20241217.\n",
            "- Graphlit now supports the latest Gemini Flash 2.0 Experimental model, with the model enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "- Graphlit now supports the latest Cohere R7B model, with the model enum COMMAND_R7B_202412.\n",
            "- Graphlit now supports returning the low-level details from prompting RAG conversations, by adding the includeDetailsparameter and setting to True. This includes details on the number of sources, the exact list of messages provided to the LLM, and more.\n",
            "- We have added support for filtering of observables, such as Person or Organization, by URI property.\n",
            "- We have added the ability to bypass semantic search in content retrieval with conversations. You can assign NONEfor the conversation search type, and it will ignore the user prompt when retrieving content. It will inject all contents resulting from the content filter into the RAG prompt context.\n",
            "- We have added a new createdInLastproperty to all entity filters, which allows easier filtering of entities created within a recent time period. Also, we have added a new inLastproperty to the content filter, which allows easier filtering of content authored within a recent time period. For example, find all images taken in the last 3 days, or find me all emails I received yesterday.\n",
            "- We have added support for the latest Azure AI Document Intelligence models, with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "- We have added support for Google Drive and OneDrive feeds to ingest specific files by providing a list of file identifiers (files), in addition to the folder identifier (folderId). If files identifiers are provided, they take precedence over the folder identifier.\n",
            "- ⚡ For projects upgraded to the Starter Tier after Dec 9, 2024, we have removed the content items limit. Now you can store an unlimited number of content items (i.e. files, web pages, Slack messages) on the Starter or Growth Tiers. If you have an existing project on the Starter Tier, please reach out and we will manually remove that content item limit on the project.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3529: Can't assign collection to multitenant content\n",
            "- GPLA-3579: Should decode HTML characters when parsing HTML email\n",
            "- GPLA-3576: Ingesting content in-place doesn't handle isSynchronous properly\n",
            "- GPLA-3457: IsFeedDone doesn't return True for finished feed with no contents\n",
            "- GPLA-3572: Not handling HTTP 400 error on uploading from URI\n",
            "\n",
            "NextDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "Last updated3 days ago \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the ElevenLabs voice ID to use\n",
        "voice_id = \"ZF6FPAbjXT4488VcRRnw\" # ElevenLabs Amelia voice\n",
        "\n",
        "# Prompt which gets run on each web page to summarize key points\n",
        "summary_prompt = \"\"\"\n",
        "You are an AI assistant that extracts the most important information from product changelog pages.\n",
        "\n",
        "You are being provided a changelog web page for one of many releases of the Graphlit Platform in 2024.\n",
        "\n",
        "Your task is to produce a concise summary that covers:\n",
        "\n",
        "New Features – Briefly list or describe each new capability.\n",
        "Enhancements/Improvements – Any notable improvements or changes.\n",
        "Bug Fixes – Summaries of what was fixed and why it matters.\n",
        "Other Key Details – Any version numbers, feature flags, or breaking changes.\n",
        "Dates - When a feature was released\n",
        "Value - What this offers to developers.\n",
        "Keep it succinct, accurate, and organized. Use short sentences or bullet points so it’s easy to incorporate into a map/reduce pipeline. Omit any superfluous text.\n",
        "\n",
        "Output:\n",
        "A concise summary in bullet points highlighting the essential updates from the changelog.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt which gets run against all summaries (in map/reduce manner) to generate final script for ElevenLabs audio\n",
        "publish_prompt = \"\"\"\n",
        "You are an enthusiastic host focused on developer marketing, and you work for Graphlit who is creating a 2024 year-in-review of their API-based platform.\n",
        "\n",
        "Don't refer to yourself in the script. Just talk to the audience.\n",
        "\n",
        "Don't add in any podcast-like references like intro music, sound effects, etc.  This will be used with a text-to-speech API to generate an audio recording.\n",
        "\n",
        "Your audience is somewhat technical — software engineers, product builders, and tech-savvy product managers — so the script should be clear, concise, and sprinkled with a bit of technical depth.\n",
        "\n",
        "Using the provided changelog for the Graphlit Platform, create a podcast-like script that:\n",
        "\n",
        "- Sets the stage with a warm, engaging introduction.\n",
        "- Highlights each new feature, explaining how it helps developers or teams be more productive, efficient, or creative.\n",
        "- Refers to when a feature was released.\n",
        "- Mentions any model updates and why they matter for technical use cases.\n",
        "- Reviews notable bug fixes, providing just enough context to show the improvements without overwhelming detail.\n",
        "- Closes with a quick recap and a call to action, encouraging listeners to try out the new features or learn more.\n",
        "\n",
        "At the very end, mention that the listener can signup for free at graphlit.com and try out all these features.\n",
        "Also, mention that in 2025, Graphlit will be offering exciting new features to accelerate the building of AI agents.\n",
        "\n",
        "The tone should be friendly, positive, and confident—like a technology evangelist who’s genuinely excited about these updates.\n",
        "\n",
        "Keep it interesting and conversational, but maintain enough depth to engage developers who care about how things work under the hood.\n",
        "Use analogies or practical examples to illustrate why certain features are useful.\n",
        "Feel free to add transitions such as “Now, let’s dive in,” or “Moving on to our next highlight” to keep it flowing.\n",
        "\n",
        "Output: A detailed, TTS-ready 10-minute long script that hits all the points above.\n",
        "\"\"\"\n",
        "\n",
        "if feed_id is not None:\n",
        "    summary_specification_id = await create_specification(enums.OpenAIModels.GPT4O_MINI_128K)\n",
        "\n",
        "    if summary_specification_id is not None:\n",
        "        print(f'Created summary specification [{summary_specification_id}]:')\n",
        "\n",
        "        publish_specification_id = await create_specification(enums.OpenAIModels.O1_200K)\n",
        "\n",
        "        if publish_specification_id is not None:\n",
        "            print(f'Created publish specification [{publish_specification_id}]:')\n",
        "\n",
        "            display(Markdown(f'### Publishing Contents...'))\n",
        "\n",
        "            published_content_id = await publish_contents(feed_id, summary_specification_id, publish_specification_id, summary_prompt, publish_prompt, publish_correlation_id, voice_id)\n",
        "\n",
        "            if published_content_id is not None:\n",
        "                print(f'Completed publishing content [{published_content_id}].')\n",
        "\n",
        "                # Need to reload content to get presigned URL to MP3\n",
        "                published_content = await get_content(published_content_id)\n",
        "\n",
        "                if published_content is not None:\n",
        "                    display(Markdown(f'### Published [{published_content.name}]({published_content.audio_uri})'))\n",
        "\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <audio controls>\n",
        "                    <source src=\"{published_content.audio_uri}\" type=\"audio/mp3\">\n",
        "                    Your browser does not support the audio element.\n",
        "                    </audio>\n",
        "                    \"\"\"))\n",
        "\n",
        "                    # After the audio is generated, we ingest the MP3 as a new content object in Graphlit, and it gets auto-transcribed\n",
        "                    display(Markdown('### Transcript'))\n",
        "                    display(Markdown(published_content.markdown))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ieBzAp6Z2Zew",
        "outputId": "29c9d7e8-a0ad-43c9-fa6a-5d419d46ec9e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created summary specification [6d783a98-02fe-448c-82bb-58eea30ee57c]:\n",
            "Created publish specification [cdb7d090-35a5-4caa-bfb3-051ed100ddf2]:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Publishing Contents..."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed publishing content [d8907685-f022-4829-ba51-4d9aa8eaf380].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Published [Published Summary.mp3](https://graphlit20241212dc396403.blob.core.windows.net/files/d8907685-f022-4829-ba51-4d9aa8eaf380/Mezzanine/Published%20Summary.mp3?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                    <audio controls>\n",
              "                    <source src=\"https://graphlit20241212dc396403.blob.core.windows.net/files/d8907685-f022-4829-ba51-4d9aa8eaf380/Mezzanine/Published%20Summary.mp3?sv=2025-01-05&se=2024-12-28T13%3A40%3A12Z&sr=c&sp=rl&sig=g3un1gh3Y4zkDeh%2BOlgZAtzeHDZsWAIHtQcuHLaJiG0%3D\" type=\"audio/mp3\">\n",
              "                    Your browser does not support the audio element.\n",
              "                    </audio>\n",
              "                    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Transcript"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[00:00:00] Hello, and welcome.\n\n[00:00:01] Today, the spotlight is on everything the Graphlet platform rolled out during 2023\n\n[00:00:07] 2024.\n\n[00:00:08] Whether you've been using Graphlet for a while or you're newly curious about tapping into smarter content,\n\n[00:00:14] ingestion,\n\n[00:00:15] retrieval, and large language model integration,\n\n[00:00:18] this year in review will get you up to speed.\n\n[00:00:22] There's plenty to cover, so let's jump right in. Let's start back in August\n\n[00:00:28] 2023.\n\n[00:00:29] That month brought a new data model for observations,\n\n[00:00:34] including a reworked approach for storing occurrences\n\n[00:00:37] of people,\n\n[00:00:39] organizations,\n\n[00:00:40] and more.\n\n[00:00:42] Alongside that came a new category entity for classifying sensitive data.\n\n[00:00:48] It might sound abstract, but trust me. Anyone who needs to classify or redact personally\n\n[00:00:54] identifiable\n\n[00:00:55] information\n\n[00:00:56] will find that these changes make content handling feel more organized and secure.\n\n[00:01:01] Also, in August,\n\n[00:01:03] we introduced usage based billing infrastructure,\n\n[00:01:06] which helps teams scale by only paying for what they actually use.\n\n[00:01:10] Plus, we began appending SAS tokens to URIs\n\n[00:01:13] so you can directly access processed data.\n\n[00:01:17] Very handy if you're building an application that needs near instant retrieval.\n\n[00:01:22] Fast forward a bit to December 2023\n\n[00:01:25] when we introduced some major expansions to our large language model lineup.\n\n[00:01:30] We added support for open AI, gpt4,\n\n[00:01:34] turbo\n\n[00:01:35] 128 k, llama 2, Mistral 7 b, and anthropic Claude 2.1,\n\n[00:01:40] just to name a few.\n\n[00:01:42] That was also when query by example launched,\n\n[00:01:45] letting you quickly search your content or conversations\n\n[00:01:48] by providing a short snippet.\n\n[00:01:50] No\n\n[00:01:51] complicated query syntax needed.\n\n[00:01:54] On top of that, we tackled a few important bug fixes,\n\n[00:01:59] like ignoring RSS\n\n[00:02:00] dot etml\n\n[00:02:02] in site maps and addressing an issue where\n\n[00:02:05] GPT 3.5\n\n[00:02:07] turbo might inject phantom citation number placeholders.\n\n[00:02:11] By December's end, teams were seeing more robust search and retrieval,\n\n[00:02:16] and these LLM upgrades opened the door to brand new use cases\n\n[00:02:20] from summarizing large documents\n\n[00:02:22] to analyzing images right within a conversation.\n\n[00:02:26] Then came February 2024.\n\n[00:02:29] On the second, we introduced\n\n[00:02:32] semantic alerts to schedule automatic LLM,\n\n[00:02:35] summarizations, and content publications.\n\n[00:02:38] Imagine generating daily or weekly reports without manual oversight.\n\n[00:02:43] Perfect for dev teams who want quick snapshots of Slack messages,\n\n[00:02:47] email threads, or tickets.\n\n[00:02:49] Later that month, on 21st,\n\n[00:02:52] support arrived for OneDrive and Google Drive feeds,\n\n[00:02:55] plus the ability to automatically extract embedded images from PDFs.\n\n[00:03:00] That means you can ingest files or entire shared folders\n\n[00:03:03] and trust the system to do the heavy lifting\n\n[00:03:06] of pulling out text, attachments,\n\n[00:03:09] and\n\n[00:03:10] images.\n\n[00:03:11] We also introduced better email backup file handling.\n\n[00:03:15] Think EML or MSG.\n\n[00:03:18] And it's never just about new features.\n\n[00:03:20] We smoothed out PDF passing errors and improved credit usage notifications\n\n[00:03:26] so you know when your usage is approaching its quota.\n\n[00:03:29] April 24 was especially busy.\n\n[00:03:33] On April 7th, we added the ability to ingest Discord channel content complete with attachments\n\n[00:03:39] so you can unify all your chats and file data in one place.\n\n[00:03:43] We also introduced Cohere Re Ranking,\n\n[00:03:46] giving you the option to reorder semantic search results with Cohere's models for more precise content retrieval.\n\n[00:03:53] Section aware, text chunking,\n\n[00:03:55] chunk based retrieval strategies,\n\n[00:03:57] and an asynchronous\n\n[00:03:59] flag for ingest operations rounded out that release.\n\n[00:04:02] Then on April 23rd,\n\n[00:04:04] we took a major step forward with official Python and TypeScript\n\n[00:04:08] SDKs.\n\n[00:04:09] Each is cogenerated from our GraphQL schema, so you don't need deep GraphQL knowledge to get started.\n\n[00:04:16] We also updated our model roster yet again with\n\n[00:04:19] GPT 4,\n\n[00:04:22] turbo 128 k, llama 3, Grok, and Fresh Cohere models like command r.\n\n[00:04:28] All of this helps you seamlessly\n\n[00:04:31] integrate Graphlet\n\n[00:04:32] into your application stack,\n\n[00:04:34] whether you're building a Python microservice\n\n[00:04:37] or a Node. Js content pipeline.\n\n[00:04:40] Moving along to June 2024,\n\n[00:04:43] we introduced support for deep seek LLMs for prompt completion.\n\n[00:04:47] We also started passing embedded JSON LD from web pages to automatically enrich the knowledge graph,\n\n[00:04:55] which is a huge win if your team is building robust data pipelines that unify multiple data sources.\n\n[00:05:03] Later that month, on June 21st,\n\n[00:05:05] we added the anthropic Claude 3.5\n\n[00:05:07] Sonnet model,\n\n[00:05:09] plus improvements for knowledge graph semantic search.\n\n[00:05:12] If you've spent any time writing entity extraction\n\n[00:05:15] and linking logic, this new approach can save you from building custom solutions from scratch.\n\n[00:05:21] July 24 saw quite a few enhancements too. On July 4th, webhook alerts arrived,\n\n[00:05:28] letting your application\n\n[00:05:29] receive HTTP\n\n[00:05:31] post notifications\n\n[00:05:32] whenever certain content events or summaries get published.\n\n[00:05:37] We also added a 128 k context window for deep seek models, giving you more space for bigger or more detailed prompts.\n\n[00:05:45] Then on July 19th, we introduced the gpt4o\n\n[00:05:49] mini model,\n\n[00:05:50] which can handle up to 16 k output tokens\n\n[00:05:53] plus improved summarization\n\n[00:05:55] features for your content.\n\n[00:05:58] Another update landed on July 25th,\n\n[00:06:00] focusing on Mistral Large 2 and Nemo,\n\n[00:06:03] plus the llama 3.1 series on.\n\n[00:06:07] And just a few days later, on July 28th,\n\n[00:06:10] we added an indexing\n\n[00:06:12] workflow stage and Azure\n\n[00:06:14] AI language detection,\n\n[00:06:16] making it easier to identify languages across large corpora.\n\n[00:06:20] August 2024\n\n[00:06:22] might be a favorite for those building specialized products.\n\n[00:06:26] On 8th, we provided LLM based document preparation\n\n[00:06:30] using GPT 4 0 or Anthropic Sonnet 3.5\n\n[00:06:35] as well as an open source dot net SDK.\n\n[00:06:38] Right after that, on August 11th,\n\n[00:06:41] Azure AI Document Intelligence\n\n[00:06:43] became our default recommendation\n\n[00:06:45] for complex PDFs and tricky table extractions,\n\n[00:06:49] boosting the accuracy of your retrieval and generation tasks.\n\n[00:06:53] And on August 20th, we introduced support for medical related entities,\n\n[00:06:59] everything from medical drug to medical test,\n\n[00:07:03] making Graphlet a more appealing option for health and life sciences apps that require thorough data classification.\n\n[00:07:10] Meanwhile, we tackled bug fixes to ensure stable performance,\n\n[00:07:14] especially in entity extraction and LLM caches.\n\n[00:07:18] Our next stop is December 2024.\n\n[00:07:21] On December 1st, we polished up the retrieval only rag pipeline features, giving you more ways to format prompts for large language models without forcing a generation step each time.\n\n[00:07:32] Then on December 9th, we added website mapping,\n\n[00:07:36] web page screenshots,\n\n[00:07:37] and extraction commands like summarize text and extract text.\n\n[00:07:42] With screenshot page, you can grab images of web pages for follow-up processing.\n\n[00:07:47] And the new flattened citations option\n\n[00:07:50] helps unify references\n\n[00:07:52] in one place.\n\n[00:07:54] And just a couple weeks later, on December 22nd,\n\n[00:07:57] we capped off the year with feed integrations for Dropbox, Box, Intercom, and Zendesk.\n\n[00:08:04] We also introduced an experimental Gemini 2.0 model and the brand new OpenAI\n\n[00:08:10] o one model\n\n[00:08:11] capable of handling up to 200 k tokens.\n\n[00:08:15] This final release of the year\n\n[00:08:17] also removed the content item limit for projects on our starter tier. Super handy if you're archiving huge volumes of documents or logs.\n\n[00:08:27] Of course, scattered among all these launches are countless bug fixes.\n\n[00:08:32] We've tackled everything from better PDF table extraction\n\n[00:08:36] to preventing timeouts\n\n[00:08:38] when passing large Slack message histories.\n\n[00:08:41] We've also refined how we handle hallucinations\n\n[00:08:44] when LLMs generate citations that don't exist.\n\n[00:08:48] When you see notes like GPL1726\n\n[00:08:52] or GPLA\n\n[00:08:53] 314,\n\n[00:08:54] those refer to issues we've identified and officially patched,\n\n[00:08:58] making the platform more reliable and accurate over time.\n\n[00:09:02] The overall goal is to ensure you don't run into quirky edge cases when building your app. And if you do, we aim to patch them swiftly.\n\n[00:09:11] Now that we've taken a whirlwind tour of these updates,\n\n[00:09:14] let's recap.\n\n[00:09:16] Throughout 2023\n\n[00:09:18] and 2054,\n\n[00:09:19] Graphlet grew from a powerful ingestion engine\n\n[00:09:22] into a complete platform\n\n[00:09:32] OneDrive, Box,\n\n[00:09:34] Intercom, Zendesk,\n\n[00:09:36] OneDrive, Box,\n\n[00:09:38] Intercom, Zendesk,\n\n[00:09:39] Notion, and more. We expanded the range of large language models from GPT 3.5 Turbo\n\n[00:09:46] all the way to advanced models like Grok Llama 3.3 or Gemini 2.0\n\n[00:09:52] and introduced\n\n[00:09:53] re ranking strategies that help you find precisely the data you need faster.\n\n[00:09:59] Meanwhile, we improved the platform's ability to pass complex PDFs,\n\n[00:10:03] handle images, produce custom summaries,\n\n[00:10:07] and integrate with your own custom code, thanks to official SDKs in Python, TypeScript, and dot net.\n\n[00:10:14] If all of this inspires you to give Graphlet a try, the perfect time is now. You can sign up for free at graphlet.com\n\n[00:10:22] and start experimenting with these features right away.\n\n[00:10:25] Whether you're looking to automate your content ingestion flows, level up your search or knowledge graph, or enhance your app with the latest large language models, Graphlet has the tools to make it happen.\n\n[00:10:38] And here's a final bonus for those with an eye on the future.\n\n[00:10:42] In 2,025,\n\n[00:10:45] Graphlet will be introducing even more features to help you rapidly build AI agents.\n\n[00:10:50] It's going to be a leap forward,\n\n[00:10:52] and we can't wait to share it.\n\n[00:10:55] Thank you for tuning in to this detailed look at the past year and a half of Graphlet's evolution.\n\n[00:11:02] Have fun building, and see you next time.\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML, JSON\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "time.sleep(10) # give it some time for billing events to catch up\n",
        "\n",
        "credits = await lookup_credits(ingestion_correlation_id)\n",
        "\n",
        "if credits is not None:\n",
        "    display(Markdown(f\"### Credits used: {credits.credits:.6f} for ingestion\"))\n",
        "    print(f\"- storage [{credits.storage_ratio:.2f}%], compute [{credits.compute_ratio:.2f}%]\")\n",
        "    print(f\"- embedding [{credits.embedding_ratio:.2f}%], completion [{credits.completion_ratio:.2f}%]\")\n",
        "    print(f\"- ingestion [{credits.ingestion_ratio:.2f}%], indexing [{credits.indexing_ratio:.2f}%], preparation [{credits.preparation_ratio:.2f}%], extraction [{credits.extraction_ratio:.2f}%], enrichment [{credits.enrichment_ratio:.2f}%], publishing [{credits.publishing_ratio:.2f}%]\")\n",
        "    print(f\"- search [{credits.search_ratio:.2f}%], conversation [{credits.conversation_ratio:.2f}%]\")\n",
        "    print()\n",
        "\n",
        "usage = await lookup_usage(ingestion_correlation_id)\n",
        "\n",
        "if usage is not None:\n",
        "    display(Markdown(f\"### Usage records:\"))\n",
        "\n",
        "    for record in usage:\n",
        "        dump_usage_record(record)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s7M_W9n7ntKA",
        "outputId": "b43c10d7-b894-43d0-85af-881072e80122"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Credits used: 5.163919 for ingestion"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- storage [0.91%], compute [58.20%]\n",
            "- embedding [3.22%], completion [0.00%]\n",
            "- ingestion [0.00%], indexing [0.00%], preparation [37.68%], extraction [0.00%], enrichment [0.00%], publishing [0.00%]\n",
            "- search [0.00%], conversation [0.00%]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Usage records:"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-28T07:40:19.532Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:00.842955, used credits [0.00151767]\n",
            "- CONTENT [44c810e9-ce3d-44f0-aff5-8f091d30acda]\n",
            "\n",
            "2024-12-28T07:40:06.213Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:10.534432, used credits [0.01896637]\n",
            "- CONTENT [a943fa59-2f9c-4731-bcc2-a52b9219215b]\n",
            "\n",
            "2024-12-28T07:40:06.069Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.295815, used credits [0.00113600]\n",
            "- CONTENT [a943fa59-2f9c-4731-bcc2-a52b9219215b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [568 tokens], throughput: 1920.118 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:05.969Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.199295, used credits [0.00093600]\n",
            "- CONTENT [a943fa59-2f9c-4731-bcc2-a52b9219215b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [468 tokens], throughput: 2348.275 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:04.642Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.057558, used credits [0.00001700]\n",
            "- CONTENT [a943fa59-2f9c-4731-bcc2-a52b9219215b]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4400 bytes], throughput: 76445.028 bytes/sec\n",
            "\n",
            "2024-12-28T07:40:01.990Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.080876, used credits [0.00081012]\n",
            "- CONTENT [a943fa59-2f9c-4731-bcc2-a52b9219215b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2023/september-20-paid-subscription-plans-support-for-custom-observed-entities-and-azure-openai-gpt-4]\n",
            "- File upload [209658 bytes], throughput: 2592332.479 bytes/sec\n",
            "\n",
            "2024-12-28T07:40:01.626Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:13.723456, used credits [0.02470794]\n",
            "- CONTENT [801f60cf-bbaf-4167-a6f5-6123c5109f8a]\n",
            "\n",
            "2024-12-28T07:40:01.569Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:13.549839, used credits [0.02439536]\n",
            "- CONTENT [aa3d4b80-1e4e-4a6f-afd0-09e6fd0217a4]\n",
            "\n",
            "2024-12-28T07:40:01.505Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.277490, used credits [0.00068800]\n",
            "- CONTENT [801f60cf-bbaf-4167-a6f5-6123c5109f8a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [344 tokens], throughput: 1239.683 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:01.470Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.353697, used credits [0.00023800]\n",
            "- CONTENT [aa3d4b80-1e4e-4a6f-afd0-09e6fd0217a4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [119 tokens], throughput: 336.446 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:01.421Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.304333, used credits [0.00023800]\n",
            "- CONTENT [aa3d4b80-1e4e-4a6f-afd0-09e6fd0217a4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [119 tokens], throughput: 391.020 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:01.418Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.188742, used credits [0.00068800]\n",
            "- CONTENT [801f60cf-bbaf-4167-a6f5-6123c5109f8a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [344 tokens], throughput: 1822.592 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:01.264Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:13.353029, used credits [0.02404102]\n",
            "- CONTENT [52d4fb3e-db27-4bd2-bc42-97410cb7216c]\n",
            "\n",
            "2024-12-28T07:40:01.165Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:13.576944, used credits [0.02444416]\n",
            "- CONTENT [0f863f8a-abac-4e44-8787-c8d28bf7f323]\n",
            "\n",
            "2024-12-28T07:40:01.137Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.205387, used credits [0.00075200]\n",
            "- CONTENT [52d4fb3e-db27-4bd2-bc42-97410cb7216c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1830.693 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:01.111Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.186061, used credits [0.00075200]\n",
            "- CONTENT [52d4fb3e-db27-4bd2-bc42-97410cb7216c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 2020.842 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:01.063Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:13.507905, used credits [0.02431986]\n",
            "- CONTENT [10d4a01f-d93e-4e60-aba9-cad12418f6d7]\n",
            "\n",
            "2024-12-28T07:40:01.059Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.335300, used credits [0.00044000]\n",
            "- CONTENT [0f863f8a-abac-4e44-8787-c8d28bf7f323]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 656.129 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:01.048Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.326096, used credits [0.03800000]\n",
            "- CONTENT [a943fa59-2f9c-4731-bcc2-a52b9219215b]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:40:00.931Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.279899, used credits [0.00027600]\n",
            "- CONTENT [10d4a01f-d93e-4e60-aba9-cad12418f6d7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [138 tokens], throughput: 493.034 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:00.899Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.238389, used credits [0.00027600]\n",
            "- CONTENT [10d4a01f-d93e-4e60-aba9-cad12418f6d7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [138 tokens], throughput: 578.885 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:00.872Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.143310, used credits [0.00044000]\n",
            "- CONTENT [0f863f8a-abac-4e44-8787-c8d28bf7f323]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 1535.135 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:00.762Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:12.867689, used credits [0.02316720]\n",
            "- CONTENT [9c5622e0-48d5-44b2-9999-e9697b31c84e]\n",
            "\n",
            "2024-12-28T07:40:00.659Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.376740, used credits [0.00044200]\n",
            "- CONTENT [9c5622e0-48d5-44b2-9999-e9697b31c84e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [221 tokens], throughput: 586.612 tokens/sec\n",
            "\n",
            "2024-12-28T07:40:00.591Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.160974, used credits [0.00044200]\n",
            "- CONTENT [9c5622e0-48d5-44b2-9999-e9697b31c84e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [221 tokens], throughput: 1372.896 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:59.997Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:13.392468, used credits [0.02411202]\n",
            "- CONTENT [9529be6c-cfd7-49db-b604-c45749004809]\n",
            "\n",
            "2024-12-28T07:39:59.813Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.118798, used credits [0.00000856]\n",
            "- CONTENT [801f60cf-bbaf-4167-a6f5-6123c5109f8a]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2216 bytes], throughput: 18653.513 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:59.795Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.301962, used credits [0.00059000]\n",
            "- CONTENT [9529be6c-cfd7-49db-b604-c45749004809]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [295 tokens], throughput: 976.945 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:59.687Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.198733, used credits [0.00059000]\n",
            "- CONTENT [9529be6c-cfd7-49db-b604-c45749004809]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [295 tokens], throughput: 1484.403 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:59.642Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.208167, used credits [0.00000570]\n",
            "- CONTENT [aa3d4b80-1e4e-4a6f-afd0-09e6fd0217a4]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1474 bytes], throughput: 7080.864 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:59.364Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.080677, used credits [0.00001299]\n",
            "- CONTENT [52d4fb3e-db27-4bd2-bc42-97410cb7216c]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3363 bytes], throughput: 41684.691 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:58.824Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.206365, used credits [0.00000616]\n",
            "- CONTENT [0f863f8a-abac-4e44-8787-c8d28bf7f323]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1595 bytes], throughput: 7729.035 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:58.815Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.258202, used credits [0.00000562]\n",
            "- CONTENT [10d4a01f-d93e-4e60-aba9-cad12418f6d7]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1455 bytes], throughput: 5635.121 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:57.831Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.093346, used credits [0.00000952]\n",
            "- CONTENT [9c5622e0-48d5-44b2-9999-e9697b31c84e]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2465 bytes], throughput: 26407.187 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:57.146Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.074115, used credits [0.00001020]\n",
            "- CONTENT [9529be6c-cfd7-49db-b604-c45749004809]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2640 bytes], throughput: 35620.320 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:55.798Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.261986, used credits [0.00076953]\n",
            "- CONTENT [801f60cf-bbaf-4167-a6f5-6123c5109f8a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-3-support-for-web-search-feeds-model-deprecations]\n",
            "- File upload [199153 bytes], throughput: 760166.864 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:55.766Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.162921, used credits [0.00071264]\n",
            "- CONTENT [aa3d4b80-1e4e-4a6f-afd0-09e6fd0217a4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-9-support-for-github-repository-feeds-bug-fixes]\n",
            "- File upload [184430 bytes], throughput: 1132018.956 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:55.563Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.153049, used credits [0.00078225]\n",
            "- CONTENT [52d4fb3e-db27-4bd2-bc42-97410cb7216c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-3-support-tool-calling-ingestbatch-mutation-gemini-flash-1.5-8b-bug-fixes]\n",
            "- File upload [202445 bytes], throughput: 1322742.853 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:55.508Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:20.270885, used credits [0.03649604]\n",
            "- CONTENT [7ce5f9ae-f13c-4832-8c74-55c8bac42caf]\n",
            "\n",
            "2024-12-28T07:39:55.351Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.241966, used credits [0.00087000]\n",
            "- CONTENT [7ce5f9ae-f13c-4832-8c74-55c8bac42caf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 1797.776 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:55.287Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.171924, used credits [0.00087000]\n",
            "- CONTENT [7ce5f9ae-f13c-4832-8c74-55c8bac42caf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 2530.188 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:55.164Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.088648, used credits [0.00069990]\n",
            "- CONTENT [10d4a01f-d93e-4e60-aba9-cad12418f6d7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-7-support-for-anthropic-and-gemini-tool-calling]\n",
            "- File upload [181134 bytes], throughput: 2043287.914 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:54.938Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:06.938231, used credits [0.03800000]\n",
            "- CONTENT [801f60cf-bbaf-4167-a6f5-6123c5109f8a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:54.902Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:06.778606, used credits [0.03800000]\n",
            "- CONTENT [aa3d4b80-1e4e-4a6f-afd0-09e6fd0217a4]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:54.892Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.123055, used credits [0.00071704]\n",
            "- CONTENT [0f863f8a-abac-4e44-8787-c8d28bf7f323]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-22-support-for-latest-anthropic-sonnet-3.5-model-cohere-image-embeddings]\n",
            "- File upload [185569 bytes], throughput: 1508015.515 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:54.859Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.074595, used credits [0.00078348]\n",
            "- CONTENT [9c5622e0-48d5-44b2-9999-e9697b31c84e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2023/september-24-support-for-youtube-feeds-added-documentation-bug-fixes]\n",
            "- File upload [202764 bytes], throughput: 2718205.559 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:54.625Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:06.650969, used credits [0.03800000]\n",
            "- CONTENT [52d4fb3e-db27-4bd2-bc42-97410cb7216c]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:54.584Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.080266, used credits [0.00077468]\n",
            "- CONTENT [9529be6c-cfd7-49db-b604-c45749004809]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-1-support-for-fhir-enrichment-latest-cohere-models-bug-fixes]\n",
            "- File upload [200486 bytes], throughput: 2497757.468 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:54.478Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:06.851102, used credits [0.03800000]\n",
            "- CONTENT [10d4a01f-d93e-4e60-aba9-cad12418f6d7]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:54.288Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:06.342497, used credits [0.03800000]\n",
            "- CONTENT [9c5622e0-48d5-44b2-9999-e9697b31c84e]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:54.273Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:06.626168, used credits [0.03800000]\n",
            "- CONTENT [0f863f8a-abac-4e44-8787-c8d28bf7f323]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:54.047Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.388340, used credits [0.03800000]\n",
            "- CONTENT [9529be6c-cfd7-49db-b604-c45749004809]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:53.670Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.165418, used credits [0.00001454]\n",
            "- CONTENT [7ce5f9ae-f13c-4832-8c74-55c8bac42caf]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3764 bytes], throughput: 22754.408 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:52.077Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:35.259737, used credits [0.06348222]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]\n",
            "\n",
            "2024-12-28T07:39:51.908Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.331204, used credits [0.00111400]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 1681.742 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:51.879Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.257806, used credits [0.00138600]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 2688.067 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:50.821Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.419213, used credits [0.00138600]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 1653.099 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:50.627Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.248658, used credits [0.00111400]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 2240.021 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:48.424Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:32.373452, used credits [0.05828570]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]\n",
            "\n",
            "2024-12-28T07:39:48.352Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.448766, used credits [0.00111400]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 1241.182 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:48.348Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.293497, used credits [0.00138600]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 2361.183 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.920Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:31.867552, used credits [0.05737487]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]\n",
            "\n",
            "2024-12-28T07:39:47.912Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:31.859029, used credits [0.05735953]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]\n",
            "\n",
            "2024-12-28T07:39:47.879Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.303745, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 572.849 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.843Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.301092, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 577.897 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.687Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.402285, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 783.028 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.660Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.374789, used credits [0.00090800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1211.349 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.547Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.232265, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1356.210 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.462Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.155983, used credits [0.00109800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 3519.619 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.456Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:27.659746, used credits [0.04979907]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]\n",
            "\n",
            "2024-12-28T07:39:47.237Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.355690, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 691.613 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.205Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:31.151681, used credits [0.05608601]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]\n",
            "\n",
            "2024-12-28T07:39:47.201Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.354572, used credits [0.00138600]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 1954.468 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.195Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:31.141603, used credits [0.05606786]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]\n",
            "\n",
            "2024-12-28T07:39:47.191Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.297424, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 827.101 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.181Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:06.821103, used credits [0.01228083]\n",
            "- CONTENT [44c810e9-ce3d-44f0-aff5-8f091d30acda]\n",
            "\n",
            "2024-12-28T07:39:47.076Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.266312, used credits [0.00111400]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 2091.536 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.072Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.262393, used credits [0.00052800]\n",
            "- CONTENT [44c810e9-ce3d-44f0-aff5-8f091d30acda]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [264 tokens], throughput: 1006.124 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.071Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:31.018843, used credits [0.05584684]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]\n",
            "\n",
            "2024-12-28T07:39:47.053Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.245516, used credits [0.00052800]\n",
            "- CONTENT [44c810e9-ce3d-44f0-aff5-8f091d30acda]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [264 tokens], throughput: 1075.285 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:47.010Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:30.959842, used credits [0.05574062]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]\n",
            "\n",
            "2024-12-28T07:39:47.008Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.385782, used credits [0.00125000]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [625 tokens], throughput: 1620.088 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.970Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.311024, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 559.442 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.955Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.464729, used credits [0.00114000]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [570 tokens], throughput: 1226.522 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.913Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:30.739650, used credits [0.05534418]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]\n",
            "\n",
            "2024-12-28T07:39:46.883Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:30.831779, used credits [0.05551005]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]\n",
            "\n",
            "2024-12-28T07:39:46.853Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.192632, used credits [0.00136800]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [684 tokens], throughput: 3550.814 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.848Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.385351, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 817.436 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.836Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.238004, used credits [0.00109800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 2306.680 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.832Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.210867, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 825.166 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.821Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.417894, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 555.164 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.798Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.280215, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 967.114 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.787Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.224950, used credits [0.00090800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 2018.229 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.773Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.340025, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 770.532 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.755Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.269204, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 898.946 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.743Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.240534, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 1126.661 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.731Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.168653, used credits [0.00080479]\n",
            "- CONTENT [7ce5f9ae-f13c-4832-8c74-55c8bac42caf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-8-support-for-llm-based-document-extraction-.net-sdk-bug-fixes]\n",
            "- File upload [208278 bytes], throughput: 1234950.600 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:46.725Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.246613, used credits [0.00097200]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [486 tokens], throughput: 1970.697 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.706Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.229726, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 1053.431 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.635Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.247954, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 992.118 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.632Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.167190, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1884.084 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.603Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.141921, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 1846.100 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.588Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.128574, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 1804.407 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.567Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.194509, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 1264.722 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.342Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.525280, used credits [0.00136800]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [684 tokens], throughput: 1302.164 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.193Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.387657, used credits [0.00125000]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [625 tokens], throughput: 1612.251 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.171Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.441573, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 394.046 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.162Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.428126, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 406.423 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.137Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:30.085188, used credits [0.05416587]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]\n",
            "\n",
            "2024-12-28T07:39:46.135Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.449359, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 538.544 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.091Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.461202, used credits [0.00114000]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [570 tokens], throughput: 1235.900 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.080Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.434873, used credits [0.01338587]\n",
            "- CONTENT [2967b74e-cd6c-40bf-82e4-b8bdfe04545d]\n",
            "\n",
            "2024-12-28T07:39:46.014Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.440557, used credits [0.00138600]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 1573.008 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:46.010Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.309384, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 846.844 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.995Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.361027, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 750.636 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.970Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.266836, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 981.876 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.962Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.274270, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 882.341 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.962Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.266839, used credits [0.00117000]\n",
            "- CONTENT [2967b74e-cd6c-40bf-82e4-b8bdfe04545d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [585 tokens], throughput: 2192.336 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.960Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.341822, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 792.811 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.951Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.375434, used credits [0.00109800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 1462.309 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.938Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.414953, used credits [0.00111400]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 1342.321 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.927Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.242038, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 958.529 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.922Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.289973, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 848.355 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.918Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.265829, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 1192.496 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.909Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.249546, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1262.292 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.901Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.055963, used credits [0.00000757]\n",
            "- CONTENT [44c810e9-ce3d-44f0-aff5-8f091d30acda]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1959 bytes], throughput: 35005.021 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:45.894Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.293878, used credits [0.00097200]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [486 tokens], throughput: 1653.747 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.869Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.209396, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 1513.877 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.848Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.170740, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 1358.791 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.835Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.144830, used credits [0.00095800]\n",
            "- CONTENT [2967b74e-cd6c-40bf-82e4-b8bdfe04545d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [479 tokens], throughput: 3307.317 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.832Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.232562, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 1057.783 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.792Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.140548, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 2241.235 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.778Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.247507, used credits [0.00090800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1834.293 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.681Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:19.149191, used credits [0.03447652]\n",
            "- CONTENT [71f330b4-23a2-40ec-bd77-544a11cf764b]\n",
            "\n",
            "2024-12-28T07:39:45.672Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.980437, used credits [0.01436811]\n",
            "- CONTENT [3a5fcfa3-9f3e-4856-a4b9-afe16c9a4813]\n",
            "\n",
            "2024-12-28T07:39:45.520Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.291969, used credits [0.00075600]\n",
            "- CONTENT [3a5fcfa3-9f3e-4856-a4b9-afe16c9a4813]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1294.660 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.506Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.787303, used credits [0.00085400]\n",
            "- CONTENT [71f330b4-23a2-40ec-bd77-544a11cf764b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 542.358 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.501Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:21.371543, used credits [0.03847768]\n",
            "- CONTENT [27269714-fa48-438c-b75e-d486a030b697]\n",
            "\n",
            "2024-12-28T07:39:45.499Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.267772, used credits [0.00075600]\n",
            "- CONTENT [3a5fcfa3-9f3e-4856-a4b9-afe16c9a4813]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1411.646 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.471Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.842285, used credits [0.00136800]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [684 tokens], throughput: 812.077 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.357Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.358074, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 885.292 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.334Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.478586, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 484.761 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.143Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.224815, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 1410.049 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.133Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.273286, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 848.926 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.127Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.194556, used credits [0.00078400]\n",
            "- CONTENT [27269714-fa48-438c-b75e-d486a030b697]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 2014.841 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.107Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.319495, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 757.446 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.075Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.331717, used credits [0.00097200]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [486 tokens], throughput: 1465.105 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.073Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.327490, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 738.954 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.059Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.290869, used credits [0.00114000]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [570 tokens], throughput: 1959.647 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.056Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.266682, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 982.444 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.044Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.487038, used credits [0.03800000]\n",
            "- CONTENT [7ce5f9ae-f13c-4832-8c74-55c8bac42caf]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:45.043Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.169514, used credits [0.00078400]\n",
            "- CONTENT [27269714-fa48-438c-b75e-d486a030b697]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 2312.489 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.039Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.282483, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 927.489 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:45.015Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.417674, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 416.593 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.956Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.407433, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 665.140 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.953Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:11.888226, used credits [0.02140376]\n",
            "- CONTENT [65a42e3c-ac9a-4c93-bf43-fa296df6a1f7]\n",
            "\n",
            "2024-12-28T07:39:44.928Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.219140, used credits [0.00085400]\n",
            "- CONTENT [71f330b4-23a2-40ec-bd77-544a11cf764b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 1948.530 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.883Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.270307, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 643.713 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.837Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.278505, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1131.038 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.824Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.263820, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 932.453 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.817Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.495613, used credits [0.00074800]\n",
            "- CONTENT [65a42e3c-ac9a-4c93-bf43-fa296df6a1f7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 754.621 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.786Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.223579, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1408.900 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.765Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.201629, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 1220.065 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.761Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.137093, used credits [0.00125000]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [625 tokens], throughput: 4558.936 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.715Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:11.654130, used credits [0.02098229]\n",
            "- CONTENT [0ce3818c-7562-43b3-a554-38b014cb36b3]\n",
            "\n",
            "2024-12-28T07:39:44.711Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.559219, used credits [0.00109800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 981.726 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.697Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.056661, used credits [0.00001446]\n",
            "- CONTENT [2967b74e-cd6c-40bf-82e4-b8bdfe04545d]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3743 bytes], throughput: 66059.314 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:44.669Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.124137, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 2183.067 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.608Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.347022, used credits [0.00045200]\n",
            "- CONTENT [0ce3818c-7562-43b3-a554-38b014cb36b3]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [226 tokens], throughput: 651.256 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.604Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.349530, used credits [0.00138600]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 1982.662 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.579Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.586162, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 540.806 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.515Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.254290, used credits [0.00045200]\n",
            "- CONTENT [0ce3818c-7562-43b3-a554-38b014cb36b3]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [226 tokens], throughput: 888.750 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.469Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.324247, used credits [0.00090800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1400.167 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.421Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.096306, used credits [0.00074800]\n",
            "- CONTENT [65a42e3c-ac9a-4c93-bf43-fa296df6a1f7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 3883.455 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.409Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.459934, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 689.229 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.386Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.146205, used credits [0.00111400]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 3809.730 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.354Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.076612, used credits [0.00071696]\n",
            "- CONTENT [44c810e9-ce3d-44f0-aff5-8f091d30acda]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-30-support-for-azure-ai-inference-models-mistral-pixtral-and-latest-google-gemini-models]\n",
            "- File upload [185548 bytes], throughput: 2421930.883 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:44.227Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.063763, used credits [0.00001484]\n",
            "- CONTENT [3a5fcfa3-9f3e-4856-a4b9-afe16c9a4813]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3840 bytes], throughput: 60222.636 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:44.209Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.363583, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 638.093 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.169Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.318514, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 728.381 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.166Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.386548, used credits [0.00097200]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [486 tokens], throughput: 1257.284 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.164Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.541984, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 446.508 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.048Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.225803, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 1160.304 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.046Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.219145, used credits [0.00114000]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [570 tokens], throughput: 2601.013 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:44.039Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.434428, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 603.093 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:43.954Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.129137, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 1873.982 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:43.931Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.528868, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 512.415 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:43.833Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.434343, used credits [0.03800000]\n",
            "- CONTENT [44c810e9-ce3d-44f0-aff5-8f091d30acda]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:43.654Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.899536, used credits [0.00136800]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [684 tokens], throughput: 760.392 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:43.594Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.347908, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 778.941 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:43.392Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.251499, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 1260.444 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:43.375Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.238285, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 1330.338 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:43.251Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.060918, used credits [0.00000793]\n",
            "- CONTENT [0ce3818c-7562-43b3-a554-38b014cb36b3]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2053 bytes], throughput: 33700.985 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:43.246Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.061443, used credits [0.00001608]\n",
            "- CONTENT [65a42e3c-ac9a-4c93-bf43-fa296df6a1f7]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4161 bytes], throughput: 67721.082 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:43.122Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.525598, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 331.051 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.997Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.086940, used credits [0.00080586]\n",
            "- CONTENT [2967b74e-cd6c-40bf-82e4-b8bdfe04545d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-26-support-for-google-ai-and-cerebras-models-and-latest-groq-models]\n",
            "- File upload [208557 bytes], throughput: 2398853.006 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:42.974Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.237785, used credits [0.00125000]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [625 tokens], throughput: 2628.422 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.945Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.349801, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 900.511 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.862Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.271375, used credits [0.00138600]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 2553.662 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.854Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.409853, used credits [0.00109800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 1339.505 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.840Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.292435, used credits [0.00111400]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 1904.694 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.804Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.255436, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1233.183 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.754Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.189923, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 916.159 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.742Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.347741, used credits [0.00090800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1305.569 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.487Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.758019, used credits [0.03800000]\n",
            "- CONTENT [2967b74e-cd6c-40bf-82e4-b8bdfe04545d]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:42.329Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.074012, used credits [0.00080571]\n",
            "- CONTENT [3a5fcfa3-9f3e-4856-a4b9-afe16c9a4813]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2023/september-4-workflow-configuration-support-for-notion-feeds-document-ocr]\n",
            "- File upload [208517 bytes], throughput: 2817321.398 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:42.271Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.577043, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 402.050 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.184Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.498836, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 493.148 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.181Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.215883, used credits [0.00114000]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [570 tokens], throughput: 2640.319 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.175Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.249000, used credits [0.00097200]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [486 tokens], throughput: 1951.805 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.085Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.191686, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 1210.310 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:42.055Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.716743, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 343.219 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.906Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.128093, used credits [0.03800000]\n",
            "- CONTENT [3a5fcfa3-9f3e-4856-a4b9-afe16c9a4813]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:41.586Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.284265, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 921.675 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.586Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:19.078908, used credits [0.03434998]\n",
            "- CONTENT [f0afc1f7-b938-40f0-927d-f6a274243532]\n",
            "\n",
            "2024-12-28T07:39:41.577Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.080382, used credits [0.00072748]\n",
            "- CONTENT [0ce3818c-7562-43b3-a554-38b014cb36b3]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-31-support-for-simulated-tool-calling-bug-fixes]\n",
            "- File upload [188270 bytes], throughput: 2342179.383 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:41.542Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.186176, used credits [0.00081897]\n",
            "- CONTENT [65a42e3c-ac9a-4c93-bf43-fa296df6a1f7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2023/october-30-optimized-conversation-responses-added-observable-aliases-bug-fixes]\n",
            "- File upload [211948 bytes], throughput: 1138429.988 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:41.475Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.338047, used credits [0.00049600]\n",
            "- CONTENT [f0afc1f7-b938-40f0-927d-f6a274243532]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 733.626 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.465Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.244389, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 1072.062 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.462Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.481743, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 562.540 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.372Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.265952, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 909.940 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.318Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.208231, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 1162.171 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.301Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.160121, used credits [0.00049600]\n",
            "- CONTENT [f0afc1f7-b938-40f0-927d-f6a274243532]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 1548.828 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.288Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.221504, used credits [0.00136800]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [684 tokens], throughput: 3087.980 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.287Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:22.265232, used credits [0.04008669]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]\n",
            "\n",
            "2024-12-28T07:39:41.286Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:27.328365, used credits [0.04920244]\n",
            "- CONTENT [1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c]\n",
            "\n",
            "2024-12-28T07:39:41.285Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:19.610004, used credits [0.03530618]\n",
            "- CONTENT [dd3d6052-821d-4398-8cc5-37fd5feefed7]\n",
            "\n",
            "2024-12-28T07:39:41.279Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.363606, used credits [0.00109800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 1509.875 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.268Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.382802, used credits [0.00090800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1185.991 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.256Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.342290, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 926.115 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.175Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.184263, used credits [0.00125000]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [625 tokens], throughput: 3391.898 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.163Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.189126, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 1432.903 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.149Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.221173, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1424.223 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.127Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.149916, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 2101.179 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.113Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.535774, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 459.148 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.111Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.503604, used credits [0.00063600]\n",
            "- CONTENT [1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [318 tokens], throughput: 631.448 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.109Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.224880, used credits [0.00076800]\n",
            "- CONTENT [dd3d6052-821d-4398-8cc5-37fd5feefed7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1707.580 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.072Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.234030, used credits [0.00076800]\n",
            "- CONTENT [dd3d6052-821d-4398-8cc5-37fd5feefed7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1640.819 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.067Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.189056, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 1676.749 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:41.012Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:22.535664, used credits [0.04057358]\n",
            "- CONTENT [b9b9ea9e-9e6d-4d19-a851-ca0115eba74f]\n",
            "\n",
            "2024-12-28T07:39:40.978Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.677592, used credits [0.03800000]\n",
            "- CONTENT [0ce3818c-7562-43b3-a554-38b014cb36b3]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:40.925Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:24.666947, used credits [0.04441078]\n",
            "- CONTENT [5c81e515-f157-4e72-bff3-fbf7de488625]\n",
            "\n",
            "2024-12-28T07:39:40.919Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:28.002338, used credits [0.05041588]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]\n",
            "\n",
            "2024-12-28T07:39:40.884Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.248372, used credits [0.00063600]\n",
            "- CONTENT [1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [318 tokens], throughput: 1280.335 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.881Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.377284, used credits [0.00112600]\n",
            "- CONTENT [b9b9ea9e-9e6d-4d19-a851-ca0115eba74f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [563 tokens], throughput: 1492.245 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.822Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.187806, used credits [0.00049200]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 1309.860 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.754Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.488456, used credits [0.03800000]\n",
            "- CONTENT [65a42e3c-ac9a-4c93-bf43-fa296df6a1f7]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:40.734Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.235026, used credits [0.00095600]\n",
            "- CONTENT [b9b9ea9e-9e6d-4d19-a851-ca0115eba74f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [478 tokens], throughput: 2033.817 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.657Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.532902, used credits [0.00114000]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [570 tokens], throughput: 1069.615 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.657Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.292770, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 792.431 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.644Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.282250, used credits [0.00150600]\n",
            "- CONTENT [5c81e515-f157-4e72-bff3-fbf7de488625]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [753 tokens], throughput: 2667.850 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.580Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.218886, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 794.934 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.551Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.187162, used credits [0.00034800]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 929.674 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.549Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.205603, used credits [0.00125800]\n",
            "- CONTENT [5c81e515-f157-4e72-bff3-fbf7de488625]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [629 tokens], throughput: 3059.291 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.501Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.085700, used credits [0.00001413]\n",
            "- CONTENT [71f330b4-23a2-40ec-bd77-544a11cf764b]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3658 bytes], throughput: 42683.532 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:40.458Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.114098, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 2033.343 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.378Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.322038, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 813.568 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.326Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.239035, used credits [0.00097200]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [486 tokens], throughput: 2033.176 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.209Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.243794, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 1074.677 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:40.076Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.079872, used credits [0.00001681]\n",
            "- CONTENT [27269714-fa48-438c-b75e-d486a030b697]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4351 bytes], throughput: 54474.591 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:40.022Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:26.107033, used credits [0.04700354]\n",
            "- CONTENT [a7ae398c-7c08-4742-b509-ff8341ad0bb0]\n",
            "\n",
            "2024-12-28T07:39:40.014Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.277855, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 975.330 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.942Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.189462, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 1277.303 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.926Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.426165, used credits [0.00136800]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [684 tokens], throughput: 1605.011 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.920Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.336616, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 805.071 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.875Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.125383, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 1930.092 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.734Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.224088, used credits [0.00109800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 2449.927 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.724Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.266300, used credits [0.00090800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1704.844 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.701Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:23.038366, used credits [0.04147866]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]\n",
            "\n",
            "2024-12-28T07:39:39.674Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.203002, used credits [0.00125000]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [625 tokens], throughput: 3078.795 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.653Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.290334, used credits [0.00139200]\n",
            "- CONTENT [a7ae398c-7c08-4742-b509-ff8341ad0bb0]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [696 tokens], throughput: 2397.239 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.567Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.253928, used credits [0.00111000]\n",
            "- CONTENT [a7ae398c-7c08-4742-b509-ff8341ad0bb0]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [555 tokens], throughput: 2185.662 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.537Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.382502, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 828.753 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.529Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:27.732151, used credits [0.04992943]\n",
            "- CONTENT [88f0827d-ef2c-440c-b135-12baae1564cd]\n",
            "\n",
            "2024-12-28T07:39:39.501Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.342699, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 925.009 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.427Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:26.245762, used credits [0.04725331]\n",
            "- CONTENT [3fcc7bb7-b065-417b-9638-7e746a206271]\n",
            "\n",
            "2024-12-28T07:39:39.378Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.385672, used credits [0.00138600]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 1796.866 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.378Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.465576, used credits [0.00000878]\n",
            "- CONTENT [f0afc1f7-b938-40f0-927d-f6a274243532]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2273 bytes], throughput: 4882.122 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:39.356Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.380495, used credits [0.00111400]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 1463.884 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.264Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.509211, used credits [0.00310800]\n",
            "- CONTENT [88f0827d-ef2c-440c-b135-12baae1564cd]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1554 tokens], throughput: 3051.782 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.227Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.525879, used credits [0.00059400]\n",
            "- CONTENT [3fcc7bb7-b065-417b-9638-7e746a206271]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 564.769 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.121Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:27.132816, used credits [0.04885037]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]\n",
            "\n",
            "2024-12-28T07:39:39.016Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.273816, used credits [0.00059400]\n",
            "- CONTENT [3fcc7bb7-b065-417b-9638-7e746a206271]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 1084.670 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:39.005Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.287620, used credits [0.00239200]\n",
            "- CONTENT [88f0827d-ef2c-440c-b135-12baae1564cd]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1196 tokens], throughput: 4158.270 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:38.809Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:30.376896, used credits [0.05469107]\n",
            "- CONTENT [0457d83e-b449-40c8-9544-c268a114e122]\n",
            "\n",
            "2024-12-28T07:39:38.761Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.594722, used credits [0.00109800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 923.120 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:38.744Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.146594, used credits [0.00001288]\n",
            "- CONTENT [dd3d6052-821d-4398-8cc5-37fd5feefed7]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3334 bytes], throughput: 22743.071 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:38.739Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:29.069882, used credits [0.05233790]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]\n",
            "\n",
            "2024-12-28T07:39:38.688Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:22.633632, used credits [0.04074997]\n",
            "- CONTENT [1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c]\n",
            "\n",
            "2024-12-28T07:39:38.517Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.368998, used credits [0.00090800]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1230.358 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:38.510Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.404168, used credits [0.00069600]\n",
            "- CONTENT [0457d83e-b449-40c8-9544-c268a114e122]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [348 tokens], throughput: 861.028 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:38.400Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.480364, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 655.753 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:38.386Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.220877, used credits [0.00069600]\n",
            "- CONTENT [0457d83e-b449-40c8-9544-c268a114e122]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [348 tokens], throughput: 1575.536 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:38.112Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.928725, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 260.572 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:38.110Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.209007, used credits [0.00063000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1507.129 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:38.044Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.863503, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 313.838 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:38.022Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.650661, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 402.667 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:38.022Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.841345, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 275.749 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.940Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.571328, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 458.580 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.937Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.775760, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 349.335 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.911Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:32.799495, used credits [0.05905276]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]\n",
            "\n",
            "2024-12-28T07:39:37.884Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.109282, used credits [0.00000764]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1976 bytes], throughput: 18081.677 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:37.881Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.698706, used credits [0.00114000]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [570 tokens], throughput: 815.794 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.864Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.706725, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 328.275 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.721Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.555991, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 435.259 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.717Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.550724, used credits [0.00097200]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [486 tokens], throughput: 882.474 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.571Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.080493, used credits [0.00002022]\n",
            "- CONTENT [5c81e515-f157-4e72-bff3-fbf7de488625]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5234 bytes], throughput: 65024.369 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:37.567Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.353578, used credits [0.00136800]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [684 tokens], throughput: 1934.508 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.467Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.270320, used credits [0.00125000]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [625 tokens], throughput: 2312.071 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.456Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:31.512722, used credits [0.05673603]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]\n",
            "\n",
            "2024-12-28T07:39:37.455Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:29.024303, used credits [0.05225584]\n",
            "- CONTENT [4830e2a8-ea48-4f1e-878c-3bf37909c275]\n",
            "\n",
            "2024-12-28T07:39:37.408Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.243259, used credits [0.00000663]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1717 bytes], throughput: 7058.312 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:37.226Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.401043, used credits [0.00126800]\n",
            "- CONTENT [4830e2a8-ea48-4f1e-878c-3bf37909c275]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [634 tokens], throughput: 1580.877 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.200Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.104558, used credits [0.00001352]\n",
            "- CONTENT [b9b9ea9e-9e6d-4d19-a851-ca0115eba74f]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3499 bytes], throughput: 33464.584 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:37.200Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.446696, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 709.656 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:37.020Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.199706, used credits [0.00107200]\n",
            "- CONTENT [4830e2a8-ea48-4f1e-878c-3bf37909c275]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [536 tokens], throughput: 2683.943 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:36.994Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.180975, used credits [0.00063400]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 1751.622 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:36.932Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:27.415150, used credits [0.04935869]\n",
            "- CONTENT [6da2b811-9ec9-484c-a102-a4a7b4a89199]\n",
            "\n",
            "2024-12-28T07:39:36.895Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:29.720848, used credits [0.05350991]\n",
            "- CONTENT [7694d101-376d-4cbe-a773-3b64002727bf]\n",
            "\n",
            "2024-12-28T07:39:36.834Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:30.904997, used credits [0.05564187]\n",
            "- CONTENT [53a80fc0-7526-4cb8-a921-b66f0c941380]\n",
            "\n",
            "2024-12-28T07:39:36.752Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.286003, used credits [0.00070200]\n",
            "- CONTENT [6da2b811-9ec9-484c-a102-a4a7b4a89199]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [351 tokens], throughput: 1227.258 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:36.642Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.178853, used credits [0.00070200]\n",
            "- CONTENT [6da2b811-9ec9-484c-a102-a4a7b4a89199]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [351 tokens], throughput: 1962.509 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:36.572Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.144710, used credits [0.00002005]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5189 bytes], throughput: 35857.972 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:36.572Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.364407, used credits [0.00039400]\n",
            "- CONTENT [7694d101-376d-4cbe-a773-3b64002727bf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 540.605 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:36.478Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:25.612904, used credits [0.04611390]\n",
            "- CONTENT [14614eae-5e42-4912-aa53-2ce05a1ce2d2]\n",
            "\n",
            "2024-12-28T07:39:36.476Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.376533, used credits [0.00214200]\n",
            "- CONTENT [53a80fc0-7526-4cb8-a921-b66f0c941380]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1071 tokens], throughput: 2844.375 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:36.391Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.247524, used credits [0.00039400]\n",
            "- CONTENT [7694d101-376d-4cbe-a773-3b64002727bf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 795.882 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:36.347Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.424675, used credits [0.00166800]\n",
            "- CONTENT [53a80fc0-7526-4cb8-a921-b66f0c941380]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [834 tokens], throughput: 1963.856 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:36.330Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.209664, used credits [0.00001046]\n",
            "- CONTENT [1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2707 bytes], throughput: 12911.122 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:36.109Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.149304, used credits [0.00002159]\n",
            "- CONTENT [a7ae398c-7c08-4742-b509-ff8341ad0bb0]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5588 bytes], throughput: 37426.919 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:35.782Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.394305, used credits [0.00125200]\n",
            "- CONTENT [14614eae-5e42-4912-aa53-2ce05a1ce2d2]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [626 tokens], throughput: 1587.604 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:35.778Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.634554, used credits [0.00103400]\n",
            "- CONTENT [14614eae-5e42-4912-aa53-2ce05a1ce2d2]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [517 tokens], throughput: 814.746 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:35.606Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.486800, used credits [0.00077414]\n",
            "- CONTENT [71f330b4-23a2-40ec-bd77-544a11cf764b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-21-support-openai-cohere-jina-mistral-voyage-and-google-ai-embedding-models]\n",
            "- File upload [200346 bytes], throughput: 411557.530 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:35.567Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.237083, used credits [0.00001084]\n",
            "- CONTENT [3fcc7bb7-b065-417b-9638-7e746a206271]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2805 bytes], throughput: 11831.290 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:35.405Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:24.889777, used credits [0.04481197]\n",
            "- CONTENT [259e0a23-5ac8-49b8-b180-593fec210fdc]\n",
            "\n",
            "2024-12-28T07:39:35.252Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.275024, used credits [0.00001167]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3020 bytes], throughput: 10980.864 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:34.663Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.174262, used credits [0.00001807]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4677 bytes], throughput: 26838.898 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:34.661Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.154096, used credits [0.00003752]\n",
            "- CONTENT [88f0827d-ef2c-440c-b135-12baae1564cd]: Content type [PAGE], file type [DATA]\n",
            "- File upload [9709 bytes], throughput: 63006.301 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:34.634Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.244139, used credits [0.00071467]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-16-support-for-image-description-multi-turn-text-summarization]\n",
            "- File upload [184956 bytes], throughput: 757585.439 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:34.437Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.383204, used credits [0.00146200]\n",
            "- CONTENT [259e0a23-5ac8-49b8-b180-593fec210fdc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [731 tokens], throughput: 1907.602 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:34.428Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.153023, used credits [0.00130800]\n",
            "- CONTENT [259e0a23-5ac8-49b8-b180-593fec210fdc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [654 tokens], throughput: 4273.865 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:33.949Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.502363, used credits [0.00083014]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-19-support-for-openai-gpt-4o-mini-byo-key-for-azure-ai-similarity-by-summary-bug-fixes]\n",
            "- File upload [214839 bytes], throughput: 427657.319 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:33.748Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.041979, used credits [0.03800000]\n",
            "- CONTENT [71f330b4-23a2-40ec-bd77-544a11cf764b]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:33.680Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.106519, used credits [0.00001357]\n",
            "- CONTENT [0457d83e-b449-40c8-9544-c268a114e122]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3512 bytes], throughput: 32970.675 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:33.678Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.176356, used credits [0.00073272]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-11-support-for-azure-ai-document-intelligence-by-default-language-aware-summaries]\n",
            "- File upload [189627 bytes], throughput: 1075254.245 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:33.383Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.336483, used credits [0.00074897]\n",
            "- CONTENT [f0afc1f7-b938-40f0-927d-f6a274243532]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-4-support-for-anthropic-claude-3.5-haiku-bug-fixes]\n",
            "- File upload [193833 bytes], throughput: 576055.335 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:33.189Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.122854, used credits [0.00081108]\n",
            "- CONTENT [27269714-fa48-438c-b75e-d486a030b697]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2023/october-15-support-for-anthropic-claude-models-slack-feeds-and-entity-enrichment]\n",
            "- File upload [209906 bytes], throughput: 1708576.745 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:33.023Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.297715, used credits [0.00075420]\n",
            "- CONTENT [1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-23-support-for-linear-github-issues-and-jira-issue-feeds-ingest-files-via-web-feed-sitemap]\n",
            "- File upload [195186 bytes], throughput: 655614.471 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:32.630Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.393520, used credits [0.00086662]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/may-2024/may-5-support-for-jina-and-pongo-rerankers-microsoft-teams-feed-new-youtube-downloader-bug-fixes]\n",
            "- File upload [224281 bytes], throughput: 569936.179 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:32.615Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.639516, used credits [0.03800000]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:32.546Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:15.883600, used credits [0.03800000]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:32.264Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:31.551512, used credits [0.05680587]\n",
            "- CONTENT [7013d1b5-eaa6-4f63-b9f5-973fb33abad4]\n",
            "\n",
            "2024-12-28T07:39:32.254Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.333986, used credits [0.00078914]\n",
            "- CONTENT [dd3d6052-821d-4398-8cc5-37fd5feefed7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-24-support-for-direct-llm-prompt-multi-turn-image-analysis-bug-fixes]\n",
            "- File upload [204228 bytes], throughput: 611487.441 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:32.244Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.306937, used credits [0.00071373]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-17-prepare-for-usage-based-billing-append-sas-tokens-to-uris]\n",
            "- File upload [184714 bytes], throughput: 601798.351 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:32.215Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.189735, used credits [0.00077073]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-28-support-for-indexing-workflow-stage-azure-ai-language-detection-bug-fixes]\n",
            "- File upload [199464 bytes], throughput: 1051275.119 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:32.198Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.190935, used credits [0.00085017]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-10-support-for-claude-3-mistral-and-groq-models-usage-credits-telemetry-bug-fixes]\n",
            "- File upload [220024 bytes], throughput: 1152352.087 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:32.193Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:28.982357, used credits [0.05218032]\n",
            "- CONTENT [eec9e87f-091c-4ba9-b7d9-b8b21a41b458]\n",
            "\n",
            "2024-12-28T07:39:32.187Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.069199, used credits [0.00000795]\n",
            "- CONTENT [7694d101-376d-4cbe-a773-3b64002727bf]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2058 bytes], throughput: 29740.443 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:32.167Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:28.308332, used credits [0.05096679]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]\n",
            "\n",
            "2024-12-28T07:39:32.162Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:31.668078, used credits [0.05701574]\n",
            "- CONTENT [1433f70c-c943-4ea6-849a-0c9fd1eaeb10]\n",
            "\n",
            "2024-12-28T07:39:32.128Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.400675, used credits [0.00073974]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-20-support-for-medical-entities-anthropic-prompt-caching-bug-fixes]\n",
            "- File upload [191443 bytes], throughput: 477800.972 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:32.108Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:30.941942, used credits [0.05570839]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]\n",
            "\n",
            "2024-12-28T07:39:32.106Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:30.644592, used credits [0.05517303]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]\n",
            "\n",
            "2024-12-28T07:39:32.094Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:27.735720, used credits [0.04993585]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]\n",
            "\n",
            "2024-12-28T07:39:32.088Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.687120, used credits [0.03800000]\n",
            "- CONTENT [27269714-fa48-438c-b75e-d486a030b697]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:32.083Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.456193, used credits [0.00074911]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-25-support-for-mistral-large-2-and-nemo-groq-llama-3.1-models-bug-fixes]\n",
            "- File upload [193869 bytes], throughput: 424971.169 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:32.070Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:24.752439, used credits [0.04456470]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]\n",
            "\n",
            "2024-12-28T07:39:32.034Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:31.128453, used credits [0.05604418]\n",
            "- CONTENT [bdad98c7-3cd1-4a67-af1c-334852ad7ce2]\n",
            "\n",
            "2024-12-28T07:39:32.033Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.306366, used credits [0.00001223]\n",
            "- CONTENT [6da2b811-9ec9-484c-a102-a4a7b4a89199]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3165 bytes], throughput: 10330.791 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:32.032Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.246248, used credits [0.00001000]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2588 bytes], throughput: 10509.747 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:31.997Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.658190, used credits [0.00076468]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/february-2024/february-2-support-for-semantic-alerts-openai-0125-models-performance-enhancements-bug-fixes]\n",
            "- File upload [197899 bytes], throughput: 300671.356 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:31.891Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.553262, used credits [0.00081168]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-3-new-data-model-for-observations-new-category-entity]\n",
            "- File upload [210061 bytes], throughput: 379677.122 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:31.817Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:15.044810, used credits [0.03800000]\n",
            "- CONTENT [1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:31.772Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:14.991162, used credits [0.03800000]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:31.707Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.542543, used credits [0.00071390]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-13-support-for-claude-3-haiku-model-direct-ingestion-of-base64-encoded-files]\n",
            "- File upload [184756 bytes], throughput: 340537.252 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:31.668Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.084579, used credits [0.00001633]\n",
            "- CONTENT [4830e2a8-ea48-4f1e-878c-3bf37909c275]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4226 bytes], throughput: 49965.180 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:31.666Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.749318, used credits [0.00120000]\n",
            "- CONTENT [7013d1b5-eaa6-4f63-b9f5-973fb33abad4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [600 tokens], throughput: 800.729 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:31.642Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.335565, used credits [0.00071004]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-16-support-for-image-description-multi-turn-text-summarization]\n",
            "- File upload [183758 bytes], throughput: 547607.603 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:31.570Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.262709, used credits [0.00002728]\n",
            "- CONTENT [53a80fc0-7526-4cb8-a921-b66f0c941380]: Content type [PAGE], file type [DATA]\n",
            "- File upload [7060 bytes], throughput: 26873.821 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:31.568Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.665871, used credits [0.00098000]\n",
            "- CONTENT [7013d1b5-eaa6-4f63-b9f5-973fb33abad4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [490 tokens], throughput: 735.878 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:31.504Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.555876, used credits [0.00075576]\n",
            "- CONTENT [b9b9ea9e-9e6d-4d19-a851-ca0115eba74f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-10-support-for-web-search-multi-turn-content-summarization-deepgram-language-detection]\n",
            "- File upload [195589 bytes], throughput: 351857.312 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:31.405Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.123414, used credits [0.00001948]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5041 bytes], throughput: 40846.224 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:31.401Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:08.817323, used credits [0.03800000]\n",
            "- CONTENT [f0afc1f7-b938-40f0-927d-f6a274243532]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:31.341Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.596871, used credits [0.00059400]\n",
            "- CONTENT [eec9e87f-091c-4ba9-b7d9-b8b21a41b458]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 497.595 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:31.256Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:14.382797, used credits [0.03800000]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:31.176Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:14.389538, used credits [0.03800000]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:31.169Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.743782, used credits [0.00161800]\n",
            "- CONTENT [1433f70c-c943-4ea6-849a-0c9fd1eaeb10]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [809 tokens], throughput: 463.934 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:31.164Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.596641, used credits [0.00188000]\n",
            "- CONTENT [1433f70c-c943-4ea6-849a-0c9fd1eaeb10]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [940 tokens], throughput: 588.736 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:31.081Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.388601, used credits [0.00114000]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [570 tokens], throughput: 1466.799 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:31.076Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.444887, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 167.487 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:31.050Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.088112, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 249.055 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:31.011Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.089101, used credits [0.00001951]\n",
            "- CONTENT [14614eae-5e42-4912-aa53-2ce05a1ce2d2]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5048 bytes], throughput: 56654.617 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:31.010Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.160920, used credits [0.00002153]\n",
            "- CONTENT [259e0a23-5ac8-49b8-b180-593fec210fdc]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5572 bytes], throughput: 34625.858 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:30.994Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.351864, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 193.806 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:30.969Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.261443, used credits [0.00059400]\n",
            "- CONTENT [eec9e87f-091c-4ba9-b7d9-b8b21a41b458]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 1136.004 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:30.955Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.267553, used credits [0.00048400]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 190.919 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:30.955Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.386792, used credits [0.00155000]\n",
            "- CONTENT [bdad98c7-3cd1-4a67-af1c-334852ad7ce2]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [775 tokens], throughput: 558.844 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:30.955Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.386586, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 167.317 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:30.955Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.213341, used credits [0.00054200]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 223.350 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:30.954Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.430306, used credits [0.00137200]\n",
            "- CONTENT [bdad98c7-3cd1-4a67-af1c-334852ad7ce2]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [686 tokens], throughput: 479.618 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:30.919Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.492841, used credits [0.00052400]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 175.504 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:30.908Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.369970, used credits [0.00046400]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 169.347 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:30.873Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.507596, used credits [0.00097200]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [486 tokens], throughput: 957.455 tokens/sec\n",
            "\n",
            "2024-12-28T07:39:30.807Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:14.064778, used credits [0.03800000]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:30.777Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:13.925638, used credits [0.03800000]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:30.697Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:14.030433, used credits [0.03800000]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:30.393Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:13.626995, used credits [0.03800000]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:30.384Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:13.710422, used credits [0.03800000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:30.375Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:13.585408, used credits [0.03800000]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:30.180Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:08.452608, used credits [0.03800000]\n",
            "- CONTENT [dd3d6052-821d-4398-8cc5-37fd5feefed7]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:30.119Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.486591, used credits [0.00083445]\n",
            "- CONTENT [5c81e515-f157-4e72-bff3-fbf7de488625]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/may-2024/may-15-support-for-graphrag-openai-gpt-4o-model-performance-improvements-and-bug-fixes]\n",
            "- File upload [215956 bytes], throughput: 443814.027 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:29.740Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.415021, used credits [0.00086210]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/may-2024/may-5-support-for-jina-and-pongo-rerankers-microsoft-teams-feed-new-youtube-downloader-bug-fixes]\n",
            "- File upload [223111 bytes], throughput: 537589.794 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:29.631Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.562184, used credits [0.03800000]\n",
            "- CONTENT [ca79d129-fe97-4afb-aa98-43fd1b5386cb]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:29.628Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.424658, used credits [0.00092844]\n",
            "- CONTENT [a7ae398c-7c08-4742-b509-ff8341ad0bb0]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2023/july-15-support-for-sharepoint-feeds-new-conversation-features]\n",
            "- File upload [240280 bytes], throughput: 565820.424 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:28.760Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.542509, used credits [0.00076311]\n",
            "- CONTENT [3fcc7bb7-b065-417b-9638-7e746a206271]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/january-2024/january-22-support-for-google-and-microsoft-email-feeds-reingest-content-in-place-bug-fixes]\n",
            "- File upload [197491 bytes], throughput: 364032.469 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:28.744Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.221272, used credits [0.03800000]\n",
            "- CONTENT [b9b9ea9e-9e6d-4d19-a851-ca0115eba74f]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:28.629Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.597223, used credits [0.00101491]\n",
            "- CONTENT [88f0827d-ef2c-440c-b135-12baae1564cd]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/january-2024/january-18-support-for-content-publishing-llm-tools-clip-image-embeddings-bug-fixes]\n",
            "- File upload [262659 bytes], throughput: 439800.691 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:28.616Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.542095, used credits [0.00085112]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-10-support-for-claude-3-mistral-and-groq-models-usage-credits-telemetry-bug-fixes]\n",
            "- File upload [220268 bytes], throughput: 406327.529 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:28.606Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:12.057448, used credits [0.03800000]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:28.528Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.642922, used credits [0.00070741]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-13-support-for-claude-3-haiku-model-direct-ingestion-of-base64-encoded-files]\n",
            "- File upload [183077 bytes], throughput: 284757.760 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:28.232Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:11.885029, used credits [0.03800000]\n",
            "- CONTENT [5c81e515-f157-4e72-bff3-fbf7de488625]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:27.707Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.838882, used credits [0.00076146]\n",
            "- CONTENT [1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-23-support-for-linear-github-issues-and-jira-issue-feeds-ingest-files-via-web-feed-sitemap]\n",
            "- File upload [197064 bytes], throughput: 234912.770 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:27.035Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.308141, used credits [0.03800000]\n",
            "- CONTENT [e74c8a01-189a-4f47-87e6-2c2e33b3da7a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:26.618Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.196146, used credits [0.00001074]\n",
            "- CONTENT [eec9e87f-091c-4ba9-b7d9-b8b21a41b458]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2779 bytes], throughput: 14168.039 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:26.549Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:12.536010, used credits [0.03800000]\n",
            "- CONTENT [a7ae398c-7c08-4742-b509-ff8341ad0bb0]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:26.334Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.448281, used credits [0.00079220]\n",
            "- CONTENT [0457d83e-b449-40c8-9544-c268a114e122]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-4-support-for-webhook-alerts-keywords-summarization-deepseek-128k-context-window-bug-fixes]\n",
            "- File upload [205021 bytes], throughput: 457348.990 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:26.072Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.127652, used credits [0.00001077]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2786 bytes], throughput: 21824.927 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:26.033Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.111857, used credits [0.00001635]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4231 bytes], throughput: 37825.114 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:26.020Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.589972, used credits [0.00077154]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/february-2024/february-2-support-for-semantic-alerts-openai-0125-models-performance-enhancements-bug-fixes]\n",
            "- File upload [199673 bytes], throughput: 338444.875 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:26.015Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.137061, used credits [0.00002253]\n",
            "- CONTENT [bdad98c7-3cd1-4a67-af1c-334852ad7ce2]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5830 bytes], throughput: 42535.774 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:25.996Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.121266, used credits [0.00000811]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2098 bytes], throughput: 17300.838 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:25.994Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.130002, used credits [0.00001627]\n",
            "- CONTENT [7013d1b5-eaa6-4f63-b9f5-973fb33abad4]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4211 bytes], throughput: 32391.735 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:25.988Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:13.322670, used credits [0.03800000]\n",
            "- CONTENT [0490de5f-5c5d-4636-b76a-2b91ba640fe8]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:25.978Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.104764, used credits [0.00000906]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2345 bytes], throughput: 22383.643 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:25.973Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.114335, used credits [0.00002518]\n",
            "- CONTENT [1433f70c-c943-4ea6-849a-0c9fd1eaeb10]: Content type [PAGE], file type [DATA]\n",
            "- File upload [6517 bytes], throughput: 56999.319 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:25.933Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.064122, used credits [0.00000946]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2447 bytes], throughput: 38161.570 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:25.901Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:12.688319, used credits [0.03800000]\n",
            "- CONTENT [3fcc7bb7-b065-417b-9638-7e746a206271]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:25.815Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:13.759590, used credits [0.03800000]\n",
            "- CONTENT [88f0827d-ef2c-440c-b135-12baae1564cd]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:25.731Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.198387, used credits [0.00075921]\n",
            "- CONTENT [6da2b811-9ec9-484c-a102-a4a7b4a89199]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/june-2024/june-21-support-for-the-claude-3.5-sonnet-model-knowledge-graph-semantic-search-and-bug-fixes]\n",
            "- File upload [196483 bytes], throughput: 163956.204 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:25.580Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:12.376603, used credits [0.03800000]\n",
            "- CONTENT [1f279e50-9b05-4d17-a2af-22177d8f091a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:25.456Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.575042, used credits [0.00075051]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-25-support-for-mistral-large-2-and-nemo-groq-llama-3.1-models-bug-fixes]\n",
            "- File upload [194231 bytes], throughput: 123317.950 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:24.948Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.577183, used credits [0.03800000]\n",
            "- CONTENT [1a1dc70c-c908-4bfe-b4a7-dc8ed618e44c]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:24.885Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.399458, used credits [0.00073896]\n",
            "- CONTENT [7694d101-376d-4cbe-a773-3b64002727bf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2024/december-1-support-for-retrieval-only-rag-pipeline-bug-fixes]\n",
            "- File upload [191241 bytes], throughput: 136653.648 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:24.822Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.245254, used credits [0.00084356]\n",
            "- CONTENT [14614eae-5e42-4912-aa53-2ce05a1ce2d2]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/june-2024/june-9-support-for-deepseek-models-json-ld-webpage-parsing-performance-improvements-and-bug-fixes]\n",
            "- File upload [218312 bytes], throughput: 175315.166 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:24.287Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.508560, used credits [0.00080673]\n",
            "- CONTENT [4830e2a8-ea48-4f1e-878c-3bf37909c275]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2024/december-9-support-for-website-mapping-web-page-screenshots-groq-llama-3.3-model-bug-fixes]\n",
            "- File upload [208780 bytes], throughput: 138396.845 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:24.286Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.980188, used credits [0.00100258]\n",
            "- CONTENT [53a80fc0-7526-4cb8-a921-b66f0c941380]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2023/december-10-support-for-openai-gpt-4-turbo-llama-2-and-mistral-models-query-by-example-bug-fixes]\n",
            "- File upload [259468 bytes], throughput: 264712.376 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:24.263Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.350310, used credits [0.00082888]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-19-support-for-openai-gpt-4o-mini-byo-key-for-azure-ai-similarity-by-summary-bug-fixes]\n",
            "- File upload [214513 bytes], throughput: 158861.992 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:24.143Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:15.657107, used credits [0.03800000]\n",
            "- CONTENT [0457d83e-b449-40c8-9544-c268a114e122]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:23.276Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.743162, used credits [0.00085370]\n",
            "- CONTENT [259e0a23-5ac8-49b8-b180-593fec210fdc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/february-2024/february-21-support-for-onedrive-and-google-drive-feeds-extract-images-from-pdfs-bug-fixes]\n",
            "- File upload [220938 bytes], throughput: 297294.533 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:21.874Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:12.044980, used credits [0.03800000]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:21.193Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.217314, used credits [0.03800000]\n",
            "- CONTENT [14614eae-5e42-4912-aa53-2ce05a1ce2d2]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:20.139Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.310016, used credits [0.00075576]\n",
            "- CONTENT [eec9e87f-091c-4ba9-b7d9-b8b21a41b458]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-9-support-direct-text-markdown-and-html-ingestion-new-specification-llm-strategy]\n",
            "- File upload [195590 bytes], throughput: 149303.566 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:20.075Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.529247, used credits [0.03800000]\n",
            "- CONTENT [6da2b811-9ec9-484c-a102-a4a7b4a89199]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:18.794Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.366654, used credits [0.00076618]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-28-support-for-indexing-workflow-stage-azure-ai-language-detection-bug-fixes]\n",
            "- File upload [198287 bytes], throughput: 145089.404 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:18.794Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.210783, used credits [0.00081403]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-3-new-data-model-for-observations-new-category-entity]\n",
            "- File upload [210669 bytes], throughput: 173994.091 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:18.727Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.415547, used credits [0.00071522]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-17-prepare-for-usage-based-billing-append-sas-tokens-to-uris]\n",
            "- File upload [185098 bytes], throughput: 130760.740 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:18.466Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.621184, used credits [0.03800000]\n",
            "- CONTENT [259e0a23-5ac8-49b8-b180-593fec210fdc]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:18.458Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:11.234619, used credits [0.03800000]\n",
            "- CONTENT [7694d101-376d-4cbe-a773-3b64002727bf]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:18.441Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.981589, used credits [0.03800000]\n",
            "- CONTENT [4830e2a8-ea48-4f1e-878c-3bf37909c275]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:18.428Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:12.455724, used credits [0.03800000]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:18.387Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:13.196686, used credits [0.03800000]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:18.080Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:12.118182, used credits [0.03800000]\n",
            "- CONTENT [53a80fc0-7526-4cb8-a921-b66f0c941380]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:17.973Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.332569, used credits [0.00074018]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-20-support-for-medical-entities-anthropic-prompt-caching-bug-fixes]\n",
            "- File upload [191557 bytes], throughput: 143750.209 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:16.823Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.713815, used credits [0.00082374]\n",
            "- CONTENT [7013d1b5-eaa6-4f63-b9f5-973fb33abad4]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/april-2024/april-23-support-for-python-and-typescript-sdks-latest-openai-cohere-and-groq-models-bug-fixes]\n",
            "- File upload [213182 bytes], throughput: 298651.528 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:16.267Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.911241, used credits [0.00073202]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-11-support-for-azure-ai-document-intelligence-by-default-language-aware-summaries]\n",
            "- File upload [189446 bytes], throughput: 207898.853 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:15.452Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.405524, used credits [0.00086923]\n",
            "- CONTENT [bdad98c7-3cd1-4a67-af1c-334852ad7ce2]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/april-2024/april-7-support-for-discord-feeds-cohere-reranking-section-aware-chunking-and-retrieval]\n",
            "- File upload [224955 bytes], throughput: 160050.583 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:15.096Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.173442, used credits [0.00094142]\n",
            "- CONTENT [1433f70c-c943-4ea6-849a-0c9fd1eaeb10]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/]\n",
            "- File upload [243638 bytes], throughput: 207626.864 bytes/sec\n",
            "\n",
            "2024-12-28T07:39:14.378Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.025546, used credits [0.03800000]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:14.368Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:12.750182, used credits [0.03800000]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:13.917Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:12.724234, used credits [0.03800000]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:13.308Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.993575, used credits [0.03800000]\n",
            "- CONTENT [eec9e87f-091c-4ba9-b7d9-b8b21a41b458]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:12.881Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:08.485803, used credits [0.03800000]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:10.434Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:06.497927, used credits [0.03800000]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:10.397Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.657058, used credits [0.03800000]\n",
            "- CONTENT [7013d1b5-eaa6-4f63-b9f5-973fb33abad4]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:08.871Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.899569, used credits [0.03800000]\n",
            "- CONTENT [bdad98c7-3cd1-4a67-af1c-334852ad7ce2]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:08.717Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:08.127182, used credits [0.03800000]\n",
            "- CONTENT [1433f70c-c943-4ea6-849a-0c9fd1eaeb10]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-28T07:39:04.523Z: Serverless compute\n",
            "- Workflow [Feed] took 0:00:10.029245, used credits [0.00902841]\n",
            "\n",
            "2024-12-28T07:38:44.888Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:01.683820, used credits [0.00303158]\n",
            "\n",
            "2024-12-28T07:38:43.132Z: GraphQL\n",
            "- Operation took 0:00:00.565170, used credits [0.00000000]\n",
            "- Request:\n",
            "mutation CreateFeed($feed: FeedInput!, $correlationId: String) { createFeed(feed: $feed, correlationId: $correlationId) { id name state type } }\n",
            "- Variables:\n",
            "{\"feed\":\"{ name: \\\"https:\\\\/\\\\/changelog.graphlit.dev\\\", type: WEB, web: { uri: \\\"https:\\\\/\\\\/changelog.graphlit.dev\\\", readLimit: 100 } }\",\"correlationId\":\"\\\"2024-12-28T07:38:42.517980\\\"\"}\n",
            "- Response:\n",
            "{\"data\":{\"createFeed\":{\"id\":\"ff940e5a-1374-4664-9701-108390a9a6f3\",\"name\":\"https://changelog.graphlit.dev\",\"state\":\"ENABLED\",\"type\":\"WEB\"}}}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "credits = await lookup_credits(publish_correlation_id)\n",
        "\n",
        "if credits is not None:\n",
        "    display(Markdown(f\"### Credits used: {credits.credits:.6f} for publishing\"))\n",
        "    print(f\"- storage [{credits.storage_ratio:.2f}%], compute [{credits.compute_ratio:.2f}%]\")\n",
        "    print(f\"- embedding [{credits.embedding_ratio:.2f}%], completion [{credits.completion_ratio:.2f}%]\")\n",
        "    print(f\"- ingestion [{credits.ingestion_ratio:.2f}%], indexing [{credits.indexing_ratio:.2f}%], preparation [{credits.preparation_ratio:.2f}%], extraction [{credits.extraction_ratio:.2f}%], enrichment [{credits.enrichment_ratio:.2f}%], publishing [{credits.publishing_ratio:.2f}%]\")\n",
        "    print(f\"- search [{credits.search_ratio:.2f}%], conversation [{credits.conversation_ratio:.2f}%]\")\n",
        "    print()\n",
        "\n",
        "usage = await lookup_usage(publish_correlation_id)\n",
        "\n",
        "if usage is not None:\n",
        "    display(Markdown(f\"### Usage records:\"))\n",
        "\n",
        "    for record in usage:\n",
        "        dump_usage_record(record)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0nqb7p9Rrr1b",
        "outputId": "0cc6cc6f-5264-4d78-a43b-686526aadb52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Credits used: 80.485904 for publishing"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- storage [0.17%], compute [0.06%]\n",
            "- embedding [0.01%], completion [0.21%]\n",
            "- ingestion [0.00%], indexing [0.00%], preparation [1.76%], extraction [0.00%], enrichment [0.00%], publishing [97.77%]\n",
            "- search [0.02%], conversation [0.00%]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Usage records:"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-28T07:42:12.006Z: GraphQL\n",
            "- Operation took 0:00:08.734114, used credits [0.00000000]\n",
            "- Request:\n",
            "mutation PublishContents($summaryPrompt: String, $publishPrompt: String!, $connector: ContentPublishingConnectorInput!, $filter: ContentFilter, $isSynchronous: Boolean, $correlationId: String, $name: String, $summarySpecification: EntityReferenceInput, $publishSpecification: EntityReferenceInput, $workflow: EntityReferenceInput) { publishContents(summaryPrompt: $summaryPrompt, publishPrompt: $publishPrompt, connector: $connector, filter: $filter, isSynchronous: $isSynchronous, correlationId: $correlationId, name: $name, summarySpecification: $summarySpecification, publishSpecification: $publishSpecification, workflow: $workflow) { id name state type fileType mimeType uri collections { id name } observations { id type observable { id name } related { id name } relatedType relation occurrences { type confidence startTime endTime pageIndex boundingBox { left top width height } } state } textUri audioUri markdown } }\n",
            "- Variables:\n",
            "{\"summaryPrompt\":\"\\\"\\\\nYou are an AI assistant that extracts the most important information from product changelog pages.\\\\n\\\\nYou are being provided a changelog web page for one of many releases of the Graphlit Platform in 2024.\\\\n\\\\nYour task is to produce a concise summary that covers:\\\\n\\\\nNew Features – Briefly list or describe each new capability.\\\\nEnhancements\\\\/Improvements – Any notable improvements or changes.\\\\nBug Fixes – Summaries of what was fixed and why it matters.\\\\nOther Key Details – Any version numbers, feature flags, or breaking changes.\\\\nDates - When a feature was released\\\\nValue - What this offers to developers.\\\\nKeep it succinct, accurate, and organized. Use short sentences or bullet points so it’s easy to incorporate into a map\\\\/reduce pipeline. Omit any superfluous text.\\\\n\\\\nOutput:\\\\nA concise summary in bullet points highlighting the essential updates from the changelog.\\\\n\\\"\",\"publishPrompt\":\"\\\"\\\\nYou are an enthusiastic host focused on developer marketing, and you work for Graphlit who is creating a 2024 year-in-review of their API-based platform.\\\\n\\\\nDon't refer to yourself in the script. Just talk to the audience.\\\\n\\\\nDon't add in any podcast-like references like intro music, sound effects, etc.  This will be used with a text-to-speech API to generate an audio recording.\\\\n\\\\nYour audience is somewhat technical — software engineers, product builders, and tech-savvy product managers — so the script should be clear, concise, and sprinkled with a bit of technical depth.\\\\n\\\\nUsing the provided changelog for the Graphlit Platform, create a podcast-like script that:\\\\n\\\\n- Sets the stage with a warm, engaging introduction.\\\\n- Highlights each new feature, explaining how it helps developers or teams be more productive, efficient, or creative.\\\\n- Refers to when a feature was released.\\\\n- Mentions any model updates and why they matter for technical use cases.\\\\n- Reviews notable bug fixes, providing just enough context to show the improvements without overwhelming detail.\\\\n- Closes with a quick recap and a call to action, encouraging listeners to try out the new features or learn more.\\\\n\\\\nAt the very end, mention that the listener can signup for free at graphlit.com and try out all these features.\\\\nAlso, mention that in 2025, Graphlit will be offering exciting new features to accelerate the building of AI agents.\\\\n\\\\nThe tone should be friendly, positive, and confident—like a technology evangelist who’s genuinely excited about these updates.\\\\n\\\\nKeep it interesting and conversational, but maintain enough depth to engage developers who care about how things work under the hood.\\\\nUse analogies or practical examples to illustrate why certain features are useful.\\\\nFeel free to add transitions such as “Now, let’s dive in,” or “Moving on to our next highlight” to keep it flowing.\\\\n\\\\nOutput: A detailed, TTS-ready 10-minute long script that hits all the points above.\\\\n\\\"\",\"connector\":\"{ type: ELEVEN_LABS_AUDIO, format: MP3, elevenLabs: { model: TURBO_V2_5, voice: \\\"ZF6FPAbjXT4488VcRRnw\\\" } }\",\"filter\":\"{ feeds: [ { id: \\\"ff940e5a-1374-4664-9701-108390a9a6f3\\\" } ] }\",\"isSynchronous\":\"true\",\"correlationId\":\"\\\"2024-12-28T07:38:42.518067\\\"\",\"name\":\"\\\"Published Summary\\\"\",\"summarySpecification\":\"{ id: \\\"6d783a98-02fe-448c-82bb-58eea30ee57c\\\" }\",\"publishSpecification\":\"{ id: \\\"cdb7d090-35a5-4caa-bfb3-051ed100ddf2\\\" }\"}\n",
            "- Response:\n",
            "{\"data\":{\"publishContents\":{\"id\":\"d8907685-f022-4829-ba51-4d9aa8eaf380\",\"name\":\"Published Summary.mp3\",\"state\":\"FINISHED\",\"type\":\"FILE\",\"fileType\":\"AUDIO\",\"mimeType\":\"audio/mp3\",\"uri\":\"https://graphlit20241212dc396403.blob.core.windows.net/files/d8907685-f022-4829-ba51-4d9aa8eaf380/Published%20Summary.mp3\",\"collections\":null,\"observations\":null,\"textUri\":null,\"audioUri\":null,\"markdown\":\"[00:00:00] Hello, and welcome.\\n\\n[00:00:01] Today, the spotlight is on everything the Graphlet platform rolled out during 2023\\n\\n[00:00:07] 2024.\\n\\n[00:00:08] Whether you've been using Graphlet for a while or you're newly curious about tapping into smarter content,\\n\\n[00:00:14] ingestion,\\n\\n[00:00:15] retrieval, and large language model integration,\\n\\n[00:00:18] this year in review will get you up to speed.\\n\\n[00:00:22] There's plenty to cover, so let's jump right in. Let's start back in August\\n\\n[00:00:28] 2023.\\n\\n[00:00:29] That month brought a new data model for observations,\\n\\n[00:00:34] including a reworked approach for storing occurrences\\n\\n[00:00:37] of people,\\n\\n[00:00:39] organizations,\\n\\n[00:00:40] and more.\\n\\n[00:00:42] Alongside that came a new category entity for classifying sensitive data.\\n\\n[00:00:48] It might sound abstract, but trust me. Anyone who needs to classify or redact personally\\n\\n[00:00:54] identifiable\\n\\n[00:00:55] information\\n\\n[00:00:56] will find that these changes make content handling feel more organized and secure.\\n\\n[00:01:01] Also, in August,\\n\\n[00:01:03] we introduced usage based billing infrastructure,\\n\\n[00:01:06] which helps teams scale by only paying for what they actually use.\\n\\n[00:01:10] Plus, we began appending SAS tokens to URIs\\n\\n[00:01:13] so you can directly access processed data.\\n\\n[00:01:17] Very handy if you're building an application that needs near instant retrieval.\\n\\n[00:01:22] Fast forward a bit to December 2023\\n\\n[00:01:25] when we introduced some major expansions to our large language model lineup.\\n\\n[00:01:30] We added support for open AI, gpt4,\\n\\n[00:01:34] turbo\\n\\n[00:01:35] 128 k, llama 2, Mistral 7 b, and anthropic Claude 2.1,\\n\\n[00:01:40] just to name a few.\\n\\n[00:01:42] That was also when query by example launched,\\n\\n[00:01:45] letting you quickly search your content or conversations\\n\\n[00:01:48] by providing a short snippet.\\n\\n[00:01:50] No\\n\\n[00:01:51] complicated query syntax needed.\\n\\n[00:01:54] On top of that, we tackled a few important bug fixes,\\n\\n[00:01:59] like ignoring RSS\\n\\n[00:02:00] dot etml\\n\\n[00:02:02] in site maps and addressing an issue where\\n\\n[00:02:05] GPT 3.5\\n\\n[00:02:07] turbo might inject phantom citation number placeholders.\\n\\n[00:02:11] By December's end, teams were seeing more robust search and retrieval,\\n\\n[00:02:16] and these LLM upgrades opened the door to brand new use cases\\n\\n[00:02:20] from summarizing large documents\\n\\n[00:02:22] to analyzing images right within a conversation.\\n\\n[00:02:26] Then came February 2024.\\n\\n[00:02:29] On the second, we introduced\\n\\n[00:02:32] semantic alerts to schedule automatic LLM,\\n\\n[00:02:35] summarizations, and content publications.\\n\\n[00:02:38] Imagine generating daily or weekly reports without manual oversight.\\n\\n[00:02:43] Perfect for dev teams who want quick snapshots of Slack messages,\\n\\n[00:02:47] email threads, or tickets.\\n\\n[00:02:49] Later that month, on 21st,\\n\\n[00:02:52] support arrived for OneDrive and Google Drive feeds,\\n\\n[00:02:55] plus the ability to automatically extract embedded images from PDFs.\\n\\n[00:03:00] That means you can ingest files or entire shared folders\\n\\n[00:03:03] and trust the system to do the heavy lifting\\n\\n[00:03:06] of pulling out text, attachments,\\n\\n[00:03:09] and\\n\\n[00:03:10] images.\\n\\n[00:03:11] We also introduced better email backup file handling.\\n\\n[00:03:15] Think EML or MSG.\\n\\n[00:03:18] And it's never just about new features.\\n\\n[00:03:20] We smoothed out PDF passing errors and improved credit usage notifications\\n\\n[00:03:26] so you know when your usage is approaching its quota.\\n\\n[00:03:29] April 24 was especially busy.\\n\\n[00:03:33] On April 7th, we added the ability to ingest Discord channel content complete with attachments\\n\\n[00:03:39] so you can unify all your chats and file data in one place.\\n\\n[00:03:43] We also introduced Cohere Re Ranking,\\n\\n[00:03:46] giving you the option to reorder semantic search results with Cohere's models for more precise content retrieval.\\n\\n[00:03:53] Section aware, text chunking,\\n\\n[00:03:55] chunk based retrieval strategies,\\n\\n[00:03:57] and an asynchronous\\n\\n[00:03:59] flag for ingest operations rounded out that release.\\n\\n[00:04:02] Then on April 23rd,\\n\\n[00:04:04] we took a major step forward with official Python and TypeScript\\n\\n[00:04:08] SDKs.\\n\\n[00:04:09] Each is cogenerated from our GraphQL schema, so you don't need deep GraphQL knowledge to get started.\\n\\n[00:04:16] We also updated our model roster yet again with\\n\\n[00:04:19] GPT 4,\\n\\n[00:04:22] turbo 128 k, llama 3, Grok, and Fresh Cohere models like command r.\\n\\n[00:04:28] All of this helps you seamlessly\\n\\n[00:04:31] integrate Graphlet\\n\\n[00:04:32] into your application stack,\\n\\n[00:04:34] whether you're building a Python microservice\\n\\n[00:04:37] or a Node. Js content pipeline.\\n\\n[00:04:40] Moving along to June 2024,\\n\\n[00:04:43] we introduced support for deep seek LLMs for prompt completion.\\n\\n[00:04:47] We also started passing embedded JSON LD from web pages to automatically enrich the knowledge graph,\\n\\n[00:04:55] which is a huge win if your team is building robust data pipelines that unify multiple data sources.\\n\\n[00:05:03] Later that month, on June 21st,\\n\\n[00:05:05] we added the anthropic Claude 3.5\\n\\n[00:05:07] Sonnet model,\\n\\n[00:05:09] plus improvements for knowledge graph semantic search.\\n\\n[00:05:12] If you've spent any time writing entity extraction\\n\\n[00:05:15] and linking logic, this new approach can save you from building custom solutions from scratch.\\n\\n[00:05:21] July 24 saw quite a few enhancements too. On July 4th, webhook alerts arrived,\\n\\n[00:05:28] letting your application\\n\\n[00:05:29] receive HTTP\\n\\n[00:05:31] post notifications\\n\\n[00:05:32] whenever certain content events or summaries get published.\\n\\n[00:05:37] We also added a 128 k context window for deep seek models, giving you more space for bigger or more detailed prompts.\\n\\n[00:05:45] Then on July 19th, we introduced the gpt4o\\n\\n[00:05:49] mini model,\\n\\n[00:05:50] which can handle up to 16 k output tokens\\n\\n[00:05:53] plus improved summarization\\n\\n[00:05:55] features for your content.\\n\\n[00:05:58] Another update landed on July 25th,\\n\\n[00:06:00] focusing on Mistral Large 2 and Nemo,\\n\\n[00:06:03] plus the llama 3.1 series on.\\n\\n[00:06:07] And just a few days later, on July 28th,\\n\\n[00:06:10] we added an indexing\\n\\n[00:06:12] workflow stage and Azure\\n\\n[00:06:14] AI language detection,\\n\\n[00:06:16] making it easier to identify languages across large corpora.\\n\\n[00:06:20] August 2024\\n\\n[00:06:22] might be a favorite for those building specialized products.\\n\\n[00:06:26] On 8th, we provided LLM based document preparation\\n\\n[00:06:30] using GPT 4 0 or Anthropic Sonnet 3.5\\n\\n[00:06:35] as well as an open source dot net SDK.\\n\\n[00:06:38] Right after that, on August 11th,\\n\\n[00:06:41] Azure AI Document Intelligence\\n\\n[00:06:43] became our default recommendation\\n\\n[00:06:45] for complex PDFs and tricky table extractions,\\n\\n[00:06:49] boosting the accuracy of your retrieval and generation tasks.\\n\\n[00:06:53] And on August 20th, we introduced support for medical related entities,\\n\\n[00:06:59] everything from medical drug to medical test,\\n\\n[00:07:03] making Graphlet a more appealing option for health and life sciences apps that require thorough data classification.\\n\\n[00:07:10] Meanwhile, we tackled bug fixes to ensure stable performance,\\n\\n[00:07:14] especially in entity extraction and LLM caches.\\n\\n[00:07:18] Our next stop is December 2024.\\n\\n[00:07:21] On December 1st, we polished up the retrieval only rag pipeline features, giving you more ways to format prompts for large language models without forcing a generation step each time.\\n\\n[00:07:32] Then on December 9th, we added website mapping,\\n\\n[00:07:36] web page screenshots,\\n\\n[00:07:37] and extraction commands like summarize text and extract text.\\n\\n[00:07:42] With screenshot page, you can grab images of web pages for follow-up processing.\\n\\n[00:07:47] And the new flattened citations option\\n\\n[00:07:50] helps unify references\\n\\n[00:07:52] in one place.\\n\\n[00:07:54] And just a couple weeks later, on December 22nd,\\n\\n[00:07:57] we capped off the year with feed integrations for Dropbox, Box, Intercom, and Zendesk.\\n\\n[00:08:04] We also introduced an experimental Gemini 2.0 model and the brand new OpenAI\\n\\n[00:08:10] o one model\\n\\n[00:08:11] capable of handling up to 200 k tokens.\\n\\n[00:08:15] This final release of the year\\n\\n[00:08:17] also removed the content item limit for projects on our starter tier. Super handy if you're archiving huge volumes of documents or logs.\\n\\n[00:08:27] Of course, scattered among all these launches are countless bug fixes.\\n\\n[00:08:32] We've tackled everything from better PDF table extraction\\n\\n[00:08:36] to preventing timeouts\\n\\n[00:08:38] when passing large Slack message histories.\\n\\n[00:08:41] We've also refined how we handle hallucinations\\n\\n[00:08:44] when LLMs generate citations that don't exist.\\n\\n[00:08:48] When you see notes like GPL1726\\n\\n[00:08:52] or GPLA\\n\\n[00:08:53] 314,\\n\\n[00:08:54] those refer to issues we've identified and officially patched,\\n\\n[00:08:58] making the platform more reliable and accurate over time.\\n\\n[00:09:02] The overall goal is to ensure you don't run into quirky edge cases when building your app. And if you do, we aim to patch them swiftly.\\n\\n[00:09:11] Now that we've taken a whirlwind tour of these updates,\\n\\n[00:09:14] let's recap.\\n\\n[00:09:16] Throughout 2023\\n\\n[00:09:18] and 2054,\\n\\n[00:09:19] Graphlet grew from a powerful ingestion engine\\n\\n[00:09:22] into a complete platform\\n\\n[00:09:32] OneDrive, Box,\\n\\n[00:09:34] Intercom, Zendesk,\\n\\n[00:09:36] OneDrive, Box,\\n\\n[00:09:38] Intercom, Zendesk,\\n\\n[00:09:39] Notion, and more. We expanded the range of large language models from GPT 3.5 Turbo\\n\\n[00:09:46] all the way to advanced models like Grok Llama 3.3 or Gemini 2.0\\n\\n[00:09:52] and introduced\\n\\n[00:09:53] re ranking strategies that help you find precisely the data you need faster.\\n\\n[00:09:59] Meanwhile, we improved the platform's ability to pass complex PDFs,\\n\\n[00:10:03] handle images, produce custom summaries,\\n\\n[00:10:07] and integrate with your own custom code, thanks to official SDKs in Python, TypeScript, and dot net.\\n\\n[00:10:14] If all of this inspires you to give Graphlet a try, the perfect time is now. You can sign up for free at graphlet.com\\n\\n[00:10:22] and start experimenting with these features right away.\\n\\n[00:10:25] Whether you're looking to automate your content ingestion flows, level up your search or knowledge graph, or enhance your app with the latest large language models, Graphlet has the tools to make it happen.\\n\\n[00:10:38] And here's a final bonus for those with an eye on the future.\\n\\n[00:10:42] In 2,025,\\n\\n[00:10:45] Graphlet will be introducing even more features to help you rapidly build AI agents.\\n\\n[00:10:50] It's going to be a leap forward,\\n\\n[00:10:52] and we can't wait to share it.\\n\\n[00:10:55] Thank you for tuning in to this detailed look at the past year and a half of Graphlet's evolution.\\n\\n[00:11:02] Have fun building, and see you next time.\\n\\n\"}}}\n",
            "\n",
            "2024-12-28T07:42:09.706Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:12.941378, used credits [0.02329987]\n",
            "- CONTENT [d8907685-f022-4829-ba51-4d9aa8eaf380]\n",
            "\n",
            "2024-12-28T07:42:09.466Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.492400, used credits [0.00380000]\n",
            "- CONTENT [d8907685-f022-4829-ba51-4d9aa8eaf380]: Content type [FILE], file type [AUDIO]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1900 tokens], throughput: 3858.655 tokens/sec\n",
            "\n",
            "2024-12-28T07:42:09.268Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.298247, used credits [0.00380400]\n",
            "- CONTENT [d8907685-f022-4829-ba51-4d9aa8eaf380]: Content type [FILE], file type [AUDIO]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1902 tokens], throughput: 6377.258 tokens/sec\n",
            "\n",
            "2024-12-28T07:42:07.959Z: Upload Transcript\n",
            "- Workflow [Preparation] took 0:00:00.053406, used credits [0.00015493]\n",
            "- CONTENT [d8907685-f022-4829-ba51-4d9aa8eaf380]: Content type [FILE], file type [DATA]\n",
            "- File upload [40095 bytes], throughput: 750761.153 bytes/sec\n",
            "\n",
            "2024-12-28T07:42:07.707Z: Processed Transcript\n",
            "- Workflow [Preparation] took 0:00:03.358346, used credits [0.95305200]\n",
            "- CONTENT [d8907685-f022-4829-ba51-4d9aa8eaf380]: Content type [FILE], file type [DATA]\n",
            "- Processor name [Deepgram Audio Transcription], model name [nova-2-general], length [0:11:04.920000]\n",
            "\n",
            "2024-12-28T07:42:03.744Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.915980, used credits [0.04110806]\n",
            "- CONTENT [d8907685-f022-4829-ba51-4d9aa8eaf380]: Content type [FILE], file type [AUDIO]\n",
            "- File upload [10638733 bytes], throughput: 11614584.590 bytes/sec\n",
            "\n",
            "2024-12-28T07:41:56.402Z: Upload Master\n",
            "- Workflow [Publishing] took 0:00:01.209869, used credits [0.04110806]\n",
            "- CONTENT [d8907685-f022-4829-ba51-4d9aa8eaf380]: Content type [FILE], file type [AUDIO]\n",
            "- File upload [10638733 bytes], throughput: 8793291.870 bytes/sec\n",
            "\n",
            "2024-12-28T07:41:54.990Z: Audio publishing\n",
            "- Workflow [Publishing] took 0:00:18.756122, used credits [79.29900000]\n",
            "- Processor name [ElevenLabs], model name [Turbo_V2_5], units [8811]\n",
            "\n",
            "2024-12-28T07:40:36.930Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:05.046416, used credits [0.00818700]\n",
            "- CONTENT [1433f70c-c943-4ea6-849a-0c9fd1eaeb10]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1161 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/</name><title>December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Dropbox feeds for ingesting files on the Dropbox cloud service. Dropbox feeds require your appKey, appSecret, redirectUriand refreshTokento be assigned. The feed also accepts an optional pathparameter to read files from a specific Dropbox folder.\n",
            "💡 Graphlit now supports Box feeds for ingesting files on the Box cloud service. Box feeds require your clientId, clientSecret, redirectUriand refreshTokento be assigned.\n",
            "💡 Graphlit now supports Intercom feeds for ingesting Intercom Articles and Tickets. We will ingest Intercom Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Intercom feeds require the accessTokenproperty to be assigned.\n",
            "💡 Graphlit now supports Zendesk feeds for ingesting Zendesk Articles and Tickets.  We will ingest Zendesk Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Zendesk feeds require the accessTokenproperty and your Zendesk subdomain to be assigned.\n",
            "Graphlit now supports the latest OpenAI o1 model, with the model enums O1_200kand O1_200k_20241217.\n",
            "Graphlit now supports the latest Gemini Flash 2.0 Experimental model, with the model enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "Graphlit now supports the latest Cohere R7B model, with the model enum COMMAND_R7B_202412.\n",
            "Graphlit now supports returning the low-level details from prompting RAG conversations, by adding the includeDetailsparameter and setting to True. This includes details on the number of sources, the exact list of messages provided to the LLM, and more.\n",
            "We have added support for filtering of observables, such as Person or Organization, by URI property.\n",
            "We have added the ability to bypass semantic search in content retrieval with conversations. You can assign NONEfor the conversation search type, and it will ignore the user prompt when retrieving content.  It will inject all contents resulting from the content filter into the RAG prompt context.\n",
            "We have added a new createdInLastproperty to all entity filters, which allows easier filtering of entities created within a recent time period. Also, we have added a new inLastproperty to the content filter, which allows easier filtering of content authored within a recent time period. For example, find all images taken in the last 3 days, or find me all emails I received yesterday.\n",
            "We have added support for the latest Azure AI Document Intelligence models, with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "We have added support for Google Drive and OneDrive feeds to ingest specific files by providing a list of file identifiers (files), in addition to the folder identifier (folderId).  If files identifiers are provided, they take precedence over the folder identifier.\n",
            "⚡ For projects upgraded to the Starter Tier after Dec 9, 2024, we have removed the content items limit. Now you can store an unlimited number of content items (i.e. files, web pages, Slack messages) on the Starter or Growth Tiers.  If you have an existing project on the Starter Tier, please reach out and we will manually remove that content item limit on the project.\n",
            "Bugs Fixed\n",
            "GPLA-3529: Can't assign collection to multitenant content\n",
            "GPLA-3579: Should decode HTML characters when parsing HTML email\n",
            "GPLA-3576: Ingesting content in-place doesn't handle isSynchronous properly\n",
            "GPLA-3457: IsFeedDone doesn't return True for finished feed with no contents\n",
            "GPLA-3572: Not handling HTTP 400 error on uploading from URI\n",
            "NextDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "Last updated3 days ago\n",
            "- Completion [392 tokens (includes JSON guardrails tokens)], throughput: 77.679 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Dropbox feeds for file ingestion; requires appKey, appSecret, redirectUri, and refreshToken.\n",
            "  - Support for Box feeds for file ingestion; requires clientId, clientSecret, redirectUri, and refreshToken.\n",
            "  - Support for Intercom feeds for Articles and Tickets; requires accessToken.\n",
            "  - Support for Zendesk feeds for Articles and Tickets; requires accessToken and Zendesk subdomain.\n",
            "  - Support for OpenAI o1 model with enums O1_200k and O1_200k_20241217.\n",
            "  - Support for Gemini Flash 2.0 Experimental model with enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "  - Support for Cohere R7B model with enum COMMAND_R7B_202412.\n",
            "  - Ability to return low-level details from RAG conversations using includeDetails parameter.\n",
            "  - Support for filtering observables by URI property.\n",
            "  - Ability to bypass semantic search in content retrieval with conversations.\n",
            "  - New createdInLast property for entity filters and inLast property for content filters.\n",
            "  - Support for Azure AI Document Intelligence models with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "  - Support for Google Drive and OneDrive feeds to ingest specific files by file identifiers.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Removal of content items limit for projects upgraded to the Starter Tier after Dec 9, 2024, allowing unlimited storage of content items.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with multitenant content assignment, HTML character decoding in emails, synchronous content ingestion, feed completion status, and HTTP 400 error handling during URI uploads.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - These updates enhance file ingestion capabilities, improve content retrieval and filtering, and provide developers with more flexibility and options in using the platform.\n",
            "\n",
            "2024-12-28T07:40:34.841Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.832576, used credits [0.00681900]\n",
            "- CONTENT [bdad98c7-3cd1-4a67-af1c-334852ad7ce2]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1061 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/april-2024/april-7-support-for-discord-feeds-cohere-reranking-section-aware-chunking-and-retrieval</name><title>April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval | Graphlit Changelog</title></metadata> 🐇\tApril 2024\n",
            "April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "New Features\n",
            "💡 Graphlit now supports Discord feeds.  By connecting to a Discord channel and providing a bot token, you can ingest all Discord messages and file attachments.\n",
            "💡  Graphlit now supports Cohere reranking after content retrieval in RAG pipeline.  You can optionally use the Cohere rerank model to semantically rerank the semantic search results, before providing as context to the LLM.\n",
            "Added support for section-aware text chunking and retrieval.  Now, when using section-aware document preparation, such as Azure AI Document Intelligence, Graphlit will store the extracted text according to the semantic chunks (i.e. sections).  The text for each section will be individually chunked and embedded into the vector index.\n",
            "Added support for retrievalStrategy in Specification type. Graphlit now supports CHUNK, SECTION and CONTENT retrieval strategies.  Chunk retrieval will use the search hit chunk, section retrieval will expand the search hit chunk to the containing section (or page, if not using section-aware preparation).  Content retrieval will expand the search hit chunk to the text of the entire document.\n",
            "Added support for rerankingStrategy in Specification type. You can now configure the reranking of content sources, using the Cohere reranking model, by assigning serviceType to COHERE.  More reranking models are planned for the future.\n",
            "Added isSynchronous flag to content ingestion mutations, such as ingestUri, so the mutation will wait for the content to complete the ingestion workflow (or error) before returning.  This is useful for utilizing the API in a Jupyter notebook or Streamlit application, in a synchronous manner without polling.\n",
            "Added includeAttachments flag to SlackFeedProperties.  When enabled, Graphlit will automatically ingest any attachments within Slack messages.\n",
            "⚡ Added ingestUri mutation to replace the now deprecated ingestPage and ingestFile mutations.  We had seen confusion on when to use one vs the other, and now for any URI, whether it is a web page or hosted PDF, you can pass it to ingestUri, and we will infer the correct content ingestion workflow.\n",
            "⚡ Removed includeSummaries from the ConversationStrategyInput type.  This will re-added in the future as part of the retrieval strategy.\n",
            "⚡ Deprecated enableExpandedRetrieval in ConversationStrategyInput type.  This is now handled by setting strategyType to SECTION or CONTENT in the RetrievalStrategyInput type.\n",
            "⚡ Moved contentLimit from ConversationStrategyInput type to RetrievalStrategyInput type. You can optionally assign the contentLimit to retrievalStrategy which limits the number of content sources leveraged in the LLM prompt context. (Default is 100.)\n",
            "Bugs Fixed\n",
            "GPLA-2469: Failed to ingest PDF hosted on GitHub\n",
            "GPLA-2390: Claude 3 Haiku not adhering to JSON schema\n",
            "GPLA-2474: Prompt rewriting should ignore formatting instructions in prompt\n",
            "GPLA-2462: Missing line break after table rows\n",
            "GPLA-2417: Not extracting images from PPTX correctly\n",
            "PreviousApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "NextMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "Last updated8 months ago\n",
            "- Completion [303 tokens (includes JSON guardrails tokens)], throughput: 62.699 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Discord feeds: Ingest messages and file attachments from a Discord channel using a bot token.\n",
            "  - Cohere reranking: Optionally rerank semantic search results in the RAG pipeline using the Cohere model.\n",
            "  - Section-aware text chunking and retrieval: Store extracted text according to semantic chunks for improved indexing.\n",
            "  - Retrieval strategies: Support for CHUNK, SECTION, and CONTENT retrieval strategies.\n",
            "  - Reranking strategy: Configure reranking of content sources using the Cohere model.\n",
            "  - isSynchronous flag: Wait for content ingestion to complete before returning results.\n",
            "  - includeAttachments flag: Automatically ingest attachments in Slack messages.\n",
            "  - ingestUri mutation: Replace deprecated ingestPage and ingestFile mutations for URI content ingestion.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Removed includeSummaries from ConversationStrategyInput; will be re-added later.\n",
            "  - Deprecated enableExpandedRetrieval; now managed by strategyType in RetrievalStrategyInput.\n",
            "  - Moved contentLimit to RetrievalStrategyInput for better content source management.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with PDF ingestion from GitHub and JSON schema adherence for Claude 3 Haiku.\n",
            "  - Resolved prompt rewriting formatting issues and image extraction from PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: April 7, 2024.\n",
            "  - Value: Enhancements improve content ingestion and retrieval processes, making it easier for developers to manage and utilize data from various sources.\n",
            "\n",
            "2024-12-28T07:40:34.416Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.275271, used credits [0.00577800]\n",
            "- CONTENT [f575eec5-7ccd-47dd-a130-489b47cce8c4]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [854 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-3-new-data-model-for-observations-new-category-entity</name><title>August 3: New data model for Observations, new Category entity | Graphlit Changelog</title></metadata> 🎂\tAugust 2023\n",
            "August 3: New data model for Observations, new Category entity\n",
            "New Features\n",
            "💡 Revised data model for Observations, Occurrences and observables (i.e. Person, Organization).  Now after entity extraction, content will have one Observation for each observed entity, and a list of occurrences.  Occurrence now supports text, time and image occurrence types.  (Text: page index, time: start/end timestamp, image: bounding box)  Observations now have ObservableType and Observable fields, which specify the observed entity type and entity reference.\n",
            "💡 Added Category entity to GraphQL data model, which supports PII categories such as Phone Number or Credit Card Number.\n",
            "Added probability field to model properties, for the LLM's token probability.  (See OpenAI documentation for more detail.)\n",
            "Added error field to feeds.  If a feed fails to read from the data source, and is marked as ERRORED state, the error field will have the error description.\n",
            "Support reingestion of changed files from feeds.  For feeds, such as SharePoint or Web, where we can recognize that a file or page was updated, we will now reingest the content in-place.  Content will keep the same ID, and will restart the content workflow by re-downloading the updated content from the data source.   Existing observations will be deleted, and new observations will be created from the updated content.\n",
            "ℹ️ Ingestion of content is now idempotent, meaning if you ingest content again from the same URI, we will reingest the content in-place, while keeping the same ID.  (If we can recognize the content has not changed, such as by ETag, we will return the existing content object.)\n",
            "ℹ️ Changed GraphQL data type of SharePoint tenantId, libraryId and siteId to ID rather than String.\n",
            "✨ Performance optimization of entity extraction, and the creation of observations.\n",
            "Bugs Fixed\n",
            "GPLA-1130: Only was extracting text from first column of PDF tables.\n",
            "GPLA-1140: Text from DOCX tables was not extracted properly.\n",
            "GPLA-1154: Audio content ingested from RSS feed was not deleted when feed was deleted.\n",
            "PreviousAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "NextJuly 15: Support for SharePoint feeds, new Conversation features\n",
            "Last updated1 year ago\n",
            "- Completion [268 tokens (includes JSON guardrails tokens)], throughput: 81.825 tokens/sec:\n",
            "- New Features:\n",
            "  - Revised data model for Observations, Occurrences, and observables (e.g., Person, Organization).\n",
            "  - Each observed entity now has one Observation and a list of occurrences (supports text, time, and image types).\n",
            "  - Added Category entity to GraphQL data model for PII categories (e.g., Phone Number, Credit Card Number).\n",
            "  - Introduced probability field for LLM's token probability.\n",
            "  - Added error field to feeds for error descriptions on failed reads.\n",
            "  - Support for reingestion of changed files from feeds, maintaining the same ID and restarting the content workflow.\n",
            "  - Idempotent content ingestion from the same URI, returning existing content if unchanged.\n",
            "  - Changed GraphQL data type for SharePoint tenantId, libraryId, and siteId to ID.\n",
            "  - Performance optimization for entity extraction and observation creation.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of content ingestion and updates.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed extraction issues from PDF and DOCX tables.\n",
            "  - Resolved issue where audio content from RSS feeds was not deleted upon feed deletion.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 3, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved data handling, enhanced entity extraction, and better error management.\n",
            "\n",
            "2024-12-28T07:40:33.162Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.201761, used credits [0.00660000]\n",
            "- CONTENT [7013d1b5-eaa6-4f63-b9f5-973fb33abad4]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [872 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/april-2024/april-23-support-for-python-and-typescript-sdks-latest-openai-cohere-and-groq-models-bug-fixes</name><title>April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes | Graphlit Changelog</title></metadata> 🐇\tApril 2024\n",
            "April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports a native Python SDK, using Pydantic types. The Python SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest PyPi package here.  The Streamlit sample applications have been updated to use the new Python SDK.\n",
            "💡 Graphlit now supports a native Node.js SDK, using TypeScript types. The Node.js SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest NPM package here.\n",
            "💡 Graphlit now supports the 2024-04-09 models in the OpenAI model service. GPT4_TURBO-128K will give the latest OpenAI GPT-4 model, following this model list.  We have added the GPT4_TURBO_128K_2024_04_09 enum to specify the new model.\n",
            "💡 Graphlit now supports LLaMA3 70b, LLaMA3 8b and Gemma 7b models in the Groq model service.\n",
            "💡 Graphlit now supports the Command R and Command-R+ models in the Cohere model service.\n",
            "Added support for Jina reranking, using the JINA reranking model service type in the reranking retrieval strategy.\n",
            "Updated the Cohere reranking model to use the latest v3.0 model.\n",
            "Increased the reliability of parsing LLM responses, in cases where they don't follow the JSON schema.\n",
            "⚡ Cleaned up nullability of GraphQL parameters, so parameters better reflect if they are required or optional, or allow nulls.\n",
            "⚡ Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "⚡ Split out reranking model service type as RetrievalModelServiceTypes enum.\n",
            "Bugs Fixed\n",
            "GPLA-2114: Adding content to collections not syncing search index\n",
            "GPLA-2511: Failing to render any conversation sources with section retrieval and text content\n",
            "PreviousMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "NextApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "Last updated8 months ago\n",
            "- Completion [332 tokens (includes JSON guardrails tokens)], throughput: 79.014 tokens/sec:\n",
            "- New Features:\n",
            "  - Native Python SDK introduced, using Pydantic types; code-generated from GraphQL schema, no GraphQL knowledge required.\n",
            "  - Native Node.js SDK introduced, using TypeScript types; code-generated from GraphQL schema, no GraphQL knowledge required.\n",
            "  - Support for OpenAI's 2024-04-09 models, including GPT4_TURBO-128K.\n",
            "  - Support for Groq models: LLaMA3 70b, LLaMA3 8b, and Gemma 7b.\n",
            "  - Support for Cohere models: Command R and Command-R+.\n",
            "  - Added support for Jina reranking with the JINA reranking model service type.\n",
            "  - Updated Cohere reranking model to v3.0.\n",
            "  - Improved reliability of parsing LLM responses that don't follow JSON schema.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Cleaned up nullability of GraphQL parameters for better clarity on required, optional, or nullable parameters.\n",
            "  - Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "  - Split reranking model service type into RetrievalModelServiceTypes enum.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where adding content to collections did not sync with the search index (GPLA-2114).\n",
            "  - Resolved rendering issues with conversation sources in section retrieval and text content (GPLA-2511).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: April 23, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with improved SDKs, model support, and bug fixes, facilitating easier integration and more reliable performance.\n",
            "\n",
            "2024-12-28T07:40:31.880Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.820137, used credits [0.00354300]\n",
            "- CONTENT [a797f749-b15b-4d03-9543-92e26c2bc922]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [609 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-17-prepare-for-usage-based-billing-append-sas-tokens-to-uris</name><title>August 17: Prepare for usage-based billing; append SAS tokens to URIs | Graphlit Changelog</title></metadata> 🎂\tAugust 2023\n",
            "August 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "New Features\n",
            "ℹ️ Behind the scenes, Graphlit is preparing to launch usage-based billing.  This release put in place the infrastructure to track billable events.  Organizations now have a Stripe customer associated with them, and Graphlit projects are auto-subscribed to a Free/Hobby pricing plan.  In a future release, we will provide the ability to upgrade to a paid plan in the Graphlit Developer Portal.  Also, we will provide visualization of usage, on granular basis, in the Portal.\n",
            "💡 Content URIs now have Shared Access Signature (SAS) token appended, so they are accessible after query.  For example, content.transcriptUri will now be able to be downloaded or used directly in an application (until the SAS token expires).\n",
            "🧱 Added more robustness for error handling and retries, especially for LLM APIs and audio transcription APIs.\n",
            "PreviousSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "NextAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "Last updated1 year ago\n",
            "- Completion [143 tokens (includes JSON guardrails tokens)], throughput: 78.566 tokens/sec:\n",
            "- New Features:\n",
            "  - Infrastructure for usage-based billing implemented; organizations now have a Stripe customer and auto-subscribed to a Free/Hobby pricing plan.\n",
            "  - Content URIs now include Shared Access Signature (SAS) tokens for direct access after queries.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Improved error handling and retries for LLM APIs and audio transcription APIs.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Future release will allow upgrades to paid plans and provide usage visualization in the Graphlit Developer Portal.\n",
            "\n",
            "- Dates:\n",
            "  - Released on August 17, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers organizations a structured billing system and enhanced access to content, improving usability and reliability for developers.\n",
            "\n",
            "2024-12-28T07:40:31.135Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.361901, used credits [0.00421800]\n",
            "- CONTENT [eec9e87f-091c-4ba9-b7d9-b8b21a41b458]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [666 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-9-support-direct-text-markdown-and-html-ingestion-new-specification-llm-strategy</name><title>August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy | Graphlit Changelog</title></metadata> 🎂\tAugust 2023\n",
            "August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "New Features\n",
            "💡 Added ingestText mutation which supports direct Content ingestion of plain text, Markdown and HTML.  Now, if you have pre-scraped HTML or Markdown text, you can ingest it into Graphlit without reading from a URL.\n",
            "💡 Added Specification strategy property, which allows customization of the LLM context when prompting a conversation.  ConversationStrategy now provides Windowed and Summarized message histories, as well as configuration of the weight between existing conversation messages and Content text pages (or audio transcript segments) in the LLM context.\n",
            "💡 Added auto-summarization of extracted text and audio transcripts.  There is a new Content summary property where a list of summary bullet points can be found.  These summaries can be optionally included in the Conversation prompt context for more accurate LLM responses.\n",
            "ℹ️ Added AzureOpenAIModels and OpenAIModels types to Specification model properties to make it easier to specify the desired LLM.\n",
            "ℹ️ Renamed ConversationMessage date property to timestamp\n",
            "✨ Refined the internal LLM prompts for providing content as part of Conversation context.  This provides for much clearer and accurate results from the LLM.\n",
            "PreviousAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "NextAugust 3: New data model for Observations, new Category entity\n",
            "Last updated1 year ago\n",
            "- Completion [185 tokens (includes JSON guardrails tokens)], throughput: 78.327 tokens/sec:\n",
            "- New Features:\n",
            "  - Added ingestText mutation for direct ingestion of plain text, Markdown, and HTML without URL reading.\n",
            "  - Introduced Specification strategy property for customizing LLM context with Windowed and Summarized message histories.\n",
            "  - Implemented auto-summarization of extracted text and audio transcripts, with summaries available for Conversation prompt context.\n",
            "  - Added AzureOpenAIModels and OpenAIModels types to Specification model properties for easier LLM specification.\n",
            "  - Renamed ConversationMessage date property to timestamp.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Refined internal LLM prompts for clearer and more accurate results.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 9, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced content ingestion capabilities, improved LLM context customization, and better summarization features for more accurate responses.\n",
            "\n",
            "2024-12-28T07:40:30.055Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.291905, used credits [0.00397800]\n",
            "- CONTENT [f4865d73-f760-47d7-b918-23295e6ad2af]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [622 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-11-support-for-azure-ai-document-intelligence-by-default-language-aware-summaries</name><title>August 11: Support for Azure AI Document Intelligence by default, language-aware summaries | Graphlit Changelog</title></metadata> 🎂\tAugust 2024\n",
            "August 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "New Features\n",
            "Added support for language-aware summaries when using LLM-based document extraction.  Now the summaries for tables and sections generated by the LLM will follow the language of the source text.\n",
            "Added support for language-aware entity descriptions with using LLM-based entity extraction. Now the entity descriptions generated by the LLM will follow the language of the source text.\n",
            "⚡ We have changed the default document preparation method to use Azure AI Document Intelligence, rather than our built-in document parsers.  We have found that the fidelity of Azure AI is considerably better for complex PDFs, and provides better support for table extraction, so we have made this the default. Note: this does come with increased credit usage per-page, for PDF, DOCX and PPTX documents, but the quality of the extracted documents are noticeably higher for use in RAG pipelines.\n",
            "Bugs Fixed\n",
            "GPLA-3070: Not getting slide count assigned to metadata for PPTX files.\n",
            "PreviousAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "NextAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "Last updated4 months ago\n",
            "- Completion [176 tokens (includes JSON guardrails tokens)], throughput: 76.792 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for language-aware summaries in LLM-based document extraction, aligning summaries with the source text language.\n",
            "  - Support for language-aware entity descriptions in LLM-based entity extraction, matching descriptions to the source text language.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Default document preparation method changed to Azure AI Document Intelligence for improved fidelity in complex PDFs and better table extraction. Increased credit usage per page for PDF, DOCX, and PPTX documents noted, but quality of extracted documents enhanced for RAG pipelines.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3070: Slide count not assigned to metadata for PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 11, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved document extraction quality and language consistency, enhancing the overall functionality of the platform.\n",
            "\n",
            "2024-12-28T07:40:30.004Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.895237, used credits [0.00547200]\n",
            "- CONTENT [7ce5f9ae-f13c-4832-8c74-55c8bac42caf]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [804 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-8-support-for-llm-based-document-extraction-.net-sdk-bug-fixes</name><title>August 8: Support for LLM-based document extraction, .NET SDK, bug fixes | Graphlit Changelog</title></metadata> 🎂\tAugust 2024\n",
            "August 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports LLM-based document preparation, using vision-capable models such as OpenAI GPT-4o and Anthropic Sonnet 3.5.  This is available via the MODEL_DOCUMENT preparation service type, and you can assign a customspecification object and bring your own LLM keys.\n",
            "💡 Graphlit now provides an open source .NET SDK, supporting .NET 6 and .NET 8 (and above).  SDK package can be found on Nuget.org.  Code samples can be found on GitHub.\n",
            "Added identifier property to Content object for mapping content to external database identifiers.  This is supported for content filtering as well.\n",
            "Added support for Claude 3 vision models for image-based entity extraction, using the MODEL_IMAGE entity extraction service.\n",
            "Added context augmentation to conversations, via the augmentedFilter property on the Conversation object.  Any content which matches this augmented filter will be injected into the LLM prompt content, without needing to be related by vector similarity to the user prompt.  This is useful for specifying domain knowledge which should always be referenced by the RAG pipeline.\n",
            "Added support for the latest snapshot of OpenAI GPT-4o, with the model enum GPT4O_128K_20240806.\n",
            "Added reranking of related entities, when preparing the LLM prompt context for GraphRAG.  If reranking is enabled, the metadata from the related entities will be reranked with the same reranker assigned to the conversation specification.\n",
            "⚡ We have changed the type of the duration field in the AudioMetadata and VideoMetadata types to be TimeSpan rather than string, as to be more consistent with the rest of the API data model.\n",
            "Bugs Fixed\n",
            "GPLA-2884: Support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "PreviousAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "NextJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "Last updated4 months ago\n",
            "- Completion [255 tokens (includes JSON guardrails tokens)], throughput: 88.076 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for LLM-based document preparation using models like OpenAI GPT-4o and Anthropic Sonnet 3.5 via MODEL_DOCUMENT service.\n",
            "  - Open source .NET SDK available for .NET 6 and .NET 8 on Nuget.org, with code samples on GitHub.\n",
            "  - Added identifier property to Content object for mapping to external database identifiers.\n",
            "  - Support for Claude 3 vision models for image-based entity extraction using MODEL_IMAGE service.\n",
            "  - Context augmentation in conversations via augmentedFilter property on Conversation object.\n",
            "  - Support for the latest snapshot of OpenAI GPT-4o (model enum GPT4O_128K_20240806).\n",
            "  - Reranking of related entities for LLM prompt context preparation in GraphRAG.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Changed duration field type in AudioMetadata and VideoMetadata from string to TimeSpan for consistency.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed retry support for HTTP 529 (Overloaded) error from Anthropic API.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 8, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced document extraction capabilities, improved SDK support, and better content mapping and filtering options.\n",
            "\n",
            "2024-12-28T07:40:28.955Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:05.142478, used credits [0.00935100]\n",
            "- CONTENT [53a80fc0-7526-4cb8-a921-b66f0c941380]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1217 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2023/december-10-support-for-openai-gpt-4-turbo-llama-2-and-mistral-models-query-by-example-bug-fixes</name><title>December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2023\n",
            "December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the OpenAI GPT-4 Turbo 128k model, both in Azure OpenAI and native OpenAI services.  Added new model enum GPT4_TURBO_VISION_128K.\n",
            "💡 Graphlit now supports Llama 2 7b, 13b, 70b models and Mistral 7b model, via Replicate.  Developers can use their own Replicate API key, or be charged as credits for Graphlit usage.\n",
            "💡 Graphlit now supports the Anthropic Claude 2.1 model. Added new model enum CLAUDE_2_1.\n",
            "💡 Graphlit now supports the OpenAI GPT-4 Vision model for image descriptions and text extraction.  Added new model enum GPT4_TURBO_VISION_128K. See usage example in \"Multimodal RAG\" blog post.\n",
            "Added query by example to contents query.  Developers can specify one or more example contents, and query will use vector embeddings to return similar contents.\n",
            "Added query by example to conversations query.  Developers can specify one or more example conversations, and query will use vector embeddings to return similar conversations.\n",
            "Added vector search support for conversations queries.  Developers can provide search text which will use vector embeddings to return similar conversations.\n",
            "Added promptSpecifications mutation for directly prompting multiple models.  This can be used to evaluate prompts against multiple models or compare different specification parameters in parallel.\n",
            "Added promptStrategy field to Specification, which supports multiple strategy types for preprocessing the prompt before being sent to the LLM model.  For example, REWRITE prompt strategy will ask LLM to rewrite the incoming user prompt based on the previous conversation messages.\n",
            "Added suggestConversation mutation, which returns a list of suggested followup questions based on the specified conversation and related contents.  This can be used to auto-suggest questions for chatbot users.\n",
            "Added new summarization types: CHAPTERS, QUESTIONS and POSTS.   See usage examples in the \"LLMs for Podcasters\" blog post.\n",
            "Added versioned model enums such as GPT4_0613 and GPT35_TURBO_16K_1106.  Without version specified, such as GPT35_TURBO_16K, Graphlit will use the latest production model version, as defined by the LLM vendor.\n",
            "Added lookupContents query to get multiple contents by id in one query.\n",
            "⚡ In Content type, headline field was renamed to headlines and now returns an array of strings.\n",
            "⚡ Entity names are now limited to 1024 characters.  Names will be truncated if they exceed the maximum length.\n",
            "⚡ In SummarizationTypes enum, BULLET_POINTS was renamed to BULLETS.\n",
            "⚡ In ProjectStorage type, originalTotalSize was renamed to totalSize, and totalRenditionSize field was added.  totalSize is the sum of the ingested source file sizes, and totalRenditionSize is the sum of the source file sizes and any derived rendition sizes.\n",
            "⚡ In ConversationStrategy type, strategyType was renamed to type for consistency with rest of data model.\n",
            "⚡ In Specification type, optimizeSearchConversation was removed, and now is handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "Bugs Fixed\n",
            "GPLA-1725: Should ignore RSS.xml from web feed sitemap\n",
            "GPLA-1726: GPT-3.5 Turbo 16k LLM is adding \"Citation #\" to response\n",
            "GPLA-1698: Workflow not applied to link-crawled content\n",
            "GPLA-1692: Mismatched project storage total size, when some content has errored\n",
            "GPLA-1237: Add relevance threshold for semantic search\n",
            "PreviousJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "NextOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "Last updated11 months ago\n",
            "- Completion [475 tokens (includes JSON guardrails tokens)], throughput: 92.368 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4 Turbo 128k model in Azure OpenAI and native services.\n",
            "  - Support for Llama 2 (7b, 13b, 70b) and Mistral 7b models via Replicate.\n",
            "  - Support for Anthropic Claude 2.1 model.\n",
            "  - Support for OpenAI GPT-4 Vision model for image descriptions and text extraction.\n",
            "  - Query by example added for contents and conversations, utilizing vector embeddings.\n",
            "  - Vector search support for conversations queries.\n",
            "  - New promptSpecifications mutation for prompting multiple models.\n",
            "  - New promptStrategy field for preprocessing prompts.\n",
            "  - SuggestConversation mutation for auto-suggesting follow-up questions.\n",
            "  - New summarization types: CHAPTERS, QUESTIONS, and POSTS.\n",
            "  - Versioned model enums introduced (e.g., GPT4_0613).\n",
            "  - LookupContents query to retrieve multiple contents by ID.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed \"headline\" field to \"headlines\" in Content type, now returns an array.\n",
            "  - Entity names limited to 1024 characters, with truncation for longer names.\n",
            "  - BULLET_POINTS renamed to BULLETS in SummarizationTypes enum.\n",
            "  - Renamed \"originalTotalSize\" to \"totalSize\" in ProjectStorage type, added \"totalRenditionSize\".\n",
            "  - Renamed \"strategyType\" to \"type\" in ConversationStrategy type for consistency.\n",
            "  - Removed \"optimizeSearchConversation\" from Specification type, now handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Ignored RSS.xml from web feed sitemap (GPLA-1725).\n",
            "  - Resolved issue with GPT-3.5 Turbo 16k LLM adding \"Citation #\" to responses (GPLA-1726).\n",
            "  - Fixed workflow application to link-crawled content (GPLA-1698).\n",
            "  - Corrected mismatched project storage total size when content errored (GPLA-1692).\n",
            "  - Added relevance threshold for semantic search (GPLA-1237).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 10, 2023.\n",
            "  - Enhancements provide developers with improved querying capabilities and model support, enhancing the overall functionality of the Graphlit Platform.\n",
            "\n",
            "2024-12-28T07:40:28.769Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.066258, used credits [0.00630300]\n",
            "- CONTENT [17831936-40db-4f70-becd-30a9499bd571]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1009 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-19-support-for-openai-gpt-4o-mini-byo-key-for-azure-ai-similarity-by-summary-bug-fixes</name><title>July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the OpenAI GPT-4o Mini model, with 16k output tokens.\n",
            "💡 Graphlit now supports 'bring-your-own-key' for Azure AI Document Intelligence models.  We have added a custom endpoint and key property, which can be assigned to use your own Azure AI resource.\n",
            "Updated to use Jina reranker v2 (jina-reranker-v2-base-multilingual) by default.\n",
            "Updated to assign the summary, bullets, etc properties when calling summarizeContents mutation.  Now when summarizing contents, we will store the resulting summary in the content itself, in addition to returning the summarized results.\n",
            "Added relevance property to all entity types, which will be assigned when searching for these entities.  Entity results will be sorted (descending) by this search relevance score.\n",
            "Added the ability to manually update summary, bullets, etc. properties when calling the updateContent mutation.\n",
            "Added offset property to AtlassianJiraFeedProperties, so the timezone offset can be properly assigned for paging of the Jira feed.  (Defaults to zero offset, i.e. UTC.)  Jira does not store dates in UTC format, and the timestamps are based on the server timezone of the hosted Jira instance.  By assigning the timezone offset with the Jira feed, we can reliably page the updated date timestamps from the Jira API.\n",
            "⚡ We have changed the content similarity search behavior to find similar content by summary, rather than text of the document, when a summary has been previously generated.  For long documents, this will provide a more accurate similarity, rather than comparing on the first few pages of text in a document.\n",
            "⚡ We have changed the behavior of assigning offset in the entity filter objects for paging through entities.  If using vector or hybrid search, this offset will be ignored (i.e. zero offset).  Paging will not be supported through vector or hybrid search results. For keyword search, the offset will continue to be used, along with the limit property, to provide paging through the search results.  We have made this change because we have found that index-based paging is not reliable with our vector/hybrid search approach.  We are investigating ways to support this reliably with vector/hybrid search in the future.\n",
            "Bugs Fixed\n",
            "GPLA-2915: Add retry on OpenAI API HTTP 524 error (gateway timeout).\n",
            "GPLA-2908: Not paging through Jira feed correctly.\n",
            "GPLA-2917: Search by similar content is not giving expected results on long documents.\n",
            "GPLA-2244: Keyword search not finding text in latter part of long PDF.\n",
            "PreviousJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "NextJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [273 tokens (includes JSON guardrails tokens)], throughput: 89.034 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4o Mini model with 16k output tokens.\n",
            "  - 'Bring-your-own-key' support for Azure AI Document Intelligence models with custom endpoint and key property.\n",
            "  - Updated to Jina reranker v2 by default.\n",
            "  - Enhanced summarizeContents mutation to store summaries in content.\n",
            "  - Added relevance property for entity types, sorting results by search relevance score.\n",
            "  - Ability to manually update summary and bullet properties in updateContent mutation.\n",
            "  - Added offset property to AtlassianJiraFeedProperties for proper timezone assignment in Jira feed.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Content similarity search now finds similar content by summary for better accuracy.\n",
            "  - Changed entity filter object behavior for paging; zero offset for vector/hybrid search.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Added retry on OpenAI API HTTP 524 error.\n",
            "  - Fixed paging issues in Jira feed.\n",
            "  - Improved search results for similar content in long documents.\n",
            "  - Resolved keyword search issues in long PDFs.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Version updates include new features and bug fixes as of July 19, 2024.\n",
            "\n",
            "- Value:\n",
            "  - These updates enhance the platform's capabilities, improve search accuracy, and streamline integration with Azure AI, offering developers more robust tools for content management and analysis.\n",
            "\n",
            "2024-12-28T07:40:27.760Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.552668, used credits [0.00428700]\n",
            "- CONTENT [ede9b530-8fd4-46c0-9dd1-05ce0514323d]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [593 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-20-support-for-medical-entities-anthropic-prompt-caching-bug-fixes</name><title>August 20: Support for medical entities, Anthropic prompt caching, bug fixes | Graphlit Changelog</title></metadata> 🎂\tAugust 2024\n",
            "August 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "💡 Graphlit now supports medical-related entities in GraphRAG, and via API for queries and mutations.\n",
            "Added support for Anthropic prompt caching. When using Anthropic Sonnet 3.5 or Haiku 3, Anthropic will now cache the entity extraction and LLM document preparation system prompts, which saves on token cost and increases performance.\n",
            "Bugs Fixed\n",
            "GPLA-3104: Should default search type to VECTOR, when performing entity similarity filter.\n",
            "GPLA-3112: Empty PDF fails entity extraction.\n",
            "PreviousSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "NextAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "Last updated4 months ago\n",
            "- Completion [209 tokens (includes JSON guardrails tokens)], throughput: 81.875 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "  - Medical-related entities supported in GraphRAG and via API for queries and mutations.\n",
            "  - Added support for Anthropic prompt caching, improving performance and reducing token costs when using Anthropic Sonnet 3.5 or Haiku 3.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved entity extraction and LLM document preparation through caching.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - GPLA-3104: Default search type now set to VECTOR for entity similarity filter.\n",
            "  - GPLA-3112: Fixed issue where empty PDFs failed entity extraction.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 20, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with improved capabilities for handling medical data and increased efficiency in using Anthropic models.\n",
            "\n",
            "2024-12-28T07:40:27.104Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.494953, used credits [0.00461700]\n",
            "- CONTENT [cf0123a6-0e0a-4e5d-afe7-3c3d7efe96b1]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [703 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-25-support-for-mistral-large-2-and-nemo-groq-llama-3.1-models-bug-fixes</name><title>July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the Mistral Large 2 and Mistral Nemo models. The existing MISTRAL_LARGE model enum now will use Mistral Large 2.\n",
            "💡 Graphlit now supports the Llama 3.1 8b, 70b and 405b models on Groq.  (Note, these are rate-limited according to Groq's platform constraints.)\n",
            "Added support for revision strategy on data extraction specifications.  Now you can prompt the LLM to revise its previous data extraction response, similar to the existing completion revision strategy.\n",
            "Added version property for AzureDocumentPreparationProperties type for assigning the API version used by Azure AI Document Intelligence.   By default, Graphlit will continue to use the v4.0 (Preview) API version, but you can override this to assign version to V2023_10_31 to use the v3.1 (GA) API version instead.  For some documents, the General Availability (GA) version of the API can provide better results.\n",
            "Bugs Fixed\n",
            "GPLA-2988: Not extracting hyperlinks from Office documents.\n",
            "PreviousJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "NextJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [209 tokens (includes JSON guardrails tokens)], throughput: 83.769 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Mistral Large 2 and Mistral Nemo models.\n",
            "  - Support for Llama 3.1 models (8b, 70b, 405b) on Groq, with rate limits per Groq's platform.\n",
            "  - Added revision strategy for data extraction specifications, allowing LLM to revise previous responses.\n",
            "  - Version property added for AzureDocumentPreparationProperties to assign API version for Azure AI Document Intelligence, with options for v4.0 (Preview) or v3.1 (GA).\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Existing MISTRAL_LARGE model enum updated to use Mistral Large 2.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with hyperlinks not being extracted from Office documents (GPLA-2988).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 25, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve model support and data extraction capabilities, offering developers more flexibility and better results with Azure AI.\n",
            "\n",
            "2024-12-28T07:40:25.698Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.913756, used credits [0.00340500]\n",
            "- CONTENT [7694d101-376d-4cbe-a773-3b64002727bf]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [555 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-1-support-for-retrieval-only-rag-pipeline-bug-fixes</name><title>December 1: Support for retrieval-only RAG pipeline, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports formatting of LLM-ready prompts with our RAG pipeline, via the new formatConversation and completeConversation mutations.  This is valuable for supporting LLM streaming by directly calling the LLM from your application, and using Graphlit for RAG retrieval and conversation history. (Colab Notebook Example)\n",
            "We have added support for inline hyperlinks in extracted text from documents and web pages.\n",
            "Bugs Fixed\n",
            "GPLA-3466: Owner ID should accept any non-whitespace string\n",
            "GPLA-3458: Not getting Person-to-Organization edges from entity extraction\n",
            "PreviousDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "NextNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "Last updated26 days ago\n",
            "- Completion [145 tokens (includes JSON guardrails tokens)], throughput: 75.767 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for formatting LLM-ready prompts with RAG pipeline using formatConversation and completeConversation mutations.\n",
            "  - Inline hyperlinks support in extracted text from documents and web pages.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved LLM streaming capabilities by allowing direct calls from applications.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where Owner ID should accept any non-whitespace string (GPLA-3466).\n",
            "  - Resolved problem with missing Person-to-Organization edges from entity extraction (GPLA-3458).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 1, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances developer experience by improving prompt formatting and text extraction capabilities.\n",
            "\n",
            "2024-12-28T07:40:25.202Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:07.510107, used credits [0.00680100]\n",
            "- CONTENT [14614eae-5e42-4912-aa53-2ce05a1ce2d2]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [887 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/june-2024/june-9-support-for-deepseek-models-json-ld-webpage-parsing-performance-improvements-and-bug-fixes</name><title>June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes | Graphlit Changelog</title></metadata> 🎓\tJune 2024\n",
            "June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Deepseek LLMs for prompt completion.  We offer the deepseek-chat and deepseek-coder models.\n",
            "💡 Graphlit now supports parsing embedded JSON-LD from web pages.  If a web page contains 'script' tags with JSON-LD, we will automatically parse and inject into the knowledge graph.\n",
            "⚡ We have changed the default model for entity extraction and image completions to be OpenAI GPT-4o.  This provides faster performance and better quality output.\n",
            "⚡ We have changed the behavior of knowledge graph generation, from a prompted conversation, to be opt-in.  In order to receive the graph's nodes and edges with the response, you will now need to set generateGraph to True in the specification's graphStrategy object.  This provides improved performance when the graph is not needed for visualization.\n",
            "Added thing property for observable entities, which returns the JSON-LD metadata associated with the entity.\n",
            "Added regex-based filtering for URI paths during feed ingestion, link crawling, and workflow filtering.  You can assign regex patterns in allowedPaths and excludedPaths.\n",
            "Added observableLimit to configure the limit of how many observed entities will be added to the GraphRAG context, defaults to 1000.\n",
            "Added prompt to suggestConversation mutation, which allows customization of the followup question generation.\n",
            "Updated suggestConversation to incorporate the past conversation message history, in addition to the filtered set of content sources.\n",
            "🔥  We have improved performance in knowledge graph retrieval and generation, via better parallelization and batching.\n",
            "Bugs Fixed\n",
            "GPLA-2748: Optimize the retrieval performance of observed entities during GraphRAG\n",
            "GPLA-2732: Invalid user-provided URI causing parsing exception\n",
            "GPLA-2666: Shouldn't require tenant ID for Microsoft email or Teams\n",
            "GPLA-2772: Not returning labels or categories from graph in API\n",
            "GPLA-2762: Failed to extract spreadsheet images\n",
            "GPLA-2687: Email to/from not getting added as observations on emails\n",
            "GPLA-2738: API is returning 'audio' metadata from podcast HTML document\n",
            "PreviousJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "NextMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [345 tokens (includes JSON guardrails tokens)], throughput: 45.938 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Deepseek LLMs for prompt completion (deepseek-chat and deepseek-coder models).\n",
            "  - Parsing of embedded JSON-LD from web pages, automatically injecting into the knowledge graph.\n",
            "  - Default model for entity extraction and image completions changed to OpenAI GPT-4o for improved performance and quality.\n",
            "  - Knowledge graph generation behavior changed to opt-in; requires setting generateGraph to True for response inclusion.\n",
            "  - Added thing property for observable entities to return associated JSON-LD metadata.\n",
            "  - Regex-based filtering for URI paths during feed ingestion and workflow filtering.\n",
            "  - Added observableLimit to configure the number of observed entities in GraphRAG context (default 1000).\n",
            "  - Customization of follow-up question generation via prompt in suggestConversation mutation.\n",
            "  - Updated suggestConversation to include past conversation message history.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved performance in knowledge graph retrieval and generation through better parallelization and batching.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Optimized retrieval performance of observed entities during GraphRAG.\n",
            "  - Fixed parsing exception caused by invalid user-provided URI.\n",
            "  - Removed requirement for tenant ID for Microsoft email or Teams.\n",
            "  - Resolved issue of not returning labels or categories from graph in API.\n",
            "  - Fixed failure to extract spreadsheet images.\n",
            "  - Addressed issue of email to/from not being added as observations.\n",
            "  - Corrected API returning 'audio' metadata from podcast HTML documents.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: June 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for model support, improved performance, and better handling of data through new features and bug fixes.\n",
            "\n",
            "2024-12-28T07:40:24.604Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.970479, used credits [0.00425100]\n",
            "- CONTENT [a69b7ff1-2c57-4b20-894b-0c924bebad57]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [633 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-28-support-for-indexing-workflow-stage-azure-ai-language-detection-bug-fixes</name><title>July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "New Features\n",
            "Added indexing workflow stage. This provides for configuration of indexing services, which may infer metadata from the content.\n",
            "Added AZURE_AI_LANGUAGE content indexing service, which supports inferring the language of extracted text or transcript.\n",
            "Added support for language content metadata.  This returns a list of languages in ISO 639-1 format, which may have been inferred from the extracted text or transcript.\n",
            "Added support for MODEL_IMAGE extraction service.  This provides integration with vision models beyond those provided by OpenAI.  You can assign a custom specification and bring-your-own API key for image extraction models.\n",
            "⚡ We have deprecated the OPENAI_IMAGE service type, and developers should now use the LLM image service instead.\n",
            "⚡ We have removed the language field from AudioMetadata type, which has been replaced by the new LanguageMetadata type.\n",
            "Bugs Fixed\n",
            "GPLA-2987: Extracting text with Azure Doc Intelligence does not extract hyperlinks\n",
            "PreviousAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "NextJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [196 tokens (includes JSON guardrails tokens)], throughput: 65.983 tokens/sec:\n",
            "- New Features:\n",
            "  - Added indexing workflow stage for configuring indexing services to infer metadata from content.\n",
            "  - Introduced AZURE_AI_LANGUAGE content indexing service for inferring language from extracted text or transcripts.\n",
            "  - Support for language content metadata returning a list of languages in ISO 639-1 format.\n",
            "  - Added MODEL_IMAGE extraction service for integration with various vision models, allowing custom specifications and API keys.\n",
            "  - Deprecated OPENAI_IMAGE service type; developers should use LLM image service instead.\n",
            "  - Removed language field from AudioMetadata type, replaced by new LanguageMetadata type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2987 where Azure Doc Intelligence did not extract hyperlinks.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 28, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for content indexing, language detection, and image extraction, improving overall functionality and flexibility.\n",
            "\n",
            "2024-12-28T07:40:23.807Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.880045, used credits [0.00579000]\n",
            "- CONTENT [4830e2a8-ea48-4f1e-878c-3bf37909c275]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [914 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-9-support-for-website-mapping-web-page-screenshots-groq-llama-3.3-model-bug-fixes</name><title>December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports mapping a website with the mapWebmutation. You can provide a URL to a website, and the query will return a list of URLs based on the sitemap.xml (or sitemap-index.xml) file, at or underneath the provided URL.\n",
            "💡 Graphlit now supports the generation of web page screenshots with the screenshotPagemutation. By providing the URL of a web page, and optionally, the maximum desired height of the screenshot, we will screenshot the webpage and ingest it automatically as content.  You can provide an optional workflow, which will be applied to the ingested image content, for operations like generating image descriptions with a vision LLM.\n",
            "💡 Graphlit now supports the direct summarization of text with the summarizeTextmutation. By providing the desired summarization strategy, we will summarize the text (i.e. bullet points, social media posts) and return the summarization.\n",
            "💡 Graphlit now supports the direct extraction of text with the extractTextmutation. By providing the LLM tool definitions and an optional LLM specification, we will prompt the desired LLM (or OpenAI GPT-4o, by default) to invoke the provided tools, and return the JSON responses from the LLM tool calling.\n",
            "Graphlit now supports the latest Groq Llama 3.3 model, with the model enum LLAMA_3_3_70B.\n",
            "We have updated Cohere reranking to use the latest Cohere rerank-v3.5model by default.\n",
            "⚡ We have added a new flattenCitationsfield to the ConversationStrategyInputtype.  By assigning this field to True, when calling promptConversation,we will combine multiple citations from the same content into a single citation.\n",
            "⚡ For Microsoft email, Microsoft Teams and OneDrive feeds, we have added the clientIdand clientSecretfields as required feed properties. These properties must be assigned, in addition to the refreshTokenfield for proper authentication to the Microsoft Graph API used by these feeds. (Colab Notebook Example)\n",
            "Bugs Fixed\n",
            "GPLA-3492: Not finding sitemap at parent web path\n",
            "GPLA-3500: Failed to handle mismatch of Deepgram model/language\n",
            "PreviousDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "NextDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "Last updated17 days ago\n",
            "- Completion [254 tokens (includes JSON guardrails tokens)], throughput: 88.193 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for website mapping using mapWebmutation to return URLs from sitemap.xml.\n",
            "  - Generation of web page screenshots with screenshotPagemutation, including optional workflows for image processing.\n",
            "  - Direct summarization of text with summarizeTextmutation, allowing various summarization strategies.\n",
            "  - Direct extraction of text with extractTextmutation, utilizing LLM tools for JSON responses.\n",
            "  - Support for Groq Llama 3.3 model (LLAMA_3_3_70B).\n",
            "  - Updated Cohere reranking to use rerank-v3.5 model by default.\n",
            "  - New flattenCitations field in ConversationStrategyInput type to combine multiple citations.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Required clientId and clientSecret fields added for Microsoft email, Teams, and OneDrive feeds for proper authentication.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with not finding sitemap at parent web path (GPLA-3492).\n",
            "  - Resolved mismatch handling of Deepgram model/language (GPLA-3500).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for web mapping, content ingestion, and improved integration with Microsoft services.\n",
            "\n",
            "2024-12-28T07:40:23.780Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.428895, used credits [0.00524100]\n",
            "- CONTENT [0457d83e-b449-40c8-9544-c268a114e122]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [723 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-4-support-for-webhook-alerts-keywords-summarization-deepseek-128k-context-window-bug-fixes</name><title>July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports webhook Alerts.  In addition to Slack notifications, you can now receive an HTTP POST webhook with the results of the published text (or text and audio URI) from a prompted alert.\n",
            "Updated the Deepseek chat and coder models to support a 128k token context window.\n",
            "Added customSummary property to Content object, which returns the custom summary generated via preparation workflow.\n",
            "Added keywords summarization type, which is now stored in keywords property in Content object.\n",
            "Added slackChannels query, which returns the list of Slack channels from the workspace authenticated by the Slack bot token.\n",
            "⚡ We have changed the response from the credits query to return a single ProjectCredits object, rather than the list of correlated objects previously returned.  The credits response now covers all credit usage over the time period specified.\n",
            "Bugs Fixed\n",
            "GPLA-2874: Processing entities is taking longer than 30min for 300+ page PDF\n",
            "GPLA-2875: Messages in queue expiring too early\n",
            "GPLA-2881: Feed read count increasing, after hitting read limit\n",
            "GPLA-2884: Need to handle Anthropic 'overloaded' API response\n",
            "GPLA-2906: JIRA issue identifier not assigned to issue metadata\n",
            "PreviousJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "NextJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [256 tokens (includes JSON guardrails tokens)], throughput: 74.660 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for webhook Alerts, allowing HTTP POST notifications with published text results.\n",
            "  - Updated Deepseek models to support a 128k token context window.\n",
            "  - Added customSummary property to Content object for custom summaries.\n",
            "  - Introduced keywords summarization type, stored in the keywords property of Content object.\n",
            "  - Added slackChannels query to list Slack channels from the authenticated workspace.\n",
            "  - Changed credits query response to return a single ProjectCredits object covering all credit usage.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved response structure for credits query.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - GPLA-2874: Reduced processing time for large PDFs.\n",
            "  - GPLA-2875: Resolved issue with messages in queue expiring too early.\n",
            "  - GPLA-2881: Fixed feed read count issue after hitting read limit.\n",
            "  - GPLA-2884: Handled Anthropic 'overloaded' API response.\n",
            "  - GPLA-2906: Assigned JIRA issue identifier to issue metadata.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 4, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve notification capabilities and processing efficiency, offering developers more robust tools for managing alerts and content summarization.\n",
            "\n",
            "2024-12-28T07:40:21.627Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.935858, used credits [0.00677700]\n",
            "- CONTENT [259e0a23-5ac8-49b8-b180-593fec210fdc]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1035 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-21-support-for-onedrive-and-google-drive-feeds-extract-images-from-pdfs-bug-fixes</name><title>February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes | Graphlit Changelog</title></metadata> 🌧️\tFebruary 2024\n",
            "February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports OneDrive and Google Drive feeds.  Files can be ingested from OneDrive or Google Drive, including shared drives where the authenticated user has access.  Both OneDrive and Google Drive support the reading of existing files, and tracking new files added to storage with recurrent feeds.\n",
            "💡 Graphlit now supports email backup files, such as EML or MSG, which will be assigned the EMAIL file type.  During email file preparation, we will automatically extract and ingest any file attachments.\n",
            "💡 Graphlit now automatically extracts embedded images in PDF files, ingests them as content objects, and links them as children of the parent PDF.\n",
            "💡 Graphlit now supports recursive Notion feeds.  When the isRecursive flag is true in the Notion feed properties, we will crawl child pages and databases, and recursively ingest them in addition to the specified pages and databases.\n",
            "Added support for assigning collections to content ingested with the ingestPage, ingestFile or ingestText mutations.  This saves a step where the content will automatically be added to the collection(s) without requiring another mutation call.\n",
            "Added support for the CODE file type for a wide variety of source code formats, i.e. Python .py, Javascript .js.  Code files use optimized text splitting for enhanced search and retrieval.\n",
            "Added support for customGuidance in Specification object, which can be used for injecting a guidance prompt during the RAG process.  For example, you can instruct the LLM to return a default response string if no content sources are found via semantic search.\n",
            "Added tenants field to Project object, which returns a list of all tenant IDs which have been used to create an entity in Graphlit.\n",
            "Added email metadata, separate from document metadata.  Now emails will contain indexed metadata such as to, from, or subject.\n",
            "⚡ The contents field for content objects has been replaced with children and parent fields.  For example, when a ZIP file is unpacked, the unpacked files will be added as children of the ZIP file, and the ZIP file will be the parent of each of the unpacked files.\n",
            "⚡ Removed enableImageAnalysis field from image preparation properties in workflow object.  Now is enabled by default.\n",
            "⚡ Moved disableSmartCapture field to preparation workflow stage from page preparation properties.  This is used to disable the use of headless Chrome browser to capture HTML from web pages.  It is enabled by default, and if disabled, Graphlit will simply download the HTML from the web page rather than rendering on headless Chrome browser.\n",
            "Bugs Fixed\n",
            "GPLA-2099: Failed to ingest ArXiV PDF.  Fixed PDF parsing error.\n",
            "GPLA-2174: LLM response is incorrect with conversation history, but no content sources.\n",
            "GPLA-2199: ZIP package left in Indexed state after content workflow.\n",
            "PreviousMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "NextFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "Last updated9 months ago\n",
            "- Completion [306 tokens (includes JSON guardrails tokens)], throughput: 77.747 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OneDrive and Google Drive feeds, allowing ingestion of files from shared drives.\n",
            "  - Support for email backup files (EML, MSG) with automatic extraction of attachments.\n",
            "  - Automatic extraction of embedded images from PDF files, linking them as children of the parent PDF.\n",
            "  - Support for recursive Notion feeds with the isRecursive flag for crawling child pages and databases.\n",
            "  - Collections can now be assigned to ingested content automatically.\n",
            "  - Introduction of CODE file type for various source code formats with optimized text splitting.\n",
            "  - Custom guidance support in Specification object for RAG process.\n",
            "  - Added tenants field to Project object for listing tenant IDs.\n",
            "  - Email metadata indexing for fields like to, from, and subject.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Contents field replaced with children and parent fields for content objects.\n",
            "  - Removed enableImageAnalysis field; now enabled by default.\n",
            "  - Moved disableSmartCapture field to preparation workflow stage, enabled by default.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed PDF parsing error for ArXiV PDFs (GPLA-2099).\n",
            "  - Corrected LLM response issues with conversation history (GPLA-2174).\n",
            "  - Resolved issue with ZIP package remaining in Indexed state (GPLA-2199).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve file ingestion and metadata handling, streamline workflows, and enhance search capabilities for developers.\n",
            "\n",
            "2024-12-28T07:40:20.919Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.227334, used credits [0.00484500]\n",
            "- CONTENT [cf01faf7-c155-4d70-8dcc-44c97cbf7c8b]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [683 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-2-support-for-semantic-alerts-openai-0125-models-performance-enhancements-bug-fixes</name><title>February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes | Graphlit Changelog</title></metadata> 🌧️\tFebruary 2024\n",
            "February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Semantic Alerts, which allows for LLM summarization and publishing of content, on a periodic basis.  This is useful for generating daily reports from email, Slack or other time-based feeds.  Alerts support the same publishing options, i.e. audio and text, as the publishContents mutation.\n",
            "💡 Graphlit now supports the latest OpenAI 0125 model versions, for GPT-4 and GPT-3.5 Turbo.  We will add support for Azure OpenAI when Microsoft releases support for these.\n",
            "Slack feeds now support a listing type field, where you can specify if you want PAST or NEW Slack messages in the feed.\n",
            "🔥 This release provides many performance enhancements, which will speed up the content workflows for ingested content.\n",
            "Bugs Fixed\n",
            "GPLA-2114: Collections not being added to text embedding index documents.\n",
            "GPLA-2063: Not handling hallucinated citations.\n",
            "GPLA-1916: Collections not inherited from project-scope into tenant-scope.\n",
            "GPLA-2105: Should error on add/remove of contents to/from collections if content does not exist.\n",
            "PreviousFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "NextJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "Last updated10 months ago\n",
            "- Completion [233 tokens (includes JSON guardrails tokens)], throughput: 72.196 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Semantic Alerts for LLM summarization and content publishing on a periodic basis, useful for generating daily reports from various feeds.\n",
            "  - Support for OpenAI 0125 model versions for GPT-4 and GPT-3.5 Turbo, with plans to add Azure OpenAI support when available.\n",
            "  - Slack feeds now include a listing type field to specify PAST or NEW Slack messages.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Performance enhancements to speed up content workflows for ingested content.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with collections not being added to text embedding index documents (GPLA-2114).\n",
            "  - Resolved handling of hallucinated citations (GPLA-2063).\n",
            "  - Fixed inheritance of collections from project-scope to tenant-scope (GPLA-1916).\n",
            "  - Added error handling for adding/removing contents to/from collections if content does not exist (GPLA-2105).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 2, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved content management capabilities, enhanced performance, and better error handling.\n",
            "\n",
            "2024-12-28T07:40:20.338Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.644046, used credits [0.00491400]\n",
            "- CONTENT [6da2b811-9ec9-484c-a102-a4a7b4a89199]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [730 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/june-2024/june-21-support-for-the-claude-3.5-sonnet-model-knowledge-graph-semantic-search-and-bug-fixes</name><title>June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes | Graphlit Changelog</title></metadata> 🎓\tJune 2024\n",
            "June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the Anthropic Claude 3.5 Sonnet model, which can be assigned with the CLAUDE_3_5_SONNET model enum.\n",
            "💡 Graphlit now supports semantic search of observable entities in the knowledge graph, such as Person, Organization and Place.  These entity types will now have vector embeddings created from their enriched metadata, and support searching by similar text, and searching by similar entities.\n",
            "⚡ We have changed the Google Drive and Google Email feed properties to require the Google OAuth client ID and client secret, along with the existing refresh token, for proper authentication against Google APIs.\n",
            "⚡ We have added a credits quota on the Free Tier.  Once 1000 credits have been used on the Free Tier, no more content can be ingested, and an upgrade to a paid tier is required.  Customers will receive an email when the credits, storage or contents quota has been reached.\n",
            "Bugs Fixed\n",
            "GPLA-2837: Failed to ingest LinkedIn page as Web feed\n",
            "GPLA-2831: Zero-byte file was left in Indexed state\n",
            "GPLA-2834: Not reading any files from Azure blob feed with space in prefix\n",
            "GPLA-2828: Better handling for files with unknown (or missing) file extensions\n",
            "PreviousJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "NextJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [227 tokens (includes JSON guardrails tokens)], throughput: 85.853 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for the Anthropic Claude 3.5 Sonnet model (model enum: CLAUDE_3_5_SONNET).\n",
            "  - Semantic search for observable entities in the knowledge graph (Person, Organization, Place) with vector embeddings for enriched metadata.\n",
            "  - Google Drive and Google Email feed properties now require Google OAuth client ID, client secret, and refresh token for authentication.\n",
            "  - Introduction of a credits quota on the Free Tier; ingestion stops after 1000 credits, requiring an upgrade to a paid tier.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved authentication process for Google APIs.\n",
            "  - Notification system for reaching credits, storage, or content quotas.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with ingesting LinkedIn pages, handling zero-byte files, reading files from Azure blob feeds with spaces, and better handling of files with unknown or missing extensions.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: June 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve model support and search capabilities, while bug fixes enhance overall platform reliability and user experience.\n",
            "\n",
            "2024-12-28T07:40:16.869Z: Search entities\n",
            "- Workflow [Semantic search] took 0:00:00.120785, used credits [0.00940000]\n",
            "- Processor name [Azure AI Search], units [47]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}