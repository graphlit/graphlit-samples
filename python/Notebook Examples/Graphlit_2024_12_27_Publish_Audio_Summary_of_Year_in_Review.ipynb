{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyOSGBVf3t/YR0pEJvhQqr0V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_12_27_Publish_Audio_Summary_of_Year_in_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to ingest Graphlit changelog, use OpenAI O1 to write a comprehensive year-in-review, and published using an [ElevenLabs](https://elevenlabs.io/) voice."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d45201-22af-4d2e-f620-d2bb6bdcda66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphlit-client in /usr/local/lib/python3.10/dist-packages (1.0.20241228002)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.3)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade isodate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqZf2TOgnXsD",
        "outputId": "6bdbd586-7e4e-4d37-8b44-e519b8226bb0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting isodate\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate\n",
            "Successfully installed isodate-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "async def create_specification(model: enums.OpenAIModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"OpenAI {model}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.OPEN_AI,\n",
        "        openAI=input_types.OpenAIModelPropertiesInput(\n",
        "            model=model,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_web_feed(uri: str, correlation_id: Optional[str], limit: Optional[int] = None):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.FeedInput(\n",
        "        name=uri,\n",
        "        type=enums.FeedTypes.WEB,\n",
        "        web=input_types.WebFeedPropertiesInput(\n",
        "            uri=uri,\n",
        "            readLimit=limit if limit is not None else 100\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_feed(input, correlation_id=correlation_id)\n",
        "\n",
        "        return response.create_feed.id if response.create_feed is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def is_feed_done(feed_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    response = await graphlit.client.is_feed_done(feed_id)\n",
        "\n",
        "    return response.is_feed_done.result if response.is_feed_done is not None else None\n",
        "\n",
        "\n",
        "async def lookup_usage(correlation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.lookup_usage(correlation_id)\n",
        "\n",
        "        return response.lookup_usage if response.lookup_usage is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def lookup_credits(correlation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.lookup_credits(correlation_id)\n",
        "\n",
        "        return response.lookup_credits if response.lookup_credits is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "\n",
        "def dump_usage_record(record):\n",
        "    print(f\"{record.date}: {record.name}\")\n",
        "\n",
        "    duration = isodate.parse_duration(record.duration)\n",
        "\n",
        "    if record.workflow:\n",
        "        print(f\"- Workflow [{record.workflow}] took {duration}, used credits [{record.credits:.8f}]\")\n",
        "    else:\n",
        "        print(f\"- Operation took {duration}, used credits [{record.credits:.8f}]\")\n",
        "\n",
        "    if record.entity_id:\n",
        "        if record.entity_type:\n",
        "            if record.entity_type == enums.EntityTypes.CONTENT and record.content_type:\n",
        "                print(f\"- {record.entity_type} [{record.entity_id}]: Content type [{record.content_type}], file type [{record.file_type}]\")\n",
        "            else:\n",
        "                print(f\"- {record.entity_type} [{record.entity_id}]\")\n",
        "        else:\n",
        "            print(f\"- Entity [{record.entity_id}]\")\n",
        "\n",
        "    if record.model_service:\n",
        "        print(f\"- Model service [{record.model_service}], model name [{record.model_name}]\")\n",
        "\n",
        "    if record.processor_name:\n",
        "        if record.processor_name in [\"Deepgram Audio Transcription\", \"Assembly.AI Audio Transcription\"]:\n",
        "            length = timedelta(milliseconds=record.count or 0)\n",
        "\n",
        "            if record.model_name:\n",
        "                print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}], length [{length}]\")\n",
        "            else:\n",
        "                print(f\"- Processor name [{record.processor_name}], length [{length}]\")\n",
        "        else:\n",
        "            if record.count:\n",
        "                if record.model_name:\n",
        "                    print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}], units [{record.count}]\")\n",
        "                else:\n",
        "                    print(f\"- Processor name [{record.processor_name}], units [{record.count}]\")\n",
        "            else:\n",
        "                if record.model_name:\n",
        "                    print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}]\")\n",
        "                else:\n",
        "                    print(f\"- Processor name [{record.processor_name}]\")\n",
        "\n",
        "    if record.uri:\n",
        "        print(f\"- URI [{record.uri}]\")\n",
        "\n",
        "    if record.name == \"Prompt completion\":\n",
        "        if record.prompt:\n",
        "            print(f\"- Prompt [{record.prompt_tokens} tokens (includes RAG context tokens)]:\")\n",
        "            print(record.prompt)\n",
        "\n",
        "        if record.completion:\n",
        "            print(f\"- Completion [{record.completion_tokens} tokens (includes JSON guardrails tokens)], throughput: {record.throughput:.3f} tokens/sec:\")\n",
        "            print(record.completion)\n",
        "\n",
        "    elif record.name == \"Text embedding\":\n",
        "        if record.prompt_tokens is not None:\n",
        "            print(f\"- Text embedding [{record.prompt_tokens} tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"Document preparation\":\n",
        "        if record.prompt_tokens is not None and record.completion_tokens is not None:\n",
        "            print(f\"- Document preparation [{record.prompt_tokens} input tokens, {record.completion_tokens} output tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"Data extraction\":\n",
        "        if record.prompt_tokens is not None and record.completion_tokens is not None:\n",
        "            print(f\"- Data extraction [{record.prompt_tokens} input tokens, {record.completion_tokens} output tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"GraphQL\":\n",
        "        if record.request:\n",
        "            print(f\"- Request:\")\n",
        "            print(record.request)\n",
        "\n",
        "        if record.variables:\n",
        "            print(f\"- Variables:\")\n",
        "            print(record.variables)\n",
        "\n",
        "        if record.response:\n",
        "            print(f\"- Response:\")\n",
        "            print(record.response)\n",
        "\n",
        "    if record.name.startswith(\"Upload\"):\n",
        "        print(f\"- File upload [{record.count} bytes], throughput: {record.throughput:.3f} bytes/sec\")\n",
        "\n",
        "    print()\n",
        "\n",
        "async def get_content(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    response = await graphlit.client.get_content(content_id)\n",
        "\n",
        "    return response.content\n",
        "\n",
        "async def query_contents(feed_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.query_contents(\n",
        "            filter=input_types.ContentFilter(\n",
        "                feeds=[\n",
        "                    input_types.EntityReferenceFilter(\n",
        "                        id=feed_id\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return response.contents.results if response.contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def publish_contents(feed_id: str, summary_specification_id: str, publish_specification_id: str, summary_prompt: str, publish_prompt: str, correlation_id: str, voice_id: Optional[str] = None):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.publish_contents(\n",
        "            name=\"Published Summary\",\n",
        "            connector=input_types.ContentPublishingConnectorInput(\n",
        "               type=enums.ContentPublishingServiceTypes.ELEVEN_LABS_AUDIO,\n",
        "               format=enums.ContentPublishingFormats.MP3,\n",
        "               elevenLabs=input_types.ElevenLabsPublishingPropertiesInput(\n",
        "                   model=enums.ElevenLabsModels.TURBO_V2_5,\n",
        "                   voice=voice_id if voice_id is not None else \"ZF6FPAbjXT4488VcRRnw\" # ElevenLabs Amelia voice\n",
        "               )\n",
        "            ),\n",
        "            summary_prompt=summary_prompt,\n",
        "            summary_specification=input_types.EntityReferenceInput(\n",
        "                id=summary_specification_id\n",
        "            ),\n",
        "            publish_prompt = publish_prompt,\n",
        "            publish_specification=input_types.EntityReferenceInput(\n",
        "                id=publish_specification_id\n",
        "            ),\n",
        "            filter=input_types.ContentFilter(\n",
        "                feeds=[input_types.EntityReferenceFilter(id=feed_id)]\n",
        "            ),\n",
        "            is_synchronous=True,\n",
        "            correlation_id=correlation_id\n",
        "        )\n",
        "\n",
        "        return response.publish_contents.content.id if response.publish_contents is not None and response.publish_contents.content is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_all_specifications():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_specifications(is_synchronous=True)\n",
        "\n",
        "async def delete_all_feeds():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_feeds(is_synchronous=True)\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import isodate\n",
        "from IPython.display import display, Markdown, HTML\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Remove any existing feeds, contents and specifications; only needed for notebook example\n",
        "await delete_all_feeds()\n",
        "await delete_all_specifications()\n",
        "await delete_all_contents()\n",
        "\n",
        "print('Deleted all feeds, contents and specifications.')\n",
        "\n",
        "# NOTE: create a unique cost correlation ID\n",
        "ingestion_correlation_id = datetime.now().isoformat()\n",
        "publish_correlation_id = datetime.now().isoformat()\n",
        "\n",
        "uri = \"https://changelog.graphlit.dev\"\n",
        "limit = 100 # maximum number of web pages to ingest\n",
        "\n",
        "feed_id = await create_web_feed(uri, ingestion_correlation_id, limit)\n",
        "\n",
        "if feed_id is not None:\n",
        "    print(f'Created feed [{feed_id}]: {uri}')\n",
        "\n",
        "    # Wait for feed to complete, since ingestion happens asychronously\n",
        "    done = False\n",
        "    time.sleep(5)\n",
        "    while not done:\n",
        "        done = await is_feed_done(feed_id)\n",
        "\n",
        "        if not done:\n",
        "            time.sleep(10)\n",
        "\n",
        "    print(f'Completed feed [{feed_id}].')\n",
        "\n",
        "    # Query contents by feed\n",
        "    contents = await query_contents(feed_id)\n",
        "\n",
        "    if contents is not None:\n",
        "        print(f'Found {len(contents)} contents in feed [{feed_id}].')\n",
        "        print()\n",
        "\n",
        "        for content in contents:\n",
        "            if content is not None:\n",
        "\n",
        "                display(Markdown(f'# Ingested content [{content.id}]'))\n",
        "\n",
        "                print(f'Text Mezzanine: {content.text_uri}')\n",
        "\n",
        "                print(content.markdown)"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0285bfc6-c593-4096-c00e-c7650f6a8c4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all feeds, contents and specifications.\n",
            "Created feed [b3609ece-d81d-4a38-b359-30951f4a1931]: https://changelog.graphlit.dev\n",
            "Completed feed [b3609ece-d81d-4a38-b359-30951f4a1931].\n",
            "Found 48 contents in feed [b3609ece-d81d-4a38-b359-30951f4a1931].\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [989c5568-cfd4-4ee4-ba79-73e3552cfdc2]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/989c5568-cfd4-4ee4-ba79-73e3552cfdc2/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéí\tSeptember 2024\n",
            "\n",
            "# September 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the Azure AI Model Inference API (aka Models as a Service) model service which offers serverless hosting to many models such as Meta Llama 3.2, Cohere Command-R, and many more. For Azure AI, all models are 'custom', and you will need to provide the serverless endpoint, API key and number of tokens accepted in context window, after provisioning the model of your choice.\n",
            "- We have added support for the multimodal Mistral Pixtral model, under the model enum PIXTRAL_12B_2409.\n",
            "- We have added versioned model enums for Google Gemini, so you can access GEMINI_1_5_FLASH_001, GEMINI_1_5_FLASH_002, GEMINI_1_5_PRO_001 and GEMINI_1_5_PRO_002.\n",
            "\n",
            "PreviousOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "NextSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [9e6ef8e1-5448-4933-9705-d9d9502b384c]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/9e6ef8e1-5448-4933-9705-d9d9502b384c/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéí\tSeptember 2024\n",
            "\n",
            "# September 3: Support for web search feeds, model deprecations\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports web search feeds, using the Tavily and Exa.AI web search APIs. You can choose the SEARCH feed type, and assign your search text property, and we will ingest the referenced web pages from the search results. Optionally, you can select the search service via the serviceType property under search feed properties. By default, Graphlit will use the Tavily API.\n",
            "- ‚ö° We have deprecated these OpenAI models, according to the future support OpenAI is providing to these legacy models: GPT35_TURBO, GPT35_TURBO_0613, GPT35_TURBO_16K, GPT35_TURBO_16K_0125, GPT35_TURBO_16K_0613, GPT35_TURBO_16K_1106, GPT4, GPT4_0613, GPT4_32K, GPT4_32K_0613, GPT4_TURBO_VISION_128K, and GPT4_TURBO_VISION_128K_1106. We suggest using GPT-4o or GPT-4o Mini instead.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2523: Can't ingest from same feed URI multiple times and wait on isFeedDone\n",
            "\n",
            "PreviousSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "NextSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "Last updated3 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [090640e2-097b-4684-90fa-86ab0cba94fc]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/090640e2-097b-4684-90fa-86ab0cba94fc/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üõ†Ô∏è\tSeptember 2023\n",
            "\n",
            "# September 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "### New Features\n",
            "\n",
            "- üî• Added Workflow entity to data model for configuring stages of content workflow; can be assigned to Feed or with ingestPage, ingestFile, or ingestText mutations to control how content is ingested, prepared, extracted and enriched into the knowledge graph.\n",
            "- üí° Added support for Notion feeds: now can create feed to ingest files from Notion pages or databases (i.e. wikis).\n",
            "- üí° Added support for API-created Observation entities, which allow for custom observations of observable entities (i.e. Person, Label) on Content.\n",
            "- üí° Added support for Azure AI Document Intelligence as an optional method for preparing PDF files, using OCR and advanced layout analysis.\n",
            "- üí° Added summarization strategies, where content can be summarized into paragraphs, bullet points or headline.\n",
            "- Added ability to assign default Workflow and Specification to project.\n",
            "- Added more well-known link types, during link crawling, such as Discord, Airtable and TypeForm.\n",
            "- ‚ÑπÔ∏è Free/Hobby plan now has 5GB storage quota; any content ingested past that limit will be auto-deleted.\n",
            "- ‚ö° Actions have been moved into Workflow entity.\n",
            "- ‚ö° Link enrichment for Feeds has been moved into the Workflow enrichment stage, now called link crawling. ExcludeContentDomain property has been reversed and is now called IncludeContentDomain.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1204: Failed to ingest content with backslash in name.\n",
            "- GPLA-1276: Failed to ingest RSS posts which contained enclosure URI, but no post URI.\n",
            "\n",
            "PreviousSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "NextAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [a704d7fa-1161-4d3a-9be7-569d3af4d1cf]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/a704d7fa-1161-4d3a-9be7-569d3af4d1cf/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéí\tSeptember 2024\n",
            "\n",
            "# September 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the Cerebras model service which offers the LLAMA_3_1_70B and LLAMA_3_1_8B models.\n",
            "- üí° Graphlit now supports the Google AI model service which offers the GEMINI_1_5_PRO and GEMINI_1_5_FLASH models.\n",
            "- We have added support for the latest Groq Llama 3.2 preview models, including LLAMA_3_2_1B_PREVIEW, LLAMA_3_2_3B_PREVIEW, LLAMA_3_2_11B_TEXT_PREVIEW, and LLAMA_3_2_90B_TEXT_PREVIEW. We have also added support for the Llama 3.2 multimodal model LLAMA_3_2_11B_VISION_PREVIEW.\n",
            "- We have added a new specification parameter to the promptConversation mutation. Now you can specify your initial specification for a new conversation, or update an existing conversation, without requiring additional API calls.\n",
            "- ‚ö° We have changed the retrieval behavior of the promptConversation mutation. Now, if no relevant content was found via vector-based semantic search (given the user prompt), we will fallback to any relevant content from the message in the conversation. If there was no content from the conversation to fallback to, we will fallback to the last ingested content in the project. This solves an issue where a first prompt like 'Summarize this' would find no relevant content. Now it will fallback to retrieve the last ingested content.\n",
            "- ‚ö° We have renamed the Groq model enum from LLAVA_1_5_7B to LLAVA_1_5_7B_PREVIEW.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3083: Not sending custom instructions/guidance with extraction prompt\n",
            "- GPLA-3146: Filtering Persons by email not working\n",
            "- GPLA-3171: Not failing on deprecated OpenAI model\n",
            "- GPLA-3158: Summarization not using revision strategy\n",
            "\n",
            "PreviousSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "NextSeptember 3: Support for web search feeds, model deprecations\n",
            "Last updated3 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [3a2727e0-f370-4b5c-9d04-59e54995f166]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/3a2727e0-f370-4b5c-9d04-59e54995f166/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üõ†Ô∏è\tSeptember 2023\n",
            "\n",
            "# September 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "### New Features\n",
            "\n",
            "- üî• Graphlit now supports YouTube feeds, where you can ingest a set of YouTube videos, or an entire YouTube playlist or channel. Note, we currently support only the ingestion of audio from YouTube videos, which gets transcribed and added to your conversational knowledge graph.\n",
            "\n",
            "### New Documentation\n",
            "\n",
            "- Added documentation for observable entities mutations and queries (Label, Category, Person, Organization, Place, Event, Product, Repo, Software).\n",
            "- Added documentation for using custom Azure OpenAI and OpenAI models with Specifications\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1459: LLM prompt formatting was exceeding the token budget with long user prompts.\n",
            "- GPLA-1445: Failed to ingest PDF from URL where filename in Content-Disposition header contained a backslash.\n",
            "\n",
            "PreviousOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "NextSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "Last updated7 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [550d544c-3dc6-4fae-a4d1-89577544d9fc]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/550d544c-3dc6-4fae-a4d1-89577544d9fc/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üõ†Ô∏è\tSeptember 2023\n",
            "\n",
            "# September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "### New Features\n",
            "\n",
            "- üî• Graphlit now supports paid Hobby, Starter and Growth tiers for projects, in addition to the existing Free tier. Starting at $49/mo, plus $0.10/credit for usage, we now support higher quota based on your subscribed tier. By providing a payment method for your organization in the Developer Portal, you can upgrade each project individually to the tier that fits your application's needs.\n",
            "- üí° Added GraphQL mutations for the creation, update and deletion of observed entities (i.e. Person, Organization, Place, Product, Event, Label, Category).\n",
            "- üí° Added new observed entity types to knowledge graph: Repo (i.e. Git repo), Software.\n",
            "- üí° Added searchType and numberSimilar fields to Specification object for configuring semantic search in conversations. In situations where the user prompt is limited in length, HYBRID search type can provide better semantic search results for the prompt context.\n",
            "- üí° Added support for the Azure OpenAI GPT-4 model.\n",
            "- Added support for project quota field. Project quotas are based on the subscribed pricing tier. Quota limits are now applied as content is ingested, and as feeds and conversations are created.\n",
            "- Added contentLimit to conversation strategy object to limit the number of semantic search content results which are formatted into prompt context.\n",
            "- Better relevance ranking on semantic search results when formatting prompt context in conversations.\n",
            "- ‚ÑπÔ∏è Free tier has updated quota: 1GB storage, 100 contents, 3 feeds and 10 conversations.\n",
            "- ‚ö° Now using the Deepgram Nova-2 audio transcription model, which is 18% more accurate, and 5-40x faster.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1373: Failed to extract multiple text pages from DOCX without page breaks. Now we support token-aware page chunking.\n",
            "- GPLA-1377: Failed during semantic search with no content results, when prompting conversation.\n",
            "- GPLA-1415: Failed when user prompt couldn't generate text embeddings.\n",
            "\n",
            "PreviousSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "NextSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "Last updated11 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [85a254e5-593b-4823-b1b9-d48506b794f8]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/85a254e5-593b-4823-b1b9-d48506b794f8/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÉ\tOctober 2024\n",
            "\n",
            "# October 9: Support for GitHub repository feeds, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports GitHub feeds, by providing the repository owner and name similar to GitHub Issues feeds, and will ingest code files from any GitHub repository.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3262: Missing row separator in table markdown formatting\n",
            "\n",
            "PreviousOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "NextOctober 7: Support for Anthropic and Gemini tool calling\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [3f9aa839-735f-4aa9-a599-c7d59d2898aa]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/3f9aa839-735f-4aa9-a599-c7d59d2898aa/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÉ\tOctober 2024\n",
            "\n",
            "# October 7: Support for Anthropic and Gemini tool calling\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports tool calling with Anthropic and Google Gemini models.\n",
            "- ‚ö° We have removed the uri property for tools from ToolDefinitionInput, such that inline webhook tools are no longer supported. Now you can define any external tools to be called, and those can support sync or async data access to fulfill the tool call.\n",
            "\n",
            "PreviousOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "NextOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [49836c55-86df-40ce-9f5e-98b3eb2ebc41]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/49836c55-86df-40ce-9f5e-98b3eb2ebc41/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÉ\tOctober 2024\n",
            "\n",
            "# October 31: Support for simulated tool calling, bug fixes\n",
            "### New Features\n",
            "\n",
            "- Graphlit now supports simulated tool calling for LLMs which don't natively support it, such as OpenAI o1-preview and o1-mini. Tool schema will be formatted into the LLM prompt context, and tool responses are parsed out of the JSON formatted response.\n",
            "- ‚ö° Given customer feedback, we have lowered the vector and hybrid thresholds used by the semantic search. Previously, some content at a low relevance was being excluded from the semantic search results. Now, more low-relevance content will be included in the results, used by the RAG pipeline. Reranking can be used to sort the search results for relevance.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3357: Not extracting all images from PDF, and should filter out single-color images.\n",
            "\n",
            "PreviousNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "NextOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [0e61c44c-cae3-4a7a-95f0-081150f5860e]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/0e61c44c-cae3-4a7a-95f0-081150f5860e/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÉ\tOctober 2024\n",
            "\n",
            "# October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the ingestBatch mutation, which accepts an array of URIs to files or web pages, and will asynchronously ingest these into content objects.\n",
            "- üí° Graphlit now supports the continueConversation mutation, which accepts an array of called tool responses. Also, promptConversation now accepts an array of tool definitions. When tools are called by the LLM, the assistant message returned from promptConversation will have a list of toolCalls which need to responded to from your calling code. These responses are to be provided back to the LLM via the continueConversation mutation.\n",
            "- üí° Graphlit now supports tool calling with OpenAI, Mistral, Deepseek, Groq, and Cerebras model services. Anthropic, Google Gemini and Cohere support will come later.\n",
            "- Added support for prefilled user and assistant messages with createConversation mutation. Now you can send an array of messages when creating a new conversation, which will bootstrap the conversation with the LLM. These must be provided in user/assistant pairs.\n",
            "- Added support for Google Gemini Flash 1.5 8b model.\n",
            "- ‚ö° We have deprecated the tools property in the Specification object. These will be removed at a later date. Tools are now to be sent directly to the extractContents and promptConversation mutations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3207: Models shouldn't be required on update specification call\n",
            "- GPLA-3220: Don't send system prompt with OpenAI o1 models\n",
            "\n",
            "PreviousOctober 7: Support for Anthropic and Gemini tool calling\n",
            "NextSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [8e5925f7-0358-42ff-84df-8522c7f265a8]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/8e5925f7-0358-42ff-84df-8522c7f265a8/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÉ\tOctober 2024\n",
            "\n",
            "# October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "### New Features\n",
            "\n",
            "- Graphlit now supports the latest Anthropic Sonnet 3.5 model (released 10/22/2024). We have added date-versions model enums for the Anthropic models: CLAUDE_3_5_SONNET_20240620, CLAUDE_3_5_SONNET_20241022, CLAUDE_3_HAIKU_20240307, CLAUDE_3_OPUS_20240229, CLAUDE_3_SONNET_20240229. The existing model enums will target the latest released models, as specified by Anthropic.\n",
            "- Graphlit now supports image embeddings using the Cohere Embed 3.0 models.\n",
            "\n",
            "PreviousOctober 31: Support for simulated tool calling, bug fixes\n",
            "NextOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/bf1f3903-d17d-4ec6-b76a-cd7687b49dbf/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéí\tSeptember 2024\n",
            "\n",
            "# September 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports entity enrichment from Fast Healthcare Interoperability Resources (FHIR) servers. You can provide the endpoint for a FHIR server, and Graphlit will enrich medical-related entities from the data found in the FHIR server.\n",
            "- Added support for latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408) and added datestamped model enums for the previous versions (COMMAND_R_202403, COMMAND_R_PLUS_202404). The latest model enums (COMMAND_R and COMMAND_R_PLUS) currently point to the models (COMMAND_R_202403 and COMMAND_R_PLUS_202404) as specified by the Cohere API.\n",
            "- Added support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31), now used by default.\n",
            "- ‚ö° We have changed the name of the LinkReferenceType to LinkReference to follow the existing data model standard.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3120: LLM is adding source tags to end of completed messages\n",
            "- GPLA-3133: Failed to load sitemap on child page of website.\n",
            "\n",
            "PreviousSeptember 3: Support for web search feeds, model deprecations\n",
            "NextAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "Last updated3 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [cfa6a062-2cfa-4994-a066-6443539c4553]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/cfa6a062-2cfa-4994-a066-6443539c4553/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÉ\tOctober 2024\n",
            "\n",
            "# October 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the configuration of image and text embedding models, at the Project level. You can create an embedding specification for a text or image embedding model, and then assign that to the Project, and all further embedding requests will use that embedding model. See this Colab notebook for an example of how to configure the project.\n",
            "- üí° Graphlit now supports the OpenAI Embedding-3-Small and Embedding-3-Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0 and 3.0 text embedding models. Graphlit also now supports Jina CLIP image embeddings, which are used by default for image search.\n",
            "- Graphlit now supports the chunkTokenLimit property in Specifications, which specifies the number of tokens for each embedded text chunk. If this is not configured, Graphlit uses 600 tokens for each embedded text chunk.\n",
            "- Graphlit now supports the Voyage reranking model.\n",
            "- Graphlit now supports the ingestTextBatch mutation, which accepts an array of text and name pairs, and will asynchronously ingest these into content objects.\n",
            "- ‚ö° We have moved the chunkTokenLimit property from the Workflow storage embeddings strategy to the Specification object. The Workflow storage property has now been deprecated.\n",
            "- ‚ö° We have deprecated the openAIImage property from Workflow entity extraction properties. Use the modelImage property instead.\n",
            "\n",
            "Once a text embedding model has been updated at the project level, any content, conversations or observed entities will no longer be semantically searchable.\n",
            "Text embeddings are not compatible across models, so you will need to delete and reingest any content, or recreate conversations or knowledge graph entities, with the new embedding model to become searchable.\n",
            "PreviousOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "NextOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [53a0f36a-7c33-4514-901f-4865aa3fe0a9]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/53a0f36a-7c33-4514-901f-4865aa3fe0a9/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÉ\tOctober 2023\n",
            "\n",
            "# October 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports 'aliases' of observable names, as the alternateNames property. When an observed entity, such as Organization, is enriched, we store the original name and the enriched name as an alias. For example, \"OpenAI\" may be enriched to \"OpenAI, Inc.\", and we store \"OpenAI\" as an alias, and update the name to \"OpenAI, Inc.\".\n",
            "- üí° Added workflows filter to ContentCriteriaInput type, for filtering content by workflow(s) when creating conversation.\n",
            "- Optimized formatting of content sources into prompt context, for more accurate conversation responses.\n",
            "- Optimized formatting of extracted text from Slack messages, for better knowledge retrieval.\n",
            "- Updated text tokenizer for more accurate token counting.\n",
            "- Upgraded Azure Text Analytics to latest preview API version.\n",
            "- Authors found in RSS feeds are now stored as observations of Person entities.\n",
            "- Added rate limiting for Reddit feeds.\n",
            "- Added rate limiting for Wikipedia enrichment.\n",
            "- Added support for reading Reddit post comments when reading Reddit feed.\n",
            "- ‚ö° EmbedFacets has been renamed to EnableFacets in the conversation strategy.\n",
            "- ‚ö° Removed extra content level in IngestionWorkflowStage type. Now, the if property is of type IngestionContentFilter.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1556: Better handling of very long user prompts.\n",
            "- GPLA-1627: Optimized token budget for more accurate prompt completion.\n",
            "- GPLA-1585: More accurate entity matching in Wikipedia entity enrichment.\n",
            "\n",
            "PreviousDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "NextOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [e1cd970b-fde4-41b4-892a-1a64cb531334]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/e1cd970b-fde4-41b4-892a-1a64cb531334/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÉ\tOctober 2023\n",
            "\n",
            "# October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "### New Features\n",
            "\n",
            "- üî• Graphlit now supports Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "- üî• Graphlit now supports Slack feeds, and will ingest Slack messages and linked file attachments from a Slack channel. Note, this requires the creation of a Slack bot which has been added to the appropriate Slack channel.\n",
            "- üí° Added support for entity enrichment to workflow object, which offers Diffbot, Wikipedia and Crunchbase enrichment of observed entities, such as Person, Organization and Place.\n",
            "- üí° Added support for text extraction from images. When using Azure Image Analytics for entity extraction, Graphlit will extract and store any identified text which then becomes searchable.\n",
            "- Added embedFacets property to conversation strategy in specification object.\n",
            "- Added embedCitations property to conversation strategy in specification object. This makes content citations optional with the completed conversation message.\n",
            "- Added GraphQL mutations for multi-delete of entities, such as deleteCollections, deleteLabels, or deleteConversations.\n",
            "- Added GraphQL deleteAllConversations mutation to delete all conversations.\n",
            "- Added support for automatically adding ingested content to one or more collections, via ingestion stage of workflow object.\n",
            "- Added specification property to preparation workflow stage, which will be used to select the LLM for text summarization.\n",
            "- Expanded the properties for observed entities, such as Person, Organization or Product. Now supports a wider range of properties for entity enrichment.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1520: Unlimited conversation quota not assigned when upgrading project tier\n",
            "- GPLA-1285: Entity enrichment not firing event, which can be sent to actions\n",
            "- GPLA-1361: Web page left in ingested state, when URL not accessible.\n",
            "\n",
            "PreviousOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "NextSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [119976d5-ad8a-4a2a-8c4d-e008bc61e848]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/119976d5-ad8a-4a2a-8c4d-e008bc61e848/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "ü¶É\tNovember 2024\n",
            "\n",
            "# November 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "### New Features\n",
            "\n",
            "- Graphlit now supports the latest Anthropic Haiku 3.5 model, with the model enum CLAUDE_3_5_HAIKU_20241022.\n",
            "- ‚ö° Once a project has hit the free tier quota, we will now automatically disable all feeds. Once the project has been upgraded to a paid tier, you can use the enableFeed mutation to re-enable your existing feeds to continue ingestion.\n",
            "- ‚ö° We have added the disableFallback flag to the RetrievalStrategyInput type, so you can disable the default behavior of falling back to the previous conversation's contents, or worst-case, falling back to the most recently uploaded content. By setting disableFallback to true, conversations will only attempt to retrieve contents based on the provided filter and/or augmentedFilter properties.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3367: Not extracting text from HTML button element\n",
            "\n",
            "PreviousNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "NextOctober 31: Support for simulated tool calling, bug fixes\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [91123175-dc50-4c4e-9421-971e59c75969]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/91123175-dc50-4c4e-9421-971e59c75969/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "ü¶É\tNovember 2024\n",
            "\n",
            "# November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports multi-turn analysis of images with the reviseImage and reviseEncodedImage mutations. You can provide an LLM prompt and either a URI or Base-64 encoded image and MIME type, along with an optional LLM specification. This can be used for analyzing any image and having a multi-turn conversation with the LLM to revise the output from the LLM. (Colab Notebook Example)\n",
            "- üí° Graphlit now supports directly prompting an LLM with the prompt mutation, bypassing any RAG content retrieval, while providing an optional list of previous conversation messages. This also accepts an optional LLM specification. (Colab Notebook Example)\n",
            "- We have added support for the new Mistral Pixtral Large model, with PIXTRAL_LARGE model enum, which can be used with LLM completion or entity extraction LLM specifications.\n",
            "- We have added support for the OpenAI 2024-11-20 version of GPT-4o, with GPT4O_128K_20241120 model enum.\n",
            "- ‚ö° We have added Microsoft Entra ID (fka Azure Active Directory) clientId and clientSecret properties to the SharePointFeedPropertiesInput type, which are now required when creating a SharePoint feed using user authentication with refreshToken property. (Colab Notebook Example)\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3438: Not filtering on desktop presentation when scraping web pages\n",
            "- GPLA-3340: Failed to parse invalid JSON from extracted PDF page\n",
            "- GPLA-3427: Not formatting extracted tables properly from Sonnet 3.5\n",
            "\n",
            "PreviousDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "NextNovember 16: Support for image description, multi-turn text summarization\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [55afbdb7-30a8-4eaa-92aa-02ae98fb57a5]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/55afbdb7-30a8-4eaa-92aa-02ae98fb57a5/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "ü¶É\tNovember 2024\n",
            "\n",
            "# November 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports web search with the searchWeb mutation. You can select the search service, either Tavily or Exa.AI, and provide the search query and number of search results to be returned. This is different than the web search feed, in that searchWeb returns the relevant text from the web page and the web page URL from each search hit, but does not ingest each of the web pages. This new mutation is optimized to be used from within an LLM tool.\n",
            "- üí° Graphlit now supports multi-turn summarization of content with the reviseContent mutation. You can provide an LLM prompt and a content reference, along with an optional specification. This can be used for summarizing any content (documents, web pages, audio transcripts, etc.), and having a multi-turn conversation with the LLM to revise the output from the LLM. Internally, this creates a conversation locked to a single piece of content. This works especially well with the OpenAI o1-preview and o1-mini models, because they provide a longer LLM output from each turn.\n",
            "- Graphlit now supports the configuration of the Deepgram transcription language, and whether detectLanguage is enabled in DeepgramAudioPreparationPropertiesInput. Language detection is now enabled by default, and can be disabled by setting detectLanguage to false.\n",
            "- ‚ö° We have added a requireTool option to promptConversation mutation, so you can control whether the LLM must call one of the provided tool, or if tool calling is optional.\n",
            "- ‚ö° For accounts created after Nov 8, 2024, we have lowered the credits quota on the Free tier from 1000 credits to 100 credits, and now offer unlimited feeds on the Hobby Tier.\n",
            "- ‚ö° The Graphlit Data API will now return HTTP 402 (Payment Required) when you have exceeded the credits quota on the free tier. You must upgrade to the Hobby Tier (or higher) to continue using the API, once the credits quota has been reached.\n",
            "\n",
            "PreviousNovember 16: Support for image description, multi-turn text summarization\n",
            "NextNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f9db5d97-0274-4ca3-b867-3f2c46a5359e]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/f9db5d97-0274-4ca3-b867-3f2c46a5359e/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "ü¶É\tNovember 2024\n",
            "\n",
            "# November 16: Support for image description, multi-turn text summarization\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports multi-turn summarization of text with the reviseText mutation. You can provide an LLM prompt and text string, along with an optional specification. This can be used for summarizing any raw text and having a multi-turn conversation with the LLM to revise the output from the LLM. (Colab Notebook Example)\n",
            "- üí° Graphlit now supports image descriptions using vision LLMs, without needing to ingest the image first. With the new describeImage mutation, which takes a URI, and describeEncodedImage mutation, which takes a Base-64 encoded image and MIME type, you can use any vision LLM to prompt an image description. These mutations accept an optional specification, where you can select your vision LLM. If not provided, OpenAI GPT-4o will be used. (Colab Notebook Example)\n",
            "\n",
            "PreviousNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "NextNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [375b16d0-9e97-44ae-b1a9-4933a2e34836]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/375b16d0-9e97-44ae-b1a9-4933a2e34836/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üíê\tMay 2024\n",
            "\n",
            "# May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the Jina reranker and Pongo semantic filtering (reranking), in the Specification object. Now you can choose between COHERE, PONGO and JINA for your reranking serviceType.\n",
            "- üí° Graphlit now supports Microsoft Teams feeds for reading messages from Teams channels.\n",
            "- Given changes in YouTube video player HTML, we have rewritten the YouTube downloader to support the new page format.\n",
            "- Added better handling of HTTP errors when validating URIs. Previously some websites were returning HTTP 403 (Forbidden) errors when validating their URI, or downloading content. Now Graphlit is able to scrape these sites, which previously returned errors.\n",
            "- Added support for updating content metadata in updateContent mutation. Now the video, audio, document, etc. metadata can be updated after the content workflow has finished.\n",
            "- Added query_contents_graph (and queryContentsGraph) functions to SDKs, which can be used to return nodes and edges from knowledge graph for visualization.\n",
            "- ‚ö° Citation indices have been changed to be one-based from zero-based. For example, you will now see \"This is a citation. [1]\" as the first citation in the list.\n",
            "- ‚ö° Added isSynchronous flag to deleteAll and multiple delete mutations. By default, bulk delete operations are now asynchronous (and completed after the mutation returns), unless the isSynchronous flag is set to true.\n",
            "- ‚ö° Added missing count mutations, such as countAlerts, countFeeds, etc.\n",
            "- ‚ö° Renamed query_content_facets to query_contents_facets in Python SDK\n",
            "- ‚ö° Renamed queryContentFacets to queryContentsFacets in Node.js SDK\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2544: Page relevance not filled-in in all situations\n",
            "- GPLA-2546: Not extracting links from PDF with Azure AI Doc Intelligence\n",
            "- GPLA-2557: Sporadically returning HTTP 500 from GraphQL API\n",
            "- GPLA-2573: Failed to re-ingest content which was deleted immediately after initial ingestion\n",
            "- GPLA-2575: Not validating for empty (non-null) parameters in mutations\n",
            "- GPLA-2578: Need to handle invalid JSON from LLMs; improper escaping or formatting\n",
            "- GPLA-2585: Failed to ingest encoded file with colon (:) in name\n",
            "\n",
            "PreviousMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "NextApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "Last updated7 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [8d95b393-a271-4cb7-956b-6f2808937151]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/8d95b393-a271-4cb7-956b-6f2808937151/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üíê\tMay 2024\n",
            "\n",
            "# May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports GraphRAG, where the extracted entities in the knowledge graph can be added as additional context to your RAG con,versation. Also, with GraphRAG, entities can be extracted from the user prompt, and used as additional content filters - or can be used to query related content sources, which are combined with the vector search results. This can be configured by specifying your graphStrategy in the Specification object.\n",
            "- üí° Graphlit now supports LLM revisions within RAG conversations, where the LLM can be prompted to revise its initial completion response. From our testing, this has been shown to provide 35% more output tokens with higher quality responses. This can be configured by specifying your revisionStrategy, and you can use our built-in revision prompt, or provide a custom one, and specify how many revisions you want the LLM to make.\n",
            "- üí° Graphlit now supports the new OpenAI GPT-4o model for RAG conversations.\n",
            "- ‚ö° We have changed the default model for Conversations to be OpenAI GPT-4o, from Azure OpenAI GPT-3.5 16k. This provides faster performance and better quality output.\n",
            "- Added graph to promptConversation response, so you can visualize or leverage the nodes and edges of the knowledge graph, resulting from the content retrieval. For example, if a Person and Organization were observed in the cited content sources used by the RAG pipeline, you will get back those entities and their relationship (such as Person 'works-for' Organization).\n",
            "- Expanded the enriched data from WIkipedia to include the long description of an entity.\n",
            "- Added getSharePointLibraries, getSharePointFolders, and getOneDriveFolders queries to the API, which can be used to enumerate the storage services. This makes locating the SharePoint libraryId easier, for example.\n",
            "- Added getTeams and getTeamsChannels queries to the API for enumerating Microsoft Teams workspaces.\n",
            "- Added extractedCount to the entity extraction connector to limit the number of extracted entities, per entity type. I.e. if extracted count is 10, it will extract at most ten each of Persons, Organizations, etc.\n",
            "- üî• We have improved performance in several areas: creation of observations after entity extraction, access to cloud storage, rendering the RAG context.\n",
            "- üî• We have optimized the LLM entity extraction process to identify more properties, as well as entity-to-entity relationships.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2652: Not extracting text from HTML in RSS post\n",
            "- GPLA-2627: Limit filter only returning half the results\n",
            "- GPLA-2613: Not properly extracting structured text from JSON/XML formats\n",
            "\n",
            "PreviousJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "NextMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "Last updated6 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [2611e484-0933-494b-bebe-511835b64bb9]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/2611e484-0933-494b-bebe-511835b64bb9/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üçÄ\tMarch 2024\n",
            "\n",
            "# March 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports Linear, GitHub Issues and Atlassian Jira feeds. Graphlit will ingest issues (aka tasks, stories) from these issue-tracking services as individual content items, which will be made searchable and conversational.\n",
            "- üí° Added support for ISSUEcontent type, which includes metadata such as title, authors, commenters, status, type, project and team.\n",
            "- üí° Added support for default feed read limit. Now, if you don't specify the readLimit property on feeds, it will default to reading 100 content items. You can override this default by assigning a custom read limit, which has no upper bounds. However, one-shot feeds much complete within 15 minutes, or they will be stopped automatically.\n",
            "- Added support for ingesting files referenced in a Web sitemap. Previously any files (i.e. PDF, MP3) referenced in a sitemap.xml would be ignored. Now you can optionally enable includeFiles in the WebFeedPropertiesInput object to have Graphlit ingest non-HTML pages as part of the Web feed.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2374: Failed to ingest MP4 with large XMP metadata.\n",
            "\n",
            "PreviousApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "NextMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [feb66b5f-ba9e-448e-9909-ba5e2fdbf56d]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/feb66b5f-ba9e-448e-9909-ba5e2fdbf56d/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üçÄ\tMarch 2024\n",
            "\n",
            "# March 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the Claude 3 Haiku model.\n",
            "- Added support for direct ingestion of Base64 encoded files with the ingestEncodedFile mutation. You can pass a Base64 encoded string and MIME type of the file, and it will be ingested into the Graphlit Platform.\n",
            "- Added modelService and model properties to ConversationMessage type, which return the model service and model which was used for the LLM completion.\n",
            "\n",
            "PreviousMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "NextMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [3b356809-5179-4144-80bc-baf4dd184462]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/3b356809-5179-4144-80bc-baf4dd184462/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üçÄ\tMarch 2024\n",
            "\n",
            "# March 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports a Command-Line Interface (CLI) for directly accessing the Graphlit Data API without writing code. See the documentation here.\n",
            "- üí° Graphlit now supports the Groq Platform, and models such as Mixtral 8x7b.\n",
            "- üí° Graphlit now supports Claude 3 Opus and Sonnet models.\n",
            "- üí° Graphlit now supports Mistral La Plateforme, and models such as Mistral Small, Medium, and Large and Mixtral 8x7b.\n",
            "- üí° Graphlit now supports the latest v4 of Azure Document Intelligence, including their new models such as Credit Card, Marriage Certificate, and Mortgage documents.\n",
            "- Added support for detailed usage and credits telemetry via API, with the usage, credits, lookupUsage and lookupCredits queries.\n",
            "- Added support for correlated telemetry, where an optional correlationId can be provided with GraphQL queries and mutations, so credits and usage can be tracked across requests.\n",
            "- Added support for project webhook, which will be called when credits have been consumed by the project.\n",
            "- Added support for image extraction during DOCX, XLSX, and PPTX document preparation.\n",
            "- Added text and markdown properties to Content object, which provide formatted output of extracted text from any content.\n",
            "- Added more accurate extraction of tables into mezzanine JSON format, across all content types.\n",
            "- Added throughput property to Conversation messages, which returns the tokens/second throughput of LLM.\n",
            "- ‚ö° Deprecated mezzanineUri property in Content object, which has been replaced by textUri and audioUri.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2281: Not extracting table from PPTX file.\n",
            "- GPLA-2282: Not extracting Markdown tables.\n",
            "- GPLA-2247: Not extracting relative HTML links properly.\n",
            "- GPLA-2241: Failed to post Alert to Slack with Markdown format.\n",
            "\n",
            "PreviousMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "NextFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [bbe12218-8806-409d-a79b-148d8f0eb515]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/bbe12218-8806-409d-a79b-148d8f0eb515/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéì\tJune 2024\n",
            "\n",
            "# June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the Anthropic Claude 3.5 Sonnet model, which can be assigned with the CLAUDE_3_5_SONNET model enum.\n",
            "- üí° Graphlit now supports semantic search of observable entities in the knowledge graph, such as Person, Organization and Place. These entity types will now have vector embeddings created from their enriched metadata, and support searching by similar text, and searching by similar entities.\n",
            "- ‚ö° We have changed the Google Drive and Google Email feed properties to require the Google OAuth client ID and client secret, along with the existing refresh token, for proper authentication against Google APIs.\n",
            "- ‚ö° We have added a credits quota on the Free Tier. Once 1000 credits have been used on the Free Tier, no more content can be ingested, and an upgrade to a paid tier is required. Customers will receive an email when the credits, storage or contents quota has been reached.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2837: Failed to ingest LinkedIn page as Web feed\n",
            "- GPLA-2831: Zero-byte file was left in Indexed state\n",
            "- GPLA-2834: Not reading any files from Azure blob feed with space in prefix\n",
            "- GPLA-2828: Better handling for files with unknown (or missing) file extensions\n",
            "\n",
            "PreviousJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "NextJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "Last updated6 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [dd2cef1a-5292-4654-a2e2-d40b4f82db84]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/dd2cef1a-5292-4654-a2e2-d40b4f82db84/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "‚òÄÔ∏è\tJuly 2024\n",
            "\n",
            "# July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports webhook Alerts. In addition to Slack notifications, you can now receive an HTTP POST webhook with the results of the published text (or text and audio URI) from a prompted alert.\n",
            "- Updated the Deepseek chat and coder models to support a 128k token context window.\n",
            "- Added customSummary property to Content object, which returns the custom summary generated via preparation workflow.\n",
            "- Added keywords summarization type, which is now stored in keywords property in Content object.\n",
            "- Added slackChannels query, which returns the list of Slack channels from the workspace authenticated by the Slack bot token.\n",
            "- ‚ö° We have changed the response from the credits query to return a single ProjectCredits object, rather than the list of correlated objects previously returned. The credits response now covers all credit usage over the time period specified.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2874: Processing entities is taking longer than 30min for 300+ page PDF\n",
            "- GPLA-2875: Messages in queue expiring too early\n",
            "- GPLA-2881: Feed read count increasing, after hitting read limit\n",
            "- GPLA-2884: Need to handle Anthropic 'overloaded' API response\n",
            "- GPLA-2906: JIRA issue identifier not assigned to issue metadata\n",
            "\n",
            "PreviousJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "NextJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/595fbc8a-88d8-4e2f-a3f0-33a86c892d76/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéì\tJune 2024\n",
            "\n",
            "# June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports Deepseek LLMs for prompt completion. We offer the deepseek-chat and deepseek-coder models.\n",
            "- üí° Graphlit now supports parsing embedded JSON-LD from web pages. If a web page contains 'script' tags with JSON-LD, we will automatically parse and inject into the knowledge graph.\n",
            "- ‚ö° We have changed the default model for entity extraction and image completions to be OpenAI GPT-4o. This provides faster performance and better quality output.\n",
            "- ‚ö° We have changed the behavior of knowledge graph generation, from a prompted conversation, to be opt-in. In order to receive the graph's nodes and edges with the response, you will now need to set generateGraph to True in the specification's graphStrategy object. This provides improved performance when the graph is not needed for visualization.\n",
            "- Added thing property for observable entities, which returns the JSON-LD metadata associated with the entity.\n",
            "- Added regex-based filtering for URI paths during feed ingestion, link crawling, and workflow filtering. You can assign regex patterns in allowedPaths and excludedPaths.\n",
            "- Added observableLimit to configure the limit of how many observed entities will be added to the GraphRAG context, defaults to 1000.\n",
            "- Added prompt to suggestConversation mutation, which allows customization of the followup question generation.\n",
            "- Updated suggestConversation to incorporate the past conversation message history, in addition to the filtered set of content sources.\n",
            "- üî• We have improved performance in knowledge graph retrieval and generation, via better parallelization and batching.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2748: Optimize the retrieval performance of observed entities during GraphRAG\n",
            "- GPLA-2732: Invalid user-provided URI causing parsing exception\n",
            "- GPLA-2666: Shouldn't require tenant ID for Microsoft email or Teams\n",
            "- GPLA-2772: Not returning labels or categories from graph in API\n",
            "- GPLA-2762: Failed to extract spreadsheet images\n",
            "- GPLA-2687: Email to/from not getting added as observations on emails\n",
            "- GPLA-2738: API is returning 'audio' metadata from podcast HTML document\n",
            "\n",
            "PreviousJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "NextMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "Last updated6 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [ce68f926-3ab6-4a73-8f33-3f5e552c2714]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/ce68f926-3ab6-4a73-8f33-3f5e552c2714/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "‚òÄÔ∏è\tJuly 2024\n",
            "\n",
            "# July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "### New Features\n",
            "\n",
            "- Added indexing workflow stage. This provides for configuration of indexing services, which may infer metadata from the content.\n",
            "- Added AZURE_AI_LANGUAGE content indexing service, which supports inferring the language of extracted text or transcript.\n",
            "- Added support for language content metadata. This returns a list of languages in ISO 639-1 format, which may have been inferred from the extracted text or transcript.\n",
            "- Added support for MODEL_IMAGE extraction service. This provides integration with vision models beyond those provided by OpenAI. You can assign a custom specification and bring-your-own API key for image extraction models.\n",
            "- ‚ö° We have deprecated the OPENAI_IMAGE service type, and developers should now use the LLM image service instead.\n",
            "- ‚ö° We have removed the language field from AudioMetadata type, which has been replaced by the new LanguageMetadata type.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2987: Extracting text with Azure Doc Intelligence does not extract hyperlinks\n",
            "\n",
            "PreviousAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "NextJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [23ae5fe0-5e6d-411c-b219-5ebaf593062e]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/23ae5fe0-5e6d-411c-b219-5ebaf593062e/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "‚òÄÔ∏è\tJuly 2024\n",
            "\n",
            "# July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the Mistral Large 2 and Mistral Nemo models. The existing MISTRAL_LARGE model enum now will use Mistral Large 2.\n",
            "- üí° Graphlit now supports the Llama 3.1 8b, 70b and 405b models on Groq. (Note, these are rate-limited according to Groq's platform constraints.)\n",
            "- Added support for revision strategy on data extraction specifications. Now you can prompt the LLM to revise its previous data extraction response, similar to the existing completion revision strategy.\n",
            "- Added version property for AzureDocumentPreparationProperties type for assigning the API version used by Azure AI Document Intelligence. By default, Graphlit will continue to use the v4.0 (Preview) API version, but you can override this to assign version to V2023_10_31 to use the v3.1 (GA) API version instead. For some documents, the General Availability (GA) version of the API can provide better results.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2988: Not extracting hyperlinks from Office documents.\n",
            "\n",
            "PreviousJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "NextJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [51e379d2-307c-42f3-b5cf-eb3b02c6f068]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/51e379d2-307c-42f3-b5cf-eb3b02c6f068/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéá\tJuly 2023\n",
            "\n",
            "# July 15: Support for SharePoint feeds, new Conversation features\n",
            "### New Features\n",
            "\n",
            "- üí° Added support for SharePoint feeds: now can create feed to ingest files from SharePoint document library (and optionally, folder within document library)\n",
            "- üí° Added support for PII detection during entity extraction from text documents and audio transcripts: now we will create labels such as PII: Social Security Number automatically when PII is detected\n",
            "- üí° Added support for developer's own OpenAI API keys and Azure OpenAI deployments in Specifications\n",
            "- ‚ÑπÔ∏è Changed semantics of deleteFeed to delete the contents ingested by the feed; since contents are linked to feeds, now feeds can be disabled, while keeping the lineage to the feed, and if feeds are deleted, they will delete the linked contents, so we never lose the feed-to-content lineage\n",
            "- Added GraphQL query for SharePoint consent URI, for registered Graphlit Platform Azure AD application\n",
            "- Better handling of web sitemap indexes: now if a sitemap.xml contains a sitemapindex element, we will load all linked sitemaps for evaluating web pages to ingest from Web feed\n",
            "- Added new GraphQL mutations for openConversation, closeConversation and undoConversation\n",
            "- Added timestamps to Conversation messages\n",
            "- Added new GraphQL mutations for openCollection and closeCollection\n",
            "- Added more configuration for content search: now can specify searchType (KEYWORD, VECTOR, HYBRID) and queryType (SIMPLE, FULL - aka Lucene syntax)\n",
            "- Better parsing of iTunes podcast metadata\n",
            "- ‚ö° Renamed listingLimit field on feeds to readLimit\n",
            "- ‚ö° Renamed topK to numberSimilar for content vector search type\n",
            "- ‚ö° Changed GraphQL feed properties: split out azure into azureBlob and azureFile properties\n",
            "- ‚ö° Changed GraphQL specification properties: split out openAI into openAI and azureOpenAI properties\n",
            "- ‚ö° Removed count fields on query results, and replaced with explicit count{Entity} queries, which support search and filtering.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1043: Reddit readLimit not taking effect: now the specified limit of Reddit posts will be leveraged for Reddit feeds\n",
            "- GPLA-1064: Performance on entity extraction and observation creation for large PDFs was under expectations: now able to build knowledge graph from large PDFs much faster (4x speed improvement)\n",
            "- GPLA-1053: If rendition generation errored during content workflow, the content was not properly marked as errored\n",
            "- GPLA-1102: Large Web sitemaps were slow to load; rewrote sitemap index handling, and now can process sitemaps with 150K+ entries in seconds.\n",
            "\n",
            "PreviousAugust 3: New data model for Observations, new Category entity\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [636bd286-d807-4838-af4d-0c37d24772b7]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/636bd286-d807-4838-af4d-0c37d24772b7/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "‚òÄÔ∏è\tJuly 2024\n",
            "\n",
            "# July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the OpenAI GPT-4o Mini model, with 16k output tokens.\n",
            "- üí° Graphlit now supports 'bring-your-own-key' for Azure AI Document Intelligence models. We have added a custom endpoint and key property, which can be assigned to use your own Azure AI resource.\n",
            "- Updated to use Jina reranker v2 (jina-reranker-v2-base-multilingual) by default.\n",
            "- Updated to assign the summary, bullets, etc properties when calling summarizeContents mutation. Now when summarizing contents, we will store the resulting summary in the content itself, in addition to returning the summarized results.\n",
            "- Added relevance property to all entity types, which will be assigned when searching for these entities. Entity results will be sorted (descending) by this search relevance score.\n",
            "- Added the ability to manually update summary, bullets, etc. properties when calling the updateContent mutation.\n",
            "- Added offset property to AtlassianJiraFeedProperties, so the timezone offset can be properly assigned for paging of the Jira feed. (Defaults to zero offset, i.e. UTC.) Jira does not store dates in UTC format, and the timestamps are based on the server timezone of the hosted Jira instance. By assigning the timezone offset with the Jira feed, we can reliably page the updated date timestamps from the Jira API.\n",
            "- ‚ö° We have changed the content similarity search behavior to find similar content by summary, rather than text of the document, when a summary has been previously generated. For long documents, this will provide a more accurate similarity, rather than comparing on the first few pages of text in a document.\n",
            "- ‚ö° We have changed the behavior of assigning offset in the entity filter objects for paging through entities. If using vector or hybrid search, this offset will be ignored (i.e. zero offset). Paging will not be supported through vector or hybrid search results. For keyword search, the offset will continue to be used, along with the limit property, to provide paging through the search results. We have made this change because we have found that index-based paging is not reliable with our vector/hybrid search approach. We are investigating ways to support this reliably with vector/hybrid search in the future.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2915: Add retry on OpenAI API HTTP 524 error (gateway timeout).\n",
            "- GPLA-2908: Not paging through Jira feed correctly.\n",
            "- GPLA-2917: Search by similar content is not giving expected results on long documents.\n",
            "- GPLA-2244: Keyword search not finding text in latter part of long PDF.\n",
            "\n",
            "PreviousJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "NextJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [5c9b0f96-4efc-4319-b2c2-0fc27875f030]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/5c9b0f96-4efc-4319-b2c2-0fc27875f030/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÜ\tJanuary 2024\n",
            "\n",
            "# January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports Google and Microsoft email feeds. Email feeds can be created to ingest past emails, or poll for new emails. Emails create an EMAIL content type. Attachment files can optionally be extracted from emails, and will be linked to their parent email content. If assigning a workflow to the feed, the workflow will be applied both to the email content and the extracted attachment files.\n",
            "- üí° Graphlit now supports reingesting content in-place. The ingestText, ingestPage and ingestFile mutations now take an optional id parameter for an existing content object. If this id is provided, the existing content will be updated from the provided text or URI source, and will restart the assigned workflow.\n",
            "- Added restartAllContents mutation to restart workflow on all partially-ingested contents in project.\n",
            "- Added text field to ConversationCitation type, which returns the relevant text from the content source with the citation.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1313: Not extracting links from HTML\n",
            "- GPLA-2030: No text extracted from shapes in PPTX files\n",
            "\n",
            "PreviousFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "NextJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [3f2f43b0-49a5-462a-88c6-e0ecd92f41c1]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/3f2f43b0-49a5-462a-88c6-e0ecd92f41c1/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÜ\tJanuary 2024\n",
            "\n",
            "# January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports content publishing, where documents, audio transcripts and even image descriptions, can be summarized, and repurposed into blog posts, emails or AI-generated podcasts. With the new publishContents mutation, you can configure LLM prompts for summarization and publishing, and assign specifications to use different models and/or system prompts for each step in the process. The published content will be reingested into Graphlit, and can be searched or used for conversations, like any other form of content.\n",
            "- üí° Graphlit now supports publishing conversations as content with the new publishConversation mutation. You can generate text or audio transcripts of your conversations, to be reused in other tools.\n",
            "- üí° Graphlit now supports bulk summarization of contents with the summarizeContents mutation. You can filter a set of content, by feed, by observable or by similar text, and run a set of summarizations across each content in parallel.\n",
            "- üí° Graphlit now supports LLM entity extraction, with the new MODEL_TEXT entity extraction service type. Similar to using Azure Cognitive Service Text Analytics, you can use any OpenAI or Anthropic model for extracting entities from text. Internally the LLM returns JSON-LD entities, which we convert into Person, Organization, Place, etc. entities and assign observations to the extracted content.\n",
            "- üí° Graphlit now supports LLM tools (aka function calls) with OpenAI models. You can define the tools to be used with the LLM in the specification object. With the new extractContents mutation, you can execute a prompt against content using a specification with tools defined. The mutation will return the JSON arguments assigned by the LLM.\n",
            "- üí° Graphlit now supports callback webhooks for LLM tools. If you assign a URI in the ToolDefinition object, Graphlit will call your webhook the tool name and JSON arguments. When you respond to the webhook with JSON, we will add that response to the LLM messages, and ask the LLM to complete the original prompt.\n",
            "- üí° Graphlit now supports the selection of the Deepgram model (such as Meeting, Phonecall or Finance) with the preparation workflow. Also, you can assign your own Deepgram API key, which will be used for audio transcription using that workflow.\n",
            "- Added support for CLIP image embeddings using Roboflow, which can be used for similar image search. If you search for contents by similar contents, we will now use the content's text and/or image embeddings to find similar content.\n",
            "- Added support for dynamic web page ingestion. Graphlit now navigates to and automatically scrolls web pages using Browserless.io, so we capture the fully rendered HTML before extracting text. Also, we now support web page screenshots, if enabled with enableImageAnalysis property in preparation workflow. These screenshots can be analyzed with multimodal modals, such as GPT-4 Vision, or can be used to create image embeddings for similar image search.\n",
            "- Added table parsing when preparing documents. We now store structured (tab-delimited) text in the JSON text mezzanine which is extracted from documents in the preparation workflow.\n",
            "- Added reverse geocoding of lat/long locations found in image or other content metadata. We now store the real-world address with the content metadata, for use in conversations.\n",
            "- Added assistant messages to the conversation message history provided to the LLM. Originally we had included only user messages, but now we are formatting both user and assistant messages into the LLM prompt for conversations.\n",
            "- Added new chunking algorithm for text embeddings. We support semantic chunking at the page or transcript segment level, and now will create embeddings from smaller sized text chunks per page or segment.\n",
            "- Added content metadata to text and image embeddings. To provide better context for the text embeddings, we now include formatted content metadata, which includes fields like title, subject, author, or description. For emails, we include to, from, cc, and bcc fields.\n",
            "- Added helper mutations isContentDone and isFeedDone which can be used for polling completion of ingested content, or all content ingested by a feed.\n",
            "- Added richer image descriptions generated by the GPT-4 Vision model. Now these provide more useful detail.\n",
            "- Added validation of extracted hyperlinks. Now we test the URIs and remove any inaccessible links during content enrichment.\n",
            "- Added deleteContents, deleteFeeds, and deleteConversations mutations for multi-deletion of contents, feeds or conversations.\n",
            "- Added deleteAllContents, deleteAllFeeds, and deleteAllConversations mutations for bulk, filtered deletion of entities. You can delete all your contents, feeds, or conversations in your project, or a filtered subset of those entities.\n",
            "- ‚ÑπÔ∏è Starter tier now has a higher content limit of 100K content items.\n",
            "- ‚ö° In the OpenAIImageExtractionProperties type, the detailMode field was renamed to detailLevel.\n",
            "- ‚ö° Each SummarizationStrategy object now accepts the specification which is used by the summarization, rather than being assigned at the preparation workflow stage.\n",
            "- ‚ö° addCollectionContents and removeCollectionContents mutations have been deprecated in favor of addContentsToCollections and removeContentsFromCollection mutations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1846: Parse Markdown headings into mezzanine JSON\n",
            "- GPLA-1779: Not returning SAS token with mezzanine, master URIs\n",
            "- GPLA-1348: Summarize text content, not just file content\n",
            "- GPLA-1297: Not assigning content error message on preparation workflow failure\n",
            "\n",
            "PreviousJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "NextDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "\n",
            "---\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [a41c7ac9-806a-4fd2-9652-759775a70d8c]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/a41c7ac9-806a-4fd2-9652-759775a70d8c/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üåßÔ∏è\tFebruary 2024\n",
            "\n",
            "# February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports OneDrive and Google Drive feeds. Files can be ingested from OneDrive or Google Drive, including shared drives where the authenticated user has access. Both OneDrive and Google Drive support the reading of existing files, and tracking new files added to storage with recurrent feeds.\n",
            "- üí° Graphlit now supports email backup files, such as EML or MSG, which will be assigned the EMAIL file type. During email file preparation, we will automatically extract and ingest any file attachments.\n",
            "- üí° Graphlit now automatically extracts embedded images in PDF files, ingests them as content objects, and links them as children of the parent PDF.\n",
            "- üí° Graphlit now supports recursive Notion feeds. When the isRecursive flag is true in the Notion feed properties, we will crawl child pages and databases, and recursively ingest them in addition to the specified pages and databases.\n",
            "- Added support for assigning collections to content ingested with the ingestPage, ingestFile or ingestText mutations. This saves a step where the content will automatically be added to the collection(s) without requiring another mutation call.\n",
            "- Added support for the CODE file type for a wide variety of source code formats, i.e. Python .py, Javascript .js. Code files use optimized text splitting for enhanced search and retrieval.\n",
            "- Added support for customGuidance in Specification object, which can be used for injecting a guidance prompt during the RAG process. For example, you can instruct the LLM to return a default response string if no content sources are found via semantic search.\n",
            "- Added tenants field to Project object, which returns a list of all tenant IDs which have been used to create an entity in Graphlit.\n",
            "- Added email metadata, separate from document metadata. Now emails will contain indexed metadata such as to, from, or subject.\n",
            "- ‚ö° The contents field for content objects has been replaced with children and parent fields. For example, when a ZIP file is unpacked, the unpacked files will be added as children of the ZIP file, and the ZIP file will be the parent of each of the unpacked files.\n",
            "- ‚ö° Removed enableImageAnalysis field from image preparation properties in workflow object. Now is enabled by default.\n",
            "- ‚ö° Moved disableSmartCapture field to preparation workflow stage from page preparation properties. This is used to disable the use of headless Chrome browser to capture HTML from web pages. It is enabled by default, and if disabled, Graphlit will simply download the HTML from the web page rather than rendering on headless Chrome browser.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2099: Failed to ingest ArXiV PDF. Fixed PDF parsing error.\n",
            "- GPLA-2174: LLM response is incorrect with conversation history, but no content sources.\n",
            "- GPLA-2199: ZIP package left in Indexed state after content workflow.\n",
            "\n",
            "PreviousMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "NextFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "Last updated9 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÑ\tDecember 2024\n",
            "\n",
            "# December 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports formatting of LLM-ready prompts with our RAG pipeline, via the new formatConversation and completeConversation mutations. This is valuable for supporting LLM streaming by directly calling the LLM from your application, and using Graphlit for RAG retrieval and conversation history. (Colab Notebook Example)\n",
            "- We have added support for inline hyperlinks in extracted text from documents and web pages.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3466: Owner ID should accept any non-whitespace string\n",
            "- GPLA-3458: Not getting Person-to-Organization edges from entity extraction\n",
            "\n",
            "PreviousDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "NextNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "Last updated26 days ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [bae5b595-6a1e-4dcb-95aa-3eaf6b55cc52]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/bae5b595-6a1e-4dcb-95aa-3eaf6b55cc52/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üåßÔ∏è\tFebruary 2024\n",
            "\n",
            "# February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports Semantic Alerts, which allows for LLM summarization and publishing of content, on a periodic basis. This is useful for generating daily reports from email, Slack or other time-based feeds. Alerts support the same publishing options, i.e. audio and text, as the publishContents mutation.\n",
            "- üí° Graphlit now supports the latest OpenAI 0125 model versions, for GPT-4 and GPT-3.5 Turbo. We will add support for Azure OpenAI when Microsoft releases support for these.\n",
            "- Slack feeds now support a listing type field, where you can specify if you want PAST or NEW Slack messages in the feed.\n",
            "- üî• This release provides many performance enhancements, which will speed up the content workflows for ingested content.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2114: Collections not being added to text embedding index documents.\n",
            "- GPLA-2063: Not handling hallucinated citations.\n",
            "- GPLA-1916: Collections not inherited from project-scope into tenant-scope.\n",
            "- GPLA-2105: Should error on add/remove of contents to/from collections if content does not exist.\n",
            "\n",
            "PreviousFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "NextJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "Last updated10 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [da1c0b2c-4bbf-4467-af16-f80122aeedd3]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/da1c0b2c-4bbf-4467-af16-f80122aeedd3/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÑ\tDecember 2024\n",
            "\n",
            "# December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports mapping a website with the mapWebmutation. You can provide a URL to a website, and the query will return a list of URLs based on the sitemap.xml (or sitemap-index.xml) file, at or underneath the provided URL.\n",
            "- üí° Graphlit now supports the generation of web page screenshots with the screenshotPagemutation. By providing the URL of a web page, and optionally, the maximum desired height of the screenshot, we will screenshot the webpage and ingest it automatically as content. You can provide an optional workflow, which will be applied to the ingested image content, for operations like generating image descriptions with a vision LLM.\n",
            "- üí° Graphlit now supports the direct summarization of text with the summarizeTextmutation. By providing the desired summarization strategy, we will summarize the text (i.e. bullet points, social media posts) and return the summarization.\n",
            "- üí° Graphlit now supports the direct extraction of text with the extractTextmutation. By providing the LLM tool definitions and an optional LLM specification, we will prompt the desired LLM (or OpenAI GPT-4o, by default) to invoke the provided tools, and return the JSON responses from the LLM tool calling.\n",
            "- Graphlit now supports the latest Groq Llama 3.3 model, with the model enum LLAMA_3_3_70B.\n",
            "- We have updated Cohere reranking to use the latest Cohere rerank-v3.5model by default.\n",
            "- ‚ö° We have added a new flattenCitationsfield to the ConversationStrategyInputtype. By assigning this field to True, when calling promptConversation,we will combine multiple citations from the same content into a single citation.\n",
            "- ‚ö° For Microsoft email, Microsoft Teams and OneDrive feeds, we have added the clientIdand clientSecretfields as required feed properties. These properties must be assigned, in addition to the refreshTokenfield for proper authentication to the Microsoft Graph API used by these feeds. (Colab Notebook Example)\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3492: Not finding sitemap at parent web path\n",
            "- GPLA-3500: Failed to handle mismatch of Deepgram model/language\n",
            "\n",
            "PreviousDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "NextDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "Last updated18 days ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/7fff5e48-bdc8-4dbc-ad26-da250228e7ee/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÑ\tDecember 2024\n",
            "\n",
            "# December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports Dropbox feeds for ingesting files on the Dropbox cloud service. Dropbox feeds require your appKey, appSecret, redirectUriand refreshTokento be assigned. The feed also accepts an optional pathparameter to read files from a specific Dropbox folder.\n",
            "- üí° Graphlit now supports Box feeds for ingesting files on the Box cloud service. Box feeds require your clientId, clientSecret, redirectUriand refreshTokento be assigned.\n",
            "- üí° Graphlit now supports Intercom feeds for ingesting Intercom Articles and Tickets. We will ingest Intercom Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Intercom feeds require the accessTokenproperty to be assigned.\n",
            "- üí° Graphlit now supports Zendesk feeds for ingesting Zendesk Articles and Tickets. We will ingest Zendesk Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Zendesk feeds require the accessTokenproperty and your Zendesk subdomain to be assigned.\n",
            "- Graphlit now supports the latest OpenAI o1 model, with the model enums O1_200kand O1_200k_20241217.\n",
            "- Graphlit now supports the latest Gemini Flash 2.0 Experimental model, with the model enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "- Graphlit now supports the latest Cohere R7B model, with the model enum COMMAND_R7B_202412.\n",
            "- Graphlit now supports returning the low-level details from prompting RAG conversations, by adding the includeDetailsparameter and setting to True. This includes details on the number of sources, the exact list of messages provided to the LLM, and more.\n",
            "- We have added support for filtering of observables, such as Person or Organization, by URI property.\n",
            "- We have added the ability to bypass semantic search in content retrieval with conversations. You can assign NONEfor the conversation search type, and it will ignore the user prompt when retrieving content. It will inject all contents resulting from the content filter into the RAG prompt context.\n",
            "- We have added a new createdInLastproperty to all entity filters, which allows easier filtering of entities created within a recent time period. Also, we have added a new inLastproperty to the content filter, which allows easier filtering of content authored within a recent time period. For example, find all images taken in the last 3 days, or find me all emails I received yesterday.\n",
            "- We have added support for the latest Azure AI Document Intelligence models, with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "- We have added support for Google Drive and OneDrive feeds to ingest specific files by providing a list of file identifiers (files), in addition to the folder identifier (folderId). If files identifiers are provided, they take precedence over the folder identifier.\n",
            "- ‚ö° For projects upgraded to the Starter Tier after Dec 9, 2024, we have removed the content items limit. Now you can store an unlimited number of content items (i.e. files, web pages, Slack messages) on the Starter or Growth Tiers. If you have an existing project on the Starter Tier, please reach out and we will manually remove that content item limit on the project.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3529: Can't assign collection to multitenant content\n",
            "- GPLA-3579: Should decode HTML characters when parsing HTML email\n",
            "- GPLA-3576: Ingesting content in-place doesn't handle isSynchronous properly\n",
            "- GPLA-3457: IsFeedDone doesn't return True for finished feed with no contents\n",
            "- GPLA-3572: Not handling HTTP 400 error on uploading from URI\n",
            "\n",
            "PreviousDecember 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes\n",
            "NextDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "Last updated4 days ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [0ec87d20-b463-4f65-a2f4-e32bc44f1b0d]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/0ec87d20-b463-4f65-a2f4-e32bc44f1b0d/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÑ\tDecember 2023\n",
            "\n",
            "# December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the OpenAI GPT-4 Turbo 128k model, both in Azure OpenAI and native OpenAI services. Added new model enum GPT4_TURBO_VISION_128K.\n",
            "- üí° Graphlit now supports Llama 2 7b, 13b, 70b models and Mistral 7b model, via Replicate. Developers can use their own Replicate API key, or be charged as credits for Graphlit usage.\n",
            "- üí° Graphlit now supports the Anthropic Claude 2.1 model. Added new model enum CLAUDE_2_1.\n",
            "- üí° Graphlit now supports the OpenAI GPT-4 Vision model for image descriptions and text extraction. Added new model enum GPT4_TURBO_VISION_128K. See usage example in \"Multimodal RAG\" blog post.\n",
            "- Added query by example to contents query. Developers can specify one or more example contents, and query will use vector embeddings to return similar contents.\n",
            "- Added query by example to conversations query. Developers can specify one or more example conversations, and query will use vector embeddings to return similar conversations.\n",
            "- Added vector search support for conversations queries. Developers can provide search text which will use vector embeddings to return similar conversations.\n",
            "- Added promptSpecifications mutation for directly prompting multiple models. This can be used to evaluate prompts against multiple models or compare different specification parameters in parallel.\n",
            "- Added promptStrategy field to Specification, which supports multiple strategy types for preprocessing the prompt before being sent to the LLM model. For example, REWRITE prompt strategy will ask LLM to rewrite the incoming user prompt based on the previous conversation messages.\n",
            "- Added suggestConversation mutation, which returns a list of suggested followup questions based on the specified conversation and related contents. This can be used to auto-suggest questions for chatbot users.\n",
            "- Added new summarization types: CHAPTERS, QUESTIONS and POSTS. See usage examples in the \"LLMs for Podcasters\" blog post.\n",
            "- Added versioned model enums such as GPT4_0613 and GPT35_TURBO_16K_1106. Without version specified, such as GPT35_TURBO_16K, Graphlit will use the latest production model version, as defined by the LLM vendor.\n",
            "- Added lookupContents query to get multiple contents by id in one query.\n",
            "- ‚ö° In Content type, headline field was renamed to headlines and now returns an array of strings.\n",
            "- ‚ö° Entity names are now limited to 1024 characters. Names will be truncated if they exceed the maximum length.\n",
            "- ‚ö° In SummarizationTypes enum, BULLET_POINTS was renamed to BULLETS.\n",
            "- ‚ö° In ProjectStorage type, originalTotalSize was renamed to totalSize, and totalRenditionSize field was added. totalSize is the sum of the ingested source file sizes, and totalRenditionSize is the sum of the source file sizes and any derived rendition sizes.\n",
            "- ‚ö° In ConversationStrategy type, strategyType was renamed to type for consistency with rest of data model.\n",
            "- ‚ö° In Specification type, optimizeSearchConversation was removed, and now is handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1725: Should ignore RSS.xml from web feed sitemap\n",
            "- GPLA-1726: GPT-3.5 Turbo 16k LLM is adding \"Citation #\" to response\n",
            "- GPLA-1698: Workflow not applied to link-crawled content\n",
            "- GPLA-1692: Mismatched project storage total size, when some content has errored\n",
            "- GPLA-1237: Add relevance threshold for semantic search\n",
            "\n",
            "PreviousJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "NextOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "Last updated11 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/a9adfd3e-0f66-40b0-9039-c00cd8fbf761/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÇ\tAugust 2024\n",
            "\n",
            "# August 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports LLM-based document preparation, using vision-capable models such as OpenAI GPT-4o and Anthropic Sonnet 3.5. This is available via the MODEL_DOCUMENT preparation service type, and you can assign a customspecification object and bring your own LLM keys.\n",
            "- üí° Graphlit now provides an open source .NET SDK, supporting .NET 6 and .NET 8 (and above). SDK package can be found on Nuget.org. Code samples can be found on GitHub.\n",
            "- Added identifier property to Content object for mapping content to external database identifiers. This is supported for content filtering as well.\n",
            "- Added support for Claude 3 vision models for image-based entity extraction, using the MODEL_IMAGE entity extraction service.\n",
            "- Added context augmentation to conversations, via the augmentedFilter property on the Conversation object. Any content which matches this augmented filter will be injected into the LLM prompt content, without needing to be related by vector similarity to the user prompt. This is useful for specifying domain knowledge which should always be referenced by the RAG pipeline.\n",
            "- Added support for the latest snapshot of OpenAI GPT-4o, with the model enum GPT4O_128K_20240806.\n",
            "- Added reranking of related entities, when preparing the LLM prompt context for GraphRAG. If reranking is enabled, the metadata from the related entities will be reranked with the same reranker assigned to the conversation specification.\n",
            "- ‚ö° We have changed the type of the duration field in the AudioMetadata and VideoMetadata types to be TimeSpan rather than string, as to be more consistent with the rest of the API data model.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2884: Support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "\n",
            "PreviousAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "NextJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "Last updated4 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [26deb281-fa20-4f6d-839b-31033675141b]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/26deb281-fa20-4f6d-839b-31033675141b/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÇ\tAugust 2024\n",
            "\n",
            "# August 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports the extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "- üí° Graphlit now supports medical-related entities in GraphRAG, and via API for queries and mutations.\n",
            "- Added support for Anthropic prompt caching. When using Anthropic Sonnet 3.5 or Haiku 3, Anthropic will now cache the entity extraction and LLM document preparation system prompts, which saves on token cost and increases performance.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3104: Should default search type to VECTOR, when performing entity similarity filter.\n",
            "- GPLA-3112: Empty PDF fails entity extraction.\n",
            "\n",
            "PreviousSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "NextAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "Last updated4 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [4f6da3ce-9feb-4ecb-9c05-dcefa5c6de6b]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/4f6da3ce-9feb-4ecb-9c05-dcefa5c6de6b/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÇ\tAugust 2023\n",
            "\n",
            "# August 3: New data model for Observations, new Category entity\n",
            "### New Features\n",
            "\n",
            "- üí° Revised data model for Observations, Occurrences and observables (i.e. Person, Organization). Now after entity extraction, content will have one Observation for each observed entity, and a list of occurrences. Occurrence now supports text, time and image occurrence types. (Text: page index, time: start/end timestamp, image: bounding box) Observations now have ObservableType and Observable fields, which specify the observed entity type and entity reference.\n",
            "- üí° Added Category entity to GraphQL data model, which supports PII categories such as Phone Number or Credit Card Number.\n",
            "- Added probability field to model properties, for the LLM's token probability. (See OpenAI documentation for more detail.)\n",
            "- Added error field to feeds. If a feed fails to read from the data source, and is marked as ERRORED state, the error field will have the error description.\n",
            "- Support reingestion of changed files from feeds. For feeds, such as SharePoint or Web, where we can recognize that a file or page was updated, we will now reingest the content in-place. Content will keep the same ID, and will restart the content workflow by re-downloading the updated content from the data source. Existing observations will be deleted, and new observations will be created from the updated content.\n",
            "- ‚ÑπÔ∏è Ingestion of content is now idempotent, meaning if you ingest content again from the same URI, we will reingest the content in-place, while keeping the same ID. (If we can recognize the content has not changed, such as by ETag, we will return the existing content object.)\n",
            "- ‚ÑπÔ∏è Changed GraphQL data type of SharePoint tenantId, libraryId and siteId to ID rather than String.\n",
            "- ‚ú® Performance optimization of entity extraction, and the creation of observations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1130: Only was extracting text from first column of PDF tables.\n",
            "- GPLA-1140: Text from DOCX tables was not extracted properly.\n",
            "- GPLA-1154: Audio content ingested from RSS feed was not deleted when feed was deleted.\n",
            "\n",
            "PreviousAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "NextJuly 15: Support for SharePoint feeds, new Conversation features\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [0cbb6490-3cff-4aa5-8d05-b94ab7c3d531]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/0cbb6490-3cff-4aa5-8d05-b94ab7c3d531/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÇ\tAugust 2024\n",
            "\n",
            "# August 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "### New Features\n",
            "\n",
            "- Added support for language-aware summaries when using LLM-based document extraction. Now the summaries for tables and sections generated by the LLM will follow the language of the source text.\n",
            "- Added support for language-aware entity descriptions with using LLM-based entity extraction. Now the entity descriptions generated by the LLM will follow the language of the source text.\n",
            "- ‚ö° We have changed the default document preparation method to use Azure AI Document Intelligence, rather than our built-in document parsers. We have found that the fidelity of Azure AI is considerably better for complex PDFs, and provides better support for table extraction, so we have made this the default. Note: this does come with increased credit usage per-page, for PDF, DOCX and PPTX documents, but the quality of the extracted documents are noticeably higher for use in RAG pipelines.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3070: Not getting slide count assigned to metadata for PPTX files.\n",
            "\n",
            "PreviousAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "NextAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "Last updated4 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [448971ac-c917-4d00-a846-52a5844472f9]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/448971ac-c917-4d00-a846-52a5844472f9/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÇ\tAugust 2023\n",
            "\n",
            "# August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "### New Features\n",
            "\n",
            "- üí° Added ingestText mutation which supports direct Content ingestion of plain text, Markdown and HTML. Now, if you have pre-scraped HTML or Markdown text, you can ingest it into Graphlit without reading from a URL.\n",
            "- üí° Added Specification strategy property, which allows customization of the LLM context when prompting a conversation. ConversationStrategy now provides Windowed and Summarized message histories, as well as configuration of the weight between existing conversation messages and Content text pages (or audio transcript segments) in the LLM context.\n",
            "- üí° Added auto-summarization of extracted text and audio transcripts. There is a new Content summary property where a list of summary bullet points can be found. These summaries can be optionally included in the Conversation prompt context for more accurate LLM responses.\n",
            "- ‚ÑπÔ∏è Added AzureOpenAIModels and OpenAIModels types to Specification model properties to make it easier to specify the desired LLM.\n",
            "- ‚ÑπÔ∏è Renamed ConversationMessage date property to timestamp\n",
            "- ‚ú® Refined the internal LLM prompts for providing content as part of Conversation context. This provides for much clearer and accurate results from the LLM.\n",
            "\n",
            "PreviousAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "NextAugust 3: New data model for Observations, new Category entity\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [a540acea-c8ed-4352-a5cc-91c3dbe4ba91]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/a540acea-c8ed-4352-a5cc-91c3dbe4ba91/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÇ\tAugust 2023\n",
            "\n",
            "# August 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "### New Features\n",
            "\n",
            "- ‚ÑπÔ∏è Behind the scenes, Graphlit is preparing to launch usage-based billing. This release put in place the infrastructure to track billable events. Organizations now have a Stripe customer associated with them, and Graphlit projects are auto-subscribed to a Free/Hobby pricing plan. In a future release, we will provide the ability to upgrade to a paid plan in the Graphlit Developer Portal. Also, we will provide visualization of usage, on granular basis, in the Portal.\n",
            "- üí° Content URIs now have Shared Access Signature (SAS) token appended, so they are accessible after query. For example, content.transcriptUri will now be able to be downloaded or used directly in an application (until the SAS token expires).\n",
            "- üß± Added more robustness for error handling and retries, especially for LLM APIs and audio transcription APIs.\n",
            "\n",
            "PreviousSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "NextAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [cd1c5052-f3bb-4747-b785-5e83ed6dfd80]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/cd1c5052-f3bb-4747-b785-5e83ed6dfd80/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üêá\tApril 2024\n",
            "\n",
            "# April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports a native Python SDK, using Pydantic types. The Python SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest PyPi package here. The Streamlit sample applications have been updated to use the new Python SDK.\n",
            "- üí° Graphlit now supports a native Node.js SDK, using TypeScript types. The Node.js SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest NPM package here.\n",
            "- üí° Graphlit now supports the 2024-04-09 models in the OpenAI model service. GPT4_TURBO-128K will give the latest OpenAI GPT-4 model, following this model list. We have added the GPT4_TURBO_128K_2024_04_09 enum to specify the new model.\n",
            "- üí° Graphlit now supports LLaMA3 70b, LLaMA3 8b and Gemma 7b models in the Groq model service.\n",
            "- üí° Graphlit now supports the Command R and Command-R+ models in the Cohere model service.\n",
            "- Added support for Jina reranking, using the JINA reranking model service type in the reranking retrieval strategy.\n",
            "- Updated the Cohere reranking model to use the latest v3.0 model.\n",
            "- Increased the reliability of parsing LLM responses, in cases where they don't follow the JSON schema.\n",
            "- ‚ö° Cleaned up nullability of GraphQL parameters, so parameters better reflect if they are required or optional, or allow nulls.\n",
            "- ‚ö° Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "- ‚ö° Split out reranking model service type as RetrievalModelServiceTypes enum.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2114: Adding content to collections not syncing search index\n",
            "- GPLA-2511: Failing to render any conversation sources with section retrieval and text content\n",
            "\n",
            "PreviousMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "NextApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [39b8a12e-de8a-41c9-9224-4b0e8e0909d8]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/39b8a12e-de8a-41c9-9224-4b0e8e0909d8/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üêá\tApril 2024\n",
            "\n",
            "# April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports Discord feeds. By connecting to a Discord channel and providing a bot token, you can ingest all Discord messages and file attachments.\n",
            "- üí° Graphlit now supports Cohere reranking after content retrieval in RAG pipeline. You can optionally use the Cohere rerank model to semantically rerank the semantic search results, before providing as context to the LLM.\n",
            "- Added support for section-aware text chunking and retrieval. Now, when using section-aware document preparation, such as Azure AI Document Intelligence, Graphlit will store the extracted text according to the semantic chunks (i.e. sections). The text for each section will be individually chunked and embedded into the vector index.\n",
            "- Added support for retrievalStrategy in Specification type. Graphlit now supports CHUNK, SECTION and CONTENT retrieval strategies. Chunk retrieval will use the search hit chunk, section retrieval will expand the search hit chunk to the containing section (or page, if not using section-aware preparation). Content retrieval will expand the search hit chunk to the text of the entire document.\n",
            "- Added support for rerankingStrategy in Specification type. You can now configure the reranking of content sources, using the Cohere reranking model, by assigning serviceType to COHERE. More reranking models are planned for the future.\n",
            "- Added isSynchronous flag to content ingestion mutations, such as ingestUri, so the mutation will wait for the content to complete the ingestion workflow (or error) before returning. This is useful for utilizing the API in a Jupyter notebook or Streamlit application, in a synchronous manner without polling.\n",
            "- Added includeAttachments flag to SlackFeedProperties. When enabled, Graphlit will automatically ingest any attachments within Slack messages.\n",
            "- ‚ö° Added ingestUri mutation to replace the now deprecated ingestPage and ingestFile mutations. We had seen confusion on when to use one vs the other, and now for any URI, whether it is a web page or hosted PDF, you can pass it to ingestUri, and we will infer the correct content ingestion workflow.\n",
            "- ‚ö° Removed includeSummaries from the ConversationStrategyInput type. This will re-added in the future as part of the retrieval strategy.\n",
            "- ‚ö° Deprecated enableExpandedRetrieval in ConversationStrategyInput type. This is now handled by setting strategyType to SECTION or CONTENT in the RetrievalStrategyInput type.\n",
            "- ‚ö° Moved contentLimit from ConversationStrategyInput type to RetrievalStrategyInput type. You can optionally assign the contentLimit to retrievalStrategy which limits the number of content sources leveraged in the LLM prompt context. (Default is 100.)\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2469: Failed to ingest PDF hosted on GitHub\n",
            "- GPLA-2390: Claude 3 Haiku not adhering to JSON schema\n",
            "- GPLA-2474: Prompt rewriting should ignore formatting instructions in prompt\n",
            "- GPLA-2462: Missing line break after table rows\n",
            "- GPLA-2417: Not extracting images from PPTX correctly\n",
            "\n",
            "PreviousApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "NextMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [e46ab310-60bd-439d-9471-0cfebd1843cd]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit20241212dc396403.blob.core.windows.net/files/e46ab310-60bd-439d-9471-0cfebd1843cd/Mezzanine/page.json?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\n",
            "üéÑ\tDecember 2024\n",
            "\n",
            "# December 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes\n",
            "### New Features\n",
            "\n",
            "- üí° Graphlit now supports LLM fallbacks which can help protect your application from model provider downtime. By assigning the fallbacksproperty when creating your conversation, you can provide an optional list of LLM specifications to be used (in order). These fallback specifications will only be used when we failed to prompt the conversation via the main specification. Caveat, the RAG pipeline will only use the strategies provided in the main specification for prompt rewriting, content retrieval, etc. Content is not re-retrieved upon fallback - the formatted LLM prompt will be tried against each fallback specification in succession until one succeeds.\n",
            "- üí° Graphlit now supports querying of all available models, through the new modelsquery in the API. This returns the model enum, model service type enum, description, and several other useful details about the models.\n",
            "- Graphlit now supports the ingestion of native Google Docs, Google Sheets and Google Slides documents from Google Drive feeds. These formats will be auto-exported to the corresponding Microsoft Office format (DOCX, XLSX, PPTX) prior to ingesting as content.\n",
            "- Graphlit now supports unblocking of websites, such as those using Cloudflare. You can set enableUnblockedCaptureto true on the PreparationWorkflowStageto enable unblocking - through our integration with Browserless.io headless browser service. This does incur an additional cost per page, compared to normal web page ingestion.\n",
            "- We have added support for assigning observations to contents ingested via feeds. By assigning observationsto the IngestionWorkflowStagein workflow object, you can assign Labels, Organizations, etc. without needing to use entity extraction.\n",
            "- We have added support for assigning observations when ingesting content via ingestUri, ingestText, etc. mutations. By passing observationsas a parameter, similar to `collections`, you can assign Labels, Organizations, etc. without needing to use entity extraction.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3645: Table headers merged together on web scrape\n",
            "- GPLA-3634: Failed to extract pages from PDF with empty hyperlink text\n",
            "- GPLA-3633: Not handling empty observables properly for reranking\n",
            "\n",
            "NextDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "Last updated21 hours ago \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the ElevenLabs voice ID to use\n",
        "voice_id = \"ZF6FPAbjXT4488VcRRnw\" # ElevenLabs Amelia voice\n",
        "\n",
        "# Prompt which gets run on each web page to summarize key points\n",
        "summary_prompt = \"\"\"\n",
        "You are an AI assistant that extracts the most important information from product changelog pages.\n",
        "\n",
        "You are being provided a changelog web page for one of many releases of the Graphlit Platform in 2024.\n",
        "\n",
        "Your task is to produce a concise summary that covers:\n",
        "\n",
        "New Features ‚Äì Briefly list or describe each new capability.\n",
        "Enhancements/Improvements ‚Äì Any notable improvements or changes.\n",
        "Bug Fixes ‚Äì Summaries of what was fixed and why it matters.\n",
        "Other Key Details ‚Äì Any version numbers, feature flags, or breaking changes.\n",
        "Dates - When a feature was released\n",
        "Value - What this offers to developers.\n",
        "Keep it succinct, accurate, and organized. Use short sentences or bullet points so it‚Äôs easy to incorporate into a map/reduce pipeline. Omit any superfluous text.\n",
        "\n",
        "Output:\n",
        "A concise summary in bullet points highlighting the essential updates from the changelog.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt which gets run against all summaries (in map/reduce manner) to generate final script for ElevenLabs audio\n",
        "publish_prompt = \"\"\"\n",
        "You are an enthusiastic host focused on developer marketing, and you work for Graphlit who is creating a 2024 year-in-review of their API-based platform.\n",
        "\n",
        "Don't refer to yourself in the script. Just talk to the audience.\n",
        "\n",
        "Don't add in any podcast-like references like intro music, sound effects, etc.  This will be used with a text-to-speech API to generate an audio recording.\n",
        "\n",
        "Your audience is somewhat technical ‚Äî software engineers, product builders, and tech-savvy product managers ‚Äî so the script should be clear, concise, and sprinkled with a bit of technical depth.\n",
        "\n",
        "Using the provided changelog for the Graphlit Platform, create a podcast-like script that:\n",
        "\n",
        "- Sets the stage with a warm, engaging introduction.\n",
        "- Highlights each new feature, explaining how it helps developers or teams be more productive, efficient, or creative.\n",
        "- Refers to when a feature was released.\n",
        "- Mentions any model updates and why they matter for technical use cases.\n",
        "- Reviews notable bug fixes, providing just enough context to show the improvements without overwhelming detail.\n",
        "- Closes with a quick recap and a call to action, encouraging listeners to try out the new features or learn more.\n",
        "\n",
        "At the very end, mention that the listener can signup for free at graphlit.com and try out all these features.\n",
        "Also, mention that in 2025, Graphlit will be offering exciting new features to accelerate the building of AI agents.\n",
        "\n",
        "The tone should be friendly, positive, and confident‚Äîlike a technology evangelist who‚Äôs genuinely excited about these updates.\n",
        "\n",
        "Keep it interesting and conversational, but maintain enough depth to engage developers who care about how things work under the hood.\n",
        "Use analogies or practical examples to illustrate why certain features are useful.\n",
        "Feel free to add transitions such as ‚ÄúNow, let‚Äôs dive in,‚Äù or ‚ÄúMoving on to our next highlight‚Äù to keep it flowing.\n",
        "\n",
        "Output: A detailed, TTS-ready 10-minute long script that hits all the points above.\n",
        "\"\"\"\n",
        "\n",
        "if feed_id is not None:\n",
        "    summary_specification_id = await create_specification(enums.OpenAIModels.GPT4O_MINI_128K)\n",
        "\n",
        "    if summary_specification_id is not None:\n",
        "        print(f'Created summary specification [{summary_specification_id}]:')\n",
        "\n",
        "        publish_specification_id = await create_specification(enums.OpenAIModels.O1_200K)\n",
        "\n",
        "        if publish_specification_id is not None:\n",
        "            print(f'Created publish specification [{publish_specification_id}]:')\n",
        "\n",
        "            display(Markdown(f'### Publishing Contents...'))\n",
        "\n",
        "            published_content_id = await publish_contents(feed_id, summary_specification_id, publish_specification_id, summary_prompt, publish_prompt, publish_correlation_id, voice_id)\n",
        "\n",
        "            if published_content_id is not None:\n",
        "                print(f'Completed publishing content [{published_content_id}].')\n",
        "\n",
        "                # Need to reload content to get presigned URL to MP3\n",
        "                published_content = await get_content(published_content_id)\n",
        "\n",
        "                if published_content is not None:\n",
        "                    display(Markdown(f'### Published [{published_content.name}]({published_content.audio_uri})'))\n",
        "\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <audio controls>\n",
        "                    <source src=\"{published_content.audio_uri}\" type=\"audio/mp3\">\n",
        "                    Your browser does not support the audio element.\n",
        "                    </audio>\n",
        "                    \"\"\"))\n",
        "\n",
        "                    # After the audio is generated, we ingest the MP3 as a new content object in Graphlit, and it gets auto-transcribed\n",
        "                    display(Markdown('### Transcript'))\n",
        "                    display(Markdown(published_content.markdown))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ieBzAp6Z2Zew",
        "outputId": "b795acfc-b324-4c7a-a9ec-7d66f68db33b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created summary specification [7cf625a7-1add-4983-8c19-c15328c9aa1f]:\n",
            "Created publish specification [a58f46d3-59d6-4727-959f-06e57ca42611]:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Publishing Contents..."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed publishing content [3932f651-18ae-40d1-b548-51b6612ec4d8].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Published [Published Summary.mp3](https://graphlit20241212dc396403.blob.core.windows.net/files/3932f651-18ae-40d1-b548-51b6612ec4d8/Mezzanine/Published%20Summary.mp3?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                    <audio controls>\n",
              "                    <source src=\"https://graphlit20241212dc396403.blob.core.windows.net/files/3932f651-18ae-40d1-b548-51b6612ec4d8/Mezzanine/Published%20Summary.mp3?sv=2025-01-05&se=2024-12-29T09%3A26%3A13Z&sr=c&sp=rl&sig=k3QvrRHHMyWK6xXy5Vc2jkdg9RZ3SFIa%2BJ1qCFMQuUs%3D\" type=\"audio/mp3\">\n",
              "                    Your browser does not support the audio element.\n",
              "                    </audio>\n",
              "                    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Transcript"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[00:00:00] Hello, everyone,\n\n[00:00:01] and welcome to a special year in review for the Graphlet platform.\n\n[00:00:06] It's been an incredible stretch of milestones\n\n[00:00:10] and enhancements\n\n[00:00:11] all designed to make your development workflow simpler,\n\n[00:00:14] more powerful,\n\n[00:00:15] and more creative.\n\n[00:00:18] Let's explore how each month's updates can help you, whether you're coding in Python, dot net, JavaScript, or any other environment.\n\n[00:00:27] Let's start back in July 2023.\n\n[00:00:31] One key addition was SharePoint feed support, allowing direct file ingestion from SharePoint libraries.\n\n[00:00:37] This came with new conversation features, such as timestamped messages\n\n[00:00:42] to help keep your discussions organized and relevant.\n\n[00:00:45] These upgrades reduced friction when pulling in data from corporate SharePoint,\n\n[00:00:50] making you more efficient in building knowledge based applications.\n\n[00:00:54] Moving into August 2023,\n\n[00:00:56] Grafler introduced a new data model for observations,\n\n[00:01:00] enabling more structured ways to track entities like people or organizations.\n\n[00:01:06] Then came a direct text ingestion for plain text, markdown,\n\n[00:01:10] or HTML,\n\n[00:01:11] eliminating the need for a file or URL.\n\n[00:01:14] This is perfect when you just want to send raw data into Graphlet and get back structured content or embeddings for analysis.\n\n[00:01:23] Also, in August, usage based billing infrastructure was rolled out.\n\n[00:01:27] While still offering a free tier to get started right away, it set the stage for future expansions of credit based\n\n[00:01:34] usage, helpful for devs who prefer pay as you go models.\n\n[00:01:39] September 2023\n\n[00:01:40] ushered in a wave of productivity features.\n\n[00:01:43] Workflow configuration\n\n[00:01:44] let you define how your content passes through ingestion,\n\n[00:01:48] enrichment, or summarization\n\n[00:01:49] steps with support for Notion feeds to quickly import your team's notes and pages.\n\n[00:01:56] Paid subscription plans also launched, providing hobby, starter,\n\n[00:02:00] and growth tiers,\n\n[00:02:02] each with different quotas.\n\n[00:02:04] For those who love video content, YouTube feed support arrived,\n\n[00:02:09] complete with automatic transcription for ingesting the audio.\n\n[00:02:13] Now your dev team can treat video content just like any text document\n\n[00:02:18] and search or summarize it with ease.\n\n[00:02:21] October\n\n[00:02:22] 2023\n\n[00:02:23] focused on deeper language model integrations.\n\n[00:02:26] Anthropic Claude models joined the party for advanced text completions,\n\n[00:02:30] while Slack feed ingestion\n\n[00:02:33] made it easier for your dev teams to bring chat messages\n\n[00:02:36] and file attachments\n\n[00:02:37] into the knowledge base.\n\n[00:02:40] The platform also\n\n[00:02:41] expanded entity enrichment,\n\n[00:02:44] letting you layer in details from sources like Wikipedia\n\n[00:02:47] or Crunchbase.\n\n[00:02:49] By the end of the month,\n\n[00:02:51] conversation responses were further optimized, and you could add aliases to your observed entities,\n\n[00:02:56] especially handy when there are multiple ways to refer to the same person or place.\n\n[00:03:02] December\n\n[00:03:03] 2023\n\n[00:03:03] brought some major model updates.\n\n[00:03:06] The platform introduced OpenAI GPT 4 Turbo with a 128\n\n[00:03:11] k token window and also added llama 2, Mistral, and anthropic Claude 2.1.\n\n[00:03:19] All these models expand your options for generating text,\n\n[00:03:22] summarizing content, or performing semantic searches.\n\n[00:03:27] There was also support for query by example,\n\n[00:03:30] letting you simply submit a sample snippet of text or conversation to find related content.\n\n[00:03:37] Jumping into January 2024,\n\n[00:03:39] new content publishing features transformed how you repurpose\n\n[00:03:43] and summarize documents,\n\n[00:03:45] audio transcripts,\n\n[00:03:46] and images.\n\n[00:03:49] LLM tools, including function calls in OpenAI models,\n\n[00:03:53] make it simpler to integrate prompt driven tasks directly into your code.\n\n[00:03:59] Additionally, Google and Microsoft email feeds became 1st class citizens,\n\n[00:04:03] enabling ingestion of both historic and new emails,\n\n[00:04:07] attachments included.\n\n[00:04:10] That month also introduced reingestion capabilities\n\n[00:04:13] so you can update existing content in place with minimal fuss.\n\n[00:04:18] February\n\n[00:04:19] 2024\n\n[00:04:20] introduced semantic alerts\n\n[00:04:22] for automatically generating daily or periodic summaries,\n\n[00:04:26] perfect for receiving quick bulletins on everything new in your workspace.\n\n[00:04:31] OneDrive and Google Drive feed support was expanded,\n\n[00:04:35] including the ability to ingest images\n\n[00:04:38] from PDFs or to capture embedded image\n\n[00:04:41] images in your documents.\n\n[00:04:43] These features cut down on tedious overhead\n\n[00:04:46] so you can focus on building robust apps.\n\n[00:04:50] March 2 24 saw comprehensive usage and credits telemetry\n\n[00:04:55] so you can monitor precisely how many tokens or data credits your team is using.\n\n[00:05:01] For devs who prefer command line utilities,\n\n[00:05:04] a new CLI tool was released for efficient graphlet data\n\n[00:05:08] API interactions.\n\n[00:05:10] Further in March came support for additional issue tracking feeds,\n\n[00:05:14] linear, GitHub issues, and Jira,\n\n[00:05:17] turning your project tickets into fully searchable items.\n\n[00:05:21] And for file management, you could now ingest base 64 encoded files directly.\n\n[00:05:28] April 2024\n\n[00:05:30] was all about chat and ingestion flows.\n\n[00:05:33] Discord feed support arrived,\n\n[00:05:35] letting you bring chat messages\n\n[00:05:37] and attached media directly into Graphlet.\n\n[00:05:40] Cohere re ranking and improved text chunking\n\n[00:05:43] enhanced the retrieval augmented generation pipeline.\n\n[00:05:47] Most notably, native SDKs in Python and TypeScript\n\n[00:05:51] came out later in April.\n\n[00:05:53] Code generated from the GraphQL schema, so you no longer need to be a GraphQL expert to start building.\n\n[00:06:00] There were also brand new model integrations for OpenAI's\n\n[00:06:04] g GPT\n\n[00:06:05] 4 expansions\n\n[00:06:07] as well as support for advanced\n\n[00:06:09] and coherent models.\n\n[00:06:11] May 2024\n\n[00:06:13] introduced new re ranking services from Jina and Pongo,\n\n[00:06:17] plus the brand new graphRAG feature,\n\n[00:06:19] a system that harnesses extracted entities to supercharge your your retrieval augmented generation workflow.\n\n[00:06:27] Open AI GPT 4 o replaced Azure Open AI GPT 3.5,\n\n[00:06:33] 16 k as the default model,\n\n[00:06:35] promising a more robust experience.\n\n[00:06:38] For Microsoft Teams fans, feed support allowed you to ingest channel messages automatically\n\n[00:06:44] while performance optimizations\n\n[00:06:46] in entity extraction boosted speeds for large datasets.\n\n[00:06:51] In June 2024,\n\n[00:06:53] developers gained the ability to pass embedded JSON LD data from web pages,\n\n[00:06:59] enriching\n\n[00:07:00] your knowledge graph with schema.org\n\n[00:07:02] or other structured data.\n\n[00:07:05] Deep seek model support was introduced. So\n\n[00:07:08] if you prefer their AI completions and coders,\n\n[00:07:11] you have that option too.\n\n[00:07:14] Then in late June came the Claude 3.5 Sonnet model and semantic search for observable entities,\n\n[00:07:21] a boon for advanced knowledge graph queries when dealing with people,\n\n[00:07:26] organizations,\n\n[00:07:27] or places.\n\n[00:07:29] July 2024\n\n[00:07:31] was packed.\n\n[00:07:33] Webhook alerts went live,\n\n[00:07:35] enabling automatic notifications when new content is published,\n\n[00:07:39] and the deep seek 128 k token context was introduced\n\n[00:07:43] to handle giant transcripts or text blocks in a single pass.\n\n[00:07:48] Then soon after, gpt4\n\n[00:07:50] o Mini arrived,\n\n[00:07:52] offering a more lightweight\n\n[00:07:54] gpt4\n\n[00:07:55] variant for faster completions.\n\n[00:07:57] Updates to Mistral Large 2 and Nemo\n\n[00:08:00] plus new GrokLama\n\n[00:08:02] 3.1 models delivered even more alternatives to handle your AI workloads.\n\n[00:08:07] The next update introduced an indexing workflow stage and Azure AI language detection\n\n[00:08:14] so you can automatically detect and store the languages a piece of content is written in.\n\n[00:08:20] In August 2024,\n\n[00:08:22] the platform unveiled an open source dotnet SDK on Nougat,\n\n[00:08:26] giving\n\n[00:08:27] Csearchiles developers\n\n[00:08:28] first class\n\n[00:08:30] access to the Graphlet APIs.\n\n[00:08:33] Document preparation\n\n[00:08:34] switched to Azure AI document intelligence by default,\n\n[00:08:38] boosting accuracy for complex PDFs.\n\n[00:08:41] Medical entities were also added,\n\n[00:08:44] letting you extract detailed references\n\n[00:08:46] to medical conditions,\n\n[00:08:48] drugs, or procedures.\n\n[00:08:50] Meanwhile, anthropic prompt caching\n\n[00:08:53] reduced token costs,\n\n[00:08:55] speeding up repeated queries.\n\n[00:08:58] September 24 stepped up the AI model offerings further with FHIR enrichment for health care data and the newly updated Coherent models.\n\n[00:09:08] Google AI search feeds and Cerebras model support also landed,\n\n[00:09:12] plus more advanced grok llama versions.\n\n[00:09:15] By now, you could combine advanced conversation prompts with fallback retrieval of relevant content,\n\n[00:09:22] ensuring your AI app stays robust even if the main model hits a snag.\n\n[00:09:26] October 2, 2024 introduced a flurry of tool calling capabilities\n\n[00:09:31] across Anthropic, Google Gemini, Mistral, and more.\n\n[00:09:36] This let large language models handle tasks by calling external tools in real time, bridging the gap between pure text generation\n\n[00:09:45] and practical app logic.\n\n[00:09:47] GitHub repository feeds also debuted so you can auto ingest code files\n\n[00:09:52] while conversation interactions got new ways to simulate tool calls or to pass tool responses from JSON.\n\n[00:10:00] November\n\n[00:10:01] 2024\n\n[00:10:02] delivered web search features.\n\n[00:10:04] Imagine searching the open web,\n\n[00:10:06] retrieving\n\n[00:10:07] relevant pages,\n\n[00:10:09] and combining them with your content all in one shot.\n\n[00:10:14] Multi turn text summarization\n\n[00:10:16] and multi turn image analysis\n\n[00:10:18] made it possible to revise or refine a summary\n\n[00:10:22] over a series of LLM prompts.\n\n[00:10:24] Deepgram language detection took speech transcription to the next level,\n\n[00:10:29] and feed management was enhanced so you can control usage or upgrade easily when you run out of free tier credits.\n\n[00:10:36] Finally, December 2024\n\n[00:10:39] concluded with major expansions in the retrieval pipeline.\n\n[00:10:43] You can run a retrieval only rag process or map entire websites with the new site mapping features. Summaries and bullet points are more accurate than ever with refined chunking strategies.\n\n[00:10:54] And let's not forget the big arrival\n\n[00:10:56] of new model support for Grok Yammer\n\n[00:10:59] 3.3\n\n[00:11:00] or the ability to handle advanced content from Dropbox,\n\n[00:11:04] Box, Intercom,\n\n[00:11:05] Zendesk, and beyond.\n\n[00:11:08] Throughout these releases, loads of bugs were squashed\n\n[00:11:11] from synchronizing your search index in real time\n\n[00:11:14] to extracting images from PDFs\n\n[00:11:16] to properly handling text from every corner of your data sources.\n\n[00:11:21] Each fix was about giving you a more stable and confident development experience.\n\n[00:11:27] In short, the Graphlet platform has advanced leaps and bounds this past year.\n\n[00:11:33] From brand new SDKs\n\n[00:11:35] and improved content ingestion\n\n[00:11:37] to powerful LLM integrations\n\n[00:11:40] and next level conversation features, there's something here for every team building with AI.\n\n[00:11:46] The best way to learn more?\n\n[00:11:48] Try it. You can sign up for free right now at graphlet.com\n\n[00:11:53] and explore all these capabilities for yourself.\n\n[00:11:57] And here's one last bit of news.\n\n[00:12:00] 2025\n\n[00:12:01] is right around the corner,\n\n[00:12:03] and the team is preparing even more excitement,\n\n[00:12:07] especially for building, testing, and deploying AI agents.\n\n[00:12:13] Keep an eye out for those features because they're going to transform\n\n[00:12:17] how you build intelligent\n\n[00:12:20] automated systems.\n\n[00:12:22] Thanks for tuning in to this whirlwind tour of Graphlet's 24 updates.\n\n[00:12:27] Good luck with your next AI driven project,\n\n[00:12:30] and we can't wait to see the amazing creations you'll build with these new tools.\n\n[00:12:36] Enjoy exploring the platform,\n\n[00:12:38] and happy coding.\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML, JSON\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "time.sleep(10) # give it some time for billing events to catch up\n",
        "\n",
        "credits = await lookup_credits(ingestion_correlation_id)\n",
        "\n",
        "if credits is not None:\n",
        "    display(Markdown(f\"### Credits used: {credits.credits:.6f} for ingestion\"))\n",
        "    print(f\"- storage [{credits.storage_ratio:.2f}%], compute [{credits.compute_ratio:.2f}%]\")\n",
        "    print(f\"- embedding [{credits.embedding_ratio:.2f}%], completion [{credits.completion_ratio:.2f}%]\")\n",
        "    print(f\"- ingestion [{credits.ingestion_ratio:.2f}%], indexing [{credits.indexing_ratio:.2f}%], preparation [{credits.preparation_ratio:.2f}%], extraction [{credits.extraction_ratio:.2f}%], enrichment [{credits.enrichment_ratio:.2f}%], publishing [{credits.publishing_ratio:.2f}%]\")\n",
        "    print(f\"- search [{credits.search_ratio:.2f}%], conversation [{credits.conversation_ratio:.2f}%]\")\n",
        "    print()\n",
        "\n",
        "usage = await lookup_usage(ingestion_correlation_id)\n",
        "\n",
        "if usage is not None:\n",
        "    display(Markdown(f\"### Usage records:\"))\n",
        "\n",
        "    for record in usage:\n",
        "        dump_usage_record(record)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s7M_W9n7ntKA",
        "outputId": "3e97b7ad-da94-40a4-e99d-87e51ebf8106"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Credits used: 3.826806 for ingestion"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- storage [1.21%], compute [45.40%]\n",
            "- embedding [4.18%], completion [0.00%]\n",
            "- ingestion [0.00%], indexing [0.00%], preparation [49.21%], extraction [0.00%], enrichment [0.00%], publishing [0.00%]\n",
            "- search [0.00%], conversation [0.00%]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Usage records:"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-29T03:26:12.046Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.950695, used credits [0.01431456]\n",
            "- CONTENT [550d544c-3dc6-4fae-a4d1-89577544d9fc]\n",
            "\n",
            "2024-12-29T03:26:11.897Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.362794, used credits [0.00113600]\n",
            "- CONTENT [550d544c-3dc6-4fae-a4d1-89577544d9fc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [568 tokens], throughput: 1565.627 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:11.733Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.205283, used credits [0.00093600]\n",
            "- CONTENT [550d544c-3dc6-4fae-a4d1-89577544d9fc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [468 tokens], throughput: 2279.781 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:10.545Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.053080, used credits [0.00001700]\n",
            "- CONTENT [550d544c-3dc6-4fae-a4d1-89577544d9fc]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4400 bytes], throughput: 82893.277 bytes/sec\n",
            "\n",
            "2024-12-29T03:26:08.830Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.167664, used credits [0.00081931]\n",
            "- CONTENT [550d544c-3dc6-4fae-a4d1-89577544d9fc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2023/september-20-paid-subscription-plans-support-for-custom-observed-entities-and-azure-openai-gpt-4]\n",
            "- File upload [212038 bytes], throughput: 1264661.027 bytes/sec\n",
            "\n",
            "2024-12-29T03:26:08.220Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.078845, used credits [0.03800000]\n",
            "- CONTENT [550d544c-3dc6-4fae-a4d1-89577544d9fc]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:26:04.394Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:01.044170, used credits [0.00187994]\n",
            "\n",
            "2024-12-29T03:26:04.292Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:23.355985, used credits [0.04205051]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]\n",
            "\n",
            "2024-12-29T03:26:04.175Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.974318, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 194.498 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.944Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:23.208306, used credits [0.04178462]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]\n",
            "\n",
            "2024-12-29T03:26:03.831Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.464575, used credits [0.00125200]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [626 tokens], throughput: 1347.468 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.695Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:22.961576, used credits [0.04134040]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]\n",
            "\n",
            "2024-12-29T03:26:03.649Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:19.962310, used credits [0.03594047]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]\n",
            "\n",
            "2024-12-29T03:26:03.577Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.513523, used credits [0.00182200]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [911 tokens], throughput: 1774.021 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.547Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.339459, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1107.645 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.529Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.169414, used credits [0.00103400]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [517 tokens], throughput: 3051.698 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.497Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.446644, used credits [0.00166600]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [833 tokens], throughput: 1865.019 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.373Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.163094, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 2305.420 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.336Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:22.598972, used credits [0.04068757]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]\n",
            "\n",
            "2024-12-29T03:26:03.308Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:22.569513, used credits [0.04063453]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]\n",
            "\n",
            "2024-12-29T03:26:03.222Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.329285, used credits [0.00138600]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 2104.559 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.209Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:22.468130, used credits [0.04045200]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]\n",
            "\n",
            "2024-12-29T03:26:03.192Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.170434, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 2552.308 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.190Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.304020, used credits [0.00111400]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 1832.119 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.169Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.148941, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 2920.625 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.122Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.459222, used credits [0.00103400]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [517 tokens], throughput: 1125.817 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:03.078Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.504128, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 390.774 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.979Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.210128, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1789.389 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.967Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.194252, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1935.625 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.906Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.236555, used credits [0.00125200]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [626 tokens], throughput: 2646.317 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.858Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.285125, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 690.925 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.804Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.329367, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 1320.715 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.791Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.299291, used credits [0.00182200]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [911 tokens], throughput: 3043.858 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.719Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.239346, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 1817.456 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.693Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.216830, used credits [0.00166600]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [833 tokens], throughput: 3841.728 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.643Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.205064, used credits [0.00111400]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 2716.219 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.635Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.188942, used credits [0.00138600]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 3667.783 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.559Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.520406, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 722.513 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.491Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.304216, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1262.259 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.457Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:14.472314, used credits [0.02605619]\n",
            "- CONTENT [9e6ef8e1-5448-4933-9705-d9d9502b384c]\n",
            "\n",
            "2024-12-29T03:26:02.338Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.159804, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 1232.759 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.325Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.161926, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 1216.603 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.319Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.353142, used credits [0.00125200]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [626 tokens], throughput: 1772.656 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.290Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.267808, used credits [0.00068800]\n",
            "- CONTENT [9e6ef8e1-5448-4933-9705-d9d9502b384c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [344 tokens], throughput: 1284.504 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.288Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.246252, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1526.889 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.260Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.345682, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 1258.381 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.251Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.247260, used credits [0.00166600]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [833 tokens], throughput: 3368.919 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.228Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:11.480700, used credits [0.02067004]\n",
            "- CONTENT [e1cd970b-fde4-41b4-892a-1a64cb531334]\n",
            "\n",
            "2024-12-29T03:26:02.204Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.185915, used credits [0.00182200]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [911 tokens], throughput: 4900.094 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.171Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.153504, used credits [0.00068800]\n",
            "- CONTENT [9e6ef8e1-5448-4933-9705-d9d9502b384c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [344 tokens], throughput: 2240.991 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.131Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.219606, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 1980.822 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.106Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.181252, used credits [0.00078400]\n",
            "- CONTENT [e1cd970b-fde4-41b4-892a-1a64cb531334]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 2162.740 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.086Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.125788, used credits [0.00103400]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [517 tokens], throughput: 4110.103 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.081Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.183726, used credits [0.00138600]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 3771.919 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.066Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.204401, used credits [0.00111400]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 2725.033 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:02.025Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.103163, used credits [0.00078400]\n",
            "- CONTENT [e1cd970b-fde4-41b4-892a-1a64cb531334]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 3799.819 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.952Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.216633, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1772.583 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.938Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.220968, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1737.812 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.843Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.526507, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 374.164 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.619Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.274663, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 717.243 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.503Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:11.725702, used credits [0.02111115]\n",
            "- CONTENT [49836c55-86df-40ce-9f5e-98b3eb2ebc41]\n",
            "\n",
            "2024-12-29T03:26:01.486Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.516509, used credits [0.00182200]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [911 tokens], throughput: 1763.762 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.472Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.451868, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 832.100 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.421Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.353284, used credits [0.00103400]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [517 tokens], throughput: 1463.414 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.399Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.201153, used credits [0.00045200]\n",
            "- CONTENT [49836c55-86df-40ce-9f5e-98b3eb2ebc41]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [226 tokens], throughput: 1123.522 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.366Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.202412, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 2149.080 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.362Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.241088, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 1804.323 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.333Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.132912, used credits [0.00045200]\n",
            "- CONTENT [49836c55-86df-40ce-9f5e-98b3eb2ebc41]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [226 tokens], throughput: 1700.372 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.324Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:12.802290, used credits [0.02304946]\n",
            "- CONTENT [85a254e5-593b-4823-b1b9-d48506b794f8]\n",
            "\n",
            "2024-12-29T03:26:01.324Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.213868, used credits [0.00138600]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 3240.316 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.319Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:16.245162, used credits [0.02924806]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]\n",
            "\n",
            "2024-12-29T03:26:01.306Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.287975, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1305.668 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.302Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.214364, used credits [0.00125200]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [626 tokens], throughput: 2920.268 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.302Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.358706, used credits [0.00166600]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [833 tokens], throughput: 2322.236 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.296Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.194527, used credits [0.00111400]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 2863.351 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.222Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.311658, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1232.120 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.215Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.253736, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1513.382 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.189Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.287870, used credits [0.00059000]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [295 tokens], throughput: 1024.770 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.181Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.254154, used credits [0.00023800]\n",
            "- CONTENT [85a254e5-593b-4823-b1b9-d48506b794f8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [119 tokens], throughput: 468.219 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.141Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.212574, used credits [0.00023800]\n",
            "- CONTENT [85a254e5-593b-4823-b1b9-d48506b794f8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [119 tokens], throughput: 559.805 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:01.122Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.222370, used credits [0.00059000]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [295 tokens], throughput: 1326.618 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:00.939Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.320464, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 614.734 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:00.874Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.055956, used credits [0.00001681]\n",
            "- CONTENT [e1cd970b-fde4-41b4-892a-1a64cb531334]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4351 bytes], throughput: 77758.080 bytes/sec\n",
            "\n",
            "2024-12-29T03:26:00.727Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.100108, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 1967.877 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:00.656Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.331164, used credits [0.00059000]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [295 tokens], throughput: 890.798 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:00.616Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:15.070147, used credits [0.02713254]\n",
            "- CONTENT [53a0f36a-7c33-4514-901f-4865aa3fe0a9]\n",
            "\n",
            "2024-12-29T03:26:00.571Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.281800, used credits [0.00059000]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [295 tokens], throughput: 1046.842 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:00.469Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.298975, used credits [0.00074800]\n",
            "- CONTENT [53a0f36a-7c33-4514-901f-4865aa3fe0a9]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1250.942 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:00.357Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.190916, used credits [0.00074800]\n",
            "- CONTENT [53a0f36a-7c33-4514-901f-4865aa3fe0a9]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1958.979 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:00.255Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:12.987081, used credits [0.02338216]\n",
            "- CONTENT [a704d7fa-1161-4d3a-9be7-569d3af4d1cf]\n",
            "\n",
            "2024-12-29T03:26:00.142Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.240238, used credits [0.00117000]\n",
            "- CONTENT [a704d7fa-1161-4d3a-9be7-569d3af4d1cf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [585 tokens], throughput: 2435.083 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:00.098Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.069333, used credits [0.00000793]\n",
            "- CONTENT [49836c55-86df-40ce-9f5e-98b3eb2ebc41]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2053 bytes], throughput: 29610.677 bytes/sec\n",
            "\n",
            "2024-12-29T03:26:00.091Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.208994, used credits [0.00095800]\n",
            "- CONTENT [a704d7fa-1161-4d3a-9be7-569d3af4d1cf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [479 tokens], throughput: 2291.934 tokens/sec\n",
            "\n",
            "2024-12-29T03:26:00.059Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.481944, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 902.595 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.998Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.299832, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1280.719 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.957Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.330415, used credits [0.00138600]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 2097.360 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.922Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.228633, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1679.549 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.802Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.294503, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1276.728 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.785Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:20.781392, used credits [0.03741516]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]\n",
            "\n",
            "2024-12-29T03:25:59.771Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.152260, used credits [0.00111400]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 3658.219 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.756Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.261076, used credits [0.00125200]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [626 tokens], throughput: 2397.766 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.723Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.077126, used credits [0.00000570]\n",
            "- CONTENT [85a254e5-593b-4823-b1b9-d48506b794f8]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1474 bytes], throughput: 19111.708 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:59.697Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.193729, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1940.854 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.680Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.101314, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 4293.574 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.678Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.198599, used credits [0.00166600]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [833 tokens], throughput: 4194.380 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.651Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.153308, used credits [0.00182200]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [911 tokens], throughput: 5942.305 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.644Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.164748, used credits [0.00103400]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [517 tokens], throughput: 3138.118 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.644Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.279232, used credits [0.00059000]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [295 tokens], throughput: 1056.470 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.590Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.248528, used credits [0.00059000]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [295 tokens], throughput: 1186.987 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.560Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.129060, used credits [0.00000856]\n",
            "- CONTENT [9e6ef8e1-5448-4933-9705-d9d9502b384c]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2216 bytes], throughput: 17170.308 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:59.550Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.271675, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 725.132 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:59.536Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.275878, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 714.085 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.859Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.509330, used credits [0.00111400]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 1093.593 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.813Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.473557, used credits [0.00125200]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [626 tokens], throughput: 1321.912 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.759Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.187049, used credits [0.00081499]\n",
            "- CONTENT [e1cd970b-fde4-41b4-892a-1a64cb531334]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2023/october-15-support-for-anthropic-claude-models-slack-feeds-and-entity-enrichment]\n",
            "- File upload [210919 bytes], throughput: 1127613.017 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:58.745Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.360207, used credits [0.00182200]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [911 tokens], throughput: 2529.099 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.743Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.212446, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1807.520 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.707Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.186424, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 2059.816 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.706Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.306920, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1225.076 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.696Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.447213, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 972.691 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.676Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.293852, used credits [0.00138600]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 2358.330 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.665Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.086299, used credits [0.00001608]\n",
            "- CONTENT [53a0f36a-7c33-4514-901f-4865aa3fe0a9]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4161 bytes], throughput: 48215.862 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:58.633Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.228129, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1648.190 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.530Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.291237, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 1493.630 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.526Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.441398, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 446.309 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.525Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.221004, used credits [0.00103400]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [517 tokens], throughput: 2339.320 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.509Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.154650, used credits [0.00166600]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [833 tokens], throughput: 5386.353 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:58.465Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.065959, used credits [0.00001446]\n",
            "- CONTENT [a704d7fa-1161-4d3a-9be7-569d3af4d1cf]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3743 bytes], throughput: 56747.287 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:58.460Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.456811, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 431.251 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:57.956Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.140864, used credits [0.03800000]\n",
            "- CONTENT [e1cd970b-fde4-41b4-892a-1a64cb531334]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:57.935Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.200103, used credits [0.00073394]\n",
            "- CONTENT [49836c55-86df-40ce-9f5e-98b3eb2ebc41]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-31-support-for-simulated-tool-calling-bug-fixes]\n",
            "- File upload [189943 bytes], throughput: 949225.674 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:57.933Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.305260, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1257.942 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:57.807Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.396892, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 967.518 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:57.157Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.106570, used credits [0.00072362]\n",
            "- CONTENT [85a254e5-593b-4823-b1b9-d48506b794f8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-9-support-for-github-repository-feeds-bug-fixes]\n",
            "- File upload [187273 bytes], throughput: 1757283.503 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:57.156Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.679838, used credits [0.00125200]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [626 tokens], throughput: 920.808 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:57.116Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.642152, used credits [0.00182200]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [911 tokens], throughput: 1418.668 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:57.103Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.626303, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 694.552 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:57.091Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.212975, used credits [0.03800000]\n",
            "- CONTENT [49836c55-86df-40ce-9f5e-98b3eb2ebc41]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:57.078Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.653833, used credits [0.00103400]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [517 tokens], throughput: 790.721 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:57.077Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.652760, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 666.401 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:57.056Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.581827, used credits [0.00138600]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 1191.075 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:57.053Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.629799, used credits [0.00166600]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [833 tokens], throughput: 1322.644 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:56.982Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.515621, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 729.218 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:56.960Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.534171, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 703.895 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:56.960Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.527654, used credits [0.00111400]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 1055.616 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:56.960Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.482774, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 408.058 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:56.959Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.522723, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 376.873 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:56.743Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.084257, used credits [0.00077349]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-1-support-for-fhir-enrichment-latest-cohere-models-bug-fixes]\n",
            "- File upload [200178 bytes], throughput: 2375802.604 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:56.305Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.059460, used credits [0.00001020]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2640 bytes], throughput: 44399.746 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:56.271Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.686820, used credits [0.03800000]\n",
            "- CONTENT [85a254e5-593b-4823-b1b9-d48506b794f8]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:56.221Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:21.632586, used credits [0.03894767]\n",
            "- CONTENT [f9db5d97-0274-4ca3-b867-3f2c46a5359e]\n",
            "\n",
            "2024-12-29T03:25:56.213Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.201413, used credits [0.00081552]\n",
            "- CONTENT [a704d7fa-1161-4d3a-9be7-569d3af4d1cf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-26-support-for-google-ai-and-cerebras-models-and-latest-groq-models]\n",
            "- File upload [211055 bytes], throughput: 1047873.347 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:56.197Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.141879, used credits [0.00082329]\n",
            "- CONTENT [53a0f36a-7c33-4514-901f-4865aa3fe0a9]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2023/october-30-optimized-conversation-responses-added-observable-aliases-bug-fixes]\n",
            "- File upload [213066 bytes], throughput: 1501741.269 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:56.089Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.261588, used credits [0.00049200]\n",
            "- CONTENT [f9db5d97-0274-4ca3-b867-3f2c46a5359e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 940.408 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:56.038Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.918226, used credits [0.03800000]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:56.030Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.150359, used credits [0.00049200]\n",
            "- CONTENT [f9db5d97-0274-4ca3-b867-3f2c46a5359e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 1636.088 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:55.336Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:20.204424, used credits [0.03637638]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]\n",
            "\n",
            "2024-12-29T03:25:55.178Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.562570, used credits [0.03800000]\n",
            "- CONTENT [53a0f36a-7c33-4514-901f-4865aa3fe0a9]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:55.151Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.250334, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1533.949 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:55.124Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.155346, used credits [0.00076800]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 2471.895 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:55.096Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.786482, used credits [0.03800000]\n",
            "- CONTENT [a704d7fa-1161-4d3a-9be7-569d3af4d1cf]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:55.093Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:20.900790, used credits [0.03763013]\n",
            "- CONTENT [55afbdb7-30a8-4eaa-92aa-02ae98fb57a5]\n",
            "\n",
            "2024-12-29T03:25:54.966Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.420076, used credits [0.00112600]\n",
            "- CONTENT [55afbdb7-30a8-4eaa-92aa-02ae98fb57a5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [563 tokens], throughput: 1340.233 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:54.828Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:06.400813, used credits [0.01152413]\n",
            "- CONTENT [989c5568-cfd4-4ee4-ba79-73e3552cfdc2]\n",
            "\n",
            "2024-12-29T03:25:54.789Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.259432, used credits [0.00095600]\n",
            "- CONTENT [55afbdb7-30a8-4eaa-92aa-02ae98fb57a5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [478 tokens], throughput: 1842.483 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:54.749Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.183402, used credits [0.00078018]\n",
            "- CONTENT [9e6ef8e1-5448-4933-9705-d9d9502b384c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-3-support-for-web-search-feeds-model-deprecations]\n",
            "- File upload [201910 bytes], throughput: 1100914.330 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:54.728Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.318587, used credits [0.00052800]\n",
            "- CONTENT [989c5568-cfd4-4ee4-ba79-73e3552cfdc2]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [264 tokens], throughput: 828.659 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:54.701Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.291471, used credits [0.00052800]\n",
            "- CONTENT [989c5568-cfd4-4ee4-ba79-73e3552cfdc2]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [264 tokens], throughput: 905.750 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:54.075Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.984888, used credits [0.03800000]\n",
            "- CONTENT [9e6ef8e1-5448-4933-9705-d9d9502b384c]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:53.875Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.075865, used credits [0.00000764]\n",
            "- CONTENT [f9db5d97-0274-4ca3-b867-3f2c46a5359e]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1976 bytes], throughput: 26046.129 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:53.532Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.051292, used credits [0.00000757]\n",
            "- CONTENT [989c5568-cfd4-4ee4-ba79-73e3552cfdc2]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1959 bytes], throughput: 38193.165 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:53.455Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.098744, used credits [0.00078032]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-1-support-for-fhir-enrichment-latest-cohere-models-bug-fixes]\n",
            "- File upload [201946 bytes], throughput: 183797.119 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:52.760Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:01.156347, used credits [0.00001288]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3334 bytes], throughput: 2883.218 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:52.735Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:06.447486, used credits [0.01160816]\n",
            "- CONTENT [090640e2-097b-4684-90fa-86ab0cba94fc]\n",
            "\n",
            "2024-12-29T03:25:52.616Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.280744, used credits [0.00075600]\n",
            "- CONTENT [090640e2-097b-4684-90fa-86ab0cba94fc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1346.424 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:52.533Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.198422, used credits [0.00075600]\n",
            "- CONTENT [090640e2-097b-4684-90fa-86ab0cba94fc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1905.035 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:52.413Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.331900, used credits [0.00085314]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/june-2024/june-9-support-for-deepseek-models-json-ld-webpage-parsing-performance-improvements-and-bug-fixes]\n",
            "- File upload [220792 bytes], throughput: 665236.317 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:52.389Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.439660, used credits [0.00087032]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/may-2024/may-5-support-for-jina-and-pongo-rerankers-microsoft-teams-feed-new-youtube-downloader-bug-fixes]\n",
            "- File upload [225239 bytes], throughput: 512302.921 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:52.292Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.075425, used credits [0.00079090]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-3-support-tool-calling-ingestbatch-mutation-gemini-flash-1.5-8b-bug-fixes]\n",
            "- File upload [204685 bytes], throughput: 2713762.582 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:52.268Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.082105, used credits [0.01275074]\n",
            "- CONTENT [3a2727e0-f370-4b5c-9d04-59e54995f166]\n",
            "\n",
            "2024-12-29T03:25:52.247Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.455175, used credits [0.00079534]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-24-support-for-direct-llm-prompt-multi-turn-image-analysis-bug-fixes]\n",
            "- File upload [205833 bytes], throughput: 452205.996 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:52.237Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.072746, used credits [0.00072134]\n",
            "- CONTENT [989c5568-cfd4-4ee4-ba79-73e3552cfdc2]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-30-support-for-azure-ai-inference-models-mistral-pixtral-and-latest-google-gemini-models]\n",
            "- File upload [186681 bytes], throughput: 2566192.370 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:52.216Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.254958, used credits [0.00095787]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2024/december-22-support-for-dropbox-box-intercom-and-zendesk-feeds-openai-o1-gemini-2.0-bug-fixes]\n",
            "- File upload [247897 bytes], throughput: 972303.336 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:52.197Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.378918, used credits [0.00074178]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2024/december-1-support-for-retrieval-only-rag-pipeline-bug-fixes]\n",
            "- File upload [191971 bytes], throughput: 506629.269 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:52.177Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.421500, used credits [0.00080505]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-8-support-for-llm-based-document-extraction-.net-sdk-bug-fixes]\n",
            "- File upload [208346 bytes], throughput: 494296.677 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:52.163Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.237649, used credits [0.00044200]\n",
            "- CONTENT [3a2727e0-f370-4b5c-9d04-59e54995f166]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [221 tokens], throughput: 929.943 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:52.133Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.208976, used credits [0.00044200]\n",
            "- CONTENT [3a2727e0-f370-4b5c-9d04-59e54995f166]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [221 tokens], throughput: 1057.540 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:51.827Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.576510, used credits [0.03800000]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:51.726Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.246720, used credits [0.03800000]\n",
            "- CONTENT [989c5568-cfd4-4ee4-ba79-73e3552cfdc2]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:51.623Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.886635, used credits [0.03800000]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:51.543Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.838255, used credits [0.03800000]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:51.483Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.771291, used credits [0.03800000]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:51.473Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.053614, used credits [0.00001484]\n",
            "- CONTENT [090640e2-097b-4684-90fa-86ab0cba94fc]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3840 bytes], throughput: 71623.618 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:51.434Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.758483, used credits [0.03800000]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:51.366Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.559774, used credits [0.00001352]\n",
            "- CONTENT [55afbdb7-30a8-4eaa-92aa-02ae98fb57a5]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3499 bytes], throughput: 6250.737 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:51.121Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.051030, used credits [0.00000952]\n",
            "- CONTENT [3a2727e0-f370-4b5c-9d04-59e54995f166]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2465 bytes], throughput: 48304.919 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:50.893Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.222011, used credits [0.03800000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:50.851Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.146245, used credits [0.03800000]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:50.788Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.034643, used credits [0.01266529]\n",
            "- CONTENT [3f9aa839-735f-4aa9-a599-c7d59d2898aa]\n",
            "\n",
            "2024-12-29T03:25:50.672Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.228595, used credits [0.00027600]\n",
            "- CONTENT [3f9aa839-735f-4aa9-a599-c7d59d2898aa]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [138 tokens], throughput: 603.688 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:50.597Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.156424, used credits [0.00027600]\n",
            "- CONTENT [3f9aa839-735f-4aa9-a599-c7d59d2898aa]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [138 tokens], throughput: 882.220 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:50.179Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:20.398768, used credits [0.03672628]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]\n",
            "\n",
            "2024-12-29T03:25:50.040Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.796196, used credits [0.03800000]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:50.027Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:23.414722, used credits [0.04215626]\n",
            "- CONTENT [3f2f43b0-49a5-462a-88c6-e0ecd92f41c1]\n",
            "\n",
            "2024-12-29T03:25:49.968Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.078287, used credits [0.00081632]\n",
            "- CONTENT [090640e2-097b-4684-90fa-86ab0cba94fc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2023/september-4-workflow-configuration-support-for-notion-feeds-document-ocr]\n",
            "- File upload [211263 bytes], throughput: 2698574.091 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:49.911Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.391179, used credits [0.00125200]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [626 tokens], throughput: 1600.290 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:49.789Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.330138, used credits [0.00071719]\n",
            "- CONTENT [f9db5d97-0274-4ca3-b867-3f2c46a5359e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-16-support-for-image-description-multi-turn-text-summarization]\n",
            "- File upload [185609 bytes], throughput: 139541.118 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:49.718Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.221983, used credits [0.00103400]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [517 tokens], throughput: 2329.009 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:49.676Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:06.982305, used credits [0.01257106]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]\n",
            "\n",
            "2024-12-29T03:25:49.656Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.312579, used credits [0.00310800]\n",
            "- CONTENT [3f2f43b0-49a5-462a-88c6-e0ecd92f41c1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1554 tokens], throughput: 4971.540 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:49.579Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.201957, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1861.780 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:49.577Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.200575, used credits [0.00075200]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1874.611 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:49.576Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.073917, used credits [0.00079268]\n",
            "- CONTENT [3a2727e0-f370-4b5c-9d04-59e54995f166]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2023/september-24-support-for-youtube-feeds-added-documentation-bug-fixes]\n",
            "- File upload [205146 bytes], throughput: 2775348.633 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:49.475Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.169720, used credits [0.00239200]\n",
            "- CONTENT [3f2f43b0-49a5-462a-88c6-e0ecd92f41c1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1196 tokens], throughput: 7046.892 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:49.467Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.151597, used credits [0.03800000]\n",
            "- CONTENT [090640e2-097b-4684-90fa-86ab0cba94fc]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:49.350Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.053919, used credits [0.00000562]\n",
            "- CONTENT [3f9aa839-735f-4aa9-a599-c7d59d2898aa]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1455 bytes], throughput: 26984.722 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:49.238Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:20.447858, used credits [0.03681466]\n",
            "- CONTENT [ce68f926-3ab6-4a73-8f33-3f5e552c2714]\n",
            "\n",
            "2024-12-29T03:25:49.021Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.814080, used credits [0.03800000]\n",
            "- CONTENT [3a2727e0-f370-4b5c-9d04-59e54995f166]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:49.005Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.189613, used credits [0.00054200]\n",
            "- CONTENT [ce68f926-3ab6-4a73-8f33-3f5e552c2714]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 1429.226 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:48.967Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.244463, used credits [0.00054200]\n",
            "- CONTENT [ce68f926-3ab6-4a73-8f33-3f5e552c2714]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 1108.554 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:48.799Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.109391, used credits [0.01279987]\n",
            "- CONTENT [8e5925f7-0358-42ff-84df-8522c7f265a8]\n",
            "\n",
            "2024-12-29T03:25:48.678Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.305400, used credits [0.00044000]\n",
            "- CONTENT [8e5925f7-0358-42ff-84df-8522c7f265a8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 720.367 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:48.535Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.163186, used credits [0.00044000]\n",
            "- CONTENT [8e5925f7-0358-42ff-84df-8522c7f265a8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 1348.158 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:48.355Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.052917, used credits [0.00001299]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3363 bytes], throughput: 63552.836 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:47.941Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.398978, used credits [0.00079940]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-24-support-for-direct-llm-prompt-multi-turn-image-analysis-bug-fixes]\n",
            "- File upload [206883 bytes], throughput: 147881.567 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:47.773Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.111175, used credits [0.01280308]\n",
            "- CONTENT [cfa6a062-2cfa-4994-a066-6443539c4553]\n",
            "\n",
            "2024-12-29T03:25:47.711Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.086963, used credits [0.00070589]\n",
            "- CONTENT [3f9aa839-735f-4aa9-a599-c7d59d2898aa]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-7-support-for-anthropic-and-gemini-tool-calling]\n",
            "- File upload [182683 bytes], throughput: 2100705.245 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:47.642Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.221041, used credits [0.00085400]\n",
            "- CONTENT [cfa6a062-2cfa-4994-a066-6443539c4553]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 1931.767 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:47.637Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:18.377923, used credits [0.03308792]\n",
            "- CONTENT [bbe12218-8806-409d-a79b-148d8f0eb515]\n",
            "\n",
            "2024-12-29T03:25:47.581Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.141162, used credits [0.00085400]\n",
            "- CONTENT [cfa6a062-2cfa-4994-a066-6443539c4553]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 3024.893 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:47.368Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.247013, used credits [0.00070200]\n",
            "- CONTENT [bbe12218-8806-409d-a79b-148d8f0eb515]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [351 tokens], throughput: 1420.980 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:47.348Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.055098, used credits [0.00000616]\n",
            "- CONTENT [8e5925f7-0358-42ff-84df-8522c7f265a8]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1595 bytes], throughput: 28948.262 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:47.342Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.211908, used credits [0.00070200]\n",
            "- CONTENT [bbe12218-8806-409d-a79b-148d8f0eb515]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [351 tokens], throughput: 1656.382 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:47.209Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.429988, used credits [0.03800000]\n",
            "- CONTENT [3f9aa839-735f-4aa9-a599-c7d59d2898aa]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:47.057Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.323228, used credits [0.00076204]\n",
            "- CONTENT [55afbdb7-30a8-4eaa-92aa-02ae98fb57a5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-10-support-for-web-search-multi-turn-content-summarization-deepgram-language-detection]\n",
            "- File upload [197215 bytes], throughput: 610140.934 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:46.959Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.794724, used credits [0.00001951]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5048 bytes], throughput: 6351.892 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:46.914Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.134059, used credits [0.00003752]\n",
            "- CONTENT [3f2f43b0-49a5-462a-88c6-e0ecd92f41c1]: Content type [PAGE], file type [DATA]\n",
            "- File upload [9709 bytes], throughput: 72423.336 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:46.881Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.172888, used credits [0.00079417]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-3-support-tool-calling-ingestbatch-mutation-gemini-flash-1.5-8b-bug-fixes]\n",
            "- File upload [205530 bytes], throughput: 1188802.937 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:46.360Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.051376, used credits [0.00001413]\n",
            "- CONTENT [cfa6a062-2cfa-4994-a066-6443539c4553]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3658 bytes], throughput: 71200.838 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:46.104Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.383320, used credits [0.03800000]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:45.715Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.210229, used credits [0.00001077]\n",
            "- CONTENT [ce68f926-3ab6-4a73-8f33-3f5e552c2714]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2786 bytes], throughput: 13252.241 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:45.638Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.115956, used credits [0.00071656]\n",
            "- CONTENT [8e5925f7-0358-42ff-84df-8522c7f265a8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-22-support-for-latest-anthropic-sonnet-3.5-model-cohere-image-embeddings]\n",
            "- File upload [185445 bytes], throughput: 1599267.655 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:45.238Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.328827, used credits [0.01319494]\n",
            "- CONTENT [119976d5-ad8a-4a2a-8c4d-e008bc61e848]\n",
            "\n",
            "2024-12-29T03:25:45.143Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.176991, used credits [0.00049600]\n",
            "- CONTENT [119976d5-ad8a-4a2a-8c4d-e008bc61e848]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 1401.204 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:45.113Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.148552, used credits [0.00049600]\n",
            "- CONTENT [119976d5-ad8a-4a2a-8c4d-e008bc61e848]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 1669.451 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:45.088Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.836678, used credits [0.03800000]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:45.084Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:18.315315, used credits [0.03297520]\n",
            "- CONTENT [5c9b0f96-4efc-4319-b2c2-0fc27875f030]\n",
            "\n",
            "2024-12-29T03:25:44.950Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.315557, used credits [0.03800000]\n",
            "- CONTENT [f9db5d97-0274-4ca3-b867-3f2c46a5359e]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:44.916Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.200559, used credits [0.03800000]\n",
            "- CONTENT [8e5925f7-0358-42ff-84df-8522c7f265a8]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:44.913Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.236975, used credits [0.00059400]\n",
            "- CONTENT [5c9b0f96-4efc-4319-b2c2-0fc27875f030]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 1253.295 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:44.874Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.077129, used credits [0.00001223]\n",
            "- CONTENT [bbe12218-8806-409d-a79b-148d8f0eb515]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3165 bytes], throughput: 41035.043 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:44.862Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.109800, used credits [0.00059400]\n",
            "- CONTENT [5c9b0f96-4efc-4319-b2c2-0fc27875f030]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 2704.925 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:44.653Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.110251, used credits [0.00077581]\n",
            "- CONTENT [cfa6a062-2cfa-4994-a066-6443539c4553]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-21-support-openai-cohere-jina-mistral-voyage-and-google-ai-embedding-models]\n",
            "- File upload [200780 bytes], throughput: 1821122.224 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:44.119Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.419811, used credits [0.03800000]\n",
            "- CONTENT [cfa6a062-2cfa-4994-a066-6443539c4553]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:43.880Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.053371, used credits [0.00000878]\n",
            "- CONTENT [119976d5-ad8a-4a2a-8c4d-e008bc61e848]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2273 bytes], throughput: 42588.672 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:43.853Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.680938, used credits [0.00101656]\n",
            "- CONTENT [3f2f43b0-49a5-462a-88c6-e0ecd92f41c1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/january-2024/january-18-support-for-content-publishing-llm-tools-clip-image-embeddings-bug-fixes]\n",
            "- File upload [263084 bytes], throughput: 156510.253 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:43.649Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.366278, used credits [0.03800000]\n",
            "- CONTENT [55afbdb7-30a8-4eaa-92aa-02ae98fb57a5]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:43.555Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:20.805993, used credits [0.03745946]\n",
            "- CONTENT [da1c0b2c-4bbf-4467-af16-f80122aeedd3]\n",
            "\n",
            "2024-12-29T03:25:43.457Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:21.880946, used credits [0.03939482]\n",
            "- CONTENT [0ec87d20-b463-4f65-a2f4-e32bc44f1b0d]\n",
            "\n",
            "2024-12-29T03:25:43.383Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.332139, used credits [0.00126800]\n",
            "- CONTENT [da1c0b2c-4bbf-4467-af16-f80122aeedd3]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [634 tokens], throughput: 1908.838 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:43.268Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.271193, used credits [0.00107200]\n",
            "- CONTENT [da1c0b2c-4bbf-4467-af16-f80122aeedd3]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [536 tokens], throughput: 1976.449 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:43.261Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.463455, used credits [0.00214200]\n",
            "- CONTENT [0ec87d20-b463-4f65-a2f4-e32bc44f1b0d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1071 tokens], throughput: 2310.903 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:43.212Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.014852, used credits [0.00084830]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/june-2024/june-9-support-for-deepseek-models-json-ld-webpage-parsing-performance-improvements-and-bug-fixes]\n",
            "- File upload [219539 bytes], throughput: 216326.018 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:43.186Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.437037, used credits [0.00166800]\n",
            "- CONTENT [0ec87d20-b463-4f65-a2f4-e32bc44f1b0d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [834 tokens], throughput: 1908.306 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:42.365Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:20.572298, used credits [0.03703871]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]\n",
            "\n",
            "2024-12-29T03:25:42.363Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:21.016522, used credits [0.03783850]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]\n",
            "\n",
            "2024-12-29T03:25:42.230Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.079060, used credits [0.00001084]\n",
            "- CONTENT [5c9b0f96-4efc-4319-b2c2-0fc27875f030]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2805 bytes], throughput: 35479.158 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:42.228Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.099518, used credits [0.00075869]\n",
            "- CONTENT [119976d5-ad8a-4a2a-8c4d-e008bc61e848]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-4-support-for-anthropic-claude-3.5-haiku-bug-fixes]\n",
            "- File upload [196348 bytes], throughput: 1972983.863 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:42.227Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.404023, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 487.596 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:42.206Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.545664, used credits [0.01538576]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]\n",
            "\n",
            "2024-12-29T03:25:42.172Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.352729, used credits [0.00039400]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 558.502 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:42.085Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.350455, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 1241.243 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:42.026Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.359058, used credits [0.00138600]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 1930.050 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:42.001Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.242771, used credits [0.00087000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 1791.812 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:41.906Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.250594, used credits [0.00111400]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 2222.721 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:41.898Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.704278, used credits [0.00077311]\n",
            "- CONTENT [ce68f926-3ab6-4a73-8f33-3f5e552c2714]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-28-support-for-indexing-workflow-stage-azure-ai-language-detection-bug-fixes]\n",
            "- File upload [200079 bytes], throughput: 284091.103 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:41.743Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:19.509674, used credits [0.03512554]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]\n",
            "\n",
            "2024-12-29T03:25:41.583Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.631376, used credits [0.03800000]\n",
            "- CONTENT [119976d5-ad8a-4a2a-8c4d-e008bc61e848]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:41.552Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.482671, used credits [0.00076677]\n",
            "- CONTENT [bbe12218-8806-409d-a79b-148d8f0eb515]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/june-2024/june-21-support-for-the-claude-3.5-sonnet-model-knowledge-graph-semantic-search-and-bug-fixes]\n",
            "- File upload [198439 bytes], throughput: 133838.808 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:41.549Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.579262, used credits [0.00182200]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [911 tokens], throughput: 1572.692 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:41.475Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.463669, used credits [0.01523813]\n",
            "- CONTENT [8d95b393-a271-4cb7-956b-6f2808937151]\n",
            "\n",
            "2024-12-29T03:25:41.334Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.313219, used credits [0.00125800]\n",
            "- CONTENT [8d95b393-a271-4cb7-956b-6f2808937151]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [629 tokens], throughput: 2008.180 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:41.154Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.124478, used credits [0.00150600]\n",
            "- CONTENT [8d95b393-a271-4cb7-956b-6f2808937151]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [753 tokens], throughput: 6049.247 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:41.111Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.152700, used credits [0.00166600]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [833 tokens], throughput: 5455.123 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:41.080Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.820668, used credits [0.01588088]\n",
            "- CONTENT [2611e484-0933-494b-bebe-511835b64bb9]\n",
            "\n",
            "2024-12-29T03:25:40.967Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.136106, used credits [0.00063600]\n",
            "- CONTENT [2611e484-0933-494b-bebe-511835b64bb9]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [318 tokens], throughput: 2336.419 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:40.956Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.119195, used credits [0.00063600]\n",
            "- CONTENT [2611e484-0933-494b-bebe-511835b64bb9]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [318 tokens], throughput: 2667.895 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:40.721Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.077864, used credits [0.00001633]\n",
            "- CONTENT [da1c0b2c-4bbf-4467-af16-f80122aeedd3]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4226 bytes], throughput: 54274.189 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:40.653Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.055664, used credits [0.00002005]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5189 bytes], throughput: 93220.202 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:40.648Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.868226, used credits [0.01596650]\n",
            "- CONTENT [feb66b5f-ba9e-448e-9909-ba5e2fdbf56d]\n",
            "\n",
            "2024-12-29T03:25:40.534Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.494998, used credits [0.00034800]\n",
            "- CONTENT [feb66b5f-ba9e-448e-9909-ba5e2fdbf56d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 351.517 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:40.406Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.835309, used credits [0.01590724]\n",
            "- CONTENT [3b356809-5179-4144-80bc-baf4dd184462]\n",
            "\n",
            "2024-12-29T03:25:40.282Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.228918, used credits [0.00109800]\n",
            "- CONTENT [3b356809-5179-4144-80bc-baf4dd184462]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 2398.238 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:40.220Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.163600, used credits [0.00034800]\n",
            "- CONTENT [feb66b5f-ba9e-448e-9909-ba5e2fdbf56d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 1063.570 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:40.162Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.124803, used credits [0.00090800]\n",
            "- CONTENT [3b356809-5179-4144-80bc-baf4dd184462]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 3637.736 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:39.877Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.055651, used credits [0.00002022]\n",
            "- CONTENT [8d95b393-a271-4cb7-956b-6f2808937151]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5234 bytes], throughput: 94050.252 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:39.825Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.063598, used credits [0.00001046]\n",
            "- CONTENT [2611e484-0933-494b-bebe-511835b64bb9]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2707 bytes], throughput: 42564.232 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:39.782Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.942611, used credits [0.03800000]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:39.656Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:10.709690, used credits [0.01928190]\n",
            "- CONTENT [dd2cef1a-5292-4654-a2e2-d40b4f82db84]\n",
            "\n",
            "2024-12-29T03:25:39.655Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:12.443834, used credits [0.02240409]\n",
            "- CONTENT [636bd286-d807-4838-af4d-0c37d24772b7]\n",
            "\n",
            "2024-12-29T03:25:39.548Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.337276, used credits [0.00002728]\n",
            "- CONTENT [0ec87d20-b463-4f65-a2f4-e32bc44f1b0d]: Content type [PAGE], file type [DATA]\n",
            "- File upload [7060 bytes], throughput: 20932.430 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:39.538Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.488438, used credits [0.00069600]\n",
            "- CONTENT [dd2cef1a-5292-4654-a2e2-d40b4f82db84]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [348 tokens], throughput: 712.475 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:39.457Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.312309, used credits [0.00136800]\n",
            "- CONTENT [636bd286-d807-4838-af4d-0c37d24772b7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [684 tokens], throughput: 2190.141 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:39.415Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.282666, used credits [0.00125000]\n",
            "- CONTENT [636bd286-d807-4838-af4d-0c37d24772b7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [625 tokens], throughput: 2211.094 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:39.236Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.191043, used credits [0.00069600]\n",
            "- CONTENT [dd2cef1a-5292-4654-a2e2-d40b4f82db84]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [348 tokens], throughput: 1821.580 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:39.006Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.068341, used credits [0.00000795]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2058 bytes], throughput: 30113.695 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:38.931Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.109962, used credits [0.00086813]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/may-2024/may-5-support-for-jina-and-pongo-rerankers-microsoft-teams-feed-new-youtube-downloader-bug-fixes]\n",
            "- File upload [224672 bytes], throughput: 2043171.120 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:38.819Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.123720, used credits [0.00000663]\n",
            "- CONTENT [feb66b5f-ba9e-448e-9909-ba5e2fdbf56d]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1717 bytes], throughput: 13878.146 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:38.816Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.468676, used credits [0.00001807]\n",
            "- CONTENT [3b356809-5179-4144-80bc-baf4dd184462]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4677 bytes], throughput: 9979.182 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:38.678Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.631375, used credits [0.03800000]\n",
            "- CONTENT [ce68f926-3ab6-4a73-8f33-3f5e552c2714]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:38.674Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:11.968386, used credits [0.03800000]\n",
            "- CONTENT [3f2f43b0-49a5-462a-88c6-e0ecd92f41c1]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:38.606Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.227827, used credits [0.00001454]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3764 bytes], throughput: 16521.322 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:38.545Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.074028, used credits [0.00002560]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DATA]\n",
            "- File upload [6624 bytes], throughput: 89479.173 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:38.539Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.024049, used credits [0.03800000]\n",
            "- CONTENT [bbe12218-8806-409d-a79b-148d8f0eb515]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:38.341Z: Serverless compute\n",
            "- Workflow [Feed] took 0:00:49.776744, used credits [0.04480944]\n",
            "\n",
            "2024-12-29T03:25:37.858Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:01.192054, used credits [0.00076395]\n",
            "- CONTENT [5c9b0f96-4efc-4319-b2c2-0fc27875f030]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/january-2024/january-22-support-for-google-and-microsoft-email-feeds-reingest-content-in-place-bug-fixes]\n",
            "- File upload [197709 bytes], throughput: 165855.769 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:37.768Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.957558, used credits [0.01792775]\n",
            "- CONTENT [23ae5fe0-5e6d-411c-b219-5ebaf593062e]\n",
            "\n",
            "2024-12-29T03:25:37.624Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:01.227815, used credits [0.00063400]\n",
            "- CONTENT [23ae5fe0-5e6d-411c-b219-5ebaf593062e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 258.182 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:37.610Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.913881, used credits [0.03800000]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:37.445Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.074283, used credits [0.00001357]\n",
            "- CONTENT [dd2cef1a-5292-4654-a2e2-d40b4f82db84]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3512 bytes], throughput: 47278.778 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:37.404Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.052793, used credits [0.00001948]\n",
            "- CONTENT [636bd286-d807-4838-af4d-0c37d24772b7]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5041 bytes], throughput: 95485.421 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:37.376Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.292594, used credits [0.00083897]\n",
            "- CONTENT [8d95b393-a271-4cb7-956b-6f2808937151]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/may-2024/may-15-support-for-graphrag-openai-gpt-4o-model-performance-improvements-and-bug-fixes]\n",
            "- File upload [217124 bytes], throughput: 742065.037 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:37.257Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.255906, used credits [0.00076222]\n",
            "- CONTENT [2611e484-0933-494b-bebe-511835b64bb9]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-23-support-for-linear-github-issues-and-jira-issue-feeds-ingest-files-via-web-feed-sitemap]\n",
            "- File upload [197263 bytes], throughput: 770841.336 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:36.890Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.979591, used credits [0.01796742]\n",
            "- CONTENT [51e379d2-307c-42f3-b5cf-eb3b02c6f068]\n",
            "\n",
            "2024-12-29T03:25:36.742Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.251678, used credits [0.00111000]\n",
            "- CONTENT [51e379d2-307c-42f3-b5cf-eb3b02c6f068]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [555 tokens], throughput: 2205.195 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:36.720Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.220016, used credits [0.00139200]\n",
            "- CONTENT [51e379d2-307c-42f3-b5cf-eb3b02c6f068]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [696 tokens], throughput: 3163.409 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:36.695Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.250508, used credits [0.00071686]\n",
            "- CONTENT [feb66b5f-ba9e-448e-9909-ba5e2fdbf56d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-13-support-for-claude-3-haiku-model-direct-ingestion-of-base64-encoded-files]\n",
            "- File upload [185523 bytes], throughput: 740586.536 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:36.614Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.509229, used credits [0.03800000]\n",
            "- CONTENT [8d95b393-a271-4cb7-956b-6f2808937151]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:36.602Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.314573, used credits [0.03800000]\n",
            "- CONTENT [2611e484-0933-494b-bebe-511835b64bb9]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:36.585Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.206525, used credits [0.00063400]\n",
            "- CONTENT [23ae5fe0-5e6d-411c-b219-5ebaf593062e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 1534.926 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:36.474Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.093771, used credits [0.00085486]\n",
            "- CONTENT [3b356809-5179-4144-80bc-baf4dd184462]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-10-support-for-claude-3-mistral-and-groq-models-usage-credits-telemetry-bug-fixes]\n",
            "- File upload [221236 bytes], throughput: 2359317.146 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:36.127Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.363587, used credits [0.00081543]\n",
            "- CONTENT [da1c0b2c-4bbf-4467-af16-f80122aeedd3]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2024/december-9-support-for-website-mapping-web-page-screenshots-groq-llama-3.3-model-bug-fixes]\n",
            "- File upload [211033 bytes], throughput: 580419.063 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:36.053Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.205889, used credits [0.03800000]\n",
            "- CONTENT [feb66b5f-ba9e-448e-9909-ba5e2fdbf56d]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:35.972Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.378963, used credits [0.03800000]\n",
            "- CONTENT [3b356809-5179-4144-80bc-baf4dd184462]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:35.506Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.075673, used credits [0.00079831]\n",
            "- CONTENT [dd2cef1a-5292-4654-a2e2-d40b4f82db84]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-4-support-for-webhook-alerts-keywords-summarization-deepseek-128k-context-window-bug-fixes]\n",
            "- File upload [206601 bytes], throughput: 2730170.351 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:35.163Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.055011, used credits [0.00001000]\n",
            "- CONTENT [23ae5fe0-5e6d-411c-b219-5ebaf593062e]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2588 bytes], throughput: 47045.393 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:35.160Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.424103, used credits [0.00083595]\n",
            "- CONTENT [636bd286-d807-4838-af4d-0c37d24772b7]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-19-support-for-openai-gpt-4o-mini-byo-key-for-azure-ai-similarity-by-summary-bug-fixes]\n",
            "- File upload [216344 bytes], throughput: 510121.362 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:35.087Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.084666, used credits [0.00002159]\n",
            "- CONTENT [51e379d2-307c-42f3-b5cf-eb3b02c6f068]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5588 bytes], throughput: 66000.208 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:34.486Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.500373, used credits [0.03800000]\n",
            "- CONTENT [dd2cef1a-5292-4654-a2e2-d40b4f82db84]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:33.741Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.181371, used credits [0.00095185]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2024/december-22-support-for-dropbox-box-intercom-and-zendesk-feeds-openai-o1-gemini-2.0-bug-fixes]\n",
            "- File upload [246339 bytes], throughput: 1358205.754 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:33.669Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:06.838676, used credits [0.03800000]\n",
            "- CONTENT [5c9b0f96-4efc-4319-b2c2-0fc27875f030]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:33.624Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.205530, used credits [0.00101304]\n",
            "- CONTENT [0ec87d20-b463-4f65-a2f4-e32bc44f1b0d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2023/december-10-support-for-openai-gpt-4-turbo-llama-2-and-mistral-models-query-by-example-bug-fixes]\n",
            "- File upload [262173 bytes], throughput: 1275592.321 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:33.611Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:06.372847, used credits [0.03800000]\n",
            "- CONTENT [636bd286-d807-4838-af4d-0c37d24772b7]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:33.561Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.405906, used credits [0.00081102]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-8-support-for-llm-based-document-extraction-.net-sdk-bug-fixes]\n",
            "- File upload [209892 bytes], throughput: 517095.728 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:33.560Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.097778, used credits [0.00073943]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2024/december-1-support-for-retrieval-only-rag-pipeline-bug-fixes]\n",
            "- File upload [191365 bytes], throughput: 1957133.594 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:32.960Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:09.863057, used credits [0.03800000]\n",
            "- CONTENT [da1c0b2c-4bbf-4467-af16-f80122aeedd3]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:32.835Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.509231, used credits [0.03800000]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:32.796Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:11.183671, used credits [0.03800000]\n",
            "- CONTENT [0ec87d20-b463-4f65-a2f4-e32bc44f1b0d]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:32.783Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:10.564121, used credits [0.03800000]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:32.734Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:11.350325, used credits [0.03800000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:31.856Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.071234, used credits [0.00076068]\n",
            "- CONTENT [23ae5fe0-5e6d-411c-b219-5ebaf593062e]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-25-support-for-mistral-large-2-and-nemo-groq-llama-3.1-models-bug-fixes]\n",
            "- File upload [196863 bytes], throughput: 2763598.435 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:31.674Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.207698, used credits [0.00093197]\n",
            "- CONTENT [51e379d2-307c-42f3-b5cf-eb3b02c6f068]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2023/july-15-support-for-sharepoint-feeds-new-conversation-features]\n",
            "- File upload [241194 bytes], throughput: 1161274.294 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:31.648Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:15.237912, used credits [0.02743459]\n",
            "- CONTENT [448971ac-c917-4d00-a846-52a5844472f9]\n",
            "\n",
            "2024-12-29T03:25:31.304Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.426518, used credits [0.03800000]\n",
            "- CONTENT [23ae5fe0-5e6d-411c-b219-5ebaf593062e]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:31.095Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.153593, used credits [0.03800000]\n",
            "- CONTENT [51e379d2-307c-42f3-b5cf-eb3b02c6f068]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:31.060Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:14.875385, used credits [0.02678189]\n",
            "- CONTENT [4f6da3ce-9feb-4ecb-9c05-dcefa5c6de6b]\n",
            "\n",
            "2024-12-29T03:25:30.934Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.375001, used credits [0.00059400]\n",
            "- CONTENT [448971ac-c917-4d00-a846-52a5844472f9]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 791.998 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:30.928Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.334728, used credits [0.00059400]\n",
            "- CONTENT [448971ac-c917-4d00-a846-52a5844472f9]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 887.287 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:30.758Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.266290, used credits [0.01488277]\n",
            "- CONTENT [bae5b595-6a1e-4dcb-95aa-3eaf6b55cc52]\n",
            "\n",
            "2024-12-29T03:25:30.653Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.181245, used credits [0.00063000]\n",
            "- CONTENT [bae5b595-6a1e-4dcb-95aa-3eaf6b55cc52]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1737.975 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:30.619Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.141968, used credits [0.00063000]\n",
            "- CONTENT [bae5b595-6a1e-4dcb-95aa-3eaf6b55cc52]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 2218.807 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:30.596Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.913198, used credits [0.01424705]\n",
            "- CONTENT [a41c7ac9-806a-4fd2-9652-759775a70d8c]\n",
            "\n",
            "2024-12-29T03:25:30.575Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.305031, used credits [0.00114000]\n",
            "- CONTENT [4f6da3ce-9feb-4ecb-9c05-dcefa5c6de6b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [570 tokens], throughput: 1868.662 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:30.566Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.307416, used credits [0.00097200]\n",
            "- CONTENT [4f6da3ce-9feb-4ecb-9c05-dcefa5c6de6b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [486 tokens], throughput: 1580.922 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:30.470Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.217172, used credits [0.00130800]\n",
            "- CONTENT [a41c7ac9-806a-4fd2-9652-759775a70d8c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [654 tokens], throughput: 3011.442 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:30.465Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.207220, used credits [0.00146200]\n",
            "- CONTENT [a41c7ac9-806a-4fd2-9652-759775a70d8c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [731 tokens], throughput: 3527.648 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:29.164Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.057846, used credits [0.00001167]\n",
            "- CONTENT [bae5b595-6a1e-4dcb-95aa-3eaf6b55cc52]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3020 bytes], throughput: 52207.134 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:28.913Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.138590, used credits [0.00002153]\n",
            "- CONTENT [a41c7ac9-806a-4fd2-9652-759775a70d8c]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5572 bytes], throughput: 40205.008 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:28.826Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.690200, used credits [0.01384556]\n",
            "- CONTENT [26deb281-fa20-4f6d-839b-31033675141b]\n",
            "\n",
            "2024-12-29T03:25:28.641Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.223834, used credits [0.00046400]\n",
            "- CONTENT [26deb281-fa20-4f6d-839b-31033675141b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 1036.483 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:28.609Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.230847, used credits [0.00046400]\n",
            "- CONTENT [26deb281-fa20-4f6d-839b-31033675141b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 1004.993 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:27.577Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.411208, used credits [0.00001074]\n",
            "- CONTENT [448971ac-c917-4d00-a846-52a5844472f9]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2779 bytes], throughput: 6758.129 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:27.574Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.171362, used credits [0.00001635]\n",
            "- CONTENT [4f6da3ce-9feb-4ecb-9c05-dcefa5c6de6b]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4231 bytes], throughput: 24690.407 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:26.739Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.052689, used credits [0.00000906]\n",
            "- CONTENT [26deb281-fa20-4f6d-839b-31033675141b]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2345 bytes], throughput: 44506.528 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:26.695Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.075322, used credits [0.00077629]\n",
            "- CONTENT [bae5b595-6a1e-4dcb-95aa-3eaf6b55cc52]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/february-2024/february-2-support-for-semantic-alerts-openai-0125-models-performance-enhancements-bug-fixes]\n",
            "- File upload [200903 bytes], throughput: 2667255.251 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:26.416Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.086800, used credits [0.00085436]\n",
            "- CONTENT [a41c7ac9-806a-4fd2-9652-759775a70d8c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/february-2024/february-21-support-for-onedrive-and-google-drive-feeds-extract-images-from-pdfs-bug-fixes]\n",
            "- File upload [221107 bytes], throughput: 2547324.472 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:26.204Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.683060, used credits [0.03800000]\n",
            "- CONTENT [bae5b595-6a1e-4dcb-95aa-3eaf6b55cc52]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:25.877Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.157830, used credits [0.03800000]\n",
            "- CONTENT [a41c7ac9-806a-4fd2-9652-759775a70d8c]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:25.341Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.171990, used credits [0.00074323]\n",
            "- CONTENT [26deb281-fa20-4f6d-839b-31033675141b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-20-support-for-medical-entities-anthropic-prompt-caching-bug-fixes]\n",
            "- File upload [192347 bytes], throughput: 1118360.882 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:24.676Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.068034, used credits [0.01452582]\n",
            "- CONTENT [0cbb6490-3cff-4aa5-8d05-b94ab7c3d531]\n",
            "\n",
            "2024-12-29T03:25:24.662Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.477058, used credits [0.03800000]\n",
            "- CONTENT [26deb281-fa20-4f6d-839b-31033675141b]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:24.547Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.273534, used credits [0.00052400]\n",
            "- CONTENT [0cbb6490-3cff-4aa5-8d05-b94ab7c3d531]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 957.834 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:24.478Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.205969, used credits [0.00052400]\n",
            "- CONTENT [0cbb6490-3cff-4aa5-8d05-b94ab7c3d531]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 1272.034 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:23.975Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.138351, used credits [0.01465242]\n",
            "- CONTENT [a540acea-c8ed-4352-a5cc-91c3dbe4ba91]\n",
            "\n",
            "2024-12-29T03:25:23.869Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.470469, used credits [0.00048400]\n",
            "- CONTENT [a540acea-c8ed-4352-a5cc-91c3dbe4ba91]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 514.381 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:23.728Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.340194, used credits [0.00048400]\n",
            "- CONTENT [a540acea-c8ed-4352-a5cc-91c3dbe4ba91]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 711.358 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:22.586Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.075859, used credits [0.00000946]\n",
            "- CONTENT [0cbb6490-3cff-4aa5-8d05-b94ab7c3d531]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2447 bytes], throughput: 32257.257 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:21.959Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.082560, used credits [0.00000811]\n",
            "- CONTENT [a540acea-c8ed-4352-a5cc-91c3dbe4ba91]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2098 bytes], throughput: 25411.945 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:21.184Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.168836, used credits [0.00076603]\n",
            "- CONTENT [448971ac-c917-4d00-a846-52a5844472f9]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-9-support-direct-text-markdown-and-html-ingestion-new-specification-llm-strategy]\n",
            "- File upload [198247 bytes], throughput: 1174195.849 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:21.133Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.079115, used credits [0.00081452]\n",
            "- CONTENT [4f6da3ce-9feb-4ecb-9c05-dcefa5c6de6b]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-3-new-data-model-for-observations-new-category-entity]\n",
            "- File upload [210797 bytes], throughput: 2664424.372 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:20.725Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.082225, used credits [0.00073970]\n",
            "- CONTENT [0cbb6490-3cff-4aa5-8d05-b94ab7c3d531]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-11-support-for-azure-ai-document-intelligence-by-default-language-aware-summaries]\n",
            "- File upload [191434 bytes], throughput: 2328164.203 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:20.609Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.387832, used credits [0.03800000]\n",
            "- CONTENT [4f6da3ce-9feb-4ecb-9c05-dcefa5c6de6b]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:20.580Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.130610, used credits [0.03800000]\n",
            "- CONTENT [448971ac-c917-4d00-a846-52a5844472f9]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:20.184Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.547575, used credits [0.03800000]\n",
            "- CONTENT [0cbb6490-3cff-4aa5-8d05-b94ab7c3d531]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:19.962Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:11.007626, used credits [0.01981831]\n",
            "- CONTENT [39b8a12e-de8a-41c9-9224-4b0e8e0909d8]\n",
            "\n",
            "2024-12-29T03:25:19.942Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.112759, used credits [0.00071997]\n",
            "- CONTENT [a540acea-c8ed-4352-a5cc-91c3dbe4ba91]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-17-prepare-for-usage-based-billing-append-sas-tokens-to-uris]\n",
            "- File upload [186328 bytes], throughput: 1652438.732 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:19.833Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.311897, used credits [0.00137200]\n",
            "- CONTENT [39b8a12e-de8a-41c9-9224-4b0e8e0909d8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [686 tokens], throughput: 2199.445 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:19.833Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.302955, used credits [0.00155000]\n",
            "- CONTENT [39b8a12e-de8a-41c9-9224-4b0e8e0909d8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [775 tokens], throughput: 2558.134 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:19.436Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.575911, used credits [0.03800000]\n",
            "- CONTENT [a540acea-c8ed-4352-a5cc-91c3dbe4ba91]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:18.304Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.057508, used credits [0.00002253]\n",
            "- CONTENT [39b8a12e-de8a-41c9-9224-4b0e8e0909d8]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5830 bytes], throughput: 101376.847 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:17.230Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.547881, used credits [0.01538975]\n",
            "- CONTENT [cd1c5052-f3bb-4747-b785-5e83ed6dfd80]\n",
            "\n",
            "2024-12-29T03:25:16.954Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.600978, used credits [0.00098000]\n",
            "- CONTENT [cd1c5052-f3bb-4747-b785-5e83ed6dfd80]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [490 tokens], throughput: 815.338 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:16.737Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.375483, used credits [0.00120000]\n",
            "- CONTENT [cd1c5052-f3bb-4747-b785-5e83ed6dfd80]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [600 tokens], throughput: 1597.942 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:15.273Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.057069, used credits [0.00001627]\n",
            "- CONTENT [cd1c5052-f3bb-4747-b785-5e83ed6dfd80]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4211 bytes], throughput: 73787.742 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:14.045Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.248934, used credits [0.00087587]\n",
            "- CONTENT [39b8a12e-de8a-41c9-9224-4b0e8e0909d8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/april-2024/april-7-support-for-discord-feeds-cohere-reranking-section-aware-chunking-and-retrieval]\n",
            "- File upload [226675 bytes], throughput: 910581.262 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:13.380Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.347172, used credits [0.03800000]\n",
            "- CONTENT [39b8a12e-de8a-41c9-9224-4b0e8e0909d8]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:13.089Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.122788, used credits [0.00082967]\n",
            "- CONTENT [cd1c5052-f3bb-4747-b785-5e83ed6dfd80]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/april-2024/april-23-support-for-python-and-typescript-sdks-latest-openai-cohere-and-groq-models-bug-fixes]\n",
            "- File upload [214719 bytes], throughput: 1748691.244 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:12.552Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.830855, used credits [0.03800000]\n",
            "- CONTENT [cd1c5052-f3bb-4747-b785-5e83ed6dfd80]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:25:12.497Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:11.817864, used credits [0.02127708]\n",
            "- CONTENT [e46ab310-60bd-439d-9471-0cfebd1843cd]\n",
            "\n",
            "2024-12-29T03:25:12.377Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.343611, used credits [0.00100200]\n",
            "- CONTENT [e46ab310-60bd-439d-9471-0cfebd1843cd]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [501 tokens], throughput: 1458.043 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:12.280Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.241066, used credits [0.00121800]\n",
            "- CONTENT [e46ab310-60bd-439d-9471-0cfebd1843cd]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [609 tokens], throughput: 2526.281 tokens/sec\n",
            "\n",
            "2024-12-29T03:25:10.710Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.067037, used credits [0.00001613]\n",
            "- CONTENT [e46ab310-60bd-439d-9471-0cfebd1843cd]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4174 bytes], throughput: 62264.216 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:07.216Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.439890, used credits [0.00079225]\n",
            "- CONTENT [e46ab310-60bd-439d-9471-0cfebd1843cd]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/]\n",
            "- File upload [205033 bytes], throughput: 466100.934 bytes/sec\n",
            "\n",
            "2024-12-29T03:25:06.219Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.244571, used credits [0.03800000]\n",
            "- CONTENT [e46ab310-60bd-439d-9471-0cfebd1843cd]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2024-12-29T03:24:14.221Z: GraphQL\n",
            "- Operation took 0:00:01.241637, used credits [0.00000000]\n",
            "- Request:\n",
            "mutation CreateFeed($feed: FeedInput!, $correlationId: String) { createFeed(feed: $feed, correlationId: $correlationId) { id name state type } }\n",
            "- Variables:\n",
            "{\"feed\":\"{ name: \\\"https:\\\\/\\\\/changelog.graphlit.dev\\\", type: WEB, web: { uri: \\\"https:\\\\/\\\\/changelog.graphlit.dev\\\", readLimit: 100 } }\",\"correlationId\":\"\\\"2024-12-29T03:24:12.922815\\\"\"}\n",
            "- Response:\n",
            "{\"data\":{\"createFeed\":{\"id\":\"b3609ece-d81d-4a38-b359-30951f4a1931\",\"name\":\"https://changelog.graphlit.dev\",\"state\":\"ENABLED\",\"type\":\"WEB\"}}}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "credits = await lookup_credits(publish_correlation_id)\n",
        "\n",
        "if credits is not None:\n",
        "    display(Markdown(f\"### Credits used: {credits.credits:.6f} for publishing\"))\n",
        "    print(f\"- storage [{credits.storage_ratio:.2f}%], compute [{credits.compute_ratio:.2f}%]\")\n",
        "    print(f\"- embedding [{credits.embedding_ratio:.2f}%], completion [{credits.completion_ratio:.2f}%]\")\n",
        "    print(f\"- ingestion [{credits.ingestion_ratio:.2f}%], indexing [{credits.indexing_ratio:.2f}%], preparation [{credits.preparation_ratio:.2f}%], extraction [{credits.extraction_ratio:.2f}%], enrichment [{credits.enrichment_ratio:.2f}%], publishing [{credits.publishing_ratio:.2f}%]\")\n",
        "    print(f\"- search [{credits.search_ratio:.2f}%], conversation [{credits.conversation_ratio:.2f}%]\")\n",
        "    print()\n",
        "\n",
        "usage = await lookup_usage(publish_correlation_id)\n",
        "\n",
        "if usage is not None:\n",
        "    display(Markdown(f\"### Usage records:\"))\n",
        "\n",
        "    for record in usage:\n",
        "        dump_usage_record(record)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0nqb7p9Rrr1b",
        "outputId": "e6469f21-9f86-478b-b0c8-c5ce396e4db2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Credits used: 57.772510 for publishing"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- storage [0.25%], compute [0.09%]\n",
            "- embedding [0.02%], completion [26.31%]\n",
            "- ingestion [0.00%], indexing [0.00%], preparation [2.55%], extraction [0.00%], enrichment [0.00%], publishing [70.75%]\n",
            "- search [0.02%], conversation [0.00%]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Usage records:"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-29T03:29:18.453Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:16.635641, used credits [0.02995108]\n",
            "- CONTENT [3932f651-18ae-40d1-b548-51b6612ec4d8]\n",
            "\n",
            "2024-12-29T03:29:18.070Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.573852, used credits [0.00410200]\n",
            "- CONTENT [3932f651-18ae-40d1-b548-51b6612ec4d8]: Content type [FILE], file type [AUDIO]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [2051 tokens], throughput: 3574.094 tokens/sec\n",
            "\n",
            "2024-12-29T03:29:17.932Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.469946, used credits [0.00411200]\n",
            "- CONTENT [3932f651-18ae-40d1-b548-51b6612ec4d8]: Content type [FILE], file type [AUDIO]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [2056 tokens], throughput: 4374.972 tokens/sec\n",
            "\n",
            "2024-12-29T03:29:17.020Z: GraphQL\n",
            "- Operation took 0:02:58.736666, used credits [0.00000000]\n",
            "- Request:\n",
            "mutation PublishContents($summaryPrompt: String, $publishPrompt: String!, $connector: ContentPublishingConnectorInput!, $filter: ContentFilter, $isSynchronous: Boolean, $correlationId: String, $name: String, $summarySpecification: EntityReferenceInput, $publishSpecification: EntityReferenceInput, $workflow: EntityReferenceInput) { publishContents(summaryPrompt: $summaryPrompt, publishPrompt: $publishPrompt, connector: $connector, filter: $filter, isSynchronous: $isSynchronous, correlationId: $correlationId, name: $name, summarySpecification: $summarySpecification, publishSpecification: $publishSpecification, workflow: $workflow) { content { id name state originalDate identifier markdown uri type fileType mimeType format formatName fileExtension fileName fileSize masterUri imageUri textUri audioUri transcriptUri summary customSummary keywords bullets headlines posts chapters questions video { width height duration make model software title description keywords author } audio { keywords author series episode episodeType season publisher copyright genre title description bitrate channels sampleRate bitsPerSample duration } image { width height resolutionX resolutionY bitsPerComponent components projectionType orientation description make model software lens focalLength exposureTime fNumber iso heading pitch } document { title subject summary author publisher description keywords pageCount worksheetCount slideCount wordCount lineCount paragraphCount isEncrypted hasDigitalSignature } } details { contents { id } summaries { index relevance chunks { index pageIndex rowIndex columnIndex confidence text role relevance } } text textType summarySpecification publishSpecification summaryTime publishTime } } }\n",
            "- Variables:\n",
            "{\"summaryPrompt\":\"\\\"\\\\nYou are an AI assistant that extracts the most important information from product changelog pages.\\\\n\\\\nYou are being provided a changelog web page for one of many releases of the Graphlit Platform in 2024.\\\\n\\\\nYour task is to produce a concise summary that covers:\\\\n\\\\nNew Features ‚Äì Briefly list or describe each new capability.\\\\nEnhancements\\\\/Improvements ‚Äì Any notable improvements or changes.\\\\nBug Fixes ‚Äì Summaries of what was fixed and why it matters.\\\\nOther Key Details ‚Äì Any version numbers, feature flags, or breaking changes.\\\\nDates - When a feature was released\\\\nValue - What this offers to developers.\\\\nKeep it succinct, accurate, and organized. Use short sentences or bullet points so it‚Äôs easy to incorporate into a map\\\\/reduce pipeline. Omit any superfluous text.\\\\n\\\\nOutput:\\\\nA concise summary in bullet points highlighting the essential updates from the changelog.\\\\n\\\"\",\"publishPrompt\":\"\\\"\\\\nYou are an enthusiastic host focused on developer marketing, and you work for Graphlit who is creating a 2024 year-in-review of their API-based platform.\\\\n\\\\nDon't refer to yourself in the script. Just talk to the audience.\\\\n\\\\nDon't add in any podcast-like references like intro music, sound effects, etc.  This will be used with a text-to-speech API to generate an audio recording.\\\\n\\\\nYour audience is somewhat technical ‚Äî software engineers, product builders, and tech-savvy product managers ‚Äî so the script should be clear, concise, and sprinkled with a bit of technical depth.\\\\n\\\\nUsing the provided changelog for the Graphlit Platform, create a podcast-like script that:\\\\n\\\\n- Sets the stage with a warm, engaging introduction.\\\\n- Highlights each new feature, explaining how it helps developers or teams be more productive, efficient, or creative.\\\\n- Refers to when a feature was released.\\\\n- Mentions any model updates and why they matter for technical use cases.\\\\n- Reviews notable bug fixes, providing just enough context to show the improvements without overwhelming detail.\\\\n- Closes with a quick recap and a call to action, encouraging listeners to try out the new features or learn more.\\\\n\\\\nAt the very end, mention that the listener can signup for free at graphlit.com and try out all these features.\\\\nAlso, mention that in 2025, Graphlit will be offering exciting new features to accelerate the building of AI agents.\\\\n\\\\nThe tone should be friendly, positive, and confident‚Äîlike a technology evangelist who‚Äôs genuinely excited about these updates.\\\\n\\\\nKeep it interesting and conversational, but maintain enough depth to engage developers who care about how things work under the hood.\\\\nUse analogies or practical examples to illustrate why certain features are useful.\\\\nFeel free to add transitions such as ‚ÄúNow, let‚Äôs dive in,‚Äù or ‚ÄúMoving on to our next highlight‚Äù to keep it flowing.\\\\n\\\\nOutput: A detailed, TTS-ready 10-minute long script that hits all the points above.\\\\n\\\"\",\"connector\":\"{ type: ELEVEN_LABS_AUDIO, format: MP3, elevenLabs: { model: TURBO_V2_5, voice: \\\"ZF6FPAbjXT4488VcRRnw\\\" } }\",\"filter\":\"{ feeds: [ { id: \\\"b3609ece-d81d-4a38-b359-30951f4a1931\\\" } ] }\",\"isSynchronous\":\"true\",\"correlationId\":\"\\\"2024-12-29T03:24:12.922903\\\"\",\"name\":\"\\\"Published Summary\\\"\",\"summarySpecification\":\"{ id: \\\"7cf625a7-1add-4983-8c19-c15328c9aa1f\\\" }\",\"publishSpecification\":\"{ id: \\\"a58f46d3-59d6-4727-959f-06e57ca42611\\\" }\"}\n",
            "- Response:\n",
            "{\"data\":{\"publishContents\":{\"content\":{\"id\":\"3932f651-18ae-40d1-b548-51b6612ec4d8\",\"name\":\"Published Summary.mp3\",\"state\":\"FINISHED\",\"originalDate\":null,\"identifier\":\"8c5375c6-259e-48e3-80be-f5f846d22202\",\"markdown\":\"[00:00:00] Hello, everyone,\\n\\n[00:00:01] and welcome to a special year in review for the Graphlet platform.\\n\\n[00:00:06] It's been an incredible stretch of milestones\\n\\n[00:00:10] and enhancements\\n\\n[00:00:11] all designed to make your development workflow simpler,\\n\\n[00:00:14] more powerful,\\n\\n[00:00:15] and more creative.\\n\\n[00:00:18] Let's explore how each month's updates can help you, whether you're coding in Python, dot net, JavaScript, or any other environment.\\n\\n[00:00:27] Let's start back in July 2023.\\n\\n[00:00:31] One key addition was SharePoint feed support, allowing direct file ingestion from SharePoint libraries.\\n\\n[00:00:37] This came with new conversation features, such as timestamped messages\\n\\n[00:00:42] to help keep your discussions organized and relevant.\\n\\n[00:00:45] These upgrades reduced friction when pulling in data from corporate SharePoint,\\n\\n[00:00:50] making you more efficient in building knowledge based applications.\\n\\n[00:00:54] Moving into August 2023,\\n\\n[00:00:56] Grafler introduced a new data model for observations,\\n\\n[00:01:00] enabling more structured ways to track entities like people or organizations.\\n\\n[00:01:06] Then came a direct text ingestion for plain text, markdown,\\n\\n[00:01:10] or HTML,\\n\\n[00:01:11] eliminating the need for a file or URL.\\n\\n[00:01:14] This is perfect when you just want to send raw data into Graphlet and get back structured content or embeddings for analysis.\\n\\n[00:01:23] Also, in August, usage based billing infrastructure was rolled out.\\n\\n[00:01:27] While still offering a free tier to get started right away, it set the stage for future expansions of credit based\\n\\n[00:01:34] usage, helpful for devs who prefer pay as you go models.\\n\\n[00:01:39] September 2023\\n\\n[00:01:40] ushered in a wave of productivity features.\\n\\n[00:01:43] Workflow configuration\\n\\n[00:01:44] let you define how your content passes through ingestion,\\n\\n[00:01:48] enrichment, or summarization\\n\\n[00:01:49] steps with support for Notion feeds to quickly import your team's notes and pages.\\n\\n[00:01:56] Paid subscription plans also launched, providing hobby, starter,\\n\\n[00:02:00] and growth tiers,\\n\\n[00:02:02] each with different quotas.\\n\\n[00:02:04] For those who love video content, YouTube feed support arrived,\\n\\n[00:02:09] complete with automatic transcription for ingesting the audio.\\n\\n[00:02:13] Now your dev team can treat video content just like any text document\\n\\n[00:02:18] and search or summarize it with ease.\\n\\n[00:02:21] October\\n\\n[00:02:22] 2023\\n\\n[00:02:23] focused on deeper language model integrations.\\n\\n[00:02:26] Anthropic Claude models joined the party for advanced text completions,\\n\\n[00:02:30] while Slack feed ingestion\\n\\n[00:02:33] made it easier for your dev teams to bring chat messages\\n\\n[00:02:36] and file attachments\\n\\n[00:02:37] into the knowledge base.\\n\\n[00:02:40] The platform also\\n\\n[00:02:41] expanded entity enrichment,\\n\\n[00:02:44] letting you layer in details from sources like Wikipedia\\n\\n[00:02:47] or Crunchbase.\\n\\n[00:02:49] By the end of the month,\\n\\n[00:02:51] conversation responses were further optimized, and you could add aliases to your observed entities,\\n\\n[00:02:56] especially handy when there are multiple ways to refer to the same person or place.\\n\\n[00:03:02] December\\n\\n[00:03:03] 2023\\n\\n[00:03:03] brought some major model updates.\\n\\n[00:03:06] The platform introduced OpenAI GPT 4 Turbo with a 128\\n\\n[00:03:11] k token window and also added llama 2, Mistral, and anthropic Claude 2.1.\\n\\n[00:03:19] All these models expand your options for generating text,\\n\\n[00:03:22] summarizing content, or performing semantic searches.\\n\\n[00:03:27] There was also support for query by example,\\n\\n[00:03:30] letting you simply submit a sample snippet of text or conversation to find related content.\\n\\n[00:03:37] Jumping into January 2024,\\n\\n[00:03:39] new content publishing features transformed how you repurpose\\n\\n[00:03:43] and summarize documents,\\n\\n[00:03:45] audio transcripts,\\n\\n[00:03:46] and images.\\n\\n[00:03:49] LLM tools, including function calls in OpenAI models,\\n\\n[00:03:53] make it simpler to integrate prompt driven tasks directly into your code.\\n\\n[00:03:59] Additionally, Google and Microsoft email feeds became 1st class citizens,\\n\\n[00:04:03] enabling ingestion of both historic and new emails,\\n\\n[00:04:07] attachments included.\\n\\n[00:04:10] That month also introduced reingestion capabilities\\n\\n[00:04:13] so you can update existing content in place with minimal fuss.\\n\\n[00:04:18] February\\n\\n[00:04:19] 2024\\n\\n[00:04:20] introduced semantic alerts\\n\\n[00:04:22] for automatically generating daily or periodic summaries,\\n\\n[00:04:26] perfect for receiving quick bulletins on everything new in your workspace.\\n\\n[00:04:31] OneDrive and Google Drive feed support was expanded,\\n\\n[00:04:35] including the ability to ingest images\\n\\n[00:04:38] from PDFs or to capture embedded image\\n\\n[00:04:41] images in your documents.\\n\\n[00:04:43] These features cut down on tedious overhead\\n\\n[00:04:46] so you can focus on building robust apps.\\n\\n[00:04:50] March 2 24 saw comprehensive usage and credits telemetry\\n\\n[00:04:55] so you can monitor precisely how many tokens or data credits your team is using.\\n\\n[00:05:01] For devs who prefer command line utilities,\\n\\n[00:05:04] a new CLI tool was released for efficient graphlet data\\n\\n[00:05:08] API interactions.\\n\\n[00:05:10] Further in March came support for additional issue tracking feeds,\\n\\n[00:05:14] linear, GitHub issues, and Jira,\\n\\n[00:05:17] turning your project tickets into fully searchable items.\\n\\n[00:05:21] And for file management, you could now ingest base 64 encoded files directly.\\n\\n[00:05:28] April 2024\\n\\n[00:05:30] was all about chat and ingestion flows.\\n\\n[00:05:33] Discord feed support arrived,\\n\\n[00:05:35] letting you bring chat messages\\n\\n[00:05:37] and attached media directly into Graphlet.\\n\\n[00:05:40] Cohere re ranking and improved text chunking\\n\\n[00:05:43] enhanced the retrieval augmented generation pipeline.\\n\\n[00:05:47] Most notably, native SDKs in Python and TypeScript\\n\\n[00:05:51] came out later in April.\\n\\n[00:05:53] Code generated from the GraphQL schema, so you no longer need to be a GraphQL expert to start building.\\n\\n[00:06:00] There were also brand new model integrations for OpenAI's\\n\\n[00:06:04] g GPT\\n\\n[00:06:05] 4 expansions\\n\\n[00:06:07] as well as support for advanced\\n\\n[00:06:09] and coherent models.\\n\\n[00:06:11] May 2024\\n\\n[00:06:13] introduced new re ranking services from Jina and Pongo,\\n\\n[00:06:17] plus the brand new graphRAG feature,\\n\\n[00:06:19] a system that harnesses extracted entities to supercharge your your retrieval augmented generation workflow.\\n\\n[00:06:27] Open AI GPT 4 o replaced Azure Open AI GPT 3.5,\\n\\n[00:06:33] 16 k as the default model,\\n\\n[00:06:35] promising a more robust experience.\\n\\n[00:06:38] For Microsoft Teams fans, feed support allowed you to ingest channel messages automatically\\n\\n[00:06:44] while performance optimizations\\n\\n[00:06:46] in entity extraction boosted speeds for large datasets.\\n\\n[00:06:51] In June 2024,\\n\\n[00:06:53] developers gained the ability to pass embedded JSON LD data from web pages,\\n\\n[00:06:59] enriching\\n\\n[00:07:00] your knowledge graph with schema.org\\n\\n[00:07:02] or other structured data.\\n\\n[00:07:05] Deep seek model support was introduced. So\\n\\n[00:07:08] if you prefer their AI completions and coders,\\n\\n[00:07:11] you have that option too.\\n\\n[00:07:14] Then in late June came the Claude 3.5 Sonnet model and semantic search for observable entities,\\n\\n[00:07:21] a boon for advanced knowledge graph queries when dealing with people,\\n\\n[00:07:26] organizations,\\n\\n[00:07:27] or places.\\n\\n[00:07:29] July 2024\\n\\n[00:07:31] was packed.\\n\\n[00:07:33] Webhook alerts went live,\\n\\n[00:07:35] enabling automatic notifications when new content is published,\\n\\n[00:07:39] and the deep seek 128 k token context was introduced\\n\\n[00:07:43] to handle giant transcripts or text blocks in a single pass.\\n\\n[00:07:48] Then soon after, gpt4\\n\\n[00:07:50] o Mini arrived,\\n\\n[00:07:52] offering a more lightweight\\n\\n[00:07:54] gpt4\\n\\n[00:07:55] variant for faster completions.\\n\\n[00:07:57] Updates to Mistral Large 2 and Nemo\\n\\n[00:08:00] plus new GrokLama\\n\\n[00:08:02] 3.1 models delivered even more alternatives to handle your AI workloads.\\n\\n[00:08:07] The next update introduced an indexing workflow stage and Azure AI language detection\\n\\n[00:08:14] so you can automatically detect and store the languages a piece of content is written in.\\n\\n[00:08:20] In August 2024,\\n\\n[00:08:22] the platform unveiled an open source dotnet SDK on Nougat,\\n\\n[00:08:26] giving\\n\\n[00:08:27] Csearchiles developers\\n\\n[00:08:28] first class\\n\\n[00:08:30] access to the Graphlet APIs.\\n\\n[00:08:33] Document preparation\\n\\n[00:08:34] switched to Azure AI document intelligence by default,\\n\\n[00:08:38] boosting accuracy for complex PDFs.\\n\\n[00:08:41] Medical entities were also added,\\n\\n[00:08:44] letting you extract detailed references\\n\\n[00:08:46] to medical conditions,\\n\\n[00:08:48] drugs, or procedures.\\n\\n[00:08:50] Meanwhile, anthropic prompt caching\\n\\n[00:08:53] reduced token costs,\\n\\n[00:08:55] speeding up repeated queries.\\n\\n[00:08:58] September 24 stepped up the AI model offerings further with FHIR enrichment for health care data and the newly updated Coherent models.\\n\\n[00:09:08] Google AI search feeds and Cerebras model support also landed,\\n\\n[00:09:12] plus more advanced grok llama versions.\\n\\n[00:09:15] By now, you could combine advanced conversation prompts with fallback retrieval of relevant content,\\n\\n[00:09:22] ensuring your AI app stays robust even if the main model hits a snag.\\n\\n[00:09:26] October 2, 2024 introduced a flurry of tool calling capabilities\\n\\n[00:09:31] across Anthropic, Google Gemini, Mistral, and more.\\n\\n[00:09:36] This let large language models handle tasks by calling external tools in real time, bridging the gap between pure text generation\\n\\n[00:09:45] and practical app logic.\\n\\n[00:09:47] GitHub repository feeds also debuted so you can auto ingest code files\\n\\n[00:09:52] while conversation interactions got new ways to simulate tool calls or to pass tool responses from JSON.\\n\\n[00:10:00] November\\n\\n[00:10:01] 2024\\n\\n[00:10:02] delivered web search features.\\n\\n[00:10:04] Imagine searching the open web,\\n\\n[00:10:06] retrieving\\n\\n[00:10:07] relevant pages,\\n\\n[00:10:09] and combining them with your content all in one shot.\\n\\n[00:10:14] Multi turn text summarization\\n\\n[00:10:16] and multi turn image analysis\\n\\n[00:10:18] made it possible to revise or refine a summary\\n\\n[00:10:22] over a series of LLM prompts.\\n\\n[00:10:24] Deepgram language detection took speech transcription to the next level,\\n\\n[00:10:29] and feed management was enhanced so you can control usage or upgrade easily when you run out of free tier credits.\\n\\n[00:10:36] Finally, December 2024\\n\\n[00:10:39] concluded with major expansions in the retrieval pipeline.\\n\\n[00:10:43] You can run a retrieval only rag process or map entire websites with the new site mapping features. Summaries and bullet points are more accurate than ever with refined chunking strategies.\\n\\n[00:10:54] And let's not forget the big arrival\\n\\n[00:10:56] of new model support for Grok Yammer\\n\\n[00:10:59] 3.3\\n\\n[00:11:00] or the ability to handle advanced content from Dropbox,\\n\\n[00:11:04] Box, Intercom,\\n\\n[00:11:05] Zendesk, and beyond.\\n\\n[00:11:08] Throughout these releases, loads of bugs were squashed\\n\\n[00:11:11] from synchronizing your search index in real time\\n\\n[00:11:14] to extracting images from PDFs\\n\\n[00:11:16] to properly handling text from every corner of your data sources.\\n\\n[00:11:21] Each fix was about giving you a more stable and confident development experience.\\n\\n[00:11:27] In short, the Graphlet platform has advanced leaps and bounds this past year.\\n\\n[00:11:33] From brand new SDKs\\n\\n[00:11:35] and improved content ingestion\\n\\n[00:11:37] to powerful LLM integrations\\n\\n[00:11:40] and next level conversation features, there's something here for every team building with AI.\\n\\n[00:11:46] The best way to learn more?\\n\\n[00:11:48] Try it. You can sign up for free right now at graphlet.com\\n\\n[00:11:53] and explore all these capabilities for yourself.\\n\\n[00:11:57] And here's one last bit of news.\\n\\n[00:12:00] 2025\\n\\n[00:12:01] is right around the corner,\\n\\n[00:12:03] and the team is preparing even more excitement,\\n\\n[00:12:07] especially for building, testing, and deploying AI agents.\\n\\n[00:12:13] Keep an eye out for those features because they're going to transform\\n\\n[00:12:17] how you build intelligent\\n\\n[00:12:20] automated systems.\\n\\n[00:12:22] Thanks for tuning in to this whirlwind tour of Graphlet's 24 updates.\\n\\n[00:12:27] Good luck with your next AI driven project,\\n\\n[00:12:30] and we can't wait to see the amazing creations you'll build with these new tools.\\n\\n[00:12:36] Enjoy exploring the platform,\\n\\n[00:12:38] and happy coding.\\n\\n\",\"uri\":\"https://graphlit20241212dc396403.blob.core.windows.net/files/3932f651-18ae-40d1-b548-51b6612ec4d8/Published%20Summary.mp3\",\"type\":\"FILE\",\"fileType\":\"AUDIO\",\"mimeType\":\"audio/mp3\",\"format\":null,\"formatName\":\"MP3\",\"fileExtension\":\".mp3\",\"fileName\":null,\"fileSize\":12168045,\"masterUri\":null,\"imageUri\":null,\"textUri\":null,\"audioUri\":null,\"transcriptUri\":null,\"summary\":null,\"customSummary\":null,\"keywords\":null,\"bullets\":null,\"headlines\":null,\"posts\":null,\"chapters\":null,\"questions\":null,\"video\":null,\"audio\":{\"keywords\":null,\"author\":null,\"series\":null,\"episode\":null,\"episodeType\":null,\"season\":null,\"publisher\":null,\"copyright\":null,\"genre\":null,\"title\":null,\"description\":null,\"bitrate\":128000,\"channels\":1,\"sampleRate\":44100,\"bitsPerSample\":null,\"duration\":\"PT12M40.5028125S\"},\"image\":null,\"document\":null},\"details\":null}}}\n",
            "\n",
            "2024-12-29T03:29:15.687Z: Upload Transcript\n",
            "- Workflow [Preparation] took 0:00:00.054209, used credits [0.00018232]\n",
            "- CONTENT [3932f651-18ae-40d1-b548-51b6612ec4d8]: Content type [FILE], file type [DATA]\n",
            "- File upload [47183 bytes], throughput: 870395.342 bytes/sec\n",
            "\n",
            "2024-12-29T03:29:15.472Z: Processed Transcript\n",
            "- Workflow [Preparation] took 0:00:05.059055, used credits [1.09005287]\n",
            "- CONTENT [3932f651-18ae-40d1-b548-51b6612ec4d8]: Content type [FILE], file type [DATA]\n",
            "- Processor name [Deepgram Audio Transcription], model name [nova-2-general], length [0:12:40.502000]\n",
            "\n",
            "2024-12-29T03:29:09.855Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.913434, used credits [0.04701733]\n",
            "- CONTENT [3932f651-18ae-40d1-b548-51b6612ec4d8]: Content type [FILE], file type [AUDIO]\n",
            "- File upload [12168045 bytes], throughput: 13321207.299 bytes/sec\n",
            "\n",
            "2024-12-29T03:29:01.337Z: Upload Master\n",
            "- Workflow [Publishing] took 0:00:01.157861, used credits [0.04701733]\n",
            "- CONTENT [3932f651-18ae-40d1-b548-51b6612ec4d8]: Content type [FILE], file type [AUDIO]\n",
            "- File upload [12168045 bytes], throughput: 10509068.702 bytes/sec\n",
            "\n",
            "2024-12-29T03:28:59.995Z: Audio publishing\n",
            "- Workflow [Publishing] took 0:00:21.804562, used credits [45.30600000]\n",
            "- Processor name [ElevenLabs], model name [Turbo_V2_5], units [5034]\n",
            "\n",
            "2024-12-29T03:28:37.973Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:01:32.760068, used credits [10.97970000]\n",
            "- Model service [OpenAI], model name [O1_200k]\n",
            "- Prompt [16643 tokens (includes RAG context tokens)]:\n",
            "<source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/april-2024/april-23-support-for-python-and-typescript-sdks-latest-openai-cohere-and-groq-models-bug-fixes</name><title>April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Native Python SDK introduced, using Pydantic types; code-generated from GraphQL schema, no GraphQL knowledge required.\n",
            "  - Native Node.js SDK introduced, using TypeScript types; code-generated from GraphQL schema, no GraphQL knowledge required.\n",
            "  - Support for OpenAI's 2024-04-09 models, including GPT4_TURBO-128K.\n",
            "  - Support for LLaMA3 70b, LLaMA3 8b, and Gemma 7b models in Groq model service.\n",
            "  - Support for Command R and Command-R+ models in Cohere model service.\n",
            "  - Added support for Jina reranking with the JINA reranking model service type.\n",
            "  - Updated Cohere reranking model to the latest v3.0.\n",
            "  - Improved reliability of parsing LLM responses that do not follow JSON schema.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Cleaned up nullability of GraphQL parameters for better clarity on required, optional, or nullable parameters.\n",
            "  - Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "  - Split reranking model service type into RetrievalModelServiceTypes enum.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where adding content to collections did not sync with the search index (GPLA-2114).\n",
            "  - Resolved rendering issues with conversation sources in section retrieval and text content (GPLA-2511).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: April 23, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced SDK support, improved model integration, and better handling of LLM responses, facilitating easier development and implementation.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/</name><title>December 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for LLM fallbacks to mitigate model provider downtime.\n",
            "  - New API query for all available models, providing details like model enum and service type.\n",
            "  - Ingestion of native Google Docs, Sheets, and Slides from Google Drive, auto-exported to Microsoft Office formats.\n",
            "  - Website unblocking support for sites using Cloudflare, enabled via the PreparationWorkflowStage.\n",
            "  - Ability to assign observations (Labels, Organizations) to ingested content without entity extraction.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved content ingestion process with new observation assignment features.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed table headers merging issue during web scraping.\n",
            "  - Resolved PDF extraction failure with empty hyperlink text.\n",
            "  - Corrected handling of empty observables for reranking.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 27, 2024.\n",
            "  - Additional costs for unblocked website ingestion.\n",
            "\n",
            "- Value:\n",
            "  - Enhances application reliability, expands content ingestion capabilities, and improves user experience with bug fixes.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-22-support-for-dropbox-box-intercom-and-zendesk-feeds-openai-o1-gemini-2.0-bug-fixes</name><title>December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for Dropbox feeds for ingesting files; requires appKey, appSecret, redirectUri, and refreshToken.\n",
            "  - Support for Box feeds for ingesting files; requires clientId, clientSecret, redirectUri, and refreshToken.\n",
            "  - Support for Intercom feeds for ingesting Articles and Tickets; requires accessToken.\n",
            "  - Support for Zendesk feeds for ingesting Articles and Tickets; requires accessToken and Zendesk subdomain.\n",
            "  - Support for OpenAI o1 model with enums O1_200k and O1_200k_20241217.\n",
            "  - Support for Gemini Flash 2.0 Experimental model with enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "  - Support for Cohere R7B model with enum COMMAND_R7B_202412.\n",
            "  - Ability to return low-level details from RAG conversations with includeDetails parameter.\n",
            "  - Support for filtering observables by URI property.\n",
            "  - Ability to bypass semantic search in content retrieval with conversations.\n",
            "  - New createdInLast property for entity filters and inLast property for content filters.\n",
            "  - Support for Azure AI Document Intelligence models: US_PAY_STUB, US_BANK_STATEMENT, US_BANK_CHECK.\n",
            "  - Support for Google Drive and OneDrive feeds to ingest specific files by file identifiers.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Removal of content items limit for projects upgraded to the Starter Tier after Dec 9, 2024.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with multitenant content assignment, HTML character decoding, synchronous content ingestion, feed completion status, and HTTP error handling during uploads.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Version updates include support for various new models and feeds.\n",
            "  - Significant changes to content item limits for Starter Tier projects.\n",
            "\n",
            "- Dates:\n",
            "  - Features and fixes released on December 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced integration capabilities with popular services, improved content management, and flexibility in data retrieval.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-21-support-for-onedrive-and-google-drive-feeds-extract-images-from-pdfs-bug-fixes</name><title>February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for OneDrive and Google Drive feeds, allowing ingestion of files from shared drives.\n",
            "  - Support for email backup files (EML, MSG) with automatic extraction of attachments.\n",
            "  - Automatic extraction of embedded images from PDF files, linking them as children of the parent PDF.\n",
            "  - Support for recursive Notion feeds with the isRecursive flag for crawling child pages and databases.\n",
            "  - Collections can now be assigned to content during ingestion without additional mutation calls.\n",
            "  - Introduction of CODE file type for various source code formats with optimized text splitting.\n",
            "  - Custom guidance can be injected during the RAG process via the Specification object.\n",
            "  - Added tenants field to Project object for listing tenant IDs used in entity creation.\n",
            "  - Email metadata is now indexed separately from document metadata.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Contents field in content objects replaced with children and parent fields for better structure.\n",
            "  - Removed enableImageAnalysis field; it is now enabled by default.\n",
            "  - Moved disableSmartCapture field to preparation workflow stage, enabled by default.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed PDF parsing error for ArXiV PDFs (GPLA-2099).\n",
            "  - Corrected LLM response issues with conversation history and no content sources (GPLA-2174).\n",
            "  - Resolved issue of ZIP package remaining in Indexed state after content workflow (GPLA-2199).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - These updates enhance file ingestion capabilities, improve metadata handling, and streamline workflows for developers.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/january-2024/january-18-support-for-content-publishing-llm-tools-clip-image-embeddings-bug-fixes</name><title>January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for content publishing, allowing documents, audio transcripts, and image descriptions to be summarized and repurposed.\n",
            "  - New mutation for publishing conversations as content, generating text or audio transcripts.\n",
            "  - Bulk summarization of contents with the summarizeContents mutation.\n",
            "  - LLM entity extraction using MODEL_TEXT service type, returning JSON-LD entities.\n",
            "  - Support for LLM tools (function calls) with OpenAI models, including the new extractContents mutation.\n",
            "  - Callback webhooks for LLM tools, enabling interaction with external services.\n",
            "  - Selection of Deepgram models for audio transcription with custom API key support.\n",
            "  - CLIP image embeddings support for similar image search.\n",
            "  - Dynamic web page ingestion with automatic scrolling and screenshot capabilities.\n",
            "  - Table parsing for structured text extraction from documents.\n",
            "  - Reverse geocoding of lat/long locations in content metadata.\n",
            "  - Inclusion of assistant messages in conversation history for LLM prompts.\n",
            "  - New chunking algorithm for semantic text embeddings.\n",
            "  - Enhanced content metadata for text and image embeddings.\n",
            "  - Helper mutations for polling content ingestion completion.\n",
            "  - Richer image descriptions generated by GPT-4 Vision model.\n",
            "  - Validation of extracted hyperlinks to remove inaccessible links.\n",
            "  - Multi-deletion mutations for contents, feeds, and conversations.\n",
            "  - Bulk deletion mutations for filtered subsets of entities.\n",
            "  - Increased content limit for Starter tier to 100K items.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed detailMode field to detailLevel in OpenAIImageExtractionProperties.\n",
            "  - SummarizationStrategy objects now accept specifications directly.\n",
            "  - Deprecated addCollectionContents and removeCollectionContents mutations in favor of new naming conventions.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with parsing Markdown headings, returning SAS tokens, summarizing text content, and assigning error messages on workflow failures.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: January 18, 2024.\n",
            "  - Value: Offers developers enhanced content management capabilities, improved LLM interactions, and better data handling for various content types.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-2-support-for-semantic-alerts-openai-0125-models-performance-enhancements-bug-fixes</name><title>February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for Semantic Alerts for LLM summarization and content publishing on a periodic basis, useful for generating daily reports from various feeds.\n",
            "  - Support for OpenAI 0125 model versions for GPT-4 and GPT-3.5 Turbo, with plans to add Azure OpenAI support when available.\n",
            "  - Slack feeds now include a listing type field to specify PAST or NEW messages.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Performance enhancements to speed up content workflows for ingested content.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with collections not being added to text embedding index documents (GPLA-2114).\n",
            "  - Resolved handling of hallucinated citations (GPLA-2063).\n",
            "  - Fixed inheritance of collections from project-scope to tenant-scope (GPLA-1916).\n",
            "  - Added error handling for adding/removing contents to/from collections if content does not exist (GPLA-2105).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 2, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved content management capabilities, enhanced performance, and better error handling.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/january-2024/january-22-support-for-google-and-microsoft-email-feeds-reingest-content-in-place-bug-fixes</name><title>January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for Google and Microsoft email feeds, allowing ingestion of past and new emails, creating an EMAIL content type. Attachments can be extracted and linked to their parent emails.\n",
            "  - Support for reingesting content in-place with optional id parameter for existing content objects, updating them from provided text or URI source, and restarting the assigned workflow.\n",
            "  - Added restartAllContents mutation to restart workflows on all partially-ingested contents in a project.\n",
            "  - Added text field to ConversationCitation type to return relevant text from the content source with the citation.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved content ingestion capabilities with new mutations and features.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with link extraction from HTML (GPLA-1313).\n",
            "  - Resolved problem of no text being extracted from shapes in PPTX files (GPLA-2030).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: January 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced email integration and content management capabilities, improving workflow efficiency and content handling.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-19-support-for-openai-gpt-4o-mini-byo-key-for-azure-ai-similarity-by-summary-bug-fixes</name><title>July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4o Mini model with 16k output tokens.\n",
            "  - 'Bring-your-own-key' support for Azure AI Document Intelligence models with custom endpoint and key property.\n",
            "  - Updated to Jina reranker v2 by default.\n",
            "  - Enhanced summarizeContents mutation to store summaries in content.\n",
            "  - Added relevance property for entity types, sorting results by search relevance score.\n",
            "  - Ability to manually update summary and bullet properties in updateContent mutation.\n",
            "  - Added offset property to AtlassianJiraFeedProperties for proper timezone assignment in Jira feed.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Content similarity search now finds similar content by summary for better accuracy.\n",
            "  - Changed entity filter object behavior for paging; zero offset for vector/hybrid search.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Added retry on OpenAI API HTTP 524 error.\n",
            "  - Fixed paging issues in Jira feed.\n",
            "  - Improved search results for similar content in long documents.\n",
            "  - Resolved keyword search issues in long PDFs.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 19, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved model support, enhanced search capabilities, and better handling of content summaries and entity relevance.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-28-support-for-indexing-workflow-stage-azure-ai-language-detection-bug-fixes</name><title>July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Added indexing workflow stage for configuring indexing services to infer metadata from content.\n",
            "  - Introduced AZURE_AI_LANGUAGE content indexing service for inferring language from extracted text or transcripts.\n",
            "  - Support for language content metadata returning a list of languages in ISO 639-1 format.\n",
            "  - Added MODEL_IMAGE extraction service for integration with various vision models, allowing custom specifications and API keys.\n",
            "  - Deprecated OPENAI_IMAGE service type; developers should use LLM image service instead.\n",
            "  - Removed language field from AudioMetadata type, replaced by new LanguageMetadata type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2987 where Azure Doc Intelligence did not extract hyperlinks.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 28, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for content indexing, language detection, and image extraction, improving overall functionality and flexibility.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-8-support-for-llm-based-document-extraction-.net-sdk-bug-fixes</name><title>August 8: Support for LLM-based document extraction, .NET SDK, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for LLM-based document preparation using models like OpenAI GPT-4o and Anthropic Sonnet 3.5 via MODEL_DOCUMENT service.\n",
            "  - Introduction of an open source .NET SDK for .NET 6 and .NET 8, available on Nuget.org with code samples on GitHub.\n",
            "  - Added identifier property to Content object for mapping to external database identifiers, aiding in content filtering.\n",
            "  - Support for Claude 3 vision models for image-based entity extraction using MODEL_IMAGE service.\n",
            "  - Context augmentation in conversations via augmentedFilter property, allowing injection of domain knowledge into LLM prompts.\n",
            "  - Support for the latest snapshot of OpenAI GPT-4o (model enum GPT4O_128K_20240806).\n",
            "  - Reranking of related entities for LLM prompt context preparation in GraphRAG.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Changed duration field type in AudioMetadata and VideoMetadata from string to TimeSpan for consistency in API data model.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2884 to support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 8, 2024.\n",
            "\n",
            "- Value:\n",
            "  - These updates enhance document extraction capabilities, improve SDK accessibility for developers, and streamline content management and entity extraction processes.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-3-new-data-model-for-observations-new-category-entity</name><title>August 3: New data model for Observations, new Category entity | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Revised data model for Observations, Occurrences, and observables (e.g., Person, Organization).\n",
            "  - Each observed entity now has one Observation and a list of occurrences (supports text, time, and image types).\n",
            "  - Added Category entity to GraphQL data model for PII categories (e.g., Phone Number, Credit Card Number).\n",
            "  - Introduced probability field for LLM's token probability.\n",
            "  - Added error field to feeds for error descriptions on failed reads.\n",
            "  - Support for reingestion of changed files from feeds, maintaining the same ID.\n",
            "  - Ingestion of content is now idempotent, allowing reingestion without ID changes.\n",
            "  - Changed GraphQL data type for SharePoint identifiers from String to ID.\n",
            "  - Performance optimization for entity extraction and observation creation.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of content ingestion and updates.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed extraction issues from PDF and DOCX tables.\n",
            "  - Resolved issue where audio content from RSS feeds was not deleted upon feed deletion.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 3, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved data handling, enhanced entity extraction, and better error management.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-1-support-for-fhir-enrichment-latest-cohere-models-bug-fixes</name><title>September 1: Support for FHIR enrichment, latest Cohere models, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for entity enrichment from FHIR servers for medical-related entities.\n",
            "  - Added support for latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408) with datestamped model enums for previous versions.\n",
            "  - Support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31) now used by default.\n",
            "  - Renamed LinkReferenceType to LinkReference to align with data model standards.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Updated naming conventions for consistency with existing standards.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where LLM added source tags to the end of completed messages (GPLA-3120).\n",
            "  - Resolved failure to load sitemap on child page of website (GPLA-3133).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 1, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve integration with healthcare data and AI models, providing developers with more robust tools for medical entity enrichment and document intelligence.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-3-support-tool-calling-ingestbatch-mutation-gemini-flash-1.5-8b-bug-fixes</name><title>October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for ingestBatch mutation to asynchronously ingest an array of URIs into content objects.\n",
            "  - Introduction of continueConversation mutation for handling tool responses and promptConversation now accepts an array of tool definitions.\n",
            "  - Tool calling support added for OpenAI, Mistral, Deepseek, Groq, and Cerebras model services; support for Anthropic, Google Gemini, and Cohere coming later.\n",
            "  - Prefilled user and assistant messages supported with createConversation mutation, allowing an array of messages to bootstrap conversations.\n",
            "  - Added support for Google Gemini Flash 1.5 8b model.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Deprecated tools property in the Specification object; tools now sent directly to extractContents and promptConversation mutations.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where models shouldn't be required on update specification call (GPLA-3207).\n",
            "  - Resolved issue of sending system prompt with OpenAI o1 models (GPLA-3220).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Version: Gemini Flash 1.5 8b.\n",
            "  - Deprecation of tools property noted for future removal.\n",
            "\n",
            "- Dates:\n",
            "  - Release Date: October 3, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve developer experience by streamlining tool interactions and conversation management.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2023/september-20-paid-subscription-plans-support-for-custom-observed-entities-and-azure-openai-gpt-4</name><title>September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4 | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Introduced paid subscription plans: Hobby, Starter, and Growth tiers, starting at $49/month.\n",
            "  - Added GraphQL mutations for creating, updating, and deleting observed entities (Person, Organization, Place, Product, Event, Label, Category).\n",
            "  - New observed entity types: Repo (Git repo), Software.\n",
            "  - Enhanced Specification object with searchType and numberSimilar fields for improved semantic search.\n",
            "  - Support for Azure OpenAI GPT-4 model.\n",
            "  - Project quota field added, with limits based on subscription tier.\n",
            "  - ContentLimit added to conversation strategy object for semantic search results.\n",
            "  - Improved relevance ranking for semantic search results.\n",
            "  - Updated Free tier quota: 1GB storage, 100 contents, 3 feeds, and 10 conversations.\n",
            "  - Implemented Deepgram Nova-2 audio transcription model, 18% more accurate and 5-40x faster.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Better semantic search results with HYBRID search type.\n",
            "  \n",
            "- Bug Fixes:\n",
            "  - Fixed issue with extracting multiple text pages from DOCX without page breaks (GPLA-1373).\n",
            "  - Resolved semantic search failure with no content results (GPLA-1377).\n",
            "  - Fixed failure in generating text embeddings from user prompts (GPLA-1415).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 20, 2023.\n",
            "  \n",
            "- Value:\n",
            "  - Offers developers flexible subscription options, enhanced search capabilities, and improved performance with new models and features.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/april-2024/april-7-support-for-discord-feeds-cohere-reranking-section-aware-chunking-and-retrieval</name><title>April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for Discord feeds: Ingest messages and file attachments from a Discord channel using a bot token.\n",
            "  - Cohere reranking: Optionally rerank semantic search results in the RAG pipeline using the Cohere model.\n",
            "  - Section-aware text chunking and retrieval: Store extracted text according to semantic chunks for improved indexing.\n",
            "  - Retrieval strategies: Added CHUNK, SECTION, and CONTENT retrieval strategies for more flexible content retrieval.\n",
            "  - Reranking strategy: Configure reranking of content sources using the Cohere model.\n",
            "  - Synchronous ingestion: Added isSynchronous flag for content ingestion mutations to wait for completion before returning.\n",
            "  - Slack attachments: IncludeAttachments flag for automatic ingestion of attachments in Slack messages.\n",
            "  - IngestUri mutation: Replaces deprecated ingestPage and ingestFile mutations for simplified content ingestion.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Removed includeSummaries from ConversationStrategyInput; will be re-added in the future.\n",
            "  - Deprecated enableExpandedRetrieval; now managed by strategyType in RetrievalStrategyInput.\n",
            "  - Moved contentLimit to RetrievalStrategyInput for better content source management.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with PDF ingestion from GitHub.\n",
            "  - Resolved JSON schema adherence for Claude 3 Haiku.\n",
            "  - Prompt rewriting now ignores formatting instructions.\n",
            "  - Corrected missing line breaks after table rows.\n",
            "  - Fixed image extraction from PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: April 7, 2024.\n",
            "  - Version updates include new feature flags and improved content ingestion workflows.\n",
            "\n",
            "- Value:\n",
            "  - Enhances developer capabilities for content ingestion and retrieval, improving integration with various platforms and models.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-9-support-direct-text-markdown-and-html-ingestion-new-specification-llm-strategy</name><title>August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - IngestText mutation added for direct ingestion of plain text, Markdown, and HTML without URL reading.\n",
            "  - Specification strategy property introduced for customizing LLM context in conversations, with options for Windowed and Summarized message histories.\n",
            "  - Auto-summarization feature for extracted text and audio transcripts, with summaries available for inclusion in conversation prompts.\n",
            "  - AzureOpenAIModels and OpenAIModels types added to Specification model properties for easier LLM specification.\n",
            "  - ConversationMessage date property renamed to timestamp.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Internal LLM prompts refined for clearer and more accurate results.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 9, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for content ingestion and improved LLM interaction, leading to more accurate responses and better customization options.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-11-support-for-azure-ai-document-intelligence-by-default-language-aware-summaries</name><title>August 11: Support for Azure AI Document Intelligence by default, language-aware summaries | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for language-aware summaries in LLM-based document extraction, aligning summaries with the source text language.\n",
            "  - Support for language-aware entity descriptions in LLM-based entity extraction, matching descriptions to the source text language.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Default document preparation method changed to Azure AI Document Intelligence for improved fidelity in complex PDFs and better table extraction. Increased credit usage per page for PDF, DOCX, and PPTX documents noted, but quality of extracted documents significantly enhanced for RAG pipelines.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3070: Slide count not assigned to metadata for PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 11, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved document extraction quality and language consistency, enhancing the overall functionality of the platform.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-20-support-for-medical-entities-anthropic-prompt-caching-bug-fixes</name><title>August 20: Support for medical entities, Anthropic prompt caching, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "  - Medical-related entities supported in GraphRAG and via API for queries and mutations.\n",
            "  - Added support for Anthropic prompt caching, improving performance and reducing token costs when using Anthropic Sonnet 3.5 or Haiku 3.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved entity extraction and LLM document preparation through caching.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - GPLA-3104: Default search type now set to VECTOR for entity similarity filter.\n",
            "  - GPLA-3112: Fixed issue where empty PDFs failed entity extraction.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 20, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with improved capabilities for handling medical data and increased efficiency in using Anthropic models.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2023/december-10-support-for-openai-gpt-4-turbo-llama-2-and-mistral-models-query-by-example-bug-fixes</name><title>December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4 Turbo 128k model in Azure OpenAI and native services.\n",
            "  - Support for Llama 2 (7b, 13b, 70b) and Mistral 7b models via Replicate.\n",
            "  - Support for Anthropic Claude 2.1 model.\n",
            "  - Support for OpenAI GPT-4 Vision model for image descriptions and text extraction.\n",
            "  - Added query by example for contents and conversations, utilizing vector embeddings.\n",
            "  - Vector search support for conversations queries.\n",
            "  - Added promptSpecifications mutation for prompting multiple models.\n",
            "  - Added promptStrategy field for preprocessing prompts.\n",
            "  - Added suggestConversation mutation for auto-suggesting follow-up questions.\n",
            "  - New summarization types: CHAPTERS, QUESTIONS, and POSTS.\n",
            "  - Versioned model enums introduced (e.g., GPT4_0613).\n",
            "  - Added lookupContents query for retrieving multiple contents by ID.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed headline field to headlines, returning an array of strings.\n",
            "  - Entity names limited to 1024 characters, with truncation for longer names.\n",
            "  - BULLET_POINTS renamed to BULLETS in SummarizationTypes enum.\n",
            "  - Renamed originalTotalSize to totalSize in ProjectStorage type, added totalRenditionSize.\n",
            "  - Renamed strategyType to type in ConversationStrategy type for consistency.\n",
            "  - Removed optimizeSearchConversation from Specification type, now handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Ignored RSS.xml from web feed sitemap.\n",
            "  - Resolved issue with GPT-3.5 Turbo 16k LLM adding \"Citation #\" to responses.\n",
            "  - Fixed workflow application to link-crawled content.\n",
            "  - Corrected mismatched project storage total size when content errored.\n",
            "  - Added relevance threshold for semantic search.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 10, 2023.\n",
            "  - Enhancements provide developers with improved querying capabilities and model support, enhancing the overall functionality of the Graphlit Platform.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-9-support-for-website-mapping-web-page-screenshots-groq-llama-3.3-model-bug-fixes</name><title>December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for website mapping using mapWebmutation to return URLs from sitemap.xml.\n",
            "  - Generation of web page screenshots with screenshotPagemutation, including optional image processing workflows.\n",
            "  - Direct summarization of text with summarizeTextmutation, allowing for various summarization strategies.\n",
            "  - Direct extraction of text with extractTextmutation, utilizing LLM tools for JSON responses.\n",
            "  - Support for Groq Llama 3.3 model (LLAMA_3_3_70B).\n",
            "  - Updated Cohere reranking to use rerank-v3.5 model by default.\n",
            "  - New flattenCitations field in ConversationStrategyInput to combine multiple citations.\n",
            "  - Required clientId and clientSecret fields added for Microsoft email, Teams, and OneDrive feeds.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of citations in conversation prompts.\n",
            "  - Enhanced authentication requirements for Microsoft\n",
            "- Completion [4989 tokens (includes JSON guardrails tokens)], throughput: 53.784 tokens/sec:\n",
            "Hello everyone, and welcome to a special year-in-review for the Graphlit Platform. It‚Äôs been an incredible stretch of milestones and enhancements all designed to make your development workflows simpler, more powerful, and more creative. Let‚Äôs explore how each month‚Äôs updates can help you, whether you‚Äôre coding in Python, .NET, JavaScript, or any other environment.\n",
            "\n",
            "Let‚Äôs start back in July 2023. One key addition was SharePoint feed support, allowing direct file ingestion from SharePoint libraries. This came with new conversation features‚Äîsuch as timestamped messages‚Äîto help keep your discussions organized and relevant. These upgrades reduced friction when pulling in data from corporate SharePoint, making you more efficient in building knowledge-based applications.\n",
            "\n",
            "Moving into August 2023, Graphlit introduced a new data model for Observations, enabling more structured ways to track entities like People or Organizations. Then came direct text ingestion for plain text, Markdown, or HTML, eliminating the need for a file or URL. This is perfect when you just want to send raw data into Graphlit and get back structured content or embeddings for analysis. Also in August, usage-based billing infrastructure was rolled out. While still offering a free tier to get started right away, it set the stage for future expansions of credit-based usage‚Äîhelpful for devs who prefer pay-as-you-go models.\n",
            "\n",
            "September 2023 ushered in a wave of productivity features. Workflow configuration let you define how your content passes through ingestion, enrichment, or summarization steps, with support for Notion feeds to quickly import your team‚Äôs notes and pages. Paid subscription plans also launched, providing Hobby, Starter, and Growth tiers, each with different quotas. For those who love video content, YouTube feed support arrived, complete with automatic transcription for ingesting the audio. Now your dev team can treat video content just like any text document and search or summarize it with ease.\n",
            "\n",
            "October 2023 focused on deeper language model integrations. Anthropic Claude models joined the party for advanced text completions, while Slack feed ingestion made it easier for your dev teams to bring chat messages and file attachments into the knowledge base. The platform also expanded entity enrichment, letting you layer in details from sources like Wikipedia or Crunchbase. By the end of the month, conversation responses were further optimized, and you could add aliases to your observed entities‚Äîespecially handy when there are multiple ways to refer to the same person or place.\n",
            "\n",
            "December 2023 brought some major model updates. The platform introduced OpenAI GPT-4 Turbo with a 128k token window and also added Llama 2, Mistral, and Anthropic Claude 2.1. All these models expand your options for generating text, summarizing content, or performing semantic searches. There was also support for query-by-example, letting you simply submit a sample snippet of text or conversation to find related content.\n",
            "\n",
            "Jumping into January 2024, new content publishing features transformed how you repurpose and summarize documents, audio transcripts, and images. LLM tools, including function calls in OpenAI models, make it simpler to integrate prompt-driven tasks directly into your code. Additionally, Google and Microsoft email feeds became first-class citizens, enabling ingestion of both historic and new emails‚Äîattachments included. That month also introduced reingestion capabilities so you can update existing content in place with minimal fuss.\n",
            "\n",
            "February 2024 introduced semantic alerts for automatically generating daily or periodic summaries, perfect for receiving quick bulletins on everything new in your workspace. OneDrive and Google Drive feed support was expanded, including the ability to ingest images from PDFs or to capture embedded images in your documents. These features cut down on tedious overhead so you can focus on building robust apps.\n",
            "\n",
            "March 2024 saw comprehensive usage and credits telemetry so you can monitor precisely how many tokens or data credits your team is using. For devs who prefer command-line utilities, a new CLI tool was released for efficient Graphlit Data API interactions. Further in March came support for additional issue-tracking feeds‚ÄîLinear, GitHub Issues, and Jira‚Äîturning your project tickets into fully searchable items. And for file management, you could now ingest Base64-encoded files directly.\n",
            "\n",
            "April 2024 was all about chat and ingestion flows. Discord feed support arrived, letting you bring chat messages and attached media directly into Graphlit. Cohere reranking and improved text chunking enhanced the retrieval-augmented generation pipeline. Most notably, native SDKs in Python and TypeScript came out later in April, code-generated from the GraphQL schema, so you no longer need to be a GraphQL expert to start building. There were also brand-new model integrations for OpenAI‚Äôs GPT-4 expansions, as well as support for advanced LLaMA and Cohere Command models.\n",
            "\n",
            "May 2024 introduced new reranking services from Jina and Pongo, plus the brand-new GraphRAG feature‚Äîa system that harnesses extracted entities to supercharge your retrieval-augmented generation workflow. OpenAI GPT-4o replaced Azure OpenAI GPT-3.5 16k as the default model, promising a more robust experience. For Microsoft Teams fans, feed support allowed you to ingest channel messages automatically, while performance optimizations in entity extraction boosted speeds for large datasets.\n",
            "\n",
            "In June 2024, developers gained the ability to parse embedded JSON-LD data from web pages, enriching your knowledge graph with schema.org or other structured data. Deepseek model support was introduced, so if you prefer their AI completions and coders, you have that option too. Then in late June came the Claude 3.5 Sonnet model and semantic search for observable entities, a boon for advanced knowledge graph queries when dealing with people, organizations, or places.\n",
            "\n",
            "July 2024 was packed. Webhook Alerts went live, enabling automatic notifications when new content is published, and the Deepseek 128k token context was introduced‚Äîto handle giant transcripts or text blocks in a single pass. Then soon after, GPT-4o Mini arrived, offering a more lightweight GPT-4 variant for faster completions. Updates to Mistral Large 2 and Nemo, plus new Groq Llama 3.1 models, delivered even more alternatives to handle your AI workloads. The next update introduced an indexing workflow stage and Azure AI language detection, so you can automatically detect and store the languages a piece of content is written in.\n",
            "\n",
            "In August 2024, the platform unveiled an open source .NET SDK on NuGet, giving C# developers first-class access to the Graphlit APIs. Document preparation switched to Azure AI Document Intelligence by default, boosting accuracy for complex PDFs. Medical entities were also added, letting you extract detailed references to medical conditions, drugs, or procedures. Meanwhile, Anthropic prompt caching reduced token costs, speeding up repeated queries.\n",
            "\n",
            "September 2024 stepped up the AI model offerings further with FHIR enrichment for healthcare data and the newly updated Cohere models. Google AI search feeds and Cerebras model support also landed, plus more advanced Groq Llama versions. By now, you could combine advanced conversation prompts with fallback retrieval of relevant content, ensuring your AI app stays robust even if the main model hits a snag.\n",
            "\n",
            "October 2024 introduced a flurry of ‚Äútool calling‚Äù capabilities across Anthropic, Google Gemini, Mistral, and more. This let large language models handle tasks by calling external tools in real time, bridging the gap between pure text generation and practical app logic. GitHub repository feeds also debuted so you can auto-ingest code files, while conversation interactions got new ways to simulate tool calls or to parse tool responses from JSON.\n",
            "\n",
            "November 2024 delivered web search features‚Äîimagine searching the open web, retrieving relevant pages, and combining them with your content, all in one shot. Multi-turn text summarization and multi-turn image analysis made it possible to revise or refine a summary over a series of LLM prompts. Deepgram language detection took speech transcription to the next level, and feed management was enhanced so you can control usage or upgrade easily when you run out of free-tier credits.\n",
            "\n",
            "Finally, December 2024 concluded with major expansions in the retrieval pipeline. You can run a retrieval-only RAG process, or map entire websites with the new site mapping features. Summaries and bullet points are more accurate than ever with refined chunking strategies. And let‚Äôs not forget the big arrival of new model support for Groq Llama 3.3, or the ability to handle advanced content from Dropbox, Box, Intercom, Zendesk, and beyond.\n",
            "\n",
            "Throughout these releases, loads of bugs were squashed‚Äîfrom synchronizing your search index in real time, to extracting images from PDFs, to properly handling text from every corner of your data sources. Each fix was about giving you a more stable and confident development experience.\n",
            "\n",
            "In short, the Graphlit Platform has advanced leaps and bounds this past year. From brand-new SDKs and improved content ingestion to powerful LLM integrations and next-level conversation features, there‚Äôs something here for every team building with AI. The best way to learn more? Try it. You can sign up for free right now at graphlit.com and explore all these capabilities for yourself.\n",
            "\n",
            "And here‚Äôs one last bit of news: 2025 is right around the corner, and the team is preparing even more excitement‚Äîespecially for building, testing, and deploying AI agents. Keep an eye out for those features, because they‚Äôre going to transform how you build intelligent, automated systems.\n",
            "\n",
            "Thanks for tuning in to this whirlwind tour of Graphlit‚Äôs 2024 updates. Good luck with your next AI-driven project, and we can‚Äôt wait to see the amazing creations you‚Äôll build with these new tools. Enjoy exploring the platform, and happy coding!\n",
            "\n",
            "2024-12-29T03:27:05.066Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.682265, used credits [0.00737100]\n",
            "- CONTENT [39b8a12e-de8a-41c9-9224-4b0e8e0909d8]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1061 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/april-2024/april-7-support-for-discord-feeds-cohere-reranking-section-aware-chunking-and-retrieval</name><title>April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval | Graphlit Changelog</title></metadata> üêá\tApril 2024\n",
            "April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "New Features\n",
            "üí° Graphlit now supports Discord feeds.  By connecting to a Discord channel and providing a bot token, you can ingest all Discord messages and file attachments.\n",
            "üí°  Graphlit now supports Cohere reranking after content retrieval in RAG pipeline.  You can optionally use the Cohere rerank model to semantically rerank the semantic search results, before providing as context to the LLM.\n",
            "Added support for section-aware text chunking and retrieval.  Now, when using section-aware document preparation, such as Azure AI Document Intelligence, Graphlit will store the extracted text according to the semantic chunks (i.e. sections).  The text for each section will be individually chunked and embedded into the vector index.\n",
            "Added support for retrievalStrategy in Specification type. Graphlit now supports CHUNK, SECTION and CONTENT retrieval strategies.  Chunk retrieval will use the search hit chunk, section retrieval will expand the search hit chunk to the containing section (or page, if not using section-aware preparation).  Content retrieval will expand the search hit chunk to the text of the entire document.\n",
            "Added support for rerankingStrategy in Specification type. You can now configure the reranking of content sources, using the Cohere reranking model, by assigning serviceType to COHERE.  More reranking models are planned for the future.\n",
            "Added isSynchronous flag to content ingestion mutations, such as ingestUri, so the mutation will wait for the content to complete the ingestion workflow (or error) before returning.  This is useful for utilizing the API in a Jupyter notebook or Streamlit application, in a synchronous manner without polling.\n",
            "Added includeAttachments flag to SlackFeedProperties.  When enabled, Graphlit will automatically ingest any attachments within Slack messages.\n",
            "‚ö° Added ingestUri mutation to replace the now deprecated ingestPage and ingestFile mutations.  We had seen confusion on when to use one vs the other, and now for any URI, whether it is a web page or hosted PDF, you can pass it to ingestUri, and we will infer the correct content ingestion workflow.\n",
            "‚ö° Removed includeSummaries from the ConversationStrategyInput type.  This will re-added in the future as part of the retrieval strategy.\n",
            "‚ö° Deprecated enableExpandedRetrieval in ConversationStrategyInput type.  This is now handled by setting strategyType to SECTION or CONTENT in the RetrievalStrategyInput type.\n",
            "‚ö° Moved contentLimit from ConversationStrategyInput type to RetrievalStrategyInput type. You can optionally assign the contentLimit to retrievalStrategy which limits the number of content sources leveraged in the LLM prompt context. (Default is 100.)\n",
            "Bugs Fixed\n",
            "GPLA-2469: Failed to ingest PDF hosted on GitHub\n",
            "GPLA-2390: Claude 3 Haiku not adhering to JSON schema\n",
            "GPLA-2474: Prompt rewriting should ignore formatting instructions in prompt\n",
            "GPLA-2462: Missing line break after table rows\n",
            "GPLA-2417: Not extracting images from PPTX correctly\n",
            "PreviousApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "NextMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "Last updated8 months ago\n",
            "- Completion [349 tokens (includes JSON guardrails tokens)], throughput: 94.779 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Discord feeds: Ingest messages and file attachments from a Discord channel using a bot token.\n",
            "  - Cohere reranking: Optionally rerank semantic search results in the RAG pipeline using the Cohere model.\n",
            "  - Section-aware text chunking and retrieval: Store extracted text according to semantic chunks for improved indexing.\n",
            "  - Retrieval strategies: Added CHUNK, SECTION, and CONTENT retrieval strategies for more flexible content retrieval.\n",
            "  - Reranking strategy: Configure reranking of content sources using the Cohere model.\n",
            "  - Synchronous ingestion: Added isSynchronous flag for content ingestion mutations to wait for completion before returning.\n",
            "  - Slack attachments: IncludeAttachments flag for automatic ingestion of attachments in Slack messages.\n",
            "  - IngestUri mutation: Replaces deprecated ingestPage and ingestFile mutations for simplified content ingestion.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Removed includeSummaries from ConversationStrategyInput; will be re-added in the future.\n",
            "  - Deprecated enableExpandedRetrieval; now managed by strategyType in RetrievalStrategyInput.\n",
            "  - Moved contentLimit to RetrievalStrategyInput for better content source management.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with PDF ingestion from GitHub.\n",
            "  - Resolved JSON schema adherence for Claude 3 Haiku.\n",
            "  - Prompt rewriting now ignores formatting instructions.\n",
            "  - Corrected missing line breaks after table rows.\n",
            "  - Fixed image extraction from PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: April 7, 2024.\n",
            "  - Version updates include new feature flags and improved content ingestion workflows.\n",
            "\n",
            "- Value:\n",
            "  - Enhances developer capabilities for content ingestion and retrieval, improving integration with various platforms and models.\n",
            "\n",
            "2024-12-29T03:27:05.016Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.048960, used credits [0.00668400]\n",
            "- CONTENT [cd1c5052-f3bb-4747-b785-5e83ed6dfd80]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [872 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/april-2024/april-23-support-for-python-and-typescript-sdks-latest-openai-cohere-and-groq-models-bug-fixes</name><title>April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes | Graphlit Changelog</title></metadata> üêá\tApril 2024\n",
            "April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports a native Python SDK, using Pydantic types. The Python SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest PyPi package here.  The Streamlit sample applications have been updated to use the new Python SDK.\n",
            "üí° Graphlit now supports a native Node.js SDK, using TypeScript types. The Node.js SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest NPM package here.\n",
            "üí° Graphlit now supports the 2024-04-09 models in the OpenAI model service. GPT4_TURBO-128K will give the latest OpenAI GPT-4 model, following this model list.  We have added the GPT4_TURBO_128K_2024_04_09 enum to specify the new model.\n",
            "üí° Graphlit now supports LLaMA3 70b, LLaMA3 8b and Gemma 7b models in the Groq model service.\n",
            "üí° Graphlit now supports the Command R and Command-R+ models in the Cohere model service.\n",
            "Added support for Jina reranking, using the JINA reranking model service type in the reranking retrieval strategy.\n",
            "Updated the Cohere reranking model to use the latest v3.0 model.\n",
            "Increased the reliability of parsing LLM responses, in cases where they don't follow the JSON schema.\n",
            "‚ö° Cleaned up nullability of GraphQL parameters, so parameters better reflect if they are required or optional, or allow nulls.\n",
            "‚ö° Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "‚ö° Split out reranking model service type as RetrievalModelServiceTypes enum.\n",
            "Bugs Fixed\n",
            "GPLA-2114: Adding content to collections not syncing search index\n",
            "GPLA-2511: Failing to render any conversation sources with section retrieval and text content\n",
            "PreviousMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "NextApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "Last updated8 months ago\n",
            "- Completion [339 tokens (includes JSON guardrails tokens)], throughput: 83.725 tokens/sec:\n",
            "- New Features:\n",
            "  - Native Python SDK introduced, using Pydantic types; code-generated from GraphQL schema, no GraphQL knowledge required.\n",
            "  - Native Node.js SDK introduced, using TypeScript types; code-generated from GraphQL schema, no GraphQL knowledge required.\n",
            "  - Support for OpenAI's 2024-04-09 models, including GPT4_TURBO-128K.\n",
            "  - Support for LLaMA3 70b, LLaMA3 8b, and Gemma 7b models in Groq model service.\n",
            "  - Support for Command R and Command-R+ models in Cohere model service.\n",
            "  - Added support for Jina reranking with the JINA reranking model service type.\n",
            "  - Updated Cohere reranking model to the latest v3.0.\n",
            "  - Improved reliability of parsing LLM responses that do not follow JSON schema.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Cleaned up nullability of GraphQL parameters for better clarity on required, optional, or nullable parameters.\n",
            "  - Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "  - Split reranking model service type into RetrievalModelServiceTypes enum.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where adding content to collections did not sync with the search index (GPLA-2114).\n",
            "  - Resolved rendering issues with conversation sources in section retrieval and text content (GPLA-2511).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: April 23, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced SDK support, improved model integration, and better handling of LLM responses, facilitating easier development and implementation.\n",
            "\n",
            "2024-12-29T03:27:04.777Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.978868, used credits [0.00508800]\n",
            "- CONTENT [e46ab310-60bd-439d-9471-0cfebd1843cd]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [848 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/</name><title>December 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes | Graphlit Changelog</title></metadata> üéÑ\tDecember 2024\n",
            "December 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports LLM fallbacks which can help protect your application from model provider downtime.  By assigning the fallbacksproperty when creating your conversation, you can provide an optional list of LLM specifications to be used (in order).  These fallback specifications will only be used when we failed to prompt the conversation via the main specification.  Caveat, the RAG pipeline will only use the strategies provided in the main specification for prompt rewriting, content retrieval, etc.  Content is not re-retrieved upon fallback - the formatted LLM prompt will be tried against each fallback specification in succession until one succeeds.\n",
            "üí° Graphlit now supports querying of all available models, through the new modelsquery in the API.  This returns the model enum, model service type enum, description, and several other useful details about the models.\n",
            "Graphlit now supports the ingestion of native Google Docs, Google Sheets and Google Slides documents from Google Drive feeds.  These formats will be auto-exported to the corresponding Microsoft Office format (DOCX, XLSX, PPTX) prior to ingesting as content.\n",
            "Graphlit now supports unblocking of websites, such as those using Cloudflare.  You can set enableUnblockedCaptureto true on the PreparationWorkflowStageto enable unblocking - through our integration with Browserless.io headless browser service.  This does incur an additional cost per page, compared to normal web page ingestion.\n",
            "We have added support for assigning observations to contents ingested via feeds.  By assigning observationsto the IngestionWorkflowStagein workflow object, you can assign Labels, Organizations, etc. without needing to use entity extraction.\n",
            "We have added support for assigning observations when ingesting content via ingestUri, ingestText, etc. mutations. By passing observationsas a parameter, similar to `collections`, you can assign Labels, Organizations, etc. without needing to use entity extraction.\n",
            "Bugs Fixed\n",
            "GPLA-3645: Table headers merged together on web scrape\n",
            "GPLA-3634: Failed to extract pages from PDF with empty hyperlink text\n",
            "GPLA-3633: Not handling empty observables properly for reranking\n",
            "NextDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "Last updated21 hours ago\n",
            "- Completion [212 tokens (includes JSON guardrails tokens)], throughput: 71.168 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for LLM fallbacks to mitigate model provider downtime.\n",
            "  - New API query for all available models, providing details like model enum and service type.\n",
            "  - Ingestion of native Google Docs, Sheets, and Slides from Google Drive, auto-exported to Microsoft Office formats.\n",
            "  - Website unblocking support for sites using Cloudflare, enabled via the PreparationWorkflowStage.\n",
            "  - Ability to assign observations (Labels, Organizations) to ingested content without entity extraction.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved content ingestion process with new observation assignment features.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed table headers merging issue during web scraping.\n",
            "  - Resolved PDF extraction failure with empty hyperlink text.\n",
            "  - Corrected handling of empty observables for reranking.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 27, 2024.\n",
            "  - Additional costs for unblocked website ingestion.\n",
            "\n",
            "- Value:\n",
            "  - Enhances application reliability, expands content ingestion capabilities, and improves user experience with bug fixes.\n",
            "\n",
            "2024-12-29T03:27:02.920Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.713658, used credits [0.00353100]\n",
            "- CONTENT [a540acea-c8ed-4352-a5cc-91c3dbe4ba91]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [609 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-17-prepare-for-usage-based-billing-append-sas-tokens-to-uris</name><title>August 17: Prepare for usage-based billing; append SAS tokens to URIs | Graphlit Changelog</title></metadata> üéÇ\tAugust 2023\n",
            "August 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "New Features\n",
            "‚ÑπÔ∏è Behind the scenes, Graphlit is preparing to launch usage-based billing.  This release put in place the infrastructure to track billable events.  Organizations now have a Stripe customer associated with them, and Graphlit projects are auto-subscribed to a Free/Hobby pricing plan.  In a future release, we will provide the ability to upgrade to a paid plan in the Graphlit Developer Portal.  Also, we will provide visualization of usage, on granular basis, in the Portal.\n",
            "üí° Content URIs now have Shared Access Signature (SAS) token appended, so they are accessible after query.  For example, content.transcriptUri will now be able to be downloaded or used directly in an application (until the SAS token expires).\n",
            "üß± Added more robustness for error handling and retries, especially for LLM APIs and audio transcription APIs.\n",
            "PreviousSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "NextAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "Last updated1 year ago\n",
            "- Completion [142 tokens (includes JSON guardrails tokens)], throughput: 52.328 tokens/sec:\n",
            "- New Features:\n",
            "  - Infrastructure for usage-based billing implemented; organizations now have a Stripe customer and auto-subscribed to a Free/Hobby pricing plan.\n",
            "  - Content URIs now include Shared Access Signature (SAS) tokens for direct access post-query.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Improved error handling and retries for LLM APIs and audio transcription APIs.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Future release will allow upgrades to paid plans and provide usage visualization in the Graphlit Developer Portal.\n",
            "\n",
            "- Dates:\n",
            "  - Released on August 17, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers organizations a billing structure and enhanced access to content, improving usability and reliability for developers.\n",
            "\n",
            "2024-12-29T03:27:01.754Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.192107, used credits [0.00425400]\n",
            "- CONTENT [448971ac-c917-4d00-a846-52a5844472f9]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [666 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-9-support-direct-text-markdown-and-html-ingestion-new-specification-llm-strategy</name><title>August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy | Graphlit Changelog</title></metadata> üéÇ\tAugust 2023\n",
            "August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "New Features\n",
            "üí° Added ingestText mutation which supports direct Content ingestion of plain text, Markdown and HTML.  Now, if you have pre-scraped HTML or Markdown text, you can ingest it into Graphlit without reading from a URL.\n",
            "üí° Added Specification strategy property, which allows customization of the LLM context when prompting a conversation.  ConversationStrategy now provides Windowed and Summarized message histories, as well as configuration of the weight between existing conversation messages and Content text pages (or audio transcript segments) in the LLM context.\n",
            "üí° Added auto-summarization of extracted text and audio transcripts.  There is a new Content summary property where a list of summary bullet points can be found.  These summaries can be optionally included in the Conversation prompt context for more accurate LLM responses.\n",
            "‚ÑπÔ∏è Added AzureOpenAIModels and OpenAIModels types to Specification model properties to make it easier to specify the desired LLM.\n",
            "‚ÑπÔ∏è Renamed ConversationMessage date property to timestamp\n",
            "‚ú® Refined the internal LLM prompts for providing content as part of Conversation context.  This provides for much clearer and accurate results from the LLM.\n",
            "PreviousAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "NextAugust 3: New data model for Observations, new Category entity\n",
            "Last updated1 year ago\n",
            "- Completion [188 tokens (includes JSON guardrails tokens)], throughput: 85.762 tokens/sec:\n",
            "- New Features:\n",
            "  - IngestText mutation added for direct ingestion of plain text, Markdown, and HTML without URL reading.\n",
            "  - Specification strategy property introduced for customizing LLM context in conversations, with options for Windowed and Summarized message histories.\n",
            "  - Auto-summarization feature for extracted text and audio transcripts, with summaries available for inclusion in conversation prompts.\n",
            "  - AzureOpenAIModels and OpenAIModels types added to Specification model properties for easier LLM specification.\n",
            "  - ConversationMessage date property renamed to timestamp.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Internal LLM prompts refined for clearer and more accurate results.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 9, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for content ingestion and improved LLM interaction, leading to more accurate responses and better customization options.\n",
            "\n",
            "2024-12-29T03:27:01.313Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.143841, used credits [0.00399000]\n",
            "- CONTENT [0cbb6490-3cff-4aa5-8d05-b94ab7c3d531]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [622 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-11-support-for-azure-ai-document-intelligence-by-default-language-aware-summaries</name><title>August 11: Support for Azure AI Document Intelligence by default, language-aware summaries | Graphlit Changelog</title></metadata> üéÇ\tAugust 2024\n",
            "August 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "New Features\n",
            "Added support for language-aware summaries when using LLM-based document extraction.  Now the summaries for tables and sections generated by the LLM will follow the language of the source text.\n",
            "Added support for language-aware entity descriptions with using LLM-based entity extraction. Now the entity descriptions generated by the LLM will follow the language of the source text.\n",
            "‚ö° We have changed the default document preparation method to use Azure AI Document Intelligence, rather than our built-in document parsers.  We have found that the fidelity of Azure AI is considerably better for complex PDFs, and provides better support for table extraction, so we have made this the default. Note: this does come with increased credit usage per-page, for PDF, DOCX and PPTX documents, but the quality of the extracted documents are noticeably higher for use in RAG pipelines.\n",
            "Bugs Fixed\n",
            "GPLA-3070: Not getting slide count assigned to metadata for PPTX files.\n",
            "PreviousAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "NextAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "Last updated4 months ago\n",
            "- Completion [177 tokens (includes JSON guardrails tokens)], throughput: 82.562 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for language-aware summaries in LLM-based document extraction, aligning summaries with the source text language.\n",
            "  - Support for language-aware entity descriptions in LLM-based entity extraction, matching descriptions to the source text language.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Default document preparation method changed to Azure AI Document Intelligence for improved fidelity in complex PDFs and better table extraction. Increased credit usage per page for PDF, DOCX, and PPTX documents noted, but quality of extracted documents significantly enhanced for RAG pipelines.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3070: Slide count not assigned to metadata for PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 11, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved document extraction quality and language consistency, enhancing the overall functionality of the platform.\n",
            "\n",
            "2024-12-29T03:27:00.882Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:06.712181, used credits [0.00582000]\n",
            "- CONTENT [a9adfd3e-0f66-40b0-9039-c00cd8fbf761]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [804 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-8-support-for-llm-based-document-extraction-.net-sdk-bug-fixes</name><title>August 8: Support for LLM-based document extraction, .NET SDK, bug fixes | Graphlit Changelog</title></metadata> üéÇ\tAugust 2024\n",
            "August 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports LLM-based document preparation, using vision-capable models such as OpenAI GPT-4o and Anthropic Sonnet 3.5.  This is available via the MODEL_DOCUMENT preparation service type, and you can assign a customspecification object and bring your own LLM keys.\n",
            "üí° Graphlit now provides an open source .NET SDK, supporting .NET 6 and .NET 8 (and above).  SDK package can be found on Nuget.org.  Code samples can be found on GitHub.\n",
            "Added identifier property to Content object for mapping content to external database identifiers.  This is supported for content filtering as well.\n",
            "Added support for Claude 3 vision models for image-based entity extraction, using the MODEL_IMAGE entity extraction service.\n",
            "Added context augmentation to conversations, via the augmentedFilter property on the Conversation object.  Any content which matches this augmented filter will be injected into the LLM prompt content, without needing to be related by vector similarity to the user prompt.  This is useful for specifying domain knowledge which should always be referenced by the RAG pipeline.\n",
            "Added support for the latest snapshot of OpenAI GPT-4o, with the model enum GPT4O_128K_20240806.\n",
            "Added reranking of related entities, when preparing the LLM prompt context for GraphRAG.  If reranking is enabled, the metadata from the related entities will be reranked with the same reranker assigned to the conversation specification.\n",
            "‚ö° We have changed the type of the duration field in the AudioMetadata and VideoMetadata types to be TimeSpan rather than string, as to be more consistent with the rest of the API data model.\n",
            "Bugs Fixed\n",
            "GPLA-2884: Support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "PreviousAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "NextJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "Last updated4 months ago\n",
            "- Completion [284 tokens (includes JSON guardrails tokens)], throughput: 42.311 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for LLM-based document preparation using models like OpenAI GPT-4o and Anthropic Sonnet 3.5 via MODEL_DOCUMENT service.\n",
            "  - Introduction of an open source .NET SDK for .NET 6 and .NET 8, available on Nuget.org with code samples on GitHub.\n",
            "  - Added identifier property to Content object for mapping to external database identifiers, aiding in content filtering.\n",
            "  - Support for Claude 3 vision models for image-based entity extraction using MODEL_IMAGE service.\n",
            "  - Context augmentation in conversations via augmentedFilter property, allowing injection of domain knowledge into LLM prompts.\n",
            "  - Support for the latest snapshot of OpenAI GPT-4o (model enum GPT4O_128K_20240806).\n",
            "  - Reranking of related entities for LLM prompt context preparation in GraphRAG.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Changed duration field type in AudioMetadata and VideoMetadata from string to TimeSpan for consistency in API data model.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2884 to support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 8, 2024.\n",
            "\n",
            "- Value:\n",
            "  - These updates enhance document extraction capabilities, improve SDK accessibility for developers, and streamline content management and entity extraction processes.\n",
            "\n",
            "2024-12-29T03:27:00.067Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.467761, used credits [0.00567000]\n",
            "- CONTENT [4f6da3ce-9feb-4ecb-9c05-dcefa5c6de6b]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [854 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-3-new-data-model-for-observations-new-category-entity</name><title>August 3: New data model for Observations, new Category entity | Graphlit Changelog</title></metadata> üéÇ\tAugust 2023\n",
            "August 3: New data model for Observations, new Category entity\n",
            "New Features\n",
            "üí° Revised data model for Observations, Occurrences and observables (i.e. Person, Organization).  Now after entity extraction, content will have one Observation for each observed entity, and a list of occurrences.  Occurrence now supports text, time and image occurrence types.  (Text: page index, time: start/end timestamp, image: bounding box)  Observations now have ObservableType and Observable fields, which specify the observed entity type and entity reference.\n",
            "üí° Added Category entity to GraphQL data model, which supports PII categories such as Phone Number or Credit Card Number.\n",
            "Added probability field to model properties, for the LLM's token probability.  (See OpenAI documentation for more detail.)\n",
            "Added error field to feeds.  If a feed fails to read from the data source, and is marked as ERRORED state, the error field will have the error description.\n",
            "Support reingestion of changed files from feeds.  For feeds, such as SharePoint or Web, where we can recognize that a file or page was updated, we will now reingest the content in-place.  Content will keep the same ID, and will restart the content workflow by re-downloading the updated content from the data source.   Existing observations will be deleted, and new observations will be created from the updated content.\n",
            "‚ÑπÔ∏è Ingestion of content is now idempotent, meaning if you ingest content again from the same URI, we will reingest the content in-place, while keeping the same ID.  (If we can recognize the content has not changed, such as by ETag, we will return the existing content object.)\n",
            "‚ÑπÔ∏è Changed GraphQL data type of SharePoint tenantId, libraryId and siteId to ID rather than String.\n",
            "‚ú® Performance optimization of entity extraction, and the creation of observations.\n",
            "Bugs Fixed\n",
            "GPLA-1130: Only was extracting text from first column of PDF tables.\n",
            "GPLA-1140: Text from DOCX tables was not extracted properly.\n",
            "GPLA-1154: Audio content ingested from RSS feed was not deleted when feed was deleted.\n",
            "PreviousAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "NextJuly 15: Support for SharePoint feeds, new Conversation features\n",
            "Last updated1 year ago\n",
            "- Completion [259 tokens (includes JSON guardrails tokens)], throughput: 74.688 tokens/sec:\n",
            "- New Features:\n",
            "  - Revised data model for Observations, Occurrences, and observables (e.g., Person, Organization).\n",
            "  - Each observed entity now has one Observation and a list of occurrences (supports text, time, and image types).\n",
            "  - Added Category entity to GraphQL data model for PII categories (e.g., Phone Number, Credit Card Number).\n",
            "  - Introduced probability field for LLM's token probability.\n",
            "  - Added error field to feeds for error descriptions on failed reads.\n",
            "  - Support for reingestion of changed files from feeds, maintaining the same ID.\n",
            "  - Ingestion of content is now idempotent, allowing reingestion without ID changes.\n",
            "  - Changed GraphQL data type for SharePoint identifiers from String to ID.\n",
            "  - Performance optimization for entity extraction and observation creation.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of content ingestion and updates.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed extraction issues from PDF and DOCX tables.\n",
            "  - Resolved issue where audio content from RSS feeds was not deleted upon feed deletion.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 3, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved data handling, enhanced entity extraction, and better error management.\n",
            "\n",
            "2024-12-29T03:26:59.476Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.777376, used credits [0.00429900]\n",
            "- CONTENT [26deb281-fa20-4f6d-839b-31033675141b]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [593 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-20-support-for-medical-entities-anthropic-prompt-caching-bug-fixes</name><title>August 20: Support for medical entities, Anthropic prompt caching, bug fixes | Graphlit Changelog</title></metadata> üéÇ\tAugust 2024\n",
            "August 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports the extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "üí° Graphlit now supports medical-related entities in GraphRAG, and via API for queries and mutations.\n",
            "Added support for Anthropic prompt caching. When using Anthropic Sonnet 3.5 or Haiku 3, Anthropic will now cache the entity extraction and LLM document preparation system prompts, which saves on token cost and increases performance.\n",
            "Bugs Fixed\n",
            "GPLA-3104: Should default search type to VECTOR, when performing entity similarity filter.\n",
            "GPLA-3112: Empty PDF fails entity extraction.\n",
            "PreviousSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "NextAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "Last updated4 months ago\n",
            "- Completion [210 tokens (includes JSON guardrails tokens)], throughput: 55.594 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "  - Medical-related entities supported in GraphRAG and via API for queries and mutations.\n",
            "  - Added support for Anthropic prompt caching, improving performance and reducing token costs when using Anthropic Sonnet 3.5 or Haiku 3.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved entity extraction and LLM document preparation through caching.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - GPLA-3104: Default search type now set to VECTOR for entity similarity filter.\n",
            "  - GPLA-3112: Fixed issue where empty PDFs failed entity extraction.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 20, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with improved capabilities for handling medical data and increased efficiency in using Anthropic models.\n",
            "\n",
            "2024-12-29T03:26:59.113Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:07.080829, used credits [0.00877500]\n",
            "- CONTENT [0ec87d20-b463-4f65-a2f4-e32bc44f1b0d]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1217 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2023/december-10-support-for-openai-gpt-4-turbo-llama-2-and-mistral-models-query-by-example-bug-fixes</name><title>December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes | Graphlit Changelog</title></metadata> üéÑ\tDecember 2023\n",
            "December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports the OpenAI GPT-4 Turbo 128k model, both in Azure OpenAI and native OpenAI services.  Added new model enum GPT4_TURBO_VISION_128K.\n",
            "üí° Graphlit now supports Llama 2 7b, 13b, 70b models and Mistral 7b model, via Replicate.  Developers can use their own Replicate API key, or be charged as credits for Graphlit usage.\n",
            "üí° Graphlit now supports the Anthropic Claude 2.1 model. Added new model enum CLAUDE_2_1.\n",
            "üí° Graphlit now supports the OpenAI GPT-4 Vision model for image descriptions and text extraction.  Added new model enum GPT4_TURBO_VISION_128K. See usage example in \"Multimodal RAG\" blog post.\n",
            "Added query by example to contents query.  Developers can specify one or more example contents, and query will use vector embeddings to return similar contents.\n",
            "Added query by example to conversations query.  Developers can specify one or more example conversations, and query will use vector embeddings to return similar conversations.\n",
            "Added vector search support for conversations queries.  Developers can provide search text which will use vector embeddings to return similar conversations.\n",
            "Added promptSpecifications mutation for directly prompting multiple models.  This can be used to evaluate prompts against multiple models or compare different specification parameters in parallel.\n",
            "Added promptStrategy field to Specification, which supports multiple strategy types for preprocessing the prompt before being sent to the LLM model.  For example, REWRITE prompt strategy will ask LLM to rewrite the incoming user prompt based on the previous conversation messages.\n",
            "Added suggestConversation mutation, which returns a list of suggested followup questions based on the specified conversation and related contents.  This can be used to auto-suggest questions for chatbot users.\n",
            "Added new summarization types: CHAPTERS, QUESTIONS and POSTS.   See usage examples in the \"LLMs for Podcasters\" blog post.\n",
            "Added versioned model enums such as GPT4_0613 and GPT35_TURBO_16K_1106.  Without version specified, such as GPT35_TURBO_16K, Graphlit will use the latest production model version, as defined by the LLM vendor.\n",
            "Added lookupContents query to get multiple contents by id in one query.\n",
            "‚ö° In Content type, headline field was renamed to headlines and now returns an array of strings.\n",
            "‚ö° Entity names are now limited to 1024 characters.  Names will be truncated if they exceed the maximum length.\n",
            "‚ö° In SummarizationTypes enum, BULLET_POINTS was renamed to BULLETS.\n",
            "‚ö° In ProjectStorage type, originalTotalSize was renamed to totalSize, and totalRenditionSize field was added.  totalSize is the sum of the ingested source file sizes, and totalRenditionSize is the sum of the source file sizes and any derived rendition sizes.\n",
            "‚ö° In ConversationStrategy type, strategyType was renamed to type for consistency with rest of data model.\n",
            "‚ö° In Specification type, optimizeSearchConversation was removed, and now is handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "Bugs Fixed\n",
            "GPLA-1725: Should ignore RSS.xml from web feed sitemap\n",
            "GPLA-1726: GPT-3.5 Turbo 16k LLM is adding \"Citation #\" to response\n",
            "GPLA-1698: Workflow not applied to link-crawled content\n",
            "GPLA-1692: Mismatched project storage total size, when some content has errored\n",
            "GPLA-1237: Add relevance threshold for semantic search\n",
            "PreviousJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "NextOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "Last updated11 months ago\n",
            "- Completion [427 tokens (includes JSON guardrails tokens)], throughput: 60.304 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4 Turbo 128k model in Azure OpenAI and native services.\n",
            "  - Support for Llama 2 (7b, 13b, 70b) and Mistral 7b models via Replicate.\n",
            "  - Support for Anthropic Claude 2.1 model.\n",
            "  - Support for OpenAI GPT-4 Vision model for image descriptions and text extraction.\n",
            "  - Added query by example for contents and conversations, utilizing vector embeddings.\n",
            "  - Vector search support for conversations queries.\n",
            "  - Added promptSpecifications mutation for prompting multiple models.\n",
            "  - Added promptStrategy field for preprocessing prompts.\n",
            "  - Added suggestConversation mutation for auto-suggesting follow-up questions.\n",
            "  - New summarization types: CHAPTERS, QUESTIONS, and POSTS.\n",
            "  - Versioned model enums introduced (e.g., GPT4_0613).\n",
            "  - Added lookupContents query for retrieving multiple contents by ID.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed headline field to headlines, returning an array of strings.\n",
            "  - Entity names limited to 1024 characters, with truncation for longer names.\n",
            "  - BULLET_POINTS renamed to BULLETS in SummarizationTypes enum.\n",
            "  - Renamed originalTotalSize to totalSize in ProjectStorage type, added totalRenditionSize.\n",
            "  - Renamed strategyType to type in ConversationStrategy type for consistency.\n",
            "  - Removed optimizeSearchConversation from Specification type, now handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Ignored RSS.xml from web feed sitemap.\n",
            "  - Resolved issue with GPT-3.5 Turbo 16k LLM adding \"Citation #\" to responses.\n",
            "  - Fixed workflow application to link-crawled content.\n",
            "  - Corrected mismatched project storage total size when content errored.\n",
            "  - Added relevance threshold for semantic search.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 10, 2023.\n",
            "  - Enhancements provide developers with improved querying capabilities and model support, enhancing the overall functionality of the Graphlit Platform.\n",
            "\n",
            "2024-12-29T03:26:56.504Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.879721, used credits [0.00856500]\n",
            "- CONTENT [7fff5e48-bdc8-4dbc-ad26-da250228e7ee]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1223 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-22-support-for-dropbox-box-intercom-and-zendesk-feeds-openai-o1-gemini-2.0-bug-fixes</name><title>December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes | Graphlit Changelog</title></metadata> üéÑ\tDecember 2024\n",
            "December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports Dropbox feeds for ingesting files on the Dropbox cloud service. Dropbox feeds require your appKey, appSecret, redirectUriand refreshTokento be assigned. The feed also accepts an optional pathparameter to read files from a specific Dropbox folder.\n",
            "üí° Graphlit now supports Box feeds for ingesting files on the Box cloud service. Box feeds require your clientId, clientSecret, redirectUriand refreshTokento be assigned.\n",
            "üí° Graphlit now supports Intercom feeds for ingesting Intercom Articles and Tickets. We will ingest Intercom Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Intercom feeds require the accessTokenproperty to be assigned.\n",
            "üí° Graphlit now supports Zendesk feeds for ingesting Zendesk Articles and Tickets.  We will ingest Zendesk Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Zendesk feeds require the accessTokenproperty and your Zendesk subdomain to be assigned.\n",
            "Graphlit now supports the latest OpenAI o1 model, with the model enums O1_200kand O1_200k_20241217.\n",
            "Graphlit now supports the latest Gemini Flash 2.0 Experimental model, with the model enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "Graphlit now supports the latest Cohere R7B model, with the model enum COMMAND_R7B_202412.\n",
            "Graphlit now supports returning the low-level details from prompting RAG conversations, by adding the includeDetailsparameter and setting to True. This includes details on the number of sources, the exact list of messages provided to the LLM, and more.\n",
            "We have added support for filtering of observables, such as Person or Organization, by URI property.\n",
            "We have added the ability to bypass semantic search in content retrieval with conversations. You can assign NONEfor the conversation search type, and it will ignore the user prompt when retrieving content.  It will inject all contents resulting from the content filter into the RAG prompt context.\n",
            "We have added a new createdInLastproperty to all entity filters, which allows easier filtering of entities created within a recent time period. Also, we have added a new inLastproperty to the content filter, which allows easier filtering of content authored within a recent time period. For example, find all images taken in the last 3 days, or find me all emails I received yesterday.\n",
            "We have added support for the latest Azure AI Document Intelligence models, with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "We have added support for Google Drive and OneDrive feeds to ingest specific files by providing a list of file identifiers (files), in addition to the folder identifier (folderId).  If files identifiers are provided, they take precedence over the folder identifier.\n",
            "‚ö° For projects upgraded to the Starter Tier after Dec 9, 2024, we have removed the content items limit. Now you can store an unlimited number of content items (i.e. files, web pages, Slack messages) on the Starter or Growth Tiers.  If you have an existing project on the Starter Tier, please reach out and we will manually remove that content item limit on the project.\n",
            "Bugs Fixed\n",
            "GPLA-3529: Can't assign collection to multitenant content\n",
            "GPLA-3579: Should decode HTML characters when parsing HTML email\n",
            "GPLA-3576: Ingesting content in-place doesn't handle isSynchronous properly\n",
            "GPLA-3457: IsFeedDone doesn't return True for finished feed with no contents\n",
            "GPLA-3572: Not handling HTTP 400 error on uploading from URI\n",
            "PreviousDecember 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes\n",
            "NextDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "Last updated4 days ago\n",
            "- Completion [408 tokens (includes JSON guardrails tokens)], throughput: 83.611 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Dropbox feeds for ingesting files; requires appKey, appSecret, redirectUri, and refreshToken.\n",
            "  - Support for Box feeds for ingesting files; requires clientId, clientSecret, redirectUri, and refreshToken.\n",
            "  - Support for Intercom feeds for ingesting Articles and Tickets; requires accessToken.\n",
            "  - Support for Zendesk feeds for ingesting Articles and Tickets; requires accessToken and Zendesk subdomain.\n",
            "  - Support for OpenAI o1 model with enums O1_200k and O1_200k_20241217.\n",
            "  - Support for Gemini Flash 2.0 Experimental model with enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "  - Support for Cohere R7B model with enum COMMAND_R7B_202412.\n",
            "  - Ability to return low-level details from RAG conversations with includeDetails parameter.\n",
            "  - Support for filtering observables by URI property.\n",
            "  - Ability to bypass semantic search in content retrieval with conversations.\n",
            "  - New createdInLast property for entity filters and inLast property for content filters.\n",
            "  - Support for Azure AI Document Intelligence models: US_PAY_STUB, US_BANK_STATEMENT, US_BANK_CHECK.\n",
            "  - Support for Google Drive and OneDrive feeds to ingest specific files by file identifiers.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Removal of content items limit for projects upgraded to the Starter Tier after Dec 9, 2024.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with multitenant content assignment, HTML character decoding, synchronous content ingestion, feed completion status, and HTTP error handling during uploads.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Version updates include support for various new models and feeds.\n",
            "  - Significant changes to content item limits for Starter Tier projects.\n",
            "\n",
            "- Dates:\n",
            "  - Features and fixes released on December 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced integration capabilities with popular services, improved content management, and flexibility in data retrieval.\n",
            "\n",
            "2024-12-29T03:26:53.653Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.761734, used credits [0.00597000]\n",
            "- CONTENT [da1c0b2c-4bbf-4467-af16-f80122aeedd3]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [914 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-9-support-for-website-mapping-web-page-screenshots-groq-llama-3.3-model-bug-fixes</name><title>December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes | Graphlit Changelog</title></metadata> üéÑ\tDecember 2024\n",
            "December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports mapping a website with the mapWebmutation. You can provide a URL to a website, and the query will return a list of URLs based on the sitemap.xml (or sitemap-index.xml) file, at or underneath the provided URL.\n",
            "üí° Graphlit now supports the generation of web page screenshots with the screenshotPagemutation. By providing the URL of a web page, and optionally, the maximum desired height of the screenshot, we will screenshot the webpage and ingest it automatically as content.  You can provide an optional workflow, which will be applied to the ingested image content, for operations like generating image descriptions with a vision LLM.\n",
            "üí° Graphlit now supports the direct summarization of text with the summarizeTextmutation. By providing the desired summarization strategy, we will summarize the text (i.e. bullet points, social media posts) and return the summarization.\n",
            "üí° Graphlit now supports the direct extraction of text with the extractTextmutation. By providing the LLM tool definitions and an optional LLM specification, we will prompt the desired LLM (or OpenAI GPT-4o, by default) to invoke the provided tools, and return the JSON responses from the LLM tool calling.\n",
            "Graphlit now supports the latest Groq Llama 3.3 model, with the model enum LLAMA_3_3_70B.\n",
            "We have updated Cohere reranking to use the latest Cohere rerank-v3.5model by default.\n",
            "‚ö° We have added a new flattenCitationsfield to the ConversationStrategyInputtype.  By assigning this field to True, when calling promptConversation,we will combine multiple citations from the same content into a single citation.\n",
            "‚ö° For Microsoft email, Microsoft Teams and OneDrive feeds, we have added the clientIdand clientSecretfields as required feed properties. These properties must be assigned, in addition to the refreshTokenfield for proper authentication to the Microsoft Graph API used by these feeds. (Colab Notebook Example)\n",
            "Bugs Fixed\n",
            "GPLA-3492: Not finding sitemap at parent web path\n",
            "GPLA-3500: Failed to handle mismatch of Deepgram model/language\n",
            "PreviousDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "NextDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "Last updated18 days ago\n",
            "- Completion [269 tokens (includes JSON guardrails tokens)], throughput: 56.492 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for website mapping using mapWebmutation to return URLs from sitemap.xml.\n",
            "  - Generation of web page screenshots with screenshotPagemutation, including optional image processing workflows.\n",
            "  - Direct summarization of text with summarizeTextmutation, allowing for various summarization strategies.\n",
            "  - Direct extraction of text with extractTextmutation, utilizing LLM tools for JSON responses.\n",
            "  - Support for Groq Llama 3.3 model (LLAMA_3_3_70B).\n",
            "  - Updated Cohere reranking to use rerank-v3.5 model by default.\n",
            "  - New flattenCitations field in ConversationStrategyInput to combine multiple citations.\n",
            "  - Required clientId and clientSecret fields added for Microsoft email, Teams, and OneDrive feeds.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of citations in conversation prompts.\n",
            "  - Enhanced authentication requirements for Microsoft Graph API feeds.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with not finding sitemap at parent web path (GPLA-3492).\n",
            "  - Resolved mismatch handling of Deepgram model/language (GPLA-3500).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for web mapping, content ingestion, and improved API interactions.\n",
            "\n",
            "2024-12-29T03:26:53.134Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:06.867289, used credits [0.00700500]\n",
            "- CONTENT [a41c7ac9-806a-4fd2-9652-759775a70d8c]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1035 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-21-support-for-onedrive-and-google-drive-feeds-extract-images-from-pdfs-bug-fixes</name><title>February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes | Graphlit Changelog</title></metadata> üåßÔ∏è\tFebruary 2024\n",
            "February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports OneDrive and Google Drive feeds.  Files can be ingested from OneDrive or Google Drive, including shared drives where the authenticated user has access.  Both OneDrive and Google Drive support the reading of existing files, and tracking new files added to storage with recurrent feeds.\n",
            "üí° Graphlit now supports email backup files, such as EML or MSG, which will be assigned the EMAIL file type.  During email file preparation, we will automatically extract and ingest any file attachments.\n",
            "üí° Graphlit now automatically extracts embedded images in PDF files, ingests them as content objects, and links them as children of the parent PDF.\n",
            "üí° Graphlit now supports recursive Notion feeds.  When the isRecursive flag is true in the Notion feed properties, we will crawl child pages and databases, and recursively ingest them in addition to the specified pages and databases.\n",
            "Added support for assigning collections to content ingested with the ingestPage, ingestFile or ingestText mutations.  This saves a step where the content will automatically be added to the collection(s) without requiring another mutation call.\n",
            "Added support for the CODE file type for a wide variety of source code formats, i.e. Python .py, Javascript .js.  Code files use optimized text splitting for enhanced search and retrieval.\n",
            "Added support for customGuidance in Specification object, which can be used for injecting a guidance prompt during the RAG process.  For example, you can instruct the LLM to return a default response string if no content sources are found via semantic search.\n",
            "Added tenants field to Project object, which returns a list of all tenant IDs which have been used to create an entity in Graphlit.\n",
            "Added email metadata, separate from document metadata.  Now emails will contain indexed metadata such as to, from, or subject.\n",
            "‚ö° The contents field for content objects has been replaced with children and parent fields.  For example, when a ZIP file is unpacked, the unpacked files will be added as children of the ZIP file, and the ZIP file will be the parent of each of the unpacked files.\n",
            "‚ö° Removed enableImageAnalysis field from image preparation properties in workflow object.  Now is enabled by default.\n",
            "‚ö° Moved disableSmartCapture field to preparation workflow stage from page preparation properties.  This is used to disable the use of headless Chrome browser to capture HTML from web pages.  It is enabled by default, and if disabled, Graphlit will simply download the HTML from the web page rather than rendering on headless Chrome browser.\n",
            "Bugs Fixed\n",
            "GPLA-2099: Failed to ingest ArXiV PDF.  Fixed PDF parsing error.\n",
            "GPLA-2174: LLM response is incorrect with conversation history, but no content sources.\n",
            "GPLA-2199: ZIP package left in Indexed state after content workflow.\n",
            "PreviousMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "NextFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "Last updated9 months ago\n",
            "- Completion [325 tokens (includes JSON guardrails tokens)], throughput: 47.326 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OneDrive and Google Drive feeds, allowing ingestion of files from shared drives.\n",
            "  - Support for email backup files (EML, MSG) with automatic extraction of attachments.\n",
            "  - Automatic extraction of embedded images from PDF files, linking them as children of the parent PDF.\n",
            "  - Support for recursive Notion feeds with the isRecursive flag for crawling child pages and databases.\n",
            "  - Collections can now be assigned to content during ingestion without additional mutation calls.\n",
            "  - Introduction of CODE file type for various source code formats with optimized text splitting.\n",
            "  - Custom guidance can be injected during the RAG process via the Specification object.\n",
            "  - Added tenants field to Project object for listing tenant IDs used in entity creation.\n",
            "  - Email metadata is now indexed separately from document metadata.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Contents field in content objects replaced with children and parent fields for better structure.\n",
            "  - Removed enableImageAnalysis field; it is now enabled by default.\n",
            "  - Moved disableSmartCapture field to preparation workflow stage, enabled by default.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed PDF parsing error for ArXiV PDFs (GPLA-2099).\n",
            "  - Corrected LLM response issues with conversation history and no content sources (GPLA-2174).\n",
            "  - Resolved issue of ZIP package remaining in Indexed state after content workflow (GPLA-2199).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - These updates enhance file ingestion capabilities, improve metadata handling, and streamline workflows for developers.\n",
            "\n",
            "2024-12-29T03:26:51.934Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:05.943934, used credits [0.00977700]\n",
            "- CONTENT [3f2f43b0-49a5-462a-88c6-e0ecd92f41c1]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1579 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/january-2024/january-18-support-for-content-publishing-llm-tools-clip-image-embeddings-bug-fixes</name><title>January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes | Graphlit Changelog</title></metadata> üéÜ\tJanuary 2024\n",
            "January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports content publishing, where documents, audio transcripts and even image descriptions, can be summarized, and repurposed into blog posts, emails or AI-generated podcasts.  With the new publishContents mutation, you can configure LLM prompts for summarization and publishing, and assign specifications to use different models and/or system prompts for each step in the process.  The published content will be reingested into Graphlit, and can be searched or used for conversations, like any other form of content.\n",
            "üí° Graphlit now supports publishing conversations as content with the new publishConversation mutation.  You can generate text or audio transcripts of your conversations, to be reused in other tools.\n",
            "üí° Graphlit now supports bulk summarization of contents with the summarizeContents mutation.  You can filter a set of content, by feed, by observable or by similar text, and run a set of summarizations across each content in parallel.\n",
            "üí° Graphlit now supports LLM entity extraction, with the new MODEL_TEXT entity extraction service type.  Similar to using Azure Cognitive Service Text Analytics, you can use any OpenAI or Anthropic model for extracting entities from text.  Internally the LLM returns JSON-LD entities, which we convert into Person, Organization, Place, etc. entities and assign observations to the extracted content.\n",
            "üí° Graphlit now supports LLM tools (aka function calls) with OpenAI models.  You can define the tools to be used with the LLM in the specification object.  With the new extractContents mutation, you can execute a prompt against content using a specification with tools defined.  The mutation will return the JSON arguments assigned by the LLM.\n",
            "üí° Graphlit now supports callback webhooks for LLM tools.  If you assign a URI in the ToolDefinition object, Graphlit will call your webhook the tool name and JSON arguments.  When you respond to the webhook with JSON, we will add that response to the LLM messages, and ask the LLM to complete the original prompt.\n",
            "üí° Graphlit now supports the selection of the Deepgram model (such as Meeting, Phonecall or Finance) with the preparation workflow.  Also, you can assign your own Deepgram API key, which will be used for audio transcription using that workflow.\n",
            "Added support for CLIP image embeddings using Roboflow, which can be used for similar image search.  If you search for contents by similar contents, we will now use the content's text and/or image embeddings to find similar content.\n",
            "Added support for dynamic web page ingestion.  Graphlit now navigates to and automatically scrolls web pages using Browserless.io, so we capture the fully rendered HTML before extracting text.  Also, we now support web page screenshots, if enabled with enableImageAnalysis property in preparation workflow.  These screenshots can be analyzed with multimodal modals, such as GPT-4 Vision, or can be used to create image embeddings for similar image search.\n",
            "Added table parsing when preparing documents.  We now store structured (tab-delimited) text in the JSON text mezzanine which is extracted from documents in the preparation workflow.\n",
            "Added reverse geocoding of lat/long locations found in image or other content metadata.  We now store the real-world address with the content metadata, for use in conversations.\n",
            "Added assistant messages to the conversation message history provided to the LLM.  Originally we had included only user messages, but now we are formatting both user and assistant messages into the LLM prompt for conversations.\n",
            "Added new chunking algorithm for text embeddings.  We support semantic chunking at the page or transcript segment level, and now will create embeddings from smaller sized text chunks per page or segment.\n",
            "Added content metadata to text and image embeddings.  To provide better context for the text embeddings, we now include formatted content metadata, which includes fields like title, subject, author, or description.  For emails, we include to, from, cc, and bcc fields.\n",
            "Added helper mutations isContentDone and isFeedDone which can be used for polling completion of ingested content, or all content ingested by a feed.\n",
            "Added richer image descriptions generated by the GPT-4 Vision model.  Now these provide more useful detail.\n",
            "Added validation of extracted hyperlinks.  Now we test the URIs and remove any inaccessible links during content enrichment.\n",
            "Added deleteContents,  deleteFeeds,  and deleteConversations mutations for multi-deletion of contents, feeds or conversations.\n",
            "Added deleteAllContents,  deleteAllFeeds,  and deleteAllConversations mutations for bulk, filtered deletion of entities.  You can delete all your contents, feeds, or conversations in your project, or a filtered subset of those entities.\n",
            "‚ÑπÔ∏è Starter tier now has a higher content limit of 100K content items.\n",
            "‚ö° In the OpenAIImageExtractionProperties type, the detailMode field was renamed to detailLevel.\n",
            "‚ö° Each SummarizationStrategy object now accepts the specification which is used by the summarization, rather than being assigned at the preparation workflow stage.\n",
            "‚ö° addCollectionContents and removeCollectionContents mutations have been deprecated in favor of addContentsToCollections and removeContentsFromCollection mutations.\n",
            "Bugs Fixed\n",
            "GPLA-1846: Parse Markdown headings into mezzanine JSON\n",
            "GPLA-1779: Not returning SAS token with mezzanine, master URIs\n",
            "GPLA-1348: Summarize text content, not just file content\n",
            "GPLA-1297: Not assigning content error message on preparation workflow failure\n",
            "PreviousJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "NextDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes Last updated8 months ago\n",
            "- Completion [420 tokens (includes JSON guardrails tokens)], throughput: 70.660 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for content publishing, allowing documents, audio transcripts, and image descriptions to be summarized and repurposed.\n",
            "  - New mutation for publishing conversations as content, generating text or audio transcripts.\n",
            "  - Bulk summarization of contents with the summarizeContents mutation.\n",
            "  - LLM entity extraction using MODEL_TEXT service type, returning JSON-LD entities.\n",
            "  - Support for LLM tools (function calls) with OpenAI models, including the new extractContents mutation.\n",
            "  - Callback webhooks for LLM tools, enabling interaction with external services.\n",
            "  - Selection of Deepgram models for audio transcription with custom API key support.\n",
            "  - CLIP image embeddings support for similar image search.\n",
            "  - Dynamic web page ingestion with automatic scrolling and screenshot capabilities.\n",
            "  - Table parsing for structured text extraction from documents.\n",
            "  - Reverse geocoding of lat/long locations in content metadata.\n",
            "  - Inclusion of assistant messages in conversation history for LLM prompts.\n",
            "  - New chunking algorithm for semantic text embeddings.\n",
            "  - Enhanced content metadata for text and image embeddings.\n",
            "  - Helper mutations for polling content ingestion completion.\n",
            "  - Richer image descriptions generated by GPT-4 Vision model.\n",
            "  - Validation of extracted hyperlinks to remove inaccessible links.\n",
            "  - Multi-deletion mutations for contents, feeds, and conversations.\n",
            "  - Bulk deletion mutations for filtered subsets of entities.\n",
            "  - Increased content limit for Starter tier to 100K items.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed detailMode field to detailLevel in OpenAIImageExtractionProperties.\n",
            "  - SummarizationStrategy objects now accept specifications directly.\n",
            "  - Deprecated addCollectionContents and removeCollectionContents mutations in favor of new naming conventions.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with parsing Markdown headings, returning SAS tokens, summarizing text content, and assigning error messages on workflow failures.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: January 18, 2024.\n",
            "  - Value: Offers developers enhanced content management capabilities, improved LLM interactions, and better data handling for various content types.\n",
            "\n",
            "2024-12-29T03:26:51.479Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.692531, used credits [0.00483300]\n",
            "- CONTENT [bae5b595-6a1e-4dcb-95aa-3eaf6b55cc52]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [683 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-2-support-for-semantic-alerts-openai-0125-models-performance-enhancements-bug-fixes</name><title>February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes | Graphlit Changelog</title></metadata> üåßÔ∏è\tFebruary 2024\n",
            "February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports Semantic Alerts, which allows for LLM summarization and publishing of content, on a periodic basis.  This is useful for generating daily reports from email, Slack or other time-based feeds.  Alerts support the same publishing options, i.e. audio and text, as the publishContents mutation.\n",
            "üí° Graphlit now supports the latest OpenAI 0125 model versions, for GPT-4 and GPT-3.5 Turbo.  We will add support for Azure OpenAI when Microsoft releases support for these.\n",
            "Slack feeds now support a listing type field, where you can specify if you want PAST or NEW Slack messages in the feed.\n",
            "üî• This release provides many performance enhancements, which will speed up the content workflows for ingested content.\n",
            "Bugs Fixed\n",
            "GPLA-2114: Collections not being added to text embedding index documents.\n",
            "GPLA-2063: Not handling hallucinated citations.\n",
            "GPLA-1916: Collections not inherited from project-scope into tenant-scope.\n",
            "GPLA-2105: Should error on add/remove of contents to/from collections if content does not exist.\n",
            "PreviousFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "NextJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "Last updated10 months ago\n",
            "- Completion [232 tokens (includes JSON guardrails tokens)], throughput: 62.830 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Semantic Alerts for LLM summarization and content publishing on a periodic basis, useful for generating daily reports from various feeds.\n",
            "  - Support for OpenAI 0125 model versions for GPT-4 and GPT-3.5 Turbo, with plans to add Azure OpenAI support when available.\n",
            "  - Slack feeds now include a listing type field to specify PAST or NEW messages.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Performance enhancements to speed up content workflows for ingested content.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with collections not being added to text embedding index documents (GPLA-2114).\n",
            "  - Resolved handling of hallucinated citations (GPLA-2063).\n",
            "  - Fixed inheritance of collections from project-scope to tenant-scope (GPLA-1916).\n",
            "  - Added error handling for adding/removing contents to/from collections if content does not exist (GPLA-2105).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 2, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved content management capabilities, enhanced performance, and better error handling.\n",
            "\n",
            "2024-12-29T03:26:48.728Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.932926, used credits [0.00347700]\n",
            "- CONTENT [cd3f1b88-90bb-4de6-a91a-4c74ecfbcf5c]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [555 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-1-support-for-retrieval-only-rag-pipeline-bug-fixes</name><title>December 1: Support for retrieval-only RAG pipeline, bug fixes | Graphlit Changelog</title></metadata> üéÑ\tDecember 2024\n",
            "December 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports formatting of LLM-ready prompts with our RAG pipeline, via the new formatConversation and completeConversation mutations.  This is valuable for supporting LLM streaming by directly calling the LLM from your application, and using Graphlit for RAG retrieval and conversation history. (Colab Notebook Example)\n",
            "We have added support for inline hyperlinks in extracted text from documents and web pages.\n",
            "Bugs Fixed\n",
            "GPLA-3466: Owner ID should accept any non-whitespace string\n",
            "GPLA-3458: Not getting Person-to-Organization edges from entity extraction\n",
            "PreviousDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "NextNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "Last updated26 days ago\n",
            "- Completion [151 tokens (includes JSON guardrails tokens)], throughput: 78.120 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for formatting LLM-ready prompts with RAG pipeline using formatConversation and completeConversation mutations.\n",
            "  - Inline hyperlinks support in extracted text from documents and web pages.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved LLM streaming capabilities by directly calling the LLM from applications.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where Owner ID should accept any non-whitespace string (GPLA-3466).\n",
            "  - Resolved problem with missing Person-to-Organization edges from entity extraction (GPLA-3458).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 1, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with better integration for LLM interactions and improved data extraction capabilities.\n",
            "\n",
            "2024-12-29T03:26:47.756Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:05.245133, used credits [0.00593400]\n",
            "- CONTENT [51e379d2-307c-42f3-b5cf-eb3b02c6f068]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [906 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2023/july-15-support-for-sharepoint-feeds-new-conversation-features</name><title>July 15: Support for SharePoint feeds, new Conversation features | Graphlit Changelog</title></metadata> üéá\tJuly 2023\n",
            "July 15: Support for SharePoint feeds, new Conversation features\n",
            "New Features\n",
            "üí° Added support for SharePoint feeds: now can create feed to ingest files from SharePoint document library (and optionally, folder within document library)\n",
            "üí° Added support for PII detection during entity extraction from text documents and audio transcripts: now we will create labels such as PII: Social Security Number automatically when PII is detected\n",
            "üí° Added support for developer's own OpenAI API keys and Azure OpenAI deployments in Specifications\n",
            "‚ÑπÔ∏è  Changed semantics of deleteFeed to delete the contents ingested by the feed; since contents are linked to feeds, now feeds can be disabled, while keeping the lineage to the feed, and if feeds are deleted, they will delete the linked contents, so we never lose the feed-to-content lineage\n",
            "Added GraphQL query for SharePoint consent URI, for registered Graphlit Platform Azure AD application\n",
            "Better handling of web sitemap indexes: now if a sitemap.xml contains a sitemapindex element, we will load all linked sitemaps for evaluating web pages to ingest from Web feed\n",
            "Added new GraphQL mutations for openConversation, closeConversation and undoConversation\n",
            "Added timestamps to Conversation messages\n",
            "Added new GraphQL mutations for openCollection and closeCollection\n",
            "Added more configuration for content search: now can specify searchType (KEYWORD, VECTOR, HYBRID) and queryType (SIMPLE, FULL - aka Lucene syntax)\n",
            "Better parsing of iTunes podcast metadata\n",
            "‚ö° Renamed listingLimit field on feeds to readLimit\n",
            "‚ö° Renamed topK to numberSimilar for content vector search type\n",
            "‚ö° Changed GraphQL feed properties: split out azure into azureBlob and azureFile properties\n",
            "‚ö° Changed GraphQL specification properties: split out openAI into openAI and azureOpenAI properties\n",
            "‚ö° Removed count fields on query results, and replaced with explicit count{Entity} queries, which support search and filtering.\n",
            "Bugs Fixed\n",
            "GPLA-1043: Reddit readLimit not taking effect: now the specified limit of Reddit posts will be leveraged for Reddit feeds\n",
            "GPLA-1064: Performance on entity extraction and observation creation for large PDFs was under expectations: now able to build knowledge graph from large PDFs much faster (4x speed improvement)\n",
            "GPLA-1053: If rendition generation errored during content workflow, the content was not properly marked as errored\n",
            "GPLA-1102: Large Web sitemaps were slow to load; rewrote sitemap index handling, and now can process sitemaps with 150K+ entries in seconds.\n",
            "PreviousAugust 3: New data model for Observations, new Category entity\n",
            "Last updated5 months ago\n",
            "- Completion [268 tokens (includes JSON guardrails tokens)], throughput: 51.095 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for SharePoint feeds to ingest files from SharePoint document libraries.\n",
            "  - PII detection during entity extraction from text documents and audio transcripts.\n",
            "  - Support for custom OpenAI API keys and Azure OpenAI deployments.\n",
            "  - GraphQL query for SharePoint consent URI for registered Azure AD applications.\n",
            "  - Improved handling of web sitemap indexes for better ingestion.\n",
            "  - New GraphQL mutations for managing conversations and collections.\n",
            "  - Timestamps added to Conversation messages.\n",
            "  - Enhanced content search configuration options.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed fields for clarity: listingLimit to readLimit, topK to numberSimilar.\n",
            "  - Split GraphQL properties for Azure and OpenAI for better organization.\n",
            "  - Removed count fields on query results, replaced with explicit count queries.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed Reddit readLimit issue for effective post limits.\n",
            "  - Improved performance for entity extraction from large PDFs (4x speed increase).\n",
            "  - Corrected error marking for content workflow rendition generation.\n",
            "  - Enhanced loading speed for large web sitemaps (150K+ entries).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 15, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced data ingestion capabilities, improved performance, and better management of content and conversations.\n",
            "\n",
            "2024-12-29T03:26:46.755Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.606375, used credits [0.00462600]\n",
            "- CONTENT [5c9b0f96-4efc-4319-b2c2-0fc27875f030]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [666 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/january-2024/january-22-support-for-google-and-microsoft-email-feeds-reingest-content-in-place-bug-fixes</name><title>January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes | Graphlit Changelog</title></metadata> üéÜ\tJanuary 2024\n",
            "January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports Google and Microsoft email feeds.  Email feeds can be created to ingest past emails, or poll for new emails.  Emails create an EMAIL content type. Attachment files can optionally be extracted from emails, and will be linked to their parent email content. If assigning a workflow to the feed, the workflow will be applied both to the email content and the extracted attachment files.\n",
            "üí° Graphlit now supports reingesting content in-place.  The ingestText, ingestPage and ingestFile mutations now take an optional id parameter for an existing content object.  If this id is provided, the existing content will be updated from the provided text or URI source, and will restart the assigned workflow.\n",
            "Added restartAllContents mutation to restart workflow on all partially-ingested contents in project.\n",
            "Added text field to ConversationCitation type, which returns the relevant text from the content source with the citation.\n",
            "Bugs Fixed\n",
            "GPLA-1313: Not extracting links from HTML\n",
            "GPLA-2030: No text extracted from shapes in PPTX files\n",
            "PreviousFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "NextJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [219 tokens (includes JSON guardrails tokens)], throughput: 60.726 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Google and Microsoft email feeds, allowing ingestion of past and new emails, creating an EMAIL content type. Attachments can be extracted and linked to their parent emails.\n",
            "  - Support for reingesting content in-place with optional id parameter for existing content objects, updating them from provided text or URI source, and restarting the assigned workflow.\n",
            "  - Added restartAllContents mutation to restart workflows on all partially-ingested contents in a project.\n",
            "  - Added text field to ConversationCitation type to return relevant text from the content source with the citation.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved content ingestion capabilities with new mutations and features.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with link extraction from HTML (GPLA-1313).\n",
            "  - Resolved problem of no text being extracted from shapes in PPTX files (GPLA-2030).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: January 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced email integration and content management capabilities, improving workflow efficiency and content handling.\n",
            "\n",
            "2024-12-29T03:26:45.991Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.001619, used credits [0.00609900]\n",
            "- CONTENT [636bd286-d807-4838-af4d-0c37d24772b7]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1009 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-19-support-for-openai-gpt-4o-mini-byo-key-for-azure-ai-similarity-by-summary-bug-fixes</name><title>July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes | Graphlit Changelog</title></metadata> ‚òÄÔ∏è\tJuly 2024\n",
            "July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports the OpenAI GPT-4o Mini model, with 16k output tokens.\n",
            "üí° Graphlit now supports 'bring-your-own-key' for Azure AI Document Intelligence models.  We have added a custom endpoint and key property, which can be assigned to use your own Azure AI resource.\n",
            "Updated to use Jina reranker v2 (jina-reranker-v2-base-multilingual) by default.\n",
            "Updated to assign the summary, bullets, etc properties when calling summarizeContents mutation.  Now when summarizing contents, we will store the resulting summary in the content itself, in addition to returning the summarized results.\n",
            "Added relevance property to all entity types, which will be assigned when searching for these entities.  Entity results will be sorted (descending) by this search relevance score.\n",
            "Added the ability to manually update summary, bullets, etc. properties when calling the updateContent mutation.\n",
            "Added offset property to AtlassianJiraFeedProperties, so the timezone offset can be properly assigned for paging of the Jira feed.  (Defaults to zero offset, i.e. UTC.)  Jira does not store dates in UTC format, and the timestamps are based on the server timezone of the hosted Jira instance.  By assigning the timezone offset with the Jira feed, we can reliably page the updated date timestamps from the Jira API.\n",
            "‚ö° We have changed the content similarity search behavior to find similar content by summary, rather than text of the document, when a summary has been previously generated.  For long documents, this will provide a more accurate similarity, rather than comparing on the first few pages of text in a document.\n",
            "‚ö° We have changed the behavior of assigning offset in the entity filter objects for paging through entities.  If using vector or hybrid search, this offset will be ignored (i.e. zero offset).  Paging will not be supported through vector or hybrid search results. For keyword search, the offset will continue to be used, along with the limit property, to provide paging through the search results.  We have made this change because we have found that index-based paging is not reliable with our vector/hybrid search approach.  We are investigating ways to support this reliably with vector/hybrid search in the future.\n",
            "Bugs Fixed\n",
            "GPLA-2915: Add retry on OpenAI API HTTP 524 error (gateway timeout).\n",
            "GPLA-2908: Not paging through Jira feed correctly.\n",
            "GPLA-2917: Search by similar content is not giving expected results on long documents.\n",
            "GPLA-2244: Keyword search not finding text in latter part of long PDF.\n",
            "PreviousJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "NextJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [256 tokens (includes JSON guardrails tokens)], throughput: 85.287 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4o Mini model with 16k output tokens.\n",
            "  - 'Bring-your-own-key' support for Azure AI Document Intelligence models with custom endpoint and key property.\n",
            "  - Updated to Jina reranker v2 by default.\n",
            "  - Enhanced summarizeContents mutation to store summaries in content.\n",
            "  - Added relevance property for entity types, sorting results by search relevance score.\n",
            "  - Ability to manually update summary and bullet properties in updateContent mutation.\n",
            "  - Added offset property to AtlassianJiraFeedProperties for proper timezone assignment in Jira feed.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Content similarity search now finds similar content by summary for better accuracy.\n",
            "  - Changed entity filter object behavior for paging; zero offset for vector/hybrid search.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Added retry on OpenAI API HTTP 524 error.\n",
            "  - Fixed paging issues in Jira feed.\n",
            "  - Improved search results for similar content in long documents.\n",
            "  - Resolved keyword search issues in long PDFs.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 19, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved model support, enhanced search capabilities, and better handling of content summaries and entity relevance.\n",
            "\n",
            "2024-12-29T03:26:45.890Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.785221, used credits [0.00461700]\n",
            "- CONTENT [23ae5fe0-5e6d-411c-b219-5ebaf593062e]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [703 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-25-support-for-mistral-large-2-and-nemo-groq-llama-3.1-models-bug-fixes</name><title>July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes | Graphlit Changelog</title></metadata> ‚òÄÔ∏è\tJuly 2024\n",
            "July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports the Mistral Large 2 and Mistral Nemo models. The existing MISTRAL_LARGE model enum now will use Mistral Large 2.\n",
            "üí° Graphlit now supports the Llama 3.1 8b, 70b and 405b models on Groq.  (Note, these are rate-limited according to Groq's platform constraints.)\n",
            "Added support for revision strategy on data extraction specifications.  Now you can prompt the LLM to revise its previous data extraction response, similar to the existing completion revision strategy.\n",
            "Added version property for AzureDocumentPreparationProperties type for assigning the API version used by Azure AI Document Intelligence.   By default, Graphlit will continue to use the v4.0 (Preview) API version, but you can override this to assign version to V2023_10_31 to use the v3.1 (GA) API version instead.  For some documents, the General Availability (GA) version of the API can provide better results.\n",
            "Bugs Fixed\n",
            "GPLA-2988: Not extracting hyperlinks from Office documents.\n",
            "PreviousJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "NextJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [209 tokens (includes JSON guardrails tokens)], throughput: 43.676 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Mistral Large 2 and Mistral Nemo models.\n",
            "  - Support for Llama 3.1 models (8b, 70b, 405b) on Groq, with rate limits per Groq's platform.\n",
            "  - Added revision strategy for data extraction specifications, allowing LLM to revise previous responses.\n",
            "  - Version property added for AzureDocumentPreparationProperties to assign API version for Azure AI Document Intelligence, with options for v4.0 (Preview) or v3.1 (GA).\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Existing MISTRAL_LARGE model enum updated to use Mistral Large 2.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with hyperlinks not being extracted from Office documents (GPLA-2988).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 25, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve model support and data extraction capabilities, offering developers more flexibility and better results with Azure AI.\n",
            "\n",
            "2024-12-29T03:26:42.914Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.026975, used credits [0.00426300]\n",
            "- CONTENT [ce68f926-3ab6-4a73-8f33-3f5e552c2714]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [633 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-28-support-for-indexing-workflow-stage-azure-ai-language-detection-bug-fixes</name><title>July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes | Graphlit Changelog</title></metadata> ‚òÄÔ∏è\tJuly 2024\n",
            "July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "New Features\n",
            "Added indexing workflow stage. This provides for configuration of indexing services, which may infer metadata from the content.\n",
            "Added AZURE_AI_LANGUAGE content indexing service, which supports inferring the language of extracted text or transcript.\n",
            "Added support for language content metadata.  This returns a list of languages in ISO 639-1 format, which may have been inferred from the extracted text or transcript.\n",
            "Added support for MODEL_IMAGE extraction service.  This provides integration with vision models beyond those provided by OpenAI.  You can assign a custom specification and bring-your-own API key for image extraction models.\n",
            "‚ö° We have deprecated the OPENAI_IMAGE service type, and developers should now use the LLM image service instead.\n",
            "‚ö° We have removed the language field from AudioMetadata type, which has been replaced by the new LanguageMetadata type.\n",
            "Bugs Fixed\n",
            "GPLA-2987: Extracting text with Azure Doc Intelligence does not extract hyperlinks\n",
            "PreviousAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "NextJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [197 tokens (includes JSON guardrails tokens)], throughput: 97.189 tokens/sec:\n",
            "- New Features:\n",
            "  - Added indexing workflow stage for configuring indexing services to infer metadata from content.\n",
            "  - Introduced AZURE_AI_LANGUAGE content indexing service for inferring language from extracted text or transcripts.\n",
            "  - Support for language content metadata returning a list of languages in ISO 639-1 format.\n",
            "  - Added MODEL_IMAGE extraction service for integration with various vision models, allowing custom specifications and API keys.\n",
            "  - Deprecated OPENAI_IMAGE service type; developers should use LLM image service instead.\n",
            "  - Removed language field from AudioMetadata type, replaced by new LanguageMetadata type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2987 where Azure Doc Intelligence did not extract hyperlinks.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 28, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for content indexing, language detection, and image extraction, improving overall functionality and flexibility.\n",
            "\n",
            "2024-12-29T03:26:42.894Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.091354, used credits [0.00681300]\n",
            "- CONTENT [595fbc8a-88d8-4e2f-a3f0-33a86c892d76]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [887 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/june-2024/june-9-support-for-deepseek-models-json-ld-webpage-parsing-performance-improvements-and-bug-fixes</name><title>June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes | Graphlit Changelog</title></metadata> üéì\tJune 2024\n",
            "June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports Deepseek LLMs for prompt completion.  We offer the deepseek-chat and deepseek-coder models.\n",
            "üí° Graphlit now supports parsing embedded JSON-LD from web pages.  If a web page contains 'script' tags with JSON-LD, we will automatically parse and inject into the knowledge graph.\n",
            "‚ö° We have changed the default model for entity extraction and image completions to be OpenAI GPT-4o.  This provides faster performance and better quality output.\n",
            "‚ö° We have changed the behavior of knowledge graph generation, from a prompted conversation, to be opt-in.  In order to receive the graph's nodes and edges with the response, you will now need to set generateGraph to True in the specification's graphStrategy object.  This provides improved performance when the graph is not needed for visualization.\n",
            "Added thing property for observable entities, which returns the JSON-LD metadata associated with the entity.\n",
            "Added regex-based filtering for URI paths during feed ingestion, link crawling, and workflow filtering.  You can assign regex patterns in allowedPaths and excludedPaths.\n",
            "Added observableLimit to configure the limit of how many observed entities will be added to the GraphRAG context, defaults to 1000.\n",
            "Added prompt to suggestConversation mutation, which allows customization of the followup question generation.\n",
            "Updated suggestConversation to incorporate the past conversation message history, in addition to the filtered set of content sources.\n",
            "üî•  We have improved performance in knowledge graph retrieval and generation, via better parallelization and batching.\n",
            "Bugs Fixed\n",
            "GPLA-2748: Optimize the retrieval performance of observed entities during GraphRAG\n",
            "GPLA-2732: Invalid user-provided URI causing parsing exception\n",
            "GPLA-2666: Shouldn't require tenant ID for Microsoft email or Teams\n",
            "GPLA-2772: Not returning labels or categories from graph in API\n",
            "GPLA-2762: Failed to extract spreadsheet images\n",
            "GPLA-2687: Email to/from not getting added as observations on emails\n",
            "GPLA-2738: API is returning 'audio' metadata from podcast HTML document\n",
            "PreviousJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "NextMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [346 tokens (includes JSON guardrails tokens)], throughput: 84.569 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Deepseek LLMs for prompt completion (deepseek-chat and deepseek-coder models).\n",
            "  - Parsing of embedded JSON-LD from web pages, automatically injecting into the knowledge graph.\n",
            "  - Default model for entity extraction and image completions changed to OpenAI GPT-4o for improved performance and quality.\n",
            "  - Knowledge graph generation behavior changed to opt-in; requires setting generateGraph to True for response inclusion.\n",
            "  - Added thing property for observable entities to return associated JSON-LD metadata.\n",
            "  - Regex-based filtering for URI paths during feed ingestion and workflow filtering.\n",
            "  - Added observableLimit to configure the number of observed entities in GraphRAG context (default 1000).\n",
            "  - Customization of follow-up question generation via prompt in suggestConversation mutation.\n",
            "  - Updated suggestConversation to include past conversation message history.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved performance in knowledge graph retrieval and generation through better parallelization and batching.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Optimized retrieval performance of observed entities during GraphRAG.\n",
            "  - Fixed parsing exception caused by invalid user-provided URI.\n",
            "  - Removed requirement for tenant ID for Microsoft email or Teams.\n",
            "  - Resolved issue of not returning labels or categories from graph in API.\n",
            "  - Fixed failure to extract spreadsheet images.\n",
            "  - Addressed issue of email to/from not being added as observations.\n",
            "  - Corrected API returning 'audio' metadata from podcast HTML documents.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: June 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for working with LLMs, improved performance in data handling, and more flexible graph generation options.\n",
            "\n",
            "2024-12-29T03:26:42.402Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.580473, used credits [0.00517800]\n",
            "- CONTENT [bbe12218-8806-409d-a79b-148d8f0eb515]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [730 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/june-2024/june-21-support-for-the-claude-3.5-sonnet-model-knowledge-graph-semantic-search-and-bug-fixes</name><title>June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes | Graphlit Changelog</title></metadata> üéì\tJune 2024\n",
            "June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports the Anthropic Claude 3.5 Sonnet model, which can be assigned with the CLAUDE_3_5_SONNET model enum.\n",
            "üí° Graphlit now supports semantic search of observable entities in the knowledge graph, such as Person, Organization and Place.  These entity types will now have vector embeddings created from their enriched metadata, and support searching by similar text, and searching by similar entities.\n",
            "‚ö° We have changed the Google Drive and Google Email feed properties to require the Google OAuth client ID and client secret, along with the existing refresh token, for proper authentication against Google APIs.\n",
            "‚ö° We have added a credits quota on the Free Tier.  Once 1000 credits have been used on the Free Tier, no more content can be ingested, and an upgrade to a paid tier is required.  Customers will receive an email when the credits, storage or contents quota has been reached.\n",
            "Bugs Fixed\n",
            "GPLA-2837: Failed to ingest LinkedIn page as Web feed\n",
            "GPLA-2831: Zero-byte file was left in Indexed state\n",
            "GPLA-2834: Not reading any files from Azure blob feed with space in prefix\n",
            "GPLA-2828: Better handling for files with unknown (or missing) file extensions\n",
            "PreviousJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "NextJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [249 tokens (includes JSON guardrails tokens)], throughput: 54.361 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for the Anthropic Claude 3.5 Sonnet model (model enum: CLAUDE_3_5_SONNET).\n",
            "  - Semantic search for observable entities in the knowledge graph (Person, Organization, Place) with vector embeddings for enriched metadata.\n",
            "  - Google Drive and Google Email feed properties now require Google OAuth client ID, client secret, and refresh token for authentication.\n",
            "  - Introduction of a credits quota on the Free Tier; ingestion stops after 1000 credits, requiring an upgrade to a paid tier.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved authentication process for Google APIs.\n",
            "  - Notification system for reaching credits, storage, or content quotas.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Resolved issues with ingesting LinkedIn pages as Web feeds.\n",
            "  - Fixed zero-byte files remaining in Indexed state.\n",
            "  - Addressed problems with reading files from Azure blob feeds with spaces in prefixes.\n",
            "  - Enhanced handling for files with unknown or missing file extensions.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: June 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support, improved search capabilities, better authentication processes, and a clearer understanding of usage limits.\n",
            "\n",
            "2024-12-29T03:26:40.970Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.458873, used credits [0.00621900]\n",
            "- CONTENT [3b356809-5179-4144-80bc-baf4dd184462]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [829 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/march-2024/march-10-support-for-claude-3-mistral-and-groq-models-usage-credits-telemetry-bug-fixes</name><title>March 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes | Graphlit Changelog</title></metadata> üçÄ\tMarch 2024\n",
            "March 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports a Command-Line Interface (CLI) for directly accessing the Graphlit Data API without writing code.  See the documentation here.\n",
            "üí° Graphlit now supports the Groq Platform, and models such as Mixtral 8x7b.\n",
            "üí° Graphlit now supports Claude 3 Opus and Sonnet models.\n",
            "üí° Graphlit now supports Mistral La Plateforme, and models such as Mistral Small, Medium, and Large and Mixtral 8x7b.\n",
            "üí° Graphlit now supports the latest v4 of Azure Document Intelligence, including their new models such as Credit Card, Marriage Certificate, and Mortgage documents.\n",
            "Added support for detailed usage and credits telemetry via API, with the usage, credits, lookupUsage and lookupCredits queries.\n",
            "Added support for correlated telemetry, where an optional correlationId can be provided with GraphQL queries and mutations, so credits and usage can be tracked across requests.\n",
            "Added support for project webhook, which will be called when credits have been consumed by the project.\n",
            "Added support for image extraction during DOCX, XLSX, and PPTX document preparation.\n",
            "Added text and markdown properties to Content object, which provide formatted output of extracted text from any content.\n",
            "Added more accurate extraction of tables into mezzanine JSON format, across all content types.\n",
            "Added throughput property to Conversation messages, which returns the tokens/second throughput of LLM.\n",
            "‚ö° Deprecated mezzanineUri property in Content object, which has been replaced by textUri and audioUri.\n",
            "Bugs Fixed\n",
            "GPLA-2281: Not extracting table from PPTX file.\n",
            "GPLA-2282: Not extracting Markdown tables.\n",
            "GPLA-2247: Not extracting relative HTML links properly.\n",
            "GPLA-2241: Failed to post Alert to Slack with Markdown format.\n",
            "PreviousMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "NextFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [311 tokens (includes JSON guardrails tokens)], throughput: 89.914 tokens/sec:\n",
            "- New Features:\n",
            "  - Command-Line Interface (CLI) for direct access to the Graphlit Data API.\n",
            "  - Support for Groq Platform and models like Mixtral 8x7b.\n",
            "  - Support for Claude 3 Opus and Sonnet models.\n",
            "  - Support for Mistral La Plateforme and models such as Mistral Small, Medium, and Large.\n",
            "  - Support for Azure Document Intelligence v4, including new models for Credit Card, Marriage Certificate, and Mortgage documents.\n",
            "  - Detailed usage and credits telemetry via API.\n",
            "  - Correlated telemetry with optional correlationId for tracking credits and usage.\n",
            "  - Project webhook for notifications when credits are consumed.\n",
            "  - Image extraction support during DOCX, XLSX, and PPTX document preparation.\n",
            "  - Text and markdown properties added to Content object for formatted output.\n",
            "  - More accurate table extraction into mezzanine JSON format.\n",
            "  - Throughput property added to Conversation messages for tokens/second throughput.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Deprecated mezzanineUri property in Content object, replaced by textUri and audioUri.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with table extraction from PPTX files and Markdown tables.\n",
            "  - Corrected extraction of relative HTML links.\n",
            "  - Resolved failure to post alerts to Slack with Markdown format.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: March 10, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for data extraction, improved API access, and better tracking of usage and credits.\n",
            "\n",
            "2024-12-29T03:26:40.827Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.713969, used credits [0.00512100]\n",
            "- CONTENT [dd2cef1a-5292-4654-a2e2-d40b4f82db84]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [723 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-4-support-for-webhook-alerts-keywords-summarization-deepseek-128k-context-window-bug-fixes</name><title>July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes | Graphlit Changelog</title></metadata> ‚òÄÔ∏è\tJuly 2024\n",
            "July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports webhook Alerts.  In addition to Slack notifications, you can now receive an HTTP POST webhook with the results of the published text (or text and audio URI) from a prompted alert.\n",
            "Updated the Deepseek chat and coder models to support a 128k token context window.\n",
            "Added customSummary property to Content object, which returns the custom summary generated via preparation workflow.\n",
            "Added keywords summarization type, which is now stored in keywords property in Content object.\n",
            "Added slackChannels query, which returns the list of Slack channels from the workspace authenticated by the Slack bot token.\n",
            "‚ö° We have changed the response from the credits query to return a single ProjectCredits object, rather than the list of correlated objects previously returned.  The credits response now covers all credit usage over the time period specified.\n",
            "Bugs Fixed\n",
            "GPLA-2874: Processing entities is taking longer than 30min for 300+ page PDF\n",
            "GPLA-2875: Messages in queue expiring too early\n",
            "GPLA-2881: Feed read count increasing, after hitting read limit\n",
            "GPLA-2884: Need to handle Anthropic 'overloaded' API response\n",
            "GPLA-2906: JIRA issue identifier not assigned to issue metadata\n",
            "PreviousJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "NextJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [246 tokens (includes JSON guardrails tokens)], throughput: 90.642 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for webhook Alerts with HTTP POST notifications for published text.\n",
            "  - Updated Deepseek models to support a 128k token context window.\n",
            "  - Added customSummary property to Content object for custom summaries.\n",
            "  - Introduced keywords summarization type stored in keywords property of Content object.\n",
            "  - Added slackChannels query to list Slack channels from the authenticated workspace.\n",
            "  - Changed credits query response to return a single ProjectCredits object covering all credit usage.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved response structure for credits query.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - GPLA-2874: Reduced processing time for large PDF entities.\n",
            "  - GPLA-2875: Fixed issue with messages in queue expiring too early.\n",
            "  - GPLA-2881: Resolved feed read count issue after hitting read limit.\n",
            "  - GPLA-2884: Handled Anthropic 'overloaded' API response.\n",
            "  - GPLA-2906: Assigned JIRA issue identifier to issue metadata.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 4, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve notification capabilities, processing efficiency, and user experience for developers.\n",
            "\n",
            "2024-12-29T03:26:38.712Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.228139, used credits [0.00316200]\n",
            "- CONTENT [feb66b5f-ba9e-448e-9909-ba5e2fdbf56d]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [542 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/march-2024/march-13-support-for-claude-3-haiku-model-direct-ingestion-of-base64-encoded-files</name><title>March 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files | Graphlit Changelog</title></metadata> üçÄ\tMarch 2024\n",
            "March 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "New Features\n",
            "üí° Graphlit now supports the Claude 3 Haiku model.\n",
            "Added support for direct ingestion of Base64 encoded files with the ingestEncodedFile mutation.  You can pass a Base64 encoded string and MIME type of the file, and it will be ingested into the Graphlit Platform.\n",
            "Added modelService and model properties to ConversationMessage type, which return the model service and model which was used for the LLM completion.\n",
            "PreviousMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "NextMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [128 tokens (includes JSON guardrails tokens)], throughput: 39.651 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Claude 3 Haiku model.\n",
            "  - Direct ingestion of Base64 encoded files via the ingestEncodedFile mutation, allowing the input of a Base64 string and MIME type.\n",
            "  - Added modelService and model properties to ConversationMessage type for LLM completion details.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - None specified.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: March 13, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances the platform's capabilities for handling different models and file formats, improving developer flexibility and integration options.\n",
            "\n",
            "2024-12-29T03:26:38.031Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.636544, used credits [0.00675900]\n",
            "- CONTENT [8d95b393-a271-4cb7-956b-6f2808937151]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1005 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/may-2024/may-15-support-for-graphrag-openai-gpt-4o-model-performance-improvements-and-bug-fixes</name><title>May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes | Graphlit Changelog</title></metadata> üíê\tMay 2024\n",
            "May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports GraphRAG, where the extracted entities in the knowledge graph can be added as additional context to your RAG con,versation.  Also, with GraphRAG, entities can be extracted from the user prompt, and used as additional content filters - or can be used to query related content sources, which are combined with the vector search results.  This can be configured by specifying your graphStrategy in the Specification object.\n",
            "üí° Graphlit now supports LLM revisions within RAG conversations, where the LLM can be prompted to revise its initial completion response. From our testing, this has been shown to provide 35% more output tokens with higher quality responses.  This can be configured by specifying your revisionStrategy, and you can use our built-in revision prompt, or provide a custom one, and specify how many revisions you want the LLM to make.\n",
            "üí° Graphlit now supports the new OpenAI GPT-4o model for RAG conversations.\n",
            "‚ö° We have changed the default model for Conversations to be OpenAI GPT-4o, from Azure OpenAI GPT-3.5 16k.  This provides faster performance and better quality output.\n",
            "Added graph to promptConversation response, so you can visualize or leverage the nodes and edges of the knowledge graph, resulting from the content retrieval.  For example, if a Person and Organization were observed in the cited content sources used by the RAG pipeline, you will get back those entities and their relationship (such as Person 'works-for' Organization).\n",
            "Expanded the enriched data from WIkipedia to include the long description of an entity.\n",
            "Added getSharePointLibraries, getSharePointFolders, and getOneDriveFolders queries to the API, which can be used to enumerate the storage services.  This makes locating the SharePoint libraryId easier, for example.\n",
            "Added getTeams and getTeamsChannels queries to the API for enumerating Microsoft Teams workspaces.\n",
            "Added extractedCount to the entity extraction connector to limit the number of extracted entities, per entity type.  I.e. if extracted count is 10, it will extract at most ten each of Persons, Organizations, etc.\n",
            "üî•  We have improved performance in several areas: creation of observations after entity extraction, access to cloud storage, rendering the RAG context.\n",
            "üî•  We have optimized the LLM entity extraction process to identify more properties, as well as entity-to-entity relationships.\n",
            "Bugs Fixed\n",
            "GPLA-2652: Not extracting text from HTML in RSS post\n",
            "GPLA-2627: Limit filter only returning half the results\n",
            "GPLA-2613: Not properly extracting structured text from JSON/XML formats\n",
            "PreviousJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "NextMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [312 tokens (includes JSON guardrails tokens)], throughput: 85.796 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for GraphRAG, allowing extracted entities to enhance RAG conversations and serve as content filters.\n",
            "  - LLM revisions in RAG conversations, improving output quality by 35% with configurable revision strategies.\n",
            "  - Integration of OpenAI GPT-4o model for RAG conversations, replacing Azure OpenAI GPT-3.5 16k as the default model.\n",
            "  - Added graph visualization in promptConversation responses to illustrate relationships in the knowledge graph.\n",
            "  - Expanded Wikipedia data to include long descriptions of entities.\n",
            "  - New API queries: getSharePointLibraries, getSharePointFolders, getOneDriveFolders for easier storage service enumeration.\n",
            "  - New API queries: getTeams and getTeamsChannels for Microsoft Teams workspace enumeration.\n",
            "  - Added extractedCount to limit the number of extracted entities per type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Performance improvements in entity extraction, cloud storage access, and RAG context rendering.\n",
            "  - Optimized LLM entity extraction to identify more properties and relationships.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with text extraction from HTML in RSS posts.\n",
            "  - Resolved limit filter returning incomplete results.\n",
            "  - Corrected structured text extraction from JSON/XML formats.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: May 15, 2024.\n",
            "  - Default model changed to OpenAI GPT-4o for improved performance and output quality.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with more powerful tools for data extraction and conversation quality, improving overall application performance.\n",
            "\n",
            "2024-12-29T03:26:37.717Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.229019, used credits [0.00702000]\n",
            "- CONTENT [375b16d0-9e97-44ae-b1a9-4933a2e34836]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [940 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/may-2024/may-5-support-for-jina-and-pongo-rerankers-microsoft-teams-feed-new-youtube-downloader-bug-fixes</name><title>May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes | Graphlit Changelog</title></metadata> üíê\tMay 2024\n",
            "May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports the Jina reranker and Pongo semantic filtering (reranking), in the Specification object.  Now you can choose between COHERE, PONGO and JINA for your reranking serviceType.\n",
            "üí° Graphlit now supports Microsoft Teams feeds for reading messages from Teams channels.\n",
            "Given changes in YouTube video player HTML, we have rewritten the YouTube downloader to support the new page format.\n",
            "Added better handling of HTTP errors when validating URIs.  Previously some websites were returning HTTP 403 (Forbidden) errors when validating their URI, or downloading content.  Now Graphlit is able to scrape these sites, which previously returned errors.\n",
            "Added support for updating content metadata in updateContent mutation.  Now the video, audio, document, etc. metadata can be updated after the content workflow has finished.\n",
            "Added query_contents_graph (and queryContentsGraph) functions to SDKs, which can be used to return nodes and edges from knowledge graph for visualization.\n",
            "‚ö° Citation indices have been changed to be one-based from zero-based.  For example, you will now see \"This is a citation. [1]\" as the first citation in the list.\n",
            "‚ö° Added isSynchronous flag to deleteAll and multiple delete mutations.  By default, bulk delete operations are now asynchronous (and completed after the mutation returns), unless the isSynchronous flag is set to true.\n",
            "‚ö° Added missing count mutations, such as countAlerts, countFeeds, etc.\n",
            "‚ö° Renamed query_content_facets to query_contents_facets in Python SDK\n",
            "‚ö° Renamed queryContentFacets to queryContentsFacets in Node.js SDK\n",
            "Bugs Fixed\n",
            "GPLA-2544: Page relevance not filled-in in all situations\n",
            "GPLA-2546: Not extracting links from PDF with Azure AI Doc Intelligence\n",
            "GPLA-2557: Sporadically returning HTTP 500 from GraphQL API\n",
            "GPLA-2573: Failed to re-ingest content which was deleted immediately after initial ingestion\n",
            "GPLA-2575: Not validating for empty (non-null) parameters in mutations\n",
            "GPLA-2578: Need to handle invalid JSON from LLMs; improper escaping or formatting\n",
            "GPLA-2585: Failed to ingest encoded file with colon (:) in name\n",
            "PreviousMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "NextApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "Last updated7 months ago\n",
            "- Completion [350 tokens (includes JSON guardrails tokens)], throughput: 82.762 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Jina and Pongo rerankers in the Specification object; options for COHERE, PONGO, and JINA as reranking service types.\n",
            "  - Microsoft Teams feeds support for reading messages from Teams channels.\n",
            "  - Rewritten YouTube downloader to accommodate changes in YouTube video player HTML.\n",
            "  - Improved handling of HTTP errors for URI validation and content downloading.\n",
            "  - Support for updating content metadata in updateContent mutation post-workflow.\n",
            "  - Added query_contents_graph functions to SDKs for knowledge graph visualization.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Citation indices changed from zero-based to one-based.\n",
            "  - Added isSynchronous flag for deleteAll and multiple delete mutations; default is asynchronous.\n",
            "  - Added missing count mutations (e.g., countAlerts, countFeeds).\n",
            "  - Renamed query_content_facets to query_contents_facets in Python SDK.\n",
            "  - Renamed queryContentFacets to queryContentsFacets in Node.js SDK.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with page relevance not being filled in.\n",
            "  - Resolved link extraction issues from PDFs using Azure AI Doc Intelligence.\n",
            "  - Addressed sporadic HTTP 500 errors from GraphQL API.\n",
            "  - Fixed re-ingestion failures for content deleted immediately after initial ingestion.\n",
            "  - Improved validation for non-null parameters in mutations.\n",
            "  - Handled invalid JSON from LLMs with proper escaping or formatting.\n",
            "  - Resolved ingestion failures for encoded files with colons in names.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: May 5, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with more flexible content management and improved error handling, facilitating better integration and user experience.\n",
            "\n",
            "2024-12-29T03:26:37.388Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.565992, used credits [0.00457500]\n",
            "- CONTENT [2611e484-0933-494b-bebe-511835b64bb9]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [693 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/march-2024/march-23-support-for-linear-github-issues-and-jira-issue-feeds-ingest-files-via-web-feed-sitemap</name><title>March 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap | Graphlit Changelog</title></metadata> üçÄ\tMarch 2024\n",
            "March 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "New Features\n",
            "üí° Graphlit now supports Linear, GitHub Issues and Atlassian Jira feeds.  Graphlit will ingest issues (aka tasks, stories) from these issue-tracking services as individual content items, which will be made searchable and conversational.\n",
            "üí° Added support for ISSUEcontent type, which includes metadata such as title, authors, commenters, status, type, project and team.\n",
            "üí° Added support for default feed read limit.  Now, if you don't specify the readLimit property on feeds, it will default to reading 100 content items.  You can override this default by assigning a custom read limit, which has no upper bounds.  However, one-shot feeds much complete within 15 minutes, or they will be stopped automatically.\n",
            "Added support for ingesting files referenced in a Web sitemap.  Previously any files (i.e. PDF, MP3) referenced in a sitemap.xml would be ignored.  Now you can optionally enable includeFiles in the WebFeedPropertiesInput object to have Graphlit ingest non-HTML pages as part of the Web feed.\n",
            "Bugs Fixed\n",
            "GPLA-2374: Failed to ingest MP4 with large XMP metadata.\n",
            "PreviousApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "NextMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "Last updated8 months ago\n",
            "- Completion [208 tokens (includes JSON guardrails tokens)], throughput: 81.060 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Linear, GitHub Issues, and Atlassian Jira feeds for ingesting issues as searchable content items.\n",
            "  - Introduction of ISSUE content type with metadata including title, authors, commenters, status, type, project, and team.\n",
            "  - Default feed read limit set to 100 content items, customizable with no upper bounds; one-shot feeds must complete within 15 minutes.\n",
            "  - Support for ingesting files referenced in a Web sitemap, allowing non-HTML pages to be included.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of files in Web sitemaps by enabling ingestion of non-HTML formats.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2374 related to failing to ingest MP4 files with large XMP metadata.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: March 23, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances integration with popular issue-tracking services, improves content ingestion capabilities, and provides flexibility in feed management for developers.\n",
            "\n",
            "2024-12-29T03:26:35.391Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.788540, used credits [0.00538800]\n",
            "- CONTENT [55afbdb7-30a8-4eaa-92aa-02ae98fb57a5]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [848 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-10-support-for-web-search-multi-turn-content-summarization-deepgram-language-detection</name><title>November 10: Support for web search, multi-turn content summarization, Deepgram language detection | Graphlit Changelog</title></metadata> ü¶É\tNovember 2024\n",
            "November 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "New Features\n",
            "üí° Graphlit now supports web search with the searchWeb mutation.  You can select the search service, either Tavily or Exa.AI, and provide the search query and number of search results to be returned.  This is different than the web search feed, in that searchWeb returns the relevant text from the web page and the web page URL from each search hit, but does not ingest each of the web pages. This new mutation is optimized to be used from within an LLM tool.\n",
            "üí° Graphlit now supports multi-turn summarization of content with the reviseContent mutation.  You can provide an LLM prompt and a content reference, along with an optional specification.  This can be used for summarizing any content (documents, web pages, audio transcripts, etc.), and having a multi-turn conversation with the LLM to revise the output from the LLM.  Internally, this creates a conversation locked to a single piece of content.  This works especially well with the OpenAI o1-preview and o1-mini models, because they provide a longer LLM output from each turn.\n",
            "Graphlit now supports the configuration of the Deepgram transcription language, and whether detectLanguage is enabled in DeepgramAudioPreparationPropertiesInput.  Language detection is now enabled by default, and can be disabled by setting detectLanguage to false.\n",
            "‚ö° We have added a requireTool option to promptConversation mutation, so you can control whether the LLM must call one of the provided tool, or if tool calling is optional.\n",
            "‚ö° For accounts created after Nov 8, 2024, we have lowered the credits quota on the Free tier from 1000 credits to 100 credits, and now offer unlimited feeds on the Hobby Tier.\n",
            "‚ö° The Graphlit Data API will now return HTTP 402 (Payment Required) when you have exceeded the credits quota on the free tier.  You must upgrade to the Hobby Tier (or higher) to continue using the API, once the credits quota has been reached.\n",
            "PreviousNovember 16: Support for image description, multi-turn text summarization\n",
            "NextNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "Last updated1 month ago\n",
            "- Completion [237 tokens (includes JSON guardrails tokens)], throughput: 84.991 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for web search via the searchWeb mutation, allowing selection of search service (Tavily or Exa.AI) and retrieval of relevant text and URLs without ingesting web pages.\n",
            "  - Multi-turn content summarization with the reviseContent mutation, enabling conversation with LLM for summarizing various content types.\n",
            "  - Configuration of Deepgram transcription language and default language detection enabled in DeepgramAudioPreparationPropertiesInput.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Added requireTool option to promptConversation mutation for controlling tool calling.\n",
            "  - Reduced credits quota on Free tier from 1000 to 100 credits for accounts created after Nov 8, 2024, with unlimited feeds on the Hobby Tier.\n",
            "  - Graphlit Data API now returns HTTP 402 (Payment Required) when exceeding credits quota on the Free tier.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 10, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for web search and content summarization, improved control over tool usage, and clearer API usage guidelines based on credit quotas.\n",
            "\n",
            "2024-12-29T03:26:34.680Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.701293, used credits [0.00355800]\n",
            "- CONTENT [f9db5d97-0274-4ca3-b867-3f2c46a5359e]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [606 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-16-support-for-image-description-multi-turn-text-summarization</name><title>November 16: Support for image description, multi-turn text summarization | Graphlit Changelog</title></metadata> ü¶É\tNovember 2024\n",
            "November 16: Support for image description, multi-turn text summarization\n",
            "New Features\n",
            "üí° Graphlit now supports multi-turn summarization of text with the reviseText mutation.  You can provide an LLM prompt and text string, along with an optional specification.  This can be used for summarizing any raw text and having a multi-turn conversation with the LLM to revise the output from the LLM.  (Colab Notebook Example)\n",
            "üí° Graphlit now supports image descriptions using vision LLMs, without needing to ingest the image first.  With the new describeImage mutation, which takes a URI, and describeEncodedImage mutation, which takes a Base-64 encoded image and MIME type, you can use any vision LLM to prompt an image description.  These mutations accept an optional specification, where you can select your vision LLM.  If not provided, OpenAI GPT-4o will be used. (Colab Notebook Example)\n",
            "PreviousNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "NextNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "Last updated1 month ago\n",
            "- Completion [145 tokens (includes JSON guardrails tokens)], throughput: 85.229 tokens/sec:\n",
            "- New Features:\n",
            "  - Multi-turn summarization of text with the reviseText mutation, allowing for LLM prompts and text revisions.\n",
            "  - Image descriptions supported via describeImage mutation (URI input) and describeEncodedImage mutation (Base-64 encoded image), utilizing vision LLMs.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Optional specifications for selecting vision LLMs; defaults to OpenAI GPT-4o if not specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned in this release.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 16, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances capabilities for developers in text summarization and image description without prior image ingestion.\n",
            "\n",
            "2024-12-29T03:26:34.350Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.958157, used credits [0.00527400]\n",
            "- CONTENT [91123175-dc50-4c4e-9421-971e59c75969]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [754 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-24-support-for-direct-llm-prompt-multi-turn-image-analysis-bug-fixes</name><title>November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes | Graphlit Changelog</title></metadata> ü¶É\tNovember 2024\n",
            "November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports multi-turn analysis of images with the reviseImage and reviseEncodedImage mutations.  You can provide an LLM prompt and either a URI or Base-64 encoded image and MIME type, along with an optional LLM specification.  This can be used for analyzing any image and having a multi-turn conversation with the LLM to revise the output from the LLM. (Colab Notebook Example)\n",
            "üí° Graphlit now supports directly prompting an LLM with the prompt mutation, bypassing any RAG content retrieval, while providing an optional list of previous conversation messages.  This also accepts an optional LLM specification. (Colab Notebook Example)\n",
            "We have added support for the new Mistral Pixtral Large model, with PIXTRAL_LARGE model enum, which can be used with LLM completion or entity extraction LLM specifications.\n",
            "We have added support for the OpenAI 2024-11-20 version of GPT-4o, with GPT4O_128K_20241120 model enum.\n",
            "‚ö° We have added Microsoft Entra ID (fka Azure Active Directory) clientId and clientSecret properties to the SharePointFeedPropertiesInput type, which are now required when creating a SharePoint feed using user authentication with refreshToken property. (Colab Notebook Example)\n",
            "Bugs Fixed\n",
            "GPLA-3438: Not filtering on desktop presentation when scraping web pages\n",
            "GPLA-3340: Failed to parse invalid JSON from extracted PDF page\n",
            "GPLA-3427: Not formatting extracted tables properly from Sonnet 3.5\n",
            "PreviousDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "NextNovember 16: Support for image description, multi-turn text summarization\n",
            "Last updated1 month ago\n",
            "- Completion [251 tokens (includes JSON guardrails tokens)], throughput: 84.850 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for multi-turn analysis of images using reviseImage and reviseEncodedImage mutations, allowing LLM prompts with image URIs or Base-64 encoded images.\n",
            "  - Direct LLM prompting with the prompt mutation, bypassing RAG content retrieval, with optional previous conversation messages.\n",
            "  - Support for Mistral Pixtral Large model (PIXTRAL_LARGE).\n",
            "  - Support for OpenAI GPT-4o (GPT4O_128K_20241120).\n",
            "  - Added Microsoft Entra ID clientId and clientSecret properties for SharePoint feed creation with user authentication.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved user authentication process for SharePoint feeds.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed filtering issue on desktop presentation when scraping web pages (GPLA-3438).\n",
            "  - Resolved JSON parsing error from extracted PDF pages (GPLA-3340).\n",
            "  - Corrected table formatting from Sonnet 3.5 (GPLA-3427).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 24, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with advanced image analysis capabilities and improved integration with LLMs, facilitating more interactive and efficient workflows.\n",
            "\n",
            "2024-12-29T03:26:33.460Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.473708, used credits [0.00589800]\n",
            "- CONTENT [e1cd970b-fde4-41b4-892a-1a64cb531334]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [758 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2023/october-15-support-for-anthropic-claude-models-slack-feeds-and-entity-enrichment</name><title>October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment | Graphlit Changelog</title></metadata> üéÉ\tOctober 2023\n",
            "October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "New Features\n",
            "üî• Graphlit now supports Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "üî• Graphlit now supports Slack feeds, and will ingest Slack messages and linked file attachments from a Slack channel.  Note, this requires the creation of a Slack bot which has been added to the appropriate Slack channel.\n",
            "üí° Added support for entity enrichment to workflow object, which offers Diffbot, Wikipedia and Crunchbase enrichment of observed entities, such as Person, Organization and Place.\n",
            "üí° Added support for text extraction from images.  When using Azure Image Analytics for entity extraction, Graphlit will extract and store any identified text which then becomes searchable.\n",
            "Added embedFacets property to conversation strategy in specification object.\n",
            "Added embedCitations property to conversation strategy in specification object.  This makes content citations optional with the completed conversation message.\n",
            "Added GraphQL mutations for multi-delete of entities, such as deleteCollections, deleteLabels, or deleteConversations.\n",
            "Added GraphQL deleteAllConversations mutation to delete all conversations.\n",
            "Added support for automatically adding ingested content to one or more collections, via ingestion stage of workflow object.\n",
            "Added specification property to preparation workflow stage, which will be used to select the LLM for text summarization.\n",
            "Expanded the properties for observed entities, such as Person, Organization or Product.  Now supports a wider range of properties for entity enrichment.\n",
            "Bugs Fixed\n",
            "GPLA-1520: Unlimited conversation quota not assigned when upgrading project tier\n",
            "GPLA-1285: Entity enrichment not firing event, which can be sent to actions\n",
            "GPLA-1361: Web page left in ingested state, when URL not accessible.\n",
            "PreviousOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "NextSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "Last updated1 year ago\n",
            "- Completion [302 tokens (includes JSON guardrails tokens)], throughput: 86.939 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "  - Ingestion of Slack messages and linked file attachments from a Slack channel (requires a Slack bot).\n",
            "  - Entity enrichment for observed entities (Person, Organization, Place) using Diffbot, Wikipedia, and Crunchbase.\n",
            "  - Text extraction from images using Azure Image Analytics, making identified text searchable.\n",
            "  - Added embedFacets and embedCitations properties to conversation strategy in specification object.\n",
            "  - GraphQL mutations for multi-delete of entities (deleteCollections, deleteLabels, deleteConversations).\n",
            "  - GraphQL deleteAllConversations mutation to delete all conversations.\n",
            "  - Automatic addition of ingested content to collections during the ingestion stage of the workflow.\n",
            "  - Specification property added to preparation workflow stage for selecting LLM for text summarization.\n",
            "  - Expanded properties for observed entities for enhanced entity enrichment.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed unlimited conversation quota not assigned when upgrading project tier (GPLA-1520).\n",
            "  - Resolved issue with entity enrichment not firing event (GPLA-1285).\n",
            "  - Fixed web page remaining in ingested state when URL is not accessible (GPLA-1361).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 15, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Enhances capabilities for developers by integrating advanced LLMs, improving data ingestion from Slack, and enriching entity data, ultimately leading to more powerful and flexible applications.\n",
            "\n",
            "2024-12-29T03:26:32.905Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.107487, used credits [0.00386400]\n",
            "- CONTENT [119976d5-ad8a-4a2a-8c4d-e008bc61e848]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [612 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-4-support-for-anthropic-claude-3.5-haiku-bug-fixes</name><title>November 4: Support for Anthropic Claude 3.5 Haiku, bug fixes | Graphlit Changelog</title></metadata> ü¶É\tNovember 2024\n",
            "November 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "New Features\n",
            "Graphlit now supports the latest Anthropic Haiku 3.5 model, with the model enum CLAUDE_3_5_HAIKU_20241022.\n",
            "‚ö° Once a project has hit the free tier quota, we will now automatically disable all feeds.  Once the project has been upgraded to a paid tier, you can use the enableFeed mutation to re-enable your existing feeds to continue ingestion.\n",
            "‚ö° We have added the disableFallback flag to the RetrievalStrategyInput type, so you can disable the default behavior of falling back to the previous conversation's contents, or worst-case, falling back to the most recently uploaded content.  By setting disableFallback to true, conversations will only attempt to retrieve contents based on the provided filter and/or augmentedFilter properties.\n",
            "Bugs Fixed\n",
            "GPLA-3367: Not extracting text from HTML button element\n",
            "PreviousNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "NextOctober 31: Support for simulated tool calling, bug fixes\n",
            "Last updated1 month ago\n",
            "- Completion [169 tokens (includes JSON guardrails tokens)], throughput: 80.190 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Anthropic Haiku 3.5 model (model enum: CLAUDE_3_5_HAIKU_20241022).\n",
            "  - Automatic disabling of all feeds upon reaching the free tier quota; re-enable feeds with enableFeed mutation after upgrading to a paid tier.\n",
            "  - Added disableFallback flag to RetrievalStrategyInput type to control fallback behavior in content retrieval.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved control over conversation content retrieval with the new disableFallback feature.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with not extracting text from HTML button elements (GPLA-3367).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 4, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support, better feed management, and improved content retrieval control.\n",
            "\n",
            "2024-12-29T03:26:32.474Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.877585, used credits [0.00561900]\n",
            "- CONTENT [53a0f36a-7c33-4514-901f-4865aa3fe0a9]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [733 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2023/october-30-optimized-conversation-responses-added-observable-aliases-bug-fixes</name><title>October 30: Optimized conversation responses; added observable aliases; bug fixes | Graphlit Changelog</title></metadata> üéÉ\tOctober 2023\n",
            "October 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports 'aliases' of observable names, as the alternateNames property.  When an observed entity, such as Organization, is enriched, we store the original name and the enriched name as an alias.  For example, \"OpenAI\" may be enriched to \"OpenAI, Inc.\", and we store \"OpenAI\" as an alias, and update the name to \"OpenAI, Inc.\".\n",
            "üí° Added workflows filter to ContentCriteriaInput type, for filtering content by workflow(s) when creating conversation.\n",
            "Optimized formatting of content sources into prompt context, for more accurate conversation responses.\n",
            "Optimized formatting of extracted text from Slack messages, for better knowledge retrieval.\n",
            "Updated text tokenizer for more accurate token counting.\n",
            "Upgraded Azure Text Analytics to latest preview API version.\n",
            "Authors found in RSS feeds are now stored as observations of Person entities.\n",
            "Added rate limiting for Reddit feeds.\n",
            "Added rate limiting for Wikipedia enrichment.\n",
            "Added support for reading Reddit post comments when reading Reddit feed.\n",
            "‚ö° EmbedFacets has been renamed to EnableFacets in the conversation strategy.\n",
            "‚ö° Removed extra content level in IngestionWorkflowStage type.  Now, the if property is of type IngestionContentFilter.\n",
            "Bugs Fixed\n",
            "GPLA-1556: Better handling of very long user prompts.\n",
            "GPLA-1627: Optimized token budget for more accurate prompt completion.\n",
            "GPLA-1585: More accurate entity matching in Wikipedia entity enrichment.\n",
            "PreviousDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "NextOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "Last updated1 year ago\n",
            "- Completion [285 tokens (includes JSON guardrails tokens)], throughput: 99.041 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for 'aliases' of observable names via the alternateNames property.\n",
            "  - Workflows filter added to ContentCriteriaInput type for filtering content by workflow(s).\n",
            "  - Optimized formatting of content sources for improved conversation responses.\n",
            "  - Enhanced formatting of extracted text from Slack messages for better knowledge retrieval.\n",
            "  - Updated text tokenizer for accurate token counting.\n",
            "  - Upgraded Azure Text Analytics to the latest preview API version.\n",
            "  - Authors in RSS feeds are now stored as observations of Person entities.\n",
            "  - Rate limiting added for Reddit feeds and Wikipedia enrichment.\n",
            "  - Support for reading Reddit post comments from Reddit feeds.\n",
            "  - EmbedFacets renamed to EnableFacets in conversation strategy.\n",
            "  - Removed extra content level in IngestionWorkflowStage type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Better handling of long user prompts.\n",
            "  - Optimized token budget for prompt completion.\n",
            "  - More accurate entity matching in Wikipedia entity enrichment.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Improved handling of long user prompts (GPLA-1556).\n",
            "  - Optimized token budget for accurate prompt completion (GPLA-1627).\n",
            "  - Enhanced entity matching in Wikipedia enrichment (GPLA-1585).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 30, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved conversation accuracy, better content filtering, and enhanced entity recognition capabilities.\n",
            "\n",
            "2024-12-29T03:26:31.313Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.322385, used credits [0.00576300]\n",
            "- CONTENT [cfa6a062-2cfa-4994-a066-6443539c4553]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [805 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-21-support-openai-cohere-jina-mistral-voyage-and-google-ai-embedding-models</name><title>October 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models | Graphlit Changelog</title></metadata> üéÉ\tOctober 2024\n",
            "October 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "New Features\n",
            "üí° Graphlit now supports the configuration of image and text embedding models, at the Project level.  You can create an embedding specification for a text or image embedding model, and then assign that to the Project, and all further embedding requests will use that embedding model.  See this Colab notebook for an example of how to configure the project.\n",
            "üí° Graphlit now supports the OpenAI Embedding-3-Small and Embedding-3-Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0 and 3.0 text embedding models.  Graphlit also now supports Jina CLIP image embeddings, which are used by default for image search.\n",
            "Graphlit now supports the chunkTokenLimit property in Specifications, which specifies the number of tokens for each embedded text chunk.  If this is not configured, Graphlit uses 600 tokens for each embedded text chunk.\n",
            "Graphlit now supports the Voyage reranking model.\n",
            "Graphlit now supports the ingestTextBatch mutation, which accepts an array of text and name pairs, and will asynchronously ingest these into content objects.\n",
            "‚ö° We have moved the chunkTokenLimit property from the Workflow storage embeddings strategy to the Specification object.  The Workflow storage property has now been deprecated.\n",
            "‚ö° We have deprecated the openAIImage property from Workflow entity extraction properties. Use the modelImage property instead.\n",
            "Once a text embedding model has been updated at the project level, any content, conversations or observed entities will no longer be semantically searchable.\n",
            "Text embeddings are not compatible across models, so you will need to delete and reingest any content, or recreate conversations or knowledge graph entities, with the new embedding model to become searchable.\n",
            "PreviousOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "NextOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "Last updated2 months ago\n",
            "- Completion [279 tokens (includes JSON guardrails tokens)], throughput: 83.976 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for configuring image and text embedding models at the Project level.\n",
            "  - Support for OpenAI Embedding-3-Small and Embedding-3-Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0 and 3.0 text embedding models.\n",
            "  - Support for Jina CLIP image embeddings for image search.\n",
            "  - Introduction of chunkTokenLimit property in Specifications for token count per embedded text chunk.\n",
            "  - Support for the Voyage reranking model.\n",
            "  - New ingestTextBatch mutation for asynchronous ingestion of text and name pairs.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Moved chunkTokenLimit property from Workflow storage embeddings strategy to Specification object; Workflow storage property deprecated.\n",
            "  - Deprecated openAIImage property from Workflow entity extraction properties; use modelImage property instead.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Updating text embedding models at the project level will make existing content, conversations, or observed entities non-searchable. Requires deletion and reingestion for compatibility.\n",
            "\n",
            "- Other Key Details:\n",
            "  - No specific version numbers mentioned.\n",
            "  - Breaking change: Text embeddings are not compatible across models.\n",
            "\n",
            "- Dates:\n",
            "  - Released on October 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced flexibility in embedding model configuration and improved ingestion capabilities.\n",
            "\n",
            "2024-12-29T03:26:30.464Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.469268, used credits [0.00445800]\n",
            "- CONTENT [8e5925f7-0358-42ff-84df-8522c7f265a8]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [590 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-22-support-for-latest-anthropic-sonnet-3.5-model-cohere-image-embeddings</name><title>October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings | Graphlit Changelog</title></metadata> üéÉ\tOctober 2024\n",
            "October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "New Features\n",
            "Graphlit now supports the latest Anthropic Sonnet 3.5 model (released 10/22/2024).  We have added date-versions model enums for the Anthropic models: CLAUDE_3_5_SONNET_20240620, CLAUDE_3_5_SONNET_20241022, CLAUDE_3_HAIKU_20240307, CLAUDE_3_OPUS_20240229, CLAUDE_3_SONNET_20240229. The existing model enums will target the latest released models, as specified by Anthropic.\n",
            "Graphlit now supports image embeddings using the Cohere Embed 3.0 models.\n",
            "PreviousOctober 31: Support for simulated tool calling, bug fixes\n",
            "NextOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "Last updated2 months ago\n",
            "- Completion [224 tokens (includes JSON guardrails tokens)], throughput: 64.567 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for the latest Anthropic Sonnet 3.5 model (released 10/22/2024).\n",
            "  - Added date-version model enums for Anthropic models: \n",
            "    - CLAUDE_3_5_SONNET_20240620\n",
            "    - CLAUDE_3_5_SONNET_20241022\n",
            "    - CLAUDE_3_HAIKU_20240307\n",
            "    - CLAUDE_3_OPUS_20240229\n",
            "    - CLAUDE_3_SONNET_20240229\n",
            "  - Support for image embeddings using Cohere Embed 3.0 models.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Existing model enums now target the latest released models as specified by Anthropic.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned in this release.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers access to the latest Anthropic models and enhanced image embedding capabilities, improving integration and functionality within the Graphlit Platform.\n",
            "\n",
            "2024-12-29T03:26:29.943Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.547655, used credits [0.00451800]\n",
            "- CONTENT [bf1f3903-d17d-4ec6-b76a-cd7687b49dbf]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [654 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-1-support-for-fhir-enrichment-latest-cohere-models-bug-fixes</name><title>September 1: Support for FHIR enrichment, latest Cohere models, bug fixes | Graphlit Changelog</title></metadata> üéí\tSeptember 2024\n",
            "September 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports entity enrichment from Fast Healthcare Interoperability Resources (FHIR) servers. You can provide the endpoint for a FHIR server, and Graphlit will enrich medical-related entities from the data found in the FHIR server.\n",
            "Added support for latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408) and added datestamped model enums for the previous versions (COMMAND_R_202403, COMMAND_R_PLUS_202404).  The latest model enums (COMMAND_R and COMMAND_R_PLUS) currently point to the models (COMMAND_R_202403 and COMMAND_R_PLUS_202404) as specified by the Cohere API.\n",
            "Added support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31), now used by default.\n",
            "‚ö° We have changed the name of the LinkReferenceType to LinkReference to follow the existing data model standard.\n",
            "Bugs Fixed\n",
            "GPLA-3120: LLM is adding source tags to end of completed messages\n",
            "GPLA-3133: Failed to load sitemap on child page of website.\n",
            "PreviousSeptember 3: Support for web search feeds, model deprecations\n",
            "NextAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "Last updated3 months ago\n",
            "- Completion [213 tokens (includes JSON guardrails tokens)], throughput: 83.606 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for entity enrichment from FHIR servers for medical-related entities.\n",
            "  - Added support for latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408) with datestamped model enums for previous versions.\n",
            "  - Support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31) now used by default.\n",
            "  - Renamed LinkReferenceType to LinkReference to align with data model standards.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Updated naming conventions for consistency with existing standards.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where LLM added source tags to the end of completed messages (GPLA-3120).\n",
            "  - Resolved failure to load sitemap on child page of website (GPLA-3133).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 1, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve integration with healthcare data and AI models, providing developers with more robust tools for medical entity enrichment and document intelligence.\n",
            "\n",
            "2024-12-29T03:26:29.511Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.205964, used credits [0.00549000]\n",
            "- CONTENT [0e61c44c-cae3-4a7a-95f0-081150f5860e]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [754 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-3-support-tool-calling-ingestbatch-mutation-gemini-flash-1.5-8b-bug-fixes</name><title>October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes | Graphlit Changelog</title></metadata> üéÉ\tOctober 2024\n",
            "October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports the ingestBatch mutation, which accepts an array of URIs to files or web pages, and will asynchronously ingest these into content objects.\n",
            "üí° Graphlit now supports the continueConversation mutation, which accepts an array of called tool responses. Also, promptConversation now accepts an array of tool definitions. When tools are called by the LLM, the assistant message returned from promptConversation will have a list of toolCalls which need to responded to from your calling code.  These responses are to be provided back to the LLM via the continueConversation mutation.\n",
            "üí° Graphlit now supports tool calling with OpenAI, Mistral, Deepseek, Groq, and Cerebras model services.  Anthropic, Google Gemini and Cohere support will come later.\n",
            "Added support for prefilled user and assistant messages with createConversation mutation. Now you can send an array of messages when creating a new conversation, which will bootstrap the conversation with the LLM.  These must be provided in user/assistant pairs.\n",
            "Added support for Google Gemini Flash 1.5 8b model.\n",
            "‚ö° We have deprecated the tools property in the Specification object. These will be removed at a later date.  Tools are now to be sent directly to the extractContents and promptConversation mutations.\n",
            "Bugs Fixed\n",
            "GPLA-3207: Models shouldn't be required on update specification call\n",
            "GPLA-3220: Don't send system prompt with OpenAI o1 models\n",
            "PreviousOctober 7: Support for Anthropic and Gemini tool calling\n",
            "NextSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "Last updated2 months ago\n",
            "- Completion [269 tokens (includes JSON guardrails tokens)], throughput: 83.906 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for ingestBatch mutation to asynchronously ingest an array of URIs into content objects.\n",
            "  - Introduction of continueConversation mutation for handling tool responses and promptConversation now accepts an array of tool definitions.\n",
            "  - Tool calling support added for OpenAI, Mistral, Deepseek, Groq, and Cerebras model services; support for Anthropic, Google Gemini, and Cohere coming later.\n",
            "  - Prefilled user and assistant messages supported with createConversation mutation, allowing an array of messages to bootstrap conversations.\n",
            "  - Added support for Google Gemini Flash 1.5 8b model.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Deprecated tools property in the Specification object; tools now sent directly to extractContents and promptConversation mutations.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where models shouldn't be required on update specification call (GPLA-3207).\n",
            "  - Resolved issue of sending system prompt with OpenAI o1 models (GPLA-3220).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Version: Gemini Flash 1.5 8b.\n",
            "  - Deprecation of tools property noted for future removal.\n",
            "\n",
            "- Dates:\n",
            "  - Release Date: October 3, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve developer experience by streamlining tool interactions and conversation management.\n",
            "\n",
            "2024-12-29T03:26:27.916Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.924662, used credits [0.00351600]\n",
            "- CONTENT [49836c55-86df-40ce-9f5e-98b3eb2ebc41]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [580 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-31-support-for-simulated-tool-calling-bug-fixes</name><title>October 31: Support for simulated tool calling, bug fixes | Graphlit Changelog</title></metadata> üéÉ\tOctober 2024\n",
            "October 31: Support for simulated tool calling, bug fixes\n",
            "New Features\n",
            "Graphlit now supports simulated tool calling for LLMs which don't natively support it, such as OpenAI o1-preview and o1-mini.  Tool schema will be formatted into the LLM prompt context, and tool responses are parsed out of the JSON formatted response.\n",
            "‚ö° Given customer feedback, we have lowered the vector and hybrid thresholds used by the semantic search.  Previously, some content at a low relevance was being excluded from the semantic search results.  Now, more low-relevance content will be included in the results, used by the RAG pipeline.  Reranking can be used to sort the search results for relevance.\n",
            "Bugs Fixed\n",
            "GPLA-3357: Not extracting all images from PDF, and should filter out single-color images.\n",
            "PreviousNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "NextOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "Last updated1 month ago\n",
            "- Completion [148 tokens (includes JSON guardrails tokens)], throughput: 76.897 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for simulated tool calling for LLMs like OpenAI o1-preview and o1-mini.\n",
            "  - Tool schema formatted into LLM prompt context; tool responses parsed from JSON.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Lowered vector and hybrid thresholds for semantic search to include more low-relevance content.\n",
            "  - Reranking feature added to sort search results by relevance.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with not extracting all images from PDFs; now filters out single-color images.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 31, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve search result relevance and broaden content inclusion, benefiting developers using the platform.\n",
            "\n",
            "2024-12-29T03:26:27.276Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.753119, used credits [0.00630600]\n",
            "- CONTENT [550d544c-3dc6-4fae-a4d1-89577544d9fc]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [838 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2023/september-20-paid-subscription-plans-support-for-custom-observed-entities-and-azure-openai-gpt-4</name><title>September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4 | Graphlit Changelog</title></metadata> üõ†Ô∏è\tSeptember 2023\n",
            "September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "New Features\n",
            "üî• Graphlit now supports paid Hobby, Starter and Growth tiers for projects, in addition to the existing Free tier.  Starting at $49/mo, plus $0.10/credit for usage, we now support higher quota based on your subscribed tier.   By providing a payment method for your organization in the Developer Portal, you can upgrade each project individually to the tier that fits your application's needs.\n",
            "üí° Added GraphQL mutations for the creation, update and deletion of observed entities (i.e. Person, Organization, Place, Product, Event, Label, Category).\n",
            "üí° Added new observed entity types to knowledge graph: Repo (i.e. Git repo), Software.\n",
            "üí° Added searchType and numberSimilar fields to Specification object for configuring semantic search in conversations.   In situations where the user prompt is limited in length, HYBRID search type can provide better semantic search results for the prompt context.\n",
            "üí° Added support for the Azure OpenAI GPT-4 model.\n",
            "Added support for project quota field.  Project quotas are based on the subscribed pricing tier.   Quota limits are now applied as content is ingested, and as feeds and conversations are created.\n",
            "Added contentLimit to conversation strategy object to limit the number of semantic search content results which are formatted into prompt context.\n",
            "Better relevance ranking on semantic search results when formatting prompt context in conversations.\n",
            "‚ÑπÔ∏è Free tier has updated quota: 1GB storage, 100 contents, 3 feeds and 10 conversations.\n",
            "‚ö° Now using the Deepgram Nova-2 audio transcription model, which is 18% more accurate, and 5-40x faster.\n",
            "Bugs Fixed\n",
            "GPLA-1373: Failed to extract multiple text pages from DOCX without page breaks.  Now we support token-aware page chunking.\n",
            "GPLA-1377: Failed during semantic search with no content results, when prompting conversation.\n",
            "GPLA-1415: Failed when user prompt couldn't generate text embeddings.\n",
            "PreviousSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "NextSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "Last updated11 months ago\n",
            "- Completion [316 tokens (includes JSON guardrails tokens)], throughput: 84.197 tokens/sec:\n",
            "- New Features:\n",
            "  - Introduced paid subscription plans: Hobby, Starter, and Growth tiers, starting at $49/month.\n",
            "  - Added GraphQL mutations for creating, updating, and deleting observed entities (Person, Organization, Place, Product, Event, Label, Category).\n",
            "  - New observed entity types: Repo (Git repo), Software.\n",
            "  - Enhanced Specification object with searchType and numberSimilar fields for improved semantic search.\n",
            "  - Support for Azure OpenAI GPT-4 model.\n",
            "  - Project quota field added, with limits based on subscription tier.\n",
            "  - ContentLimit added to conversation strategy object for semantic search results.\n",
            "  - Improved relevance ranking for semantic search results.\n",
            "  - Updated Free tier quota: 1GB storage, 100 contents, 3 feeds, and 10 conversations.\n",
            "  - Implemented Deepgram Nova-2 audio transcription model, 18% more accurate and 5-40x faster.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Better semantic search results with HYBRID search type.\n",
            "  \n",
            "- Bug Fixes:\n",
            "  - Fixed issue with extracting multiple text pages from DOCX without page breaks (GPLA-1373).\n",
            "  - Resolved semantic search failure with no content results (GPLA-1377).\n",
            "  - Fixed failure in generating text embeddings from user prompts (GPLA-1415).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 20, 2023.\n",
            "  \n",
            "- Value:\n",
            "  - Offers developers flexible subscription options, enhanced search capabilities, and improved performance with new models and features.\n",
            "\n",
            "2024-12-29T03:26:26.899Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.501213, used credits [0.00293700]\n",
            "- CONTENT [3f9aa839-735f-4aa9-a599-c7d59d2898aa]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [491 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-7-support-for-anthropic-and-gemini-tool-calling</name><title>October 7: Support for Anthropic and Gemini tool calling | Graphlit Changelog</title></metadata> üéÉ\tOctober 2024\n",
            "October 7: Support for Anthropic and Gemini tool calling\n",
            "New Features\n",
            "üí° Graphlit now supports tool calling with Anthropic and Google Gemini models.\n",
            "‚ö° We have removed the uri property for tools from ToolDefinitionInput, such that inline webhook tools are no longer supported.  Now you can define any external tools to be called, and those can support sync or async data access to fulfill the tool call.\n",
            "PreviousOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "NextOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "Last updated2 months ago\n",
            "- Completion [122 tokens (includes JSON guardrails tokens)], throughput: 81.268 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for tool calling with Anthropic and Google Gemini models.\n",
            "  - Removal of the uri property for tools from ToolDefinitionInput, eliminating support for inline webhook tools. External tools can now be defined for sync or async data access.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved flexibility in defining external tools for tool calls.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 7, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for integrating external tools and improved data access options.\n",
            "\n",
            "2024-12-29T03:26:26.226Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:06.211141, used credits [0.00675600]\n",
            "- CONTENT [a704d7fa-1161-4d3a-9be7-569d3af4d1cf]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [844 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-26-support-for-google-ai-and-cerebras-models-and-latest-groq-models</name><title>September 26: Support for Google AI and Cerebras models, and latest Groq models | Graphlit Changelog</title></metadata> üéí\tSeptember 2024\n",
            "September 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "New Features\n",
            "üí° Graphlit now supports the Cerebras model service which offers the LLAMA_3_1_70B and LLAMA_3_1_8B models.\n",
            "üí° Graphlit now supports the Google AI model service which offers the GEMINI_1_5_PRO and GEMINI_1_5_FLASH models.\n",
            "We have added support for the latest Groq Llama 3.2 preview models, including LLAMA_3_2_1B_PREVIEW, LLAMA_3_2_3B_PREVIEW, LLAMA_3_2_11B_TEXT_PREVIEW, and LLAMA_3_2_90B_TEXT_PREVIEW.  We have also added support for the Llama 3.2 multimodal model LLAMA_3_2_11B_VISION_PREVIEW.\n",
            "We have added a new specification parameter to the promptConversation mutation. Now you can specify your initial specification for a new conversation, or update an existing conversation, without requiring additional API calls.\n",
            "‚ö° We have changed the retrieval behavior of the promptConversation mutation. Now, if no relevant content was found via vector-based semantic search (given the user prompt), we will fallback to any relevant content from the message in the conversation. If there was no content from the conversation to fallback to, we will fallback to the last ingested content in the project. This solves an issue where a first prompt like 'Summarize this' would find no relevant content.  Now it will fallback to retrieve the last ingested content.\n",
            "‚ö° We have renamed the Groq model enum from LLAVA_1_5_7B to LLAVA_1_5_7B_PREVIEW.\n",
            "Bugs Fixed\n",
            "GPLA-3083: Not sending custom instructions/guidance with extraction prompt\n",
            "GPLA-3146: Filtering Persons by email not working\n",
            "GPLA-3171: Not failing on deprecated OpenAI model\n",
            "GPLA-3158: Summarization not using revision strategy\n",
            "PreviousSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "NextSeptember 3: Support for web search feeds, model deprecations\n",
            "Last updated3 months ago\n",
            "- Completion [352 tokens (includes JSON guardrails tokens)], throughput: 56.672 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Cerebras model service: LLAMA_3_1_70B and LLAMA_3_1_8B models.\n",
            "  - Support for Google AI model service: GEMINI_1_5_PRO and GEMINI_1_5_FLASH models.\n",
            "  - Added support for Groq Llama 3.2 preview models: LLAMA_3_2_1B_PREVIEW, LLAMA_3_2_3B_PREVIEW, LLAMA_3_2_11B_TEXT_PREVIEW, LLAMA_3_2_90B_TEXT_PREVIEW, and multimodal model LLAMA_3_2_11B_VISION_PREVIEW.\n",
            "  - New specification parameter for promptConversation mutation to specify or update conversation specifications without extra API calls.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Changed retrieval behavior of promptConversation mutation to fallback to relevant content from the conversation or last ingested content if no relevant content is found.\n",
            "  - Renamed Groq model enum from LLAVA_1_5_7B to LLAVA_1_5_7B_PREVIEW.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with not sending custom instructions with extraction prompt (GPLA-3083).\n",
            "  - Resolved filtering persons by email not working (GPLA-3146).\n",
            "  - Addressed issue of not failing on deprecated OpenAI model (GPLA-3171).\n",
            "  - Fixed summarization not using revision strategy (GPLA-3158).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 26, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support and improved conversation handling, leading to more efficient interactions and better content retrieval.\n",
            "\n",
            "2024-12-29T03:26:25.888Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.558835, used credits [0.00390000]\n",
            "- CONTENT [3a2727e0-f370-4b5c-9d04-59e54995f166]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [580 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2023/september-24-support-for-youtube-feeds-added-documentation-bug-fixes</name><title>September 24: Support for YouTube feeds; added documentation; bug fixes | Graphlit Changelog</title></metadata> üõ†Ô∏è\tSeptember 2023\n",
            "September 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "New Features\n",
            "üî• Graphlit now supports YouTube feeds, where you can ingest a set of YouTube videos, or an entire YouTube playlist or channel.   Note, we currently support only the ingestion of audio from YouTube videos, which gets transcribed and added to your conversational knowledge graph.\n",
            "New Documentation\n",
            "Added documentation for observable entities mutations and queries (Label, Category, Person, Organization, Place, Event, Product, Repo, Software).\n",
            "Added documentation for using custom Azure OpenAI and OpenAI models with Specifications\n",
            "Bugs Fixed\n",
            "GPLA-1459: LLM prompt formatting was exceeding the token budget with long user prompts.\n",
            "GPLA-1445: Failed to ingest PDF from URL where filename in Content-Disposition header contained a backslash.\n",
            "PreviousOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "NextSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "Last updated7 months ago\n",
            "- Completion [180 tokens (includes JSON guardrails tokens)], throughput: 70.345 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for YouTube feeds, allowing ingestion of YouTube videos, playlists, or channels (currently supports audio ingestion and transcription).\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Added documentation for observable entities mutations and queries (Label, Category, Person, Organization, Place, Event, Product, Repo, Software).\n",
            "  - Added documentation for using custom Azure OpenAI and OpenAI models with Specifications.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed LLM prompt formatting issue that exceeded token budget with long user prompts (GPLA-1459).\n",
            "  - Resolved issue with ingesting PDF from URL when filename in Content-Disposition header contained a backslash (GPLA-1445).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 24, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Enhances the platform's capabilities for video content integration and improves documentation for developers.\n",
            "\n",
            "2024-12-29T03:26:25.292Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.485418, used credits [0.00253500]\n",
            "- CONTENT [85a254e5-593b-4823-b1b9-d48506b794f8]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [477 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-9-support-for-github-repository-feeds-bug-fixes</name><title>October 9: Support for GitHub repository feeds, bug fixes | Graphlit Changelog</title></metadata> üéÉ\tOctober 2024\n",
            "October 9: Support for GitHub repository feeds, bug fixes\n",
            "New Features\n",
            "üí° Graphlit now supports GitHub feeds, by providing the repository owner and name similar to GitHub Issues feeds, and will ingest code files from any GitHub repository.\n",
            "Bugs Fixed\n",
            "GPLA-3262: Missing row separator in table markdown formatting\n",
            "PreviousOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "NextOctober 7: Support for Anthropic and Gemini tool calling\n",
            "Last updated2 months ago\n",
            "- Completion [92 tokens (includes JSON guardrails tokens)], throughput: 61.935 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for GitHub repository feeds, allowing ingestion of code files by providing the repository owner and name.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed missing row separator in table markdown formatting (GPLA-3262).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances integration with GitHub, improving workflow for developers by enabling direct access to repository code files.\n",
            "\n",
            "2024-12-29T03:26:23.664Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.649116, used credits [0.00423000]\n",
            "- CONTENT [9e6ef8e1-5448-4933-9705-d9d9502b384c]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [682 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-3-support-for-web-search-feeds-model-deprecations</name><title>September 3: Support for web search feeds, model deprecations | Graphlit Changelog</title></metadata> üéí\tSeptember 2024\n",
            "September 3: Support for web search feeds, model deprecations\n",
            "New Features\n",
            "üí° Graphlit now supports web search feeds, using the Tavily and Exa.AI web search APIs. You can choose the SEARCH feed type, and assign your search text property, and we will ingest the referenced web pages from the search results.  Optionally, you can select the search service via the serviceType property under search feed properties.  By default, Graphlit will use the Tavily API.\n",
            "‚ö° We have deprecated these OpenAI models, according to the future support OpenAI is providing to these legacy models: GPT35_TURBO, GPT35_TURBO_0613, GPT35_TURBO_16K, GPT35_TURBO_16K_0125, GPT35_TURBO_16K_0613, GPT35_TURBO_16K_1106, GPT4, GPT4_0613, GPT4_32K, GPT4_32K_0613, GPT4_TURBO_VISION_128K, and GPT4_TURBO_VISION_128K_1106.  We suggest using GPT-4o or GPT-4o Mini instead.\n",
            "Bugs Fixed\n",
            "GPLA-2523: Can't ingest from same feed URI multiple times and wait on isFeedDone\n",
            "PreviousSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "NextSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "Last updated3 months ago\n",
            "- Completion [182 tokens (includes JSON guardrails tokens)], throughput: 49.875 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for web search feeds using Tavily and Exa.AI APIs.\n",
            "  - Ability to choose SEARCH feed type and assign search text property for ingesting web pages.\n",
            "  - Option to select search service via serviceType property, defaulting to Tavily API.\n",
            "  - Deprecation of several OpenAI models; recommended alternatives are GPT-4o and GPT-4o Mini.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2523: Ingestion from the same feed URI multiple times now works correctly.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 3, 2024.\n",
            "  - Deprecated models include various versions of GPT-3.5 and GPT-4.\n",
            "\n",
            "- Value:\n",
            "  - Enhances web search capabilities for developers and encourages migration to newer models for better support.\n",
            "\n",
            "2024-12-29T03:26:23.426Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.411440, used credits [0.00403800]\n",
            "- CONTENT [989c5568-cfd4-4ee4-ba79-73e3552cfdc2]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [634 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-30-support-for-azure-ai-inference-models-mistral-pixtral-and-latest-google-gemini-models</name><title>September 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models | Graphlit Changelog</title></metadata> üéí\tSeptember 2024\n",
            "September 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "New Features\n",
            "üí° Graphlit now supports the Azure AI Model Inference API (aka Models as a Service) model service which offers serverless hosting to many models such as Meta Llama 3.2, Cohere Command-R, and many more.  For Azure AI, all models are 'custom', and you will need to provide the serverless endpoint, API key and number of tokens accepted in context window, after provisioning the model of your choice.\n",
            "We have added support for the multimodal Mistral Pixtral model, under the model enum PIXTRAL_12B_2409.\n",
            "We have added versioned model enums for Google Gemini, so you can access GEMINI_1_5_FLASH_001, GEMINI_1_5_FLASH_002, GEMINI_1_5_PRO_001 and GEMINI_1_5_PRO_002.\n",
            "PreviousOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "NextSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "Last updated2 months ago\n",
            "- Completion [178 tokens (includes JSON guardrails tokens)], throughput: 52.177 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Azure AI Model Inference API, enabling serverless hosting for models like Meta Llama 3.2 and Cohere Command-R.\n",
            "  - Support for the multimodal Mistral Pixtral model (PIXTRAL_12B_2409).\n",
            "  - Versioned model enums for Google Gemini: GEMINI_1_5_FLASH_001, GEMINI_1_5_FLASH_002, GEMINI_1_5_PRO_001, GEMINI_1_5_PRO_002.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - None specified.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 30, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers access to a wider range of AI models with serverless capabilities, enhancing flexibility and scalability in model deployment.\n",
            "\n",
            "2024-12-29T03:26:23.215Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.197300, used credits [0.00523200]\n",
            "- CONTENT [090640e2-097b-4684-90fa-86ab0cba94fc]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [732 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2023/september-4-workflow-configuration-support-for-notion-feeds-document-ocr</name><title>September 4: Workflow configuration; support for Notion feeds; document OCR | Graphlit Changelog</title></metadata> üõ†Ô∏è\tSeptember 2023\n",
            "September 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "New Features\n",
            "üî• Added Workflow entity to data model for configuring stages of content workflow; can be assigned to Feed or with ingestPage, ingestFile, or ingestText mutations to control how content is ingested, prepared, extracted and enriched into the knowledge graph.\n",
            "üí° Added support for Notion feeds: now can create feed to ingest files from Notion pages or databases (i.e. wikis).\n",
            "üí° Added support for API-created Observation entities, which allow for custom observations of observable entities (i.e. Person, Label) on Content.\n",
            "üí° Added support for Azure AI Document Intelligence as an optional method for preparing PDF files, using OCR and advanced layout analysis.\n",
            "üí° Added summarization strategies, where content can be summarized into paragraphs, bullet points or headline.\n",
            "Added ability to assign default Workflow and Specification to project.\n",
            "Added more well-known link types, during link crawling, such as Discord, Airtable and TypeForm.\n",
            "‚ÑπÔ∏è Free/Hobby plan now has 5GB storage quota; any content ingested past that limit will be auto-deleted.\n",
            "‚ö° Actions have been moved into Workflow entity.\n",
            "‚ö° Link enrichment for Feeds has been moved into the Workflow enrichment stage, now called link crawling.  ExcludeContentDomain property has been reversed and is now called IncludeContentDomain.\n",
            "Bugs Fixed\n",
            "GPLA-1204: Failed to ingest content with backslash in name.\n",
            "GPLA-1276: Failed to ingest RSS posts which contained enclosure URI, but no post URI.\n",
            "PreviousSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "NextAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "Last updated1 year ago\n",
            "- Completion [253 tokens (includes JSON guardrails tokens)], throughput: 79.129 tokens/sec:\n",
            "- New Features:\n",
            "  - Added Workflow entity for configuring content workflow stages.\n",
            "  - Support for Notion feeds to ingest files from Notion pages or databases.\n",
            "  - API-created Observation entities for custom observations on Content.\n",
            "  - Support for Azure AI Document Intelligence for PDF preparation using OCR.\n",
            "  - Summarization strategies for content into paragraphs, bullet points, or headlines.\n",
            "  - Ability to assign default Workflow and Specification to projects.\n",
            "  - Added support for more link types during link crawling (e.g., Discord, Airtable, TypeForm).\n",
            "  - Free/Hobby plan now includes a 5GB storage quota.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Actions moved into Workflow entity.\n",
            "  - Link enrichment for Feeds moved to Workflow enrichment stage, renamed to link crawling.\n",
            "  - ExcludeContentDomain property renamed to IncludeContentDomain.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with ingesting content that had a backslash in the name.\n",
            "  - Resolved failure to ingest RSS posts with enclosure URI but no post URI.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 4, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced workflow configuration, improved content ingestion from Notion, and advanced document processing capabilities.\n",
            "\n",
            "2024-12-29T03:26:18.459Z: Search entities\n",
            "- Workflow [Semantic search] took 0:00:00.151040, used credits [0.00960000]\n",
            "- Processor name [Azure AI Search], units [48]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}