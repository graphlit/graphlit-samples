{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyNKmUsn/TgYPmzdFsQ0d3Qj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2025_01_01_Publish_Graphlit_Year_in_Review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to ingest Graphlit changelog, use OpenAI O1 Mini to write a comprehensive year-in-review."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c64434-aa8f-46a7-b7c8-13992b5ff3d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting graphlit-client\n",
            "  Downloading graphlit_client-1.0.20241229004-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.3)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n",
            "Downloading graphlit_client-1.0.20241229004-py3-none-any.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.6/236.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: graphlit-client\n",
            "Successfully installed graphlit-client-1.0.20241229004\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade isodate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqZf2TOgnXsD",
        "outputId": "dd2b998f-a1f0-4b53-9d1c-3121a3324a16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting isodate\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate\n",
            "Successfully installed isodate-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI49fRzzRU-9",
        "outputId": "6e1549b9-6480-427f-c74c-ef318ef33181"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "async def create_specification(model: enums.OpenAIModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"OpenAI [{model}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.OPEN_AI,\n",
        "        openAI=input_types.OpenAIModelPropertiesInput(\n",
        "            model=model,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_web_feed(uri: str, correlation_id: Optional[str], limit: Optional[int] = None):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.FeedInput(\n",
        "        name=uri,\n",
        "        type=enums.FeedTypes.WEB,\n",
        "        web=input_types.WebFeedPropertiesInput(\n",
        "            uri=uri,\n",
        "            readLimit=limit if limit is not None else 100\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_feed(input, correlation_id=correlation_id)\n",
        "\n",
        "        return response.create_feed.id if response.create_feed is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def is_feed_done(feed_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    response = await graphlit.client.is_feed_done(feed_id)\n",
        "\n",
        "    return response.is_feed_done.result if response.is_feed_done is not None else None\n",
        "\n",
        "\n",
        "async def lookup_usage(correlation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.lookup_usage(correlation_id)\n",
        "\n",
        "        return response.lookup_usage if response.lookup_usage is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def lookup_credits(correlation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.lookup_credits(correlation_id)\n",
        "\n",
        "        return response.lookup_credits if response.lookup_credits is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "def dump_usage_record(record):\n",
        "    print(f\"{record.date}: {record.name}\")\n",
        "\n",
        "    duration = isodate.parse_duration(record.duration)\n",
        "\n",
        "    if record.workflow:\n",
        "        print(f\"- Workflow [{record.workflow}] took {duration}, used credits [{record.credits:.8f}]\")\n",
        "    else:\n",
        "        print(f\"- Operation took {duration}, used credits [{record.credits:.8f}]\")\n",
        "\n",
        "    if record.entity_id:\n",
        "        if record.entity_type:\n",
        "            if record.entity_type == enums.EntityTypes.CONTENT and record.content_type:\n",
        "                print(f\"- {record.entity_type} [{record.entity_id}]: Content type [{record.content_type}], file type [{record.file_type}]\")\n",
        "            else:\n",
        "                print(f\"- {record.entity_type} [{record.entity_id}]\")\n",
        "        else:\n",
        "            print(f\"- Entity [{record.entity_id}]\")\n",
        "\n",
        "    if record.model_service:\n",
        "        print(f\"- Model service [{record.model_service}], model name [{record.model_name}]\")\n",
        "\n",
        "    if record.processor_name:\n",
        "        if record.processor_name in [\"Deepgram Audio Transcription\", \"Assembly.AI Audio Transcription\"]:\n",
        "            length = timedelta(milliseconds=record.count or 0)\n",
        "\n",
        "            if record.model_name:\n",
        "                print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}], length [{length}]\")\n",
        "            else:\n",
        "                print(f\"- Processor name [{record.processor_name}], length [{length}]\")\n",
        "        else:\n",
        "            if record.count:\n",
        "                if record.model_name:\n",
        "                    print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}], units [{record.count}]\")\n",
        "                else:\n",
        "                    print(f\"- Processor name [{record.processor_name}], units [{record.count}]\")\n",
        "            else:\n",
        "                if record.model_name:\n",
        "                    print(f\"- Processor name [{record.processor_name}], model name [{record.model_name}]\")\n",
        "                else:\n",
        "                    print(f\"- Processor name [{record.processor_name}]\")\n",
        "\n",
        "    if record.uri:\n",
        "        print(f\"- URI [{record.uri}]\")\n",
        "\n",
        "    if record.name == \"Prompt completion\":\n",
        "        if record.prompt:\n",
        "            print(f\"- Prompt [{record.prompt_tokens} tokens (includes RAG context tokens)]:\")\n",
        "            print(record.prompt)\n",
        "\n",
        "        if record.completion:\n",
        "            print(f\"- Completion [{record.completion_tokens} tokens (includes JSON guardrails tokens)], throughput: {record.throughput:.3f} tokens/sec:\")\n",
        "            print(record.completion)\n",
        "\n",
        "    elif record.name == \"Text embedding\":\n",
        "        if record.prompt_tokens is not None:\n",
        "            print(f\"- Text embedding [{record.prompt_tokens} tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"Document preparation\":\n",
        "        if record.prompt_tokens is not None and record.completion_tokens is not None:\n",
        "            print(f\"- Document preparation [{record.prompt_tokens} input tokens, {record.completion_tokens} output tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"Data extraction\":\n",
        "        if record.prompt_tokens is not None and record.completion_tokens is not None:\n",
        "            print(f\"- Data extraction [{record.prompt_tokens} input tokens, {record.completion_tokens} output tokens], throughput: {record.throughput:.3f} tokens/sec\")\n",
        "\n",
        "    elif record.name == \"GraphQL\":\n",
        "        if record.request:\n",
        "            print(f\"- Request:\")\n",
        "            print(record.request)\n",
        "\n",
        "        if record.variables:\n",
        "            print(f\"- Variables:\")\n",
        "            print(record.variables)\n",
        "\n",
        "        if record.response:\n",
        "            print(f\"- Response:\")\n",
        "            print(record.response)\n",
        "\n",
        "    if record.name.startswith(\"Upload\"):\n",
        "        print(f\"- File upload [{record.count} bytes], throughput: {record.throughput:.3f} bytes/sec\")\n",
        "\n",
        "    print()\n",
        "\n",
        "async def get_content(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    response = await graphlit.client.get_content(content_id)\n",
        "\n",
        "    return response.content\n",
        "\n",
        "async def query_contents(feed_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.query_contents(\n",
        "            filter=input_types.ContentFilter(\n",
        "                feeds=[\n",
        "                    input_types.EntityReferenceFilter(\n",
        "                        id=feed_id\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return response.contents.results if response.contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def publish_contents(feed_id: str, summary_specification_id: str, publish_specification_id: str, summary_prompt: str, publish_prompt: str, correlation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.publish_contents(\n",
        "            name=\"Published Summary\",\n",
        "            connector=input_types.ContentPublishingConnectorInput(\n",
        "               type=enums.ContentPublishingServiceTypes.TEXT,\n",
        "               format=enums.ContentPublishingFormats.MARKDOWN\n",
        "            ),\n",
        "            summary_prompt=summary_prompt,\n",
        "            summary_specification=input_types.EntityReferenceInput(\n",
        "                id=summary_specification_id\n",
        "            ),\n",
        "            publish_prompt = publish_prompt,\n",
        "            publish_specification=input_types.EntityReferenceInput(\n",
        "                id=publish_specification_id\n",
        "            ),\n",
        "            filter=input_types.ContentFilter(\n",
        "                feeds=[input_types.EntityReferenceFilter(id=feed_id)]\n",
        "            ),\n",
        "            include_details=True,\n",
        "            is_synchronous=True,\n",
        "            correlation_id=correlation_id\n",
        "        )\n",
        "\n",
        "        return response.publish_contents.content.id if response.publish_contents is not None and response.publish_contents.content is not None else None, response.publish_contents.details if response.publish_contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None, None\n",
        "\n",
        "async def delete_all_specifications():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_specifications(is_synchronous=True)\n",
        "\n",
        "async def delete_all_feeds():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_feeds(is_synchronous=True)\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import isodate\n",
        "from IPython.display import display, Markdown, HTML\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Remove any existing feeds, contents and specifications; only needed for notebook example\n",
        "await delete_all_feeds()\n",
        "await delete_all_specifications()\n",
        "await delete_all_contents()\n",
        "\n",
        "print('Deleted all feeds, contents and specifications.')\n",
        "\n",
        "# NOTE: create a unique cost correlation ID\n",
        "ingestion_correlation_id = datetime.now().isoformat()\n",
        "publish_correlation_id = datetime.now().isoformat()\n",
        "\n",
        "uri = \"https://changelog.graphlit.dev\"\n",
        "limit = 100 # maximum number of web pages to ingest\n",
        "\n",
        "feed_id = await create_web_feed(uri, ingestion_correlation_id, limit)\n",
        "\n",
        "if feed_id is not None:\n",
        "    print(f'Created feed [{feed_id}]: {uri}')\n",
        "\n",
        "    # Wait for feed to complete, since ingestion happens asychronously\n",
        "    done = False\n",
        "    time.sleep(5)\n",
        "    while not done:\n",
        "        done = await is_feed_done(feed_id)\n",
        "\n",
        "        if not done:\n",
        "            time.sleep(10)\n",
        "\n",
        "    print(f'Completed feed [{feed_id}].')\n",
        "\n",
        "    # Query contents by feed\n",
        "    contents = await query_contents(feed_id)\n",
        "\n",
        "    if contents is not None:\n",
        "        print(f'Found {len(contents)} contents in feed [{feed_id}].')\n",
        "        print()\n",
        "\n",
        "        for content in contents:\n",
        "            if content is not None:\n",
        "\n",
        "                display(Markdown(f'# Ingested content [{content.id}]'))\n",
        "\n",
        "                print(f'Text Mezzanine: {content.text_uri}')\n",
        "\n",
        "                print(content.markdown)"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04579f2e-7ef5-48cb-bcb7-7903b71ea029"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all feeds, contents and specifications.\n",
            "Created feed [a0d5de6a-9901-42e2-b016-839e5c7c8408]: https://changelog.graphlit.dev\n",
            "Completed feed [a0d5de6a-9901-42e2-b016-839e5c7c8408].\n",
            "Found 48 contents in feed [a0d5de6a-9901-42e2-b016-839e5c7c8408].\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [65dc8b4e-c1dc-40e2-a653-d8d51fe0414c]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/65dc8b4e-c1dc-40e2-a653-d8d51fe0414c/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎒\tSeptember 2024\n",
            "\n",
            "# September 3: Support for web search feeds, model deprecations\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports web search feeds, using the Tavily and Exa.AI web search APIs. You can choose the SEARCH feed type, and assign your search text property, and we will ingest the referenced web pages from the search results. Optionally, you can select the search service via the serviceType property under search feed properties. By default, Graphlit will use the Tavily API.\n",
            "- ⚡ We have deprecated these OpenAI models, according to the future support OpenAI is providing to these legacy models: GPT35_TURBO, GPT35_TURBO_0613, GPT35_TURBO_16K, GPT35_TURBO_16K_0125, GPT35_TURBO_16K_0613, GPT35_TURBO_16K_1106, GPT4, GPT4_0613, GPT4_32K, GPT4_32K_0613, GPT4_TURBO_VISION_128K, and GPT4_TURBO_VISION_128K_1106. We suggest using GPT-4o or GPT-4o Mini instead.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2523: Can't ingest from same feed URI multiple times and wait on isFeedDone\n",
            "\n",
            "PreviousSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "NextSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "Last updated3 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [9819c811-85db-4a53-923e-f3a6beac4b6a]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/9819c811-85db-4a53-923e-f3a6beac4b6a/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎒\tSeptember 2024\n",
            "\n",
            "# September 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Azure AI Model Inference API (aka Models as a Service) model service which offers serverless hosting to many models such as Meta Llama 3.2, Cohere Command-R, and many more. For Azure AI, all models are 'custom', and you will need to provide the serverless endpoint, API key and number of tokens accepted in context window, after provisioning the model of your choice.\n",
            "- We have added support for the multimodal Mistral Pixtral model, under the model enum PIXTRAL_12B_2409.\n",
            "- We have added versioned model enums for Google Gemini, so you can access GEMINI_1_5_FLASH_001, GEMINI_1_5_FLASH_002, GEMINI_1_5_PRO_001 and GEMINI_1_5_PRO_002.\n",
            "\n",
            "PreviousOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "NextSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "Last updated3 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [4983a4c2-2d1a-4640-83f7-a187a4275778]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/4983a4c2-2d1a-4640-83f7-a187a4275778/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎒\tSeptember 2024\n",
            "\n",
            "# September 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports entity enrichment from Fast Healthcare Interoperability Resources (FHIR) servers. You can provide the endpoint for a FHIR server, and Graphlit will enrich medical-related entities from the data found in the FHIR server.\n",
            "- Added support for latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408) and added datestamped model enums for the previous versions (COMMAND_R_202403, COMMAND_R_PLUS_202404). The latest model enums (COMMAND_R and COMMAND_R_PLUS) currently point to the models (COMMAND_R_202403 and COMMAND_R_PLUS_202404) as specified by the Cohere API.\n",
            "- Added support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31), now used by default.\n",
            "- ⚡ We have changed the name of the LinkReferenceType to LinkReference to follow the existing data model standard.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3120: LLM is adding source tags to end of completed messages\n",
            "- GPLA-3133: Failed to load sitemap on child page of website.\n",
            "\n",
            "PreviousSeptember 3: Support for web search feeds, model deprecations\n",
            "NextAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "Last updated4 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [88122e0e-dc76-44ba-a14b-5f5fb698524d]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/88122e0e-dc76-44ba-a14b-5f5fb698524d/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎒\tSeptember 2024\n",
            "\n",
            "# September 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Cerebras model service which offers the LLAMA_3_1_70B and LLAMA_3_1_8B models.\n",
            "- 💡 Graphlit now supports the Google AI model service which offers the GEMINI_1_5_PRO and GEMINI_1_5_FLASH models.\n",
            "- We have added support for the latest Groq Llama 3.2 preview models, including LLAMA_3_2_1B_PREVIEW, LLAMA_3_2_3B_PREVIEW, LLAMA_3_2_11B_TEXT_PREVIEW, and LLAMA_3_2_90B_TEXT_PREVIEW. We have also added support for the Llama 3.2 multimodal model LLAMA_3_2_11B_VISION_PREVIEW.\n",
            "- We have added a new specification parameter to the promptConversation mutation. Now you can specify your initial specification for a new conversation, or update an existing conversation, without requiring additional API calls.\n",
            "- ⚡ We have changed the retrieval behavior of the promptConversation mutation. Now, if no relevant content was found via vector-based semantic search (given the user prompt), we will fallback to any relevant content from the message in the conversation. If there was no content from the conversation to fallback to, we will fallback to the last ingested content in the project. This solves an issue where a first prompt like 'Summarize this' would find no relevant content. Now it will fallback to retrieve the last ingested content.\n",
            "- ⚡ We have renamed the Groq model enum from LLAVA_1_5_7B to LLAVA_1_5_7B_PREVIEW.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3083: Not sending custom instructions/guidance with extraction prompt\n",
            "- GPLA-3146: Filtering Persons by email not working\n",
            "- GPLA-3171: Not failing on deprecated OpenAI model\n",
            "- GPLA-3158: Summarization not using revision strategy\n",
            "\n",
            "PreviousSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "NextSeptember 3: Support for web search feeds, model deprecations\n",
            "Last updated3 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [8fff37ca-bf13-47f8-b452-1010061953f8]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/8fff37ca-bf13-47f8-b452-1010061953f8/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🛠️\tSeptember 2023\n",
            "\n",
            "# September 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "### New Features\n",
            "\n",
            "- 🔥 Graphlit now supports YouTube feeds, where you can ingest a set of YouTube videos, or an entire YouTube playlist or channel. Note, we currently support only the ingestion of audio from YouTube videos, which gets transcribed and added to your conversational knowledge graph.\n",
            "\n",
            "### New Documentation\n",
            "\n",
            "- Added documentation for observable entities mutations and queries (Label, Category, Person, Organization, Place, Event, Product, Repo, Software).\n",
            "- Added documentation for using custom Azure OpenAI and OpenAI models with Specifications\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1459: LLM prompt formatting was exceeding the token budget with long user prompts.\n",
            "- GPLA-1445: Failed to ingest PDF from URL where filename in Content-Disposition header contained a backslash.\n",
            "\n",
            "PreviousOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "NextSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "Last updated7 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [811852e3-3743-4d28-af22-15c2423faa4a]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/811852e3-3743-4d28-af22-15c2423faa4a/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🛠️\tSeptember 2023\n",
            "\n",
            "# September 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "### New Features\n",
            "\n",
            "- 🔥 Added Workflow entity to data model for configuring stages of content workflow; can be assigned to Feed or with ingestPage, ingestFile, or ingestText mutations to control how content is ingested, prepared, extracted and enriched into the knowledge graph.\n",
            "- 💡 Added support for Notion feeds: now can create feed to ingest files from Notion pages or databases (i.e. wikis).\n",
            "- 💡 Added support for API-created Observation entities, which allow for custom observations of observable entities (i.e. Person, Label) on Content.\n",
            "- 💡 Added support for Azure AI Document Intelligence as an optional method for preparing PDF files, using OCR and advanced layout analysis.\n",
            "- 💡 Added summarization strategies, where content can be summarized into paragraphs, bullet points or headline.\n",
            "- Added ability to assign default Workflow and Specification to project.\n",
            "- Added more well-known link types, during link crawling, such as Discord, Airtable and TypeForm.\n",
            "- ℹ️ Free/Hobby plan now has 5GB storage quota; any content ingested past that limit will be auto-deleted.\n",
            "- ⚡ Actions have been moved into Workflow entity.\n",
            "- ⚡ Link enrichment for Feeds has been moved into the Workflow enrichment stage, now called link crawling. ExcludeContentDomain property has been reversed and is now called IncludeContentDomain.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1204: Failed to ingest content with backslash in name.\n",
            "- GPLA-1276: Failed to ingest RSS posts which contained enclosure URI, but no post URI.\n",
            "\n",
            "PreviousSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "NextAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [8413f583-bc40-4957-b03b-f62d5078fe09]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/8413f583-bc40-4957-b03b-f62d5078fe09/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🛠️\tSeptember 2023\n",
            "\n",
            "# September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "### New Features\n",
            "\n",
            "- 🔥 Graphlit now supports paid Hobby, Starter and Growth tiers for projects, in addition to the existing Free tier. Starting at $49/mo, plus $0.10/credit for usage, we now support higher quota based on your subscribed tier. By providing a payment method for your organization in the Developer Portal, you can upgrade each project individually to the tier that fits your application's needs.\n",
            "- 💡 Added GraphQL mutations for the creation, update and deletion of observed entities (i.e. Person, Organization, Place, Product, Event, Label, Category).\n",
            "- 💡 Added new observed entity types to knowledge graph: Repo (i.e. Git repo), Software.\n",
            "- 💡 Added searchType and numberSimilar fields to Specification object for configuring semantic search in conversations. In situations where the user prompt is limited in length, HYBRID search type can provide better semantic search results for the prompt context.\n",
            "- 💡 Added support for the Azure OpenAI GPT-4 model.\n",
            "- Added support for project quota field. Project quotas are based on the subscribed pricing tier. Quota limits are now applied as content is ingested, and as feeds and conversations are created.\n",
            "- Added contentLimit to conversation strategy object to limit the number of semantic search content results which are formatted into prompt context.\n",
            "- Better relevance ranking on semantic search results when formatting prompt context in conversations.\n",
            "- ℹ️ Free tier has updated quota: 1GB storage, 100 contents, 3 feeds and 10 conversations.\n",
            "- ⚡ Now using the Deepgram Nova-2 audio transcription model, which is 18% more accurate, and 5-40x faster.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1373: Failed to extract multiple text pages from DOCX without page breaks. Now we support token-aware page chunking.\n",
            "- GPLA-1377: Failed during semantic search with no content results, when prompting conversation.\n",
            "- GPLA-1415: Failed when user prompt couldn't generate text embeddings.\n",
            "\n",
            "PreviousSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "NextSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "Last updated11 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [833da803-989f-4c1c-96af-c4cf003e6e52]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/833da803-989f-4c1c-96af-c4cf003e6e52/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 9: Support for GitHub repository feeds, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports GitHub feeds, by providing the repository owner and name similar to GitHub Issues feeds, and will ingest code files from any GitHub repository.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3262: Missing row separator in table markdown formatting\n",
            "\n",
            "PreviousOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "NextOctober 7: Support for Anthropic and Gemini tool calling\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [1243312d-9b3b-4172-88ea-ee1b5c097b2c]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/1243312d-9b3b-4172-88ea-ee1b5c097b2c/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 7: Support for Anthropic and Gemini tool calling\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports tool calling with Anthropic and Google Gemini models.\n",
            "- ⚡ We have removed the uri property for tools from ToolDefinitionInput, such that inline webhook tools are no longer supported. Now you can define any external tools to be called, and those can support sync or async data access to fulfill the tool call.\n",
            "\n",
            "PreviousOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "NextOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [119c6576-e615-476e-8ab9-27d841050c5c]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/119c6576-e615-476e-8ab9-27d841050c5c/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 31: Support for simulated tool calling, bug fixes\n",
            "### New Features\n",
            "\n",
            "- Graphlit now supports simulated tool calling for LLMs which don't natively support it, such as OpenAI o1-preview and o1-mini. Tool schema will be formatted into the LLM prompt context, and tool responses are parsed out of the JSON formatted response.\n",
            "- ⚡ Given customer feedback, we have lowered the vector and hybrid thresholds used by the semantic search. Previously, some content at a low relevance was being excluded from the semantic search results. Now, more low-relevance content will be included in the results, used by the RAG pipeline. Reranking can be used to sort the search results for relevance.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3357: Not extracting all images from PDF, and should filter out single-color images.\n",
            "\n",
            "PreviousNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "NextOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [68397ab0-4d1f-4b4a-bb0d-af89d00ba47f]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/68397ab0-4d1f-4b4a-bb0d-af89d00ba47f/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the ingestBatch mutation, which accepts an array of URIs to files or web pages, and will asynchronously ingest these into content objects.\n",
            "- 💡 Graphlit now supports the continueConversation mutation, which accepts an array of called tool responses. Also, promptConversation now accepts an array of tool definitions. When tools are called by the LLM, the assistant message returned from promptConversation will have a list of toolCalls which need to responded to from your calling code. These responses are to be provided back to the LLM via the continueConversation mutation.\n",
            "- 💡 Graphlit now supports tool calling with OpenAI, Mistral, Deepseek, Groq, and Cerebras model services. Anthropic, Google Gemini and Cohere support will come later.\n",
            "- Added support for prefilled user and assistant messages with createConversation mutation. Now you can send an array of messages when creating a new conversation, which will bootstrap the conversation with the LLM. These must be provided in user/assistant pairs.\n",
            "- Added support for Google Gemini Flash 1.5 8b model.\n",
            "- ⚡ We have deprecated the tools property in the Specification object. These will be removed at a later date. Tools are now to be sent directly to the extractContents and promptConversation mutations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3207: Models shouldn't be required on update specification call\n",
            "- GPLA-3220: Don't send system prompt with OpenAI o1 models\n",
            "\n",
            "PreviousOctober 7: Support for Anthropic and Gemini tool calling\n",
            "NextSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [706fcc2b-e7bf-4d03-9660-c94e2dc27296]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/706fcc2b-e7bf-4d03-9660-c94e2dc27296/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "### New Features\n",
            "\n",
            "- Graphlit now supports the latest Anthropic Sonnet 3.5 model (released 10/22/2024). We have added date-versions model enums for the Anthropic models: CLAUDE_3_5_SONNET_20240620, CLAUDE_3_5_SONNET_20241022, CLAUDE_3_HAIKU_20240307, CLAUDE_3_OPUS_20240229, CLAUDE_3_SONNET_20240229. The existing model enums will target the latest released models, as specified by Anthropic.\n",
            "- Graphlit now supports image embeddings using the Cohere Embed 3.0 models.\n",
            "\n",
            "PreviousOctober 31: Support for simulated tool calling, bug fixes\n",
            "NextOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [9e472441-d42b-4b2e-9115-fc9a123975dc]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/9e472441-d42b-4b2e-9115-fc9a123975dc/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎃\tOctober 2024\n",
            "\n",
            "# October 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the configuration of image and text embedding models, at the Project level. You can create an embedding specification for a text or image embedding model, and then assign that to the Project, and all further embedding requests will use that embedding model. See this Colab notebook for an example of how to configure the project.\n",
            "- 💡 Graphlit now supports the OpenAI Embedding-3-Small and Embedding-3-Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0 and 3.0 text embedding models. Graphlit also now supports Jina CLIP image embeddings, which are used by default for image search.\n",
            "- Graphlit now supports the chunkTokenLimit property in Specifications, which specifies the number of tokens for each embedded text chunk. If this is not configured, Graphlit uses 600 tokens for each embedded text chunk.\n",
            "- Graphlit now supports the Voyage reranking model.\n",
            "- Graphlit now supports the ingestTextBatch mutation, which accepts an array of text and name pairs, and will asynchronously ingest these into content objects.\n",
            "- ⚡ We have moved the chunkTokenLimit property from the Workflow storage embeddings strategy to the Specification object. The Workflow storage property has now been deprecated.\n",
            "- ⚡ We have deprecated the openAIImage property from Workflow entity extraction properties. Use the modelImage property instead.\n",
            "\n",
            "Once a text embedding model has been updated at the project level, any content, conversations or observed entities will no longer be semantically searchable.\n",
            "Text embeddings are not compatible across models, so you will need to delete and reingest any content, or recreate conversations or knowledge graph entities, with the new embedding model to become searchable.\n",
            "PreviousOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "NextOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "Last updated2 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [2791558e-fd4b-4ab1-9328-fe6ac0c05786]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/2791558e-fd4b-4ab1-9328-fe6ac0c05786/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎃\tOctober 2023\n",
            "\n",
            "# October 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports 'aliases' of observable names, as the alternateNames property. When an observed entity, such as Organization, is enriched, we store the original name and the enriched name as an alias. For example, \"OpenAI\" may be enriched to \"OpenAI, Inc.\", and we store \"OpenAI\" as an alias, and update the name to \"OpenAI, Inc.\".\n",
            "- 💡 Added workflows filter to ContentCriteriaInput type, for filtering content by workflow(s) when creating conversation.\n",
            "- Optimized formatting of content sources into prompt context, for more accurate conversation responses.\n",
            "- Optimized formatting of extracted text from Slack messages, for better knowledge retrieval.\n",
            "- Updated text tokenizer for more accurate token counting.\n",
            "- Upgraded Azure Text Analytics to latest preview API version.\n",
            "- Authors found in RSS feeds are now stored as observations of Person entities.\n",
            "- Added rate limiting for Reddit feeds.\n",
            "- Added rate limiting for Wikipedia enrichment.\n",
            "- Added support for reading Reddit post comments when reading Reddit feed.\n",
            "- ⚡ EmbedFacets has been renamed to EnableFacets in the conversation strategy.\n",
            "- ⚡ Removed extra content level in IngestionWorkflowStage type. Now, the if property is of type IngestionContentFilter.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1556: Better handling of very long user prompts.\n",
            "- GPLA-1627: Optimized token budget for more accurate prompt completion.\n",
            "- GPLA-1585: More accurate entity matching in Wikipedia entity enrichment.\n",
            "\n",
            "PreviousDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "NextOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [3d6fee17-3911-4a12-9354-aaaed930f9d1]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/3d6fee17-3911-4a12-9354-aaaed930f9d1/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🦃\tNovember 2024\n",
            "\n",
            "# November 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "### New Features\n",
            "\n",
            "- Graphlit now supports the latest Anthropic Haiku 3.5 model, with the model enum CLAUDE_3_5_HAIKU_20241022.\n",
            "- ⚡ Once a project has hit the free tier quota, we will now automatically disable all feeds. Once the project has been upgraded to a paid tier, you can use the enableFeed mutation to re-enable your existing feeds to continue ingestion.\n",
            "- ⚡ We have added the disableFallback flag to the RetrievalStrategyInput type, so you can disable the default behavior of falling back to the previous conversation's contents, or worst-case, falling back to the most recently uploaded content. By setting disableFallback to true, conversations will only attempt to retrieve contents based on the provided filter and/or augmentedFilter properties.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3367: Not extracting text from HTML button element\n",
            "\n",
            "PreviousNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "NextOctober 31: Support for simulated tool calling, bug fixes\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f1678834-af32-4b3d-b6a2-54a2abc0aac5]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/f1678834-af32-4b3d-b6a2-54a2abc0aac5/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎃\tOctober 2023\n",
            "\n",
            "# October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "### New Features\n",
            "\n",
            "- 🔥 Graphlit now supports Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "- 🔥 Graphlit now supports Slack feeds, and will ingest Slack messages and linked file attachments from a Slack channel. Note, this requires the creation of a Slack bot which has been added to the appropriate Slack channel.\n",
            "- 💡 Added support for entity enrichment to workflow object, which offers Diffbot, Wikipedia and Crunchbase enrichment of observed entities, such as Person, Organization and Place.\n",
            "- 💡 Added support for text extraction from images. When using Azure Image Analytics for entity extraction, Graphlit will extract and store any identified text which then becomes searchable.\n",
            "- Added embedFacets property to conversation strategy in specification object.\n",
            "- Added embedCitations property to conversation strategy in specification object. This makes content citations optional with the completed conversation message.\n",
            "- Added GraphQL mutations for multi-delete of entities, such as deleteCollections, deleteLabels, or deleteConversations.\n",
            "- Added GraphQL deleteAllConversations mutation to delete all conversations.\n",
            "- Added support for automatically adding ingested content to one or more collections, via ingestion stage of workflow object.\n",
            "- Added specification property to preparation workflow stage, which will be used to select the LLM for text summarization.\n",
            "- Expanded the properties for observed entities, such as Person, Organization or Product. Now supports a wider range of properties for entity enrichment.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1520: Unlimited conversation quota not assigned when upgrading project tier\n",
            "- GPLA-1285: Entity enrichment not firing event, which can be sent to actions\n",
            "- GPLA-1361: Web page left in ingested state, when URL not accessible.\n",
            "\n",
            "PreviousOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "NextSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [9303dfbd-2681-4e9b-93ec-2f6a8c1c326f]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/9303dfbd-2681-4e9b-93ec-2f6a8c1c326f/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🦃\tNovember 2024\n",
            "\n",
            "# November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports multi-turn analysis of images with the reviseImage and reviseEncodedImage mutations. You can provide an LLM prompt and either a URI or Base-64 encoded image and MIME type, along with an optional LLM specification. This can be used for analyzing any image and having a multi-turn conversation with the LLM to revise the output from the LLM. (Colab Notebook Example)\n",
            "- 💡 Graphlit now supports directly prompting an LLM with the prompt mutation, bypassing any RAG content retrieval, while providing an optional list of previous conversation messages. This also accepts an optional LLM specification. (Colab Notebook Example)\n",
            "- We have added support for the new Mistral Pixtral Large model, with PIXTRAL_LARGE model enum, which can be used with LLM completion or entity extraction LLM specifications.\n",
            "- We have added support for the OpenAI 2024-11-20 version of GPT-4o, with GPT4O_128K_20241120 model enum.\n",
            "- ⚡ We have added Microsoft Entra ID (fka Azure Active Directory) clientId and clientSecret properties to the SharePointFeedPropertiesInput type, which are now required when creating a SharePoint feed using user authentication with refreshToken property. (Colab Notebook Example)\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3438: Not filtering on desktop presentation when scraping web pages\n",
            "- GPLA-3340: Failed to parse invalid JSON from extracted PDF page\n",
            "- GPLA-3427: Not formatting extracted tables properly from Sonnet 3.5\n",
            "\n",
            "PreviousDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "NextNovember 16: Support for image description, multi-turn text summarization\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f6df142b-f920-4c85-b4d9-1dc09d7d2315]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/f6df142b-f920-4c85-b4d9-1dc09d7d2315/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🦃\tNovember 2024\n",
            "\n",
            "# November 16: Support for image description, multi-turn text summarization\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports multi-turn summarization of text with the reviseText mutation. You can provide an LLM prompt and text string, along with an optional specification. This can be used for summarizing any raw text and having a multi-turn conversation with the LLM to revise the output from the LLM. (Colab Notebook Example)\n",
            "- 💡 Graphlit now supports image descriptions using vision LLMs, without needing to ingest the image first. With the new describeImage mutation, which takes a URI, and describeEncodedImage mutation, which takes a Base-64 encoded image and MIME type, you can use any vision LLM to prompt an image description. These mutations accept an optional specification, where you can select your vision LLM. If not provided, OpenAI GPT-4o will be used. (Colab Notebook Example)\n",
            "\n",
            "PreviousNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "NextNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [03103626-0933-40e5-9eb3-31d1d9694e1a]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/03103626-0933-40e5-9eb3-31d1d9694e1a/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🦃\tNovember 2024\n",
            "\n",
            "# November 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports web search with the searchWeb mutation. You can select the search service, either Tavily or Exa.AI, and provide the search query and number of search results to be returned. This is different than the web search feed, in that searchWeb returns the relevant text from the web page and the web page URL from each search hit, but does not ingest each of the web pages. This new mutation is optimized to be used from within an LLM tool.\n",
            "- 💡 Graphlit now supports multi-turn summarization of content with the reviseContent mutation. You can provide an LLM prompt and a content reference, along with an optional specification. This can be used for summarizing any content (documents, web pages, audio transcripts, etc.), and having a multi-turn conversation with the LLM to revise the output from the LLM. Internally, this creates a conversation locked to a single piece of content. This works especially well with the OpenAI o1-preview and o1-mini models, because they provide a longer LLM output from each turn.\n",
            "- Graphlit now supports the configuration of the Deepgram transcription language, and whether detectLanguage is enabled in DeepgramAudioPreparationPropertiesInput. Language detection is now enabled by default, and can be disabled by setting detectLanguage to false.\n",
            "- ⚡ We have added a requireTool option to promptConversation mutation, so you can control whether the LLM must call one of the provided tool, or if tool calling is optional.\n",
            "- ⚡ For accounts created after Nov 8, 2024, we have lowered the credits quota on the Free tier from 1000 credits to 100 credits, and now offer unlimited feeds on the Hobby Tier.\n",
            "- ⚡ The Graphlit Data API will now return HTTP 402 (Payment Required) when you have exceeded the credits quota on the free tier. You must upgrade to the Hobby Tier (or higher) to continue using the API, once the credits quota has been reached.\n",
            "\n",
            "PreviousNovember 16: Support for image description, multi-turn text summarization\n",
            "NextNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [5d06c7c1-735d-41a3-b95e-8bce0b652102]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/5d06c7c1-735d-41a3-b95e-8bce0b652102/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "💐\tMay 2024\n",
            "\n",
            "# May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Jina reranker and Pongo semantic filtering (reranking), in the Specification object. Now you can choose between COHERE, PONGO and JINA for your reranking serviceType.\n",
            "- 💡 Graphlit now supports Microsoft Teams feeds for reading messages from Teams channels.\n",
            "- Given changes in YouTube video player HTML, we have rewritten the YouTube downloader to support the new page format.\n",
            "- Added better handling of HTTP errors when validating URIs. Previously some websites were returning HTTP 403 (Forbidden) errors when validating their URI, or downloading content. Now Graphlit is able to scrape these sites, which previously returned errors.\n",
            "- Added support for updating content metadata in updateContent mutation. Now the video, audio, document, etc. metadata can be updated after the content workflow has finished.\n",
            "- Added query_contents_graph (and queryContentsGraph) functions to SDKs, which can be used to return nodes and edges from knowledge graph for visualization.\n",
            "- ⚡ Citation indices have been changed to be one-based from zero-based. For example, you will now see \"This is a citation. [1]\" as the first citation in the list.\n",
            "- ⚡ Added isSynchronous flag to deleteAll and multiple delete mutations. By default, bulk delete operations are now asynchronous (and completed after the mutation returns), unless the isSynchronous flag is set to true.\n",
            "- ⚡ Added missing count mutations, such as countAlerts, countFeeds, etc.\n",
            "- ⚡ Renamed query_content_facets to query_contents_facets in Python SDK\n",
            "- ⚡ Renamed queryContentFacets to queryContentsFacets in Node.js SDK\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2544: Page relevance not filled-in in all situations\n",
            "- GPLA-2546: Not extracting links from PDF with Azure AI Doc Intelligence\n",
            "- GPLA-2557: Sporadically returning HTTP 500 from GraphQL API\n",
            "- GPLA-2573: Failed to re-ingest content which was deleted immediately after initial ingestion\n",
            "- GPLA-2575: Not validating for empty (non-null) parameters in mutations\n",
            "- GPLA-2578: Need to handle invalid JSON from LLMs; improper escaping or formatting\n",
            "- GPLA-2585: Failed to ingest encoded file with colon (:) in name\n",
            "\n",
            "PreviousMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "NextApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [e0f9a487-28bc-4d08-8b71-0dded28e8378]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/e0f9a487-28bc-4d08-8b71-0dded28e8378/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "💐\tMay 2024\n",
            "\n",
            "# May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports GraphRAG, where the extracted entities in the knowledge graph can be added as additional context to your RAG con,versation. Also, with GraphRAG, entities can be extracted from the user prompt, and used as additional content filters - or can be used to query related content sources, which are combined with the vector search results. This can be configured by specifying your graphStrategy in the Specification object.\n",
            "- 💡 Graphlit now supports LLM revisions within RAG conversations, where the LLM can be prompted to revise its initial completion response. From our testing, this has been shown to provide 35% more output tokens with higher quality responses. This can be configured by specifying your revisionStrategy, and you can use our built-in revision prompt, or provide a custom one, and specify how many revisions you want the LLM to make.\n",
            "- 💡 Graphlit now supports the new OpenAI GPT-4o model for RAG conversations.\n",
            "- ⚡ We have changed the default model for Conversations to be OpenAI GPT-4o, from Azure OpenAI GPT-3.5 16k. This provides faster performance and better quality output.\n",
            "- Added graph to promptConversation response, so you can visualize or leverage the nodes and edges of the knowledge graph, resulting from the content retrieval. For example, if a Person and Organization were observed in the cited content sources used by the RAG pipeline, you will get back those entities and their relationship (such as Person 'works-for' Organization).\n",
            "- Expanded the enriched data from WIkipedia to include the long description of an entity.\n",
            "- Added getSharePointLibraries, getSharePointFolders, and getOneDriveFolders queries to the API, which can be used to enumerate the storage services. This makes locating the SharePoint libraryId easier, for example.\n",
            "- Added getTeams and getTeamsChannels queries to the API for enumerating Microsoft Teams workspaces.\n",
            "- Added extractedCount to the entity extraction connector to limit the number of extracted entities, per entity type. I.e. if extracted count is 10, it will extract at most ten each of Persons, Organizations, etc.\n",
            "- 🔥 We have improved performance in several areas: creation of observations after entity extraction, access to cloud storage, rendering the RAG context.\n",
            "- 🔥 We have optimized the LLM entity extraction process to identify more properties, as well as entity-to-entity relationships.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2652: Not extracting text from HTML in RSS post\n",
            "- GPLA-2627: Limit filter only returning half the results\n",
            "- GPLA-2613: Not properly extracting structured text from JSON/XML formats\n",
            "\n",
            "PreviousJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "NextMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "Last updated6 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [8f1c79b1-beaf-481d-a7a7-078105781fbb]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/8f1c79b1-beaf-481d-a7a7-078105781fbb/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🍀\tMarch 2024\n",
            "\n",
            "# March 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Linear, GitHub Issues and Atlassian Jira feeds. Graphlit will ingest issues (aka tasks, stories) from these issue-tracking services as individual content items, which will be made searchable and conversational.\n",
            "- 💡 Added support for ISSUEcontent type, which includes metadata such as title, authors, commenters, status, type, project and team.\n",
            "- 💡 Added support for default feed read limit. Now, if you don't specify the readLimit property on feeds, it will default to reading 100 content items. You can override this default by assigning a custom read limit, which has no upper bounds. However, one-shot feeds much complete within 15 minutes, or they will be stopped automatically.\n",
            "- Added support for ingesting files referenced in a Web sitemap. Previously any files (i.e. PDF, MP3) referenced in a sitemap.xml would be ignored. Now you can optionally enable includeFiles in the WebFeedPropertiesInput object to have Graphlit ingest non-HTML pages as part of the Web feed.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2374: Failed to ingest MP4 with large XMP metadata.\n",
            "\n",
            "PreviousApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "NextMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [b57cc140-8c7d-4608-8a89-7dfa783797ec]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/b57cc140-8c7d-4608-8a89-7dfa783797ec/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🍀\tMarch 2024\n",
            "\n",
            "# March 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports a Command-Line Interface (CLI) for directly accessing the Graphlit Data API without writing code. See the documentation here.\n",
            "- 💡 Graphlit now supports the Groq Platform, and models such as Mixtral 8x7b.\n",
            "- 💡 Graphlit now supports Claude 3 Opus and Sonnet models.\n",
            "- 💡 Graphlit now supports Mistral La Plateforme, and models such as Mistral Small, Medium, and Large and Mixtral 8x7b.\n",
            "- 💡 Graphlit now supports the latest v4 of Azure Document Intelligence, including their new models such as Credit Card, Marriage Certificate, and Mortgage documents.\n",
            "- Added support for detailed usage and credits telemetry via API, with the usage, credits, lookupUsage and lookupCredits queries.\n",
            "- Added support for correlated telemetry, where an optional correlationId can be provided with GraphQL queries and mutations, so credits and usage can be tracked across requests.\n",
            "- Added support for project webhook, which will be called when credits have been consumed by the project.\n",
            "- Added support for image extraction during DOCX, XLSX, and PPTX document preparation.\n",
            "- Added text and markdown properties to Content object, which provide formatted output of extracted text from any content.\n",
            "- Added more accurate extraction of tables into mezzanine JSON format, across all content types.\n",
            "- Added throughput property to Conversation messages, which returns the tokens/second throughput of LLM.\n",
            "- ⚡ Deprecated mezzanineUri property in Content object, which has been replaced by textUri and audioUri.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2281: Not extracting table from PPTX file.\n",
            "- GPLA-2282: Not extracting Markdown tables.\n",
            "- GPLA-2247: Not extracting relative HTML links properly.\n",
            "- GPLA-2241: Failed to post Alert to Slack with Markdown format.\n",
            "\n",
            "PreviousMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "NextFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [7fb352e7-6eda-419f-848d-d23527d330cc]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/7fb352e7-6eda-419f-848d-d23527d330cc/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🍀\tMarch 2024\n",
            "\n",
            "# March 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Claude 3 Haiku model.\n",
            "- Added support for direct ingestion of Base64 encoded files with the ingestEncodedFile mutation. You can pass a Base64 encoded string and MIME type of the file, and it will be ingested into the Graphlit Platform.\n",
            "- Added modelService and model properties to ConversationMessage type, which return the model service and model which was used for the LLM completion.\n",
            "\n",
            "PreviousMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "NextMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [8ca8bf83-6c04-4d64-9953-5699918fbb06]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/8ca8bf83-6c04-4d64-9953-5699918fbb06/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎓\tJune 2024\n",
            "\n",
            "# June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Anthropic Claude 3.5 Sonnet model, which can be assigned with the CLAUDE_3_5_SONNET model enum.\n",
            "- 💡 Graphlit now supports semantic search of observable entities in the knowledge graph, such as Person, Organization and Place. These entity types will now have vector embeddings created from their enriched metadata, and support searching by similar text, and searching by similar entities.\n",
            "- ⚡ We have changed the Google Drive and Google Email feed properties to require the Google OAuth client ID and client secret, along with the existing refresh token, for proper authentication against Google APIs.\n",
            "- ⚡ We have added a credits quota on the Free Tier. Once 1000 credits have been used on the Free Tier, no more content can be ingested, and an upgrade to a paid tier is required. Customers will receive an email when the credits, storage or contents quota has been reached.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2837: Failed to ingest LinkedIn page as Web feed\n",
            "- GPLA-2831: Zero-byte file was left in Indexed state\n",
            "- GPLA-2834: Not reading any files from Azure blob feed with space in prefix\n",
            "- GPLA-2828: Better handling for files with unknown (or missing) file extensions\n",
            "\n",
            "PreviousJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "NextJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "Last updated6 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [cb4a2f54-a3ae-4b9b-a66e-27232fea2d14]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/cb4a2f54-a3ae-4b9b-a66e-27232fea2d14/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎓\tJune 2024\n",
            "\n",
            "# June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Deepseek LLMs for prompt completion. We offer the deepseek-chat and deepseek-coder models.\n",
            "- 💡 Graphlit now supports parsing embedded JSON-LD from web pages. If a web page contains 'script' tags with JSON-LD, we will automatically parse and inject into the knowledge graph.\n",
            "- ⚡ We have changed the default model for entity extraction and image completions to be OpenAI GPT-4o. This provides faster performance and better quality output.\n",
            "- ⚡ We have changed the behavior of knowledge graph generation, from a prompted conversation, to be opt-in. In order to receive the graph's nodes and edges with the response, you will now need to set generateGraph to True in the specification's graphStrategy object. This provides improved performance when the graph is not needed for visualization.\n",
            "- Added thing property for observable entities, which returns the JSON-LD metadata associated with the entity.\n",
            "- Added regex-based filtering for URI paths during feed ingestion, link crawling, and workflow filtering. You can assign regex patterns in allowedPaths and excludedPaths.\n",
            "- Added observableLimit to configure the limit of how many observed entities will be added to the GraphRAG context, defaults to 1000.\n",
            "- Added prompt to suggestConversation mutation, which allows customization of the followup question generation.\n",
            "- Updated suggestConversation to incorporate the past conversation message history, in addition to the filtered set of content sources.\n",
            "- 🔥 We have improved performance in knowledge graph retrieval and generation, via better parallelization and batching.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2748: Optimize the retrieval performance of observed entities during GraphRAG\n",
            "- GPLA-2732: Invalid user-provided URI causing parsing exception\n",
            "- GPLA-2666: Shouldn't require tenant ID for Microsoft email or Teams\n",
            "- GPLA-2772: Not returning labels or categories from graph in API\n",
            "- GPLA-2762: Failed to extract spreadsheet images\n",
            "- GPLA-2687: Email to/from not getting added as observations on emails\n",
            "- GPLA-2738: API is returning 'audio' metadata from podcast HTML document\n",
            "\n",
            "PreviousJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "NextMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "Last updated6 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [79510fa9-9ddb-4b7e-8436-a2b0a867ac64]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/79510fa9-9ddb-4b7e-8436-a2b0a867ac64/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "☀️\tJuly 2024\n",
            "\n",
            "# July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports webhook Alerts. In addition to Slack notifications, you can now receive an HTTP POST webhook with the results of the published text (or text and audio URI) from a prompted alert.\n",
            "- Updated the Deepseek chat and coder models to support a 128k token context window.\n",
            "- Added customSummary property to Content object, which returns the custom summary generated via preparation workflow.\n",
            "- Added keywords summarization type, which is now stored in keywords property in Content object.\n",
            "- Added slackChannels query, which returns the list of Slack channels from the workspace authenticated by the Slack bot token.\n",
            "- ⚡ We have changed the response from the credits query to return a single ProjectCredits object, rather than the list of correlated objects previously returned. The credits response now covers all credit usage over the time period specified.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2874: Processing entities is taking longer than 30min for 300+ page PDF\n",
            "- GPLA-2875: Messages in queue expiring too early\n",
            "- GPLA-2881: Feed read count increasing, after hitting read limit\n",
            "- GPLA-2884: Need to handle Anthropic 'overloaded' API response\n",
            "- GPLA-2906: JIRA issue identifier not assigned to issue metadata\n",
            "\n",
            "PreviousJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "NextJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [47c53407-b4d6-48e4-b91d-b9677a1028de]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/47c53407-b4d6-48e4-b91d-b9677a1028de/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "☀️\tJuly 2024\n",
            "\n",
            "# July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "### New Features\n",
            "\n",
            "- Added indexing workflow stage. This provides for configuration of indexing services, which may infer metadata from the content.\n",
            "- Added AZURE_AI_LANGUAGE content indexing service, which supports inferring the language of extracted text or transcript.\n",
            "- Added support for language content metadata. This returns a list of languages in ISO 639-1 format, which may have been inferred from the extracted text or transcript.\n",
            "- Added support for MODEL_IMAGE extraction service. This provides integration with vision models beyond those provided by OpenAI. You can assign a custom specification and bring-your-own API key for image extraction models.\n",
            "- ⚡ We have deprecated the OPENAI_IMAGE service type, and developers should now use the LLM image service instead.\n",
            "- ⚡ We have removed the language field from AudioMetadata type, which has been replaced by the new LanguageMetadata type.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2987: Extracting text with Azure Doc Intelligence does not extract hyperlinks\n",
            "\n",
            "PreviousAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "NextJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [a8a2bbe9-1c58-42a7-b31c-dd4b27b74956]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/a8a2bbe9-1c58-42a7-b31c-dd4b27b74956/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "☀️\tJuly 2024\n",
            "\n",
            "# July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the OpenAI GPT-4o Mini model, with 16k output tokens.\n",
            "- 💡 Graphlit now supports 'bring-your-own-key' for Azure AI Document Intelligence models. We have added a custom endpoint and key property, which can be assigned to use your own Azure AI resource.\n",
            "- Updated to use Jina reranker v2 (jina-reranker-v2-base-multilingual) by default.\n",
            "- Updated to assign the summary, bullets, etc properties when calling summarizeContents mutation. Now when summarizing contents, we will store the resulting summary in the content itself, in addition to returning the summarized results.\n",
            "- Added relevance property to all entity types, which will be assigned when searching for these entities. Entity results will be sorted (descending) by this search relevance score.\n",
            "- Added the ability to manually update summary, bullets, etc. properties when calling the updateContent mutation.\n",
            "- Added offset property to AtlassianJiraFeedProperties, so the timezone offset can be properly assigned for paging of the Jira feed. (Defaults to zero offset, i.e. UTC.) Jira does not store dates in UTC format, and the timestamps are based on the server timezone of the hosted Jira instance. By assigning the timezone offset with the Jira feed, we can reliably page the updated date timestamps from the Jira API.\n",
            "- ⚡ We have changed the content similarity search behavior to find similar content by summary, rather than text of the document, when a summary has been previously generated. For long documents, this will provide a more accurate similarity, rather than comparing on the first few pages of text in a document.\n",
            "- ⚡ We have changed the behavior of assigning offset in the entity filter objects for paging through entities. If using vector or hybrid search, this offset will be ignored (i.e. zero offset). Paging will not be supported through vector or hybrid search results. For keyword search, the offset will continue to be used, along with the limit property, to provide paging through the search results. We have made this change because we have found that index-based paging is not reliable with our vector/hybrid search approach. We are investigating ways to support this reliably with vector/hybrid search in the future.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2915: Add retry on OpenAI API HTTP 524 error (gateway timeout).\n",
            "- GPLA-2908: Not paging through Jira feed correctly.\n",
            "- GPLA-2917: Search by similar content is not giving expected results on long documents.\n",
            "- GPLA-2244: Keyword search not finding text in latter part of long PDF.\n",
            "\n",
            "PreviousJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "NextJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [3109dca5-a705-4191-a868-ab2b2485d8ca]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/3109dca5-a705-4191-a868-ab2b2485d8ca/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "☀️\tJuly 2024\n",
            "\n",
            "# July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the Mistral Large 2 and Mistral Nemo models. The existing MISTRAL_LARGE model enum now will use Mistral Large 2.\n",
            "- 💡 Graphlit now supports the Llama 3.1 8b, 70b and 405b models on Groq. (Note, these are rate-limited according to Groq's platform constraints.)\n",
            "- Added support for revision strategy on data extraction specifications. Now you can prompt the LLM to revise its previous data extraction response, similar to the existing completion revision strategy.\n",
            "- Added version property for AzureDocumentPreparationProperties type for assigning the API version used by Azure AI Document Intelligence. By default, Graphlit will continue to use the v4.0 (Preview) API version, but you can override this to assign version to V2023_10_31 to use the v3.1 (GA) API version instead. For some documents, the General Availability (GA) version of the API can provide better results.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2988: Not extracting hyperlinks from Office documents.\n",
            "\n",
            "PreviousJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "NextJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "Last updated5 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f20064c8-0d29-4243-b163-d94ebb7913d3]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/f20064c8-0d29-4243-b163-d94ebb7913d3/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎇\tJuly 2023\n",
            "\n",
            "# July 15: Support for SharePoint feeds, new Conversation features\n",
            "### New Features\n",
            "\n",
            "- 💡 Added support for SharePoint feeds: now can create feed to ingest files from SharePoint document library (and optionally, folder within document library)\n",
            "- 💡 Added support for PII detection during entity extraction from text documents and audio transcripts: now we will create labels such as PII: Social Security Number automatically when PII is detected\n",
            "- 💡 Added support for developer's own OpenAI API keys and Azure OpenAI deployments in Specifications\n",
            "- ℹ️ Changed semantics of deleteFeed to delete the contents ingested by the feed; since contents are linked to feeds, now feeds can be disabled, while keeping the lineage to the feed, and if feeds are deleted, they will delete the linked contents, so we never lose the feed-to-content lineage\n",
            "- Added GraphQL query for SharePoint consent URI, for registered Graphlit Platform Azure AD application\n",
            "- Better handling of web sitemap indexes: now if a sitemap.xml contains a sitemapindex element, we will load all linked sitemaps for evaluating web pages to ingest from Web feed\n",
            "- Added new GraphQL mutations for openConversation, closeConversation and undoConversation\n",
            "- Added timestamps to Conversation messages\n",
            "- Added new GraphQL mutations for openCollection and closeCollection\n",
            "- Added more configuration for content search: now can specify searchType (KEYWORD, VECTOR, HYBRID) and queryType (SIMPLE, FULL - aka Lucene syntax)\n",
            "- Better parsing of iTunes podcast metadata\n",
            "- ⚡ Renamed listingLimit field on feeds to readLimit\n",
            "- ⚡ Renamed topK to numberSimilar for content vector search type\n",
            "- ⚡ Changed GraphQL feed properties: split out azure into azureBlob and azureFile properties\n",
            "- ⚡ Changed GraphQL specification properties: split out openAI into openAI and azureOpenAI properties\n",
            "- ⚡ Removed count fields on query results, and replaced with explicit count{Entity} queries, which support search and filtering.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1043: Reddit readLimit not taking effect: now the specified limit of Reddit posts will be leveraged for Reddit feeds\n",
            "- GPLA-1064: Performance on entity extraction and observation creation for large PDFs was under expectations: now able to build knowledge graph from large PDFs much faster (4x speed improvement)\n",
            "- GPLA-1053: If rendition generation errored during content workflow, the content was not properly marked as errored\n",
            "- GPLA-1102: Large Web sitemaps were slow to load; rewrote sitemap index handling, and now can process sitemaps with 150K+ entries in seconds.\n",
            "\n",
            "PreviousAugust 3: New data model for Observations, new Category entity\n",
            "Last updated6 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [1149d374-504c-443f-8fb3-ca0a475776dc]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/1149d374-504c-443f-8fb3-ca0a475776dc/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎆\tJanuary 2024\n",
            "\n",
            "# January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Google and Microsoft email feeds. Email feeds can be created to ingest past emails, or poll for new emails. Emails create an EMAIL content type. Attachment files can optionally be extracted from emails, and will be linked to their parent email content. If assigning a workflow to the feed, the workflow will be applied both to the email content and the extracted attachment files.\n",
            "- 💡 Graphlit now supports reingesting content in-place. The ingestText, ingestPage and ingestFile mutations now take an optional id parameter for an existing content object. If this id is provided, the existing content will be updated from the provided text or URI source, and will restart the assigned workflow.\n",
            "- Added restartAllContents mutation to restart workflow on all partially-ingested contents in project.\n",
            "- Added text field to ConversationCitation type, which returns the relevant text from the content source with the citation.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1313: Not extracting links from HTML\n",
            "- GPLA-2030: No text extracted from shapes in PPTX files\n",
            "\n",
            "PreviousFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "NextJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [42df7e18-5180-4ee2-99f5-72f3292c9872]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/42df7e18-5180-4ee2-99f5-72f3292c9872/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎆\tJanuary 2024\n",
            "\n",
            "# January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports content publishing, where documents, audio transcripts and even image descriptions, can be summarized, and repurposed into blog posts, emails or AI-generated podcasts. With the new publishContents mutation, you can configure LLM prompts for summarization and publishing, and assign specifications to use different models and/or system prompts for each step in the process. The published content will be reingested into Graphlit, and can be searched or used for conversations, like any other form of content.\n",
            "- 💡 Graphlit now supports publishing conversations as content with the new publishConversation mutation. You can generate text or audio transcripts of your conversations, to be reused in other tools.\n",
            "- 💡 Graphlit now supports bulk summarization of contents with the summarizeContents mutation. You can filter a set of content, by feed, by observable or by similar text, and run a set of summarizations across each content in parallel.\n",
            "- 💡 Graphlit now supports LLM entity extraction, with the new MODEL_TEXT entity extraction service type. Similar to using Azure Cognitive Service Text Analytics, you can use any OpenAI or Anthropic model for extracting entities from text. Internally the LLM returns JSON-LD entities, which we convert into Person, Organization, Place, etc. entities and assign observations to the extracted content.\n",
            "- 💡 Graphlit now supports LLM tools (aka function calls) with OpenAI models. You can define the tools to be used with the LLM in the specification object. With the new extractContents mutation, you can execute a prompt against content using a specification with tools defined. The mutation will return the JSON arguments assigned by the LLM.\n",
            "- 💡 Graphlit now supports callback webhooks for LLM tools. If you assign a URI in the ToolDefinition object, Graphlit will call your webhook the tool name and JSON arguments. When you respond to the webhook with JSON, we will add that response to the LLM messages, and ask the LLM to complete the original prompt.\n",
            "- 💡 Graphlit now supports the selection of the Deepgram model (such as Meeting, Phonecall or Finance) with the preparation workflow. Also, you can assign your own Deepgram API key, which will be used for audio transcription using that workflow.\n",
            "- Added support for CLIP image embeddings using Roboflow, which can be used for similar image search. If you search for contents by similar contents, we will now use the content's text and/or image embeddings to find similar content.\n",
            "- Added support for dynamic web page ingestion. Graphlit now navigates to and automatically scrolls web pages using Browserless.io, so we capture the fully rendered HTML before extracting text. Also, we now support web page screenshots, if enabled with enableImageAnalysis property in preparation workflow. These screenshots can be analyzed with multimodal modals, such as GPT-4 Vision, or can be used to create image embeddings for similar image search.\n",
            "- Added table parsing when preparing documents. We now store structured (tab-delimited) text in the JSON text mezzanine which is extracted from documents in the preparation workflow.\n",
            "- Added reverse geocoding of lat/long locations found in image or other content metadata. We now store the real-world address with the content metadata, for use in conversations.\n",
            "- Added assistant messages to the conversation message history provided to the LLM. Originally we had included only user messages, but now we are formatting both user and assistant messages into the LLM prompt for conversations.\n",
            "- Added new chunking algorithm for text embeddings. We support semantic chunking at the page or transcript segment level, and now will create embeddings from smaller sized text chunks per page or segment.\n",
            "- Added content metadata to text and image embeddings. To provide better context for the text embeddings, we now include formatted content metadata, which includes fields like title, subject, author, or description. For emails, we include to, from, cc, and bcc fields.\n",
            "- Added helper mutations isContentDone and isFeedDone which can be used for polling completion of ingested content, or all content ingested by a feed.\n",
            "- Added richer image descriptions generated by the GPT-4 Vision model. Now these provide more useful detail.\n",
            "- Added validation of extracted hyperlinks. Now we test the URIs and remove any inaccessible links during content enrichment.\n",
            "- Added deleteContents, deleteFeeds, and deleteConversations mutations for multi-deletion of contents, feeds or conversations.\n",
            "- Added deleteAllContents, deleteAllFeeds, and deleteAllConversations mutations for bulk, filtered deletion of entities. You can delete all your contents, feeds, or conversations in your project, or a filtered subset of those entities.\n",
            "- ℹ️ Starter tier now has a higher content limit of 100K content items.\n",
            "- ⚡ In the OpenAIImageExtractionProperties type, the detailMode field was renamed to detailLevel.\n",
            "- ⚡ Each SummarizationStrategy object now accepts the specification which is used by the summarization, rather than being assigned at the preparation workflow stage.\n",
            "- ⚡ addCollectionContents and removeCollectionContents mutations have been deprecated in favor of addContentsToCollections and removeContentsFromCollection mutations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1846: Parse Markdown headings into mezzanine JSON\n",
            "- GPLA-1779: Not returning SAS token with mezzanine, master URIs\n",
            "- GPLA-1348: Summarize text content, not just file content\n",
            "- GPLA-1297: Not assigning content error message on preparation workflow failure\n",
            "\n",
            "PreviousJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "NextDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "\n",
            "---\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [72415f65-64c2-4aca-87e0-93aa4f58cea6]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/72415f65-64c2-4aca-87e0-93aa4f58cea6/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🌧️\tFebruary 2024\n",
            "\n",
            "# February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports OneDrive and Google Drive feeds. Files can be ingested from OneDrive or Google Drive, including shared drives where the authenticated user has access. Both OneDrive and Google Drive support the reading of existing files, and tracking new files added to storage with recurrent feeds.\n",
            "- 💡 Graphlit now supports email backup files, such as EML or MSG, which will be assigned the EMAIL file type. During email file preparation, we will automatically extract and ingest any file attachments.\n",
            "- 💡 Graphlit now automatically extracts embedded images in PDF files, ingests them as content objects, and links them as children of the parent PDF.\n",
            "- 💡 Graphlit now supports recursive Notion feeds. When the isRecursive flag is true in the Notion feed properties, we will crawl child pages and databases, and recursively ingest them in addition to the specified pages and databases.\n",
            "- Added support for assigning collections to content ingested with the ingestPage, ingestFile or ingestText mutations. This saves a step where the content will automatically be added to the collection(s) without requiring another mutation call.\n",
            "- Added support for the CODE file type for a wide variety of source code formats, i.e. Python .py, Javascript .js. Code files use optimized text splitting for enhanced search and retrieval.\n",
            "- Added support for customGuidance in Specification object, which can be used for injecting a guidance prompt during the RAG process. For example, you can instruct the LLM to return a default response string if no content sources are found via semantic search.\n",
            "- Added tenants field to Project object, which returns a list of all tenant IDs which have been used to create an entity in Graphlit.\n",
            "- Added email metadata, separate from document metadata. Now emails will contain indexed metadata such as to, from, or subject.\n",
            "- ⚡ The contents field for content objects has been replaced with children and parent fields. For example, when a ZIP file is unpacked, the unpacked files will be added as children of the ZIP file, and the ZIP file will be the parent of each of the unpacked files.\n",
            "- ⚡ Removed enableImageAnalysis field from image preparation properties in workflow object. Now is enabled by default.\n",
            "- ⚡ Moved disableSmartCapture field to preparation workflow stage from page preparation properties. This is used to disable the use of headless Chrome browser to capture HTML from web pages. It is enabled by default, and if disabled, Graphlit will simply download the HTML from the web page rather than rendering on headless Chrome browser.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2099: Failed to ingest ArXiV PDF. Fixed PDF parsing error.\n",
            "- GPLA-2174: LLM response is incorrect with conversation history, but no content sources.\n",
            "- GPLA-2199: ZIP package left in Indexed state after content workflow.\n",
            "\n",
            "PreviousMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "NextFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "Last updated9 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [f53855bb-439c-48c9-a88d-f427da2962d1]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/f53855bb-439c-48c9-a88d-f427da2962d1/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🌧️\tFebruary 2024\n",
            "\n",
            "# February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Semantic Alerts, which allows for LLM summarization and publishing of content, on a periodic basis. This is useful for generating daily reports from email, Slack or other time-based feeds. Alerts support the same publishing options, i.e. audio and text, as the publishContents mutation.\n",
            "- 💡 Graphlit now supports the latest OpenAI 0125 model versions, for GPT-4 and GPT-3.5 Turbo. We will add support for Azure OpenAI when Microsoft releases support for these.\n",
            "- Slack feeds now support a listing type field, where you can specify if you want PAST or NEW Slack messages in the feed.\n",
            "- 🔥 This release provides many performance enhancements, which will speed up the content workflows for ingested content.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2114: Collections not being added to text embedding index documents.\n",
            "- GPLA-2063: Not handling hallucinated citations.\n",
            "- GPLA-1916: Collections not inherited from project-scope into tenant-scope.\n",
            "- GPLA-2105: Should error on add/remove of contents to/from collections if content does not exist.\n",
            "\n",
            "PreviousFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "NextJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "Last updated10 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [c5dc4da1-aa86-4d89-93f2-d15ec179a24d]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/c5dc4da1-aa86-4d89-93f2-d15ec179a24d/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎄\tDecember 2024\n",
            "\n",
            "# December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports mapping a website with the mapWebmutation. You can provide a URL to a website, and the query will return a list of URLs based on the sitemap.xml (or sitemap-index.xml) file, at or underneath the provided URL.\n",
            "- 💡 Graphlit now supports the generation of web page screenshots with the screenshotPagemutation. By providing the URL of a web page, and optionally, the maximum desired height of the screenshot, we will screenshot the webpage and ingest it automatically as content. You can provide an optional workflow, which will be applied to the ingested image content, for operations like generating image descriptions with a vision LLM.\n",
            "- 💡 Graphlit now supports the direct summarization of text with the summarizeTextmutation. By providing the desired summarization strategy, we will summarize the text (i.e. bullet points, social media posts) and return the summarization.\n",
            "- 💡 Graphlit now supports the direct extraction of text with the extractTextmutation. By providing the LLM tool definitions and an optional LLM specification, we will prompt the desired LLM (or OpenAI GPT-4o, by default) to invoke the provided tools, and return the JSON responses from the LLM tool calling.\n",
            "- Graphlit now supports the latest Groq Llama 3.3 model, with the model enum LLAMA_3_3_70B.\n",
            "- We have updated Cohere reranking to use the latest Cohere rerank-v3.5model by default.\n",
            "- ⚡ We have added a new flattenCitationsfield to the ConversationStrategyInputtype. By assigning this field to True, when calling promptConversation,we will combine multiple citations from the same content into a single citation.\n",
            "- ⚡ For Microsoft email, Microsoft Teams and OneDrive feeds, we have added the clientIdand clientSecretfields as required feed properties. These properties must be assigned, in addition to the refreshTokenfield for proper authentication to the Microsoft Graph API used by these feeds. (Colab Notebook Example)\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3492: Not finding sitemap at parent web path\n",
            "- GPLA-3500: Failed to handle mismatch of Deepgram model/language\n",
            "\n",
            "PreviousDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "NextDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "Last updated22 days ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [979a18c8-361a-4786-ac5e-355c7652e5ba]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/979a18c8-361a-4786-ac5e-355c7652e5ba/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎄\tDecember 2024\n",
            "\n",
            "# December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Dropbox feeds for ingesting files on the Dropbox cloud service. Dropbox feeds require your appKey, appSecret, redirectUriand refreshTokento be assigned. The feed also accepts an optional pathparameter to read files from a specific Dropbox folder.\n",
            "- 💡 Graphlit now supports Box feeds for ingesting files on the Box cloud service. Box feeds require your clientId, clientSecret, redirectUriand refreshTokento be assigned.\n",
            "- 💡 Graphlit now supports Intercom feeds for ingesting Intercom Articles and Tickets. We will ingest Intercom Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Intercom feeds require the accessTokenproperty to be assigned.\n",
            "- 💡 Graphlit now supports Zendesk feeds for ingesting Zendesk Articles and Tickets. We will ingest Zendesk Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Zendesk feeds require the accessTokenproperty and your Zendesk subdomain to be assigned.\n",
            "- Graphlit now supports the latest OpenAI o1 model, with the model enums O1_200kand O1_200k_20241217.\n",
            "- Graphlit now supports the latest Gemini Flash 2.0 Experimental model, with the model enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "- Graphlit now supports the latest Cohere R7B model, with the model enum COMMAND_R7B_202412.\n",
            "- Graphlit now supports returning the low-level details from prompting RAG conversations, by adding the includeDetailsparameter and setting to True. This includes details on the number of sources, the exact list of messages provided to the LLM, and more.\n",
            "- We have added support for filtering of observables, such as Person or Organization, by URI property.\n",
            "- We have added the ability to bypass semantic search in content retrieval with conversations. You can assign NONEfor the conversation search type, and it will ignore the user prompt when retrieving content. It will inject all contents resulting from the content filter into the RAG prompt context.\n",
            "- We have added a new createdInLastproperty to all entity filters, which allows easier filtering of entities created within a recent time period. Also, we have added a new inLastproperty to the content filter, which allows easier filtering of content authored within a recent time period. For example, find all images taken in the last 3 days, or find me all emails I received yesterday.\n",
            "- We have added support for the latest Azure AI Document Intelligence models, with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "- We have added support for Google Drive and OneDrive feeds to ingest specific files by providing a list of file identifiers (files), in addition to the folder identifier (folderId). If files identifiers are provided, they take precedence over the folder identifier.\n",
            "- ⚡ For projects upgraded to the Starter Tier after Dec 9, 2024, we have removed the content items limit. Now you can store an unlimited number of content items (i.e. files, web pages, Slack messages) on the Starter or Growth Tiers. If you have an existing project on the Starter Tier, please reach out and we will manually remove that content item limit on the project.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3529: Can't assign collection to multitenant content\n",
            "- GPLA-3579: Should decode HTML characters when parsing HTML email\n",
            "- GPLA-3576: Ingesting content in-place doesn't handle isSynchronous properly\n",
            "- GPLA-3457: IsFeedDone doesn't return True for finished feed with no contents\n",
            "- GPLA-3572: Not handling HTTP 400 error on uploading from URI\n",
            "\n",
            "PreviousDecember 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes\n",
            "NextDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "Last updated7 days ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [931a2349-8288-4a5f-846c-9f1476cd0946]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/931a2349-8288-4a5f-846c-9f1476cd0946/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎄\tDecember 2023\n",
            "\n",
            "# December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the OpenAI GPT-4 Turbo 128k model, both in Azure OpenAI and native OpenAI services. Added new model enum GPT4_TURBO_VISION_128K.\n",
            "- 💡 Graphlit now supports Llama 2 7b, 13b, 70b models and Mistral 7b model, via Replicate. Developers can use their own Replicate API key, or be charged as credits for Graphlit usage.\n",
            "- 💡 Graphlit now supports the Anthropic Claude 2.1 model. Added new model enum CLAUDE_2_1.\n",
            "- 💡 Graphlit now supports the OpenAI GPT-4 Vision model for image descriptions and text extraction. Added new model enum GPT4_TURBO_VISION_128K. See usage example in \"Multimodal RAG\" blog post.\n",
            "- Added query by example to contents query. Developers can specify one or more example contents, and query will use vector embeddings to return similar contents.\n",
            "- Added query by example to conversations query. Developers can specify one or more example conversations, and query will use vector embeddings to return similar conversations.\n",
            "- Added vector search support for conversations queries. Developers can provide search text which will use vector embeddings to return similar conversations.\n",
            "- Added promptSpecifications mutation for directly prompting multiple models. This can be used to evaluate prompts against multiple models or compare different specification parameters in parallel.\n",
            "- Added promptStrategy field to Specification, which supports multiple strategy types for preprocessing the prompt before being sent to the LLM model. For example, REWRITE prompt strategy will ask LLM to rewrite the incoming user prompt based on the previous conversation messages.\n",
            "- Added suggestConversation mutation, which returns a list of suggested followup questions based on the specified conversation and related contents. This can be used to auto-suggest questions for chatbot users.\n",
            "- Added new summarization types: CHAPTERS, QUESTIONS and POSTS. See usage examples in the \"LLMs for Podcasters\" blog post.\n",
            "- Added versioned model enums such as GPT4_0613 and GPT35_TURBO_16K_1106. Without version specified, such as GPT35_TURBO_16K, Graphlit will use the latest production model version, as defined by the LLM vendor.\n",
            "- Added lookupContents query to get multiple contents by id in one query.\n",
            "- ⚡ In Content type, headline field was renamed to headlines and now returns an array of strings.\n",
            "- ⚡ Entity names are now limited to 1024 characters. Names will be truncated if they exceed the maximum length.\n",
            "- ⚡ In SummarizationTypes enum, BULLET_POINTS was renamed to BULLETS.\n",
            "- ⚡ In ProjectStorage type, originalTotalSize was renamed to totalSize, and totalRenditionSize field was added. totalSize is the sum of the ingested source file sizes, and totalRenditionSize is the sum of the source file sizes and any derived rendition sizes.\n",
            "- ⚡ In ConversationStrategy type, strategyType was renamed to type for consistency with rest of data model.\n",
            "- ⚡ In Specification type, optimizeSearchConversation was removed, and now is handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1725: Should ignore RSS.xml from web feed sitemap\n",
            "- GPLA-1726: GPT-3.5 Turbo 16k LLM is adding \"Citation #\" to response\n",
            "- GPLA-1698: Workflow not applied to link-crawled content\n",
            "- GPLA-1692: Mismatched project storage total size, when some content has errored\n",
            "- GPLA-1237: Add relevance threshold for semantic search\n",
            "\n",
            "PreviousJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "NextOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "Last updated11 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [3ed53e18-c1a5-41f7-9a6e-ef900e439b95]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/3ed53e18-c1a5-41f7-9a6e-ef900e439b95/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎂\tAugust 2024\n",
            "\n",
            "# August 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports LLM-based document preparation, using vision-capable models such as OpenAI GPT-4o and Anthropic Sonnet 3.5. This is available via the MODEL_DOCUMENT preparation service type, and you can assign a customspecification object and bring your own LLM keys.\n",
            "- 💡 Graphlit now provides an open source .NET SDK, supporting .NET 6 and .NET 8 (and above). SDK package can be found on Nuget.org. Code samples can be found on GitHub.\n",
            "- Added identifier property to Content object for mapping content to external database identifiers. This is supported for content filtering as well.\n",
            "- Added support for Claude 3 vision models for image-based entity extraction, using the MODEL_IMAGE entity extraction service.\n",
            "- Added context augmentation to conversations, via the augmentedFilter property on the Conversation object. Any content which matches this augmented filter will be injected into the LLM prompt content, without needing to be related by vector similarity to the user prompt. This is useful for specifying domain knowledge which should always be referenced by the RAG pipeline.\n",
            "- Added support for the latest snapshot of OpenAI GPT-4o, with the model enum GPT4O_128K_20240806.\n",
            "- Added reranking of related entities, when preparing the LLM prompt context for GraphRAG. If reranking is enabled, the metadata from the related entities will be reranked with the same reranker assigned to the conversation specification.\n",
            "- ⚡ We have changed the type of the duration field in the AudioMetadata and VideoMetadata types to be TimeSpan rather than string, as to be more consistent with the rest of the API data model.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2884: Support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "\n",
            "PreviousAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "NextJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "Last updated4 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [8c9d843f-4a50-4c40-aab7-9dbeb3472396]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/8c9d843f-4a50-4c40-aab7-9dbeb3472396/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎄\tDecember 2024\n",
            "\n",
            "# December 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports formatting of LLM-ready prompts with our RAG pipeline, via the new formatConversation and completeConversation mutations. This is valuable for supporting LLM streaming by directly calling the LLM from your application, and using Graphlit for RAG retrieval and conversation history. (Colab Notebook Example)\n",
            "- We have added support for inline hyperlinks in extracted text from documents and web pages.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3466: Owner ID should accept any non-whitespace string\n",
            "- GPLA-3458: Not getting Person-to-Organization edges from entity extraction\n",
            "\n",
            "PreviousDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "NextNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "Last updated1 month ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [d40294bc-55cb-4f3b-ade6-42fab2f1d379]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/d40294bc-55cb-4f3b-ade6-42fab2f1d379/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎂\tAugust 2024\n",
            "\n",
            "# August 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports the extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "- 💡 Graphlit now supports medical-related entities in GraphRAG, and via API for queries and mutations.\n",
            "- Added support for Anthropic prompt caching. When using Anthropic Sonnet 3.5 or Haiku 3, Anthropic will now cache the entity extraction and LLM document preparation system prompts, which saves on token cost and increases performance.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3104: Should default search type to VECTOR, when performing entity similarity filter.\n",
            "- GPLA-3112: Empty PDF fails entity extraction.\n",
            "\n",
            "PreviousSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "NextAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "Last updated4 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [ce9f8f91-d948-421a-8cda-3959e0f9c5f6]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/ce9f8f91-d948-421a-8cda-3959e0f9c5f6/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎂\tAugust 2024\n",
            "\n",
            "# August 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "### New Features\n",
            "\n",
            "- Added support for language-aware summaries when using LLM-based document extraction. Now the summaries for tables and sections generated by the LLM will follow the language of the source text.\n",
            "- Added support for language-aware entity descriptions with using LLM-based entity extraction. Now the entity descriptions generated by the LLM will follow the language of the source text.\n",
            "- ⚡ We have changed the default document preparation method to use Azure AI Document Intelligence, rather than our built-in document parsers. We have found that the fidelity of Azure AI is considerably better for complex PDFs, and provides better support for table extraction, so we have made this the default. Note: this does come with increased credit usage per-page, for PDF, DOCX and PPTX documents, but the quality of the extracted documents are noticeably higher for use in RAG pipelines.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3070: Not getting slide count assigned to metadata for PPTX files.\n",
            "\n",
            "PreviousAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "NextAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "Last updated4 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [523f000e-30f7-44a0-8c75-383b18aeed8f]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/523f000e-30f7-44a0-8c75-383b18aeed8f/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎂\tAugust 2023\n",
            "\n",
            "# August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "### New Features\n",
            "\n",
            "- 💡 Added ingestText mutation which supports direct Content ingestion of plain text, Markdown and HTML. Now, if you have pre-scraped HTML or Markdown text, you can ingest it into Graphlit without reading from a URL.\n",
            "- 💡 Added Specification strategy property, which allows customization of the LLM context when prompting a conversation. ConversationStrategy now provides Windowed and Summarized message histories, as well as configuration of the weight between existing conversation messages and Content text pages (or audio transcript segments) in the LLM context.\n",
            "- 💡 Added auto-summarization of extracted text and audio transcripts. There is a new Content summary property where a list of summary bullet points can be found. These summaries can be optionally included in the Conversation prompt context for more accurate LLM responses.\n",
            "- ℹ️ Added AzureOpenAIModels and OpenAIModels types to Specification model properties to make it easier to specify the desired LLM.\n",
            "- ℹ️ Renamed ConversationMessage date property to timestamp\n",
            "- ✨ Refined the internal LLM prompts for providing content as part of Conversation context. This provides for much clearer and accurate results from the LLM.\n",
            "\n",
            "PreviousAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "NextAugust 3: New data model for Observations, new Category entity\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [da28adff-3f2c-4062-b78c-3581c2762768]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/da28adff-3f2c-4062-b78c-3581c2762768/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎂\tAugust 2023\n",
            "\n",
            "# August 3: New data model for Observations, new Category entity\n",
            "### New Features\n",
            "\n",
            "- 💡 Revised data model for Observations, Occurrences and observables (i.e. Person, Organization). Now after entity extraction, content will have one Observation for each observed entity, and a list of occurrences. Occurrence now supports text, time and image occurrence types. (Text: page index, time: start/end timestamp, image: bounding box) Observations now have ObservableType and Observable fields, which specify the observed entity type and entity reference.\n",
            "- 💡 Added Category entity to GraphQL data model, which supports PII categories such as Phone Number or Credit Card Number.\n",
            "- Added probability field to model properties, for the LLM's token probability. (See OpenAI documentation for more detail.)\n",
            "- Added error field to feeds. If a feed fails to read from the data source, and is marked as ERRORED state, the error field will have the error description.\n",
            "- Support reingestion of changed files from feeds. For feeds, such as SharePoint or Web, where we can recognize that a file or page was updated, we will now reingest the content in-place. Content will keep the same ID, and will restart the content workflow by re-downloading the updated content from the data source. Existing observations will be deleted, and new observations will be created from the updated content.\n",
            "- ℹ️ Ingestion of content is now idempotent, meaning if you ingest content again from the same URI, we will reingest the content in-place, while keeping the same ID. (If we can recognize the content has not changed, such as by ETag, we will return the existing content object.)\n",
            "- ℹ️ Changed GraphQL data type of SharePoint tenantId, libraryId and siteId to ID rather than String.\n",
            "- ✨ Performance optimization of entity extraction, and the creation of observations.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-1130: Only was extracting text from first column of PDF tables.\n",
            "- GPLA-1140: Text from DOCX tables was not extracted properly.\n",
            "- GPLA-1154: Audio content ingested from RSS feed was not deleted when feed was deleted.\n",
            "\n",
            "PreviousAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "NextJuly 15: Support for SharePoint feeds, new Conversation features\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [fbfc6199-ba6c-4e4d-a050-4bc72d7faeaf]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/fbfc6199-ba6c-4e4d-a050-4bc72d7faeaf/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎂\tAugust 2023\n",
            "\n",
            "# August 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "### New Features\n",
            "\n",
            "- ℹ️ Behind the scenes, Graphlit is preparing to launch usage-based billing. This release put in place the infrastructure to track billable events. Organizations now have a Stripe customer associated with them, and Graphlit projects are auto-subscribed to a Free/Hobby pricing plan. In a future release, we will provide the ability to upgrade to a paid plan in the Graphlit Developer Portal. Also, we will provide visualization of usage, on granular basis, in the Portal.\n",
            "- 💡 Content URIs now have Shared Access Signature (SAS) token appended, so they are accessible after query. For example, content.transcriptUri will now be able to be downloaded or used directly in an application (until the SAS token expires).\n",
            "- 🧱 Added more robustness for error handling and retries, especially for LLM APIs and audio transcription APIs.\n",
            "\n",
            "PreviousSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "NextAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "Last updated1 year ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [79e64fc7-44b1-47b3-9b24-c8aa55dd502a]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/79e64fc7-44b1-47b3-9b24-c8aa55dd502a/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🐇\tApril 2024\n",
            "\n",
            "# April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports Discord feeds. By connecting to a Discord channel and providing a bot token, you can ingest all Discord messages and file attachments.\n",
            "- 💡 Graphlit now supports Cohere reranking after content retrieval in RAG pipeline. You can optionally use the Cohere rerank model to semantically rerank the semantic search results, before providing as context to the LLM.\n",
            "- Added support for section-aware text chunking and retrieval. Now, when using section-aware document preparation, such as Azure AI Document Intelligence, Graphlit will store the extracted text according to the semantic chunks (i.e. sections). The text for each section will be individually chunked and embedded into the vector index.\n",
            "- Added support for retrievalStrategy in Specification type. Graphlit now supports CHUNK, SECTION and CONTENT retrieval strategies. Chunk retrieval will use the search hit chunk, section retrieval will expand the search hit chunk to the containing section (or page, if not using section-aware preparation). Content retrieval will expand the search hit chunk to the text of the entire document.\n",
            "- Added support for rerankingStrategy in Specification type. You can now configure the reranking of content sources, using the Cohere reranking model, by assigning serviceType to COHERE. More reranking models are planned for the future.\n",
            "- Added isSynchronous flag to content ingestion mutations, such as ingestUri, so the mutation will wait for the content to complete the ingestion workflow (or error) before returning. This is useful for utilizing the API in a Jupyter notebook or Streamlit application, in a synchronous manner without polling.\n",
            "- Added includeAttachments flag to SlackFeedProperties. When enabled, Graphlit will automatically ingest any attachments within Slack messages.\n",
            "- ⚡ Added ingestUri mutation to replace the now deprecated ingestPage and ingestFile mutations. We had seen confusion on when to use one vs the other, and now for any URI, whether it is a web page or hosted PDF, you can pass it to ingestUri, and we will infer the correct content ingestion workflow.\n",
            "- ⚡ Removed includeSummaries from the ConversationStrategyInput type. This will re-added in the future as part of the retrieval strategy.\n",
            "- ⚡ Deprecated enableExpandedRetrieval in ConversationStrategyInput type. This is now handled by setting strategyType to SECTION or CONTENT in the RetrievalStrategyInput type.\n",
            "- ⚡ Moved contentLimit from ConversationStrategyInput type to RetrievalStrategyInput type. You can optionally assign the contentLimit to retrievalStrategy which limits the number of content sources leveraged in the LLM prompt context. (Default is 100.)\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2469: Failed to ingest PDF hosted on GitHub\n",
            "- GPLA-2390: Claude 3 Haiku not adhering to JSON schema\n",
            "- GPLA-2474: Prompt rewriting should ignore formatting instructions in prompt\n",
            "- GPLA-2462: Missing line break after table rows\n",
            "- GPLA-2417: Not extracting images from PPTX correctly\n",
            "\n",
            "PreviousApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "NextMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [b47614fd-41f3-491b-bbb4-d3976687e131]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/b47614fd-41f3-491b-bbb4-d3976687e131/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🐇\tApril 2024\n",
            "\n",
            "# April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports a native Python SDK, using Pydantic types. The Python SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest PyPi package here. The Streamlit sample applications have been updated to use the new Python SDK.\n",
            "- 💡 Graphlit now supports a native Node.js SDK, using TypeScript types. The Node.js SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest NPM package here.\n",
            "- 💡 Graphlit now supports the 2024-04-09 models in the OpenAI model service. GPT4_TURBO-128K will give the latest OpenAI GPT-4 model, following this model list. We have added the GPT4_TURBO_128K_2024_04_09 enum to specify the new model.\n",
            "- 💡 Graphlit now supports LLaMA3 70b, LLaMA3 8b and Gemma 7b models in the Groq model service.\n",
            "- 💡 Graphlit now supports the Command R and Command-R+ models in the Cohere model service.\n",
            "- Added support for Jina reranking, using the JINA reranking model service type in the reranking retrieval strategy.\n",
            "- Updated the Cohere reranking model to use the latest v3.0 model.\n",
            "- Increased the reliability of parsing LLM responses, in cases where they don't follow the JSON schema.\n",
            "- ⚡ Cleaned up nullability of GraphQL parameters, so parameters better reflect if they are required or optional, or allow nulls.\n",
            "- ⚡ Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "- ⚡ Split out reranking model service type as RetrievalModelServiceTypes enum.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-2114: Adding content to collections not syncing search index\n",
            "- GPLA-2511: Failing to render any conversation sources with section retrieval and text content\n",
            "\n",
            "PreviousMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "NextApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "Last updated8 months ago \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Ingested content [c51c1b7e-b854-44a8-90c1-bc2d23043aa1]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Mezzanine: https://graphlit202412121b7b448d.blob.core.windows.net/files/c51c1b7e-b854-44a8-90c1-bc2d23043aa1/Mezzanine/page.json?sv=2025-01-05&se=2025-01-02T02%3A17%3A29Z&sr=c&sp=rl&sig=wAzQNtbqsOm76jKcBv6%2BPDaPjNcMe%2F5kXNzCSc7z7R0%3D\n",
            "🎄\tDecember 2024\n",
            "\n",
            "# December 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes\n",
            "### New Features\n",
            "\n",
            "- 💡 Graphlit now supports LLM fallbacks which can help protect your application from model provider downtime. By assigning the fallbacksproperty when creating your conversation, you can provide an optional list of LLM specifications to be used (in order). These fallback specifications will only be used when we failed to prompt the conversation via the main specification. Caveat, the RAG pipeline will only use the strategies provided in the main specification for prompt rewriting, content retrieval, etc. Content is not re-retrieved upon fallback - the formatted LLM prompt will be tried against each fallback specification in succession until one succeeds. (Colab Notebook Example)\n",
            "- 💡 Graphlit now supports querying of all available models, through the new modelsquery in the API. This returns the model enum, model service type enum, description, and several other useful details about the models.\n",
            "- Graphlit now supports the ingestion of native Google Docs, Google Sheets and Google Slides documents from Google Drive feeds. These formats will be auto-exported to the corresponding Microsoft Office format (DOCX, XLSX, PPTX) prior to ingesting as content.\n",
            "- Graphlit now supports unblocking of websites, such as those using Cloudflare. You can set enableUnblockedCaptureto true on the PreparationWorkflowStageto enable unblocking - through our integration with Browserless.io headless browser service. This does incur an additional cost per page, compared to normal web page ingestion.\n",
            "- We have added support for assigning observations to contents ingested via feeds. By assigning observationsto the IngestionWorkflowStagein workflow object, you can assign Labels, Organizations, etc. without needing to use entity extraction.\n",
            "- We have added support for assigning observations when ingesting content via ingestUri, ingestText, etc. mutations. By passing observationsas a parameter, similar to `collections`, you can assign Labels, Organizations, etc. without needing to use entity extraction.\n",
            "- ⚡ We have changed the response type of the publishContentsmutation to return PublishContentstype. This new PublishContentstype wraps the published Contentobject, and includes the new Detailsproperty of PublishingDetailstype. We have added an includeDetailsparameter to publishContentsmutation, which will fill in the Details property with a list of intermediate content summaries and the published text, among other publishing metrics.\n",
            "- ⚡ We have changed the behavior of publishContentssuch that, if no content was retrieved for publishing, the mutation returns a null content object rather than returning an error.\n",
            "\n",
            "### Bugs Fixed\n",
            "\n",
            "- GPLA-3645: Table headers merged together on web scrape\n",
            "- GPLA-3634: Failed to extract pages from PDF with empty hyperlink text\n",
            "- GPLA-3633: Not handling empty observables properly for reranking\n",
            "\n",
            "NextDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "Last updated3 days ago \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, HTML\n",
        "\n",
        "# Prompt which gets run on each web page to summarize key points\n",
        "summary_prompt = \"\"\"\n",
        "You are an AI assistant that extracts the most important information from product changelog pages.\n",
        "\n",
        "You are being provided a changelog web page for one of many releases of the Graphlit Platform in 2024.\n",
        "\n",
        "Your task is to produce a concise summary that covers:\n",
        "\n",
        "New Features – Briefly list or describe each new capability.\n",
        "Enhancements/Improvements – Any notable improvements or changes.\n",
        "Bug Fixes – Summaries of what was fixed and why it matters.\n",
        "Other Key Details – Any version numbers, feature flags, or breaking changes.\n",
        "Dates - When a feature was released, include both the month and year\n",
        "Value - What this offers to developers.\n",
        "Keep it succinct, accurate, and organized. Use short sentences or bullet points so it’s easy to incorporate into a map/reduce pipeline. Omit any superfluous text.\n",
        "\n",
        "Output:\n",
        "A concise summary in bullet points highlighting the essential updates from the changelog.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt which gets run against all summaries (in map/reduce manner) to generate report\n",
        "publish_prompt = \"\"\"\n",
        "**Prompt:**\n",
        "You are an AI writing assistant. You have been provided a collection of **concise bullet-point summaries** representing each monthly changelog from our SaaS product throughout the past year. Using those summaries as source material, your task is to craft a **complete and cohesive “Year in Review” blog post** that covers all important updates from January through December.\n",
        "\n",
        "In your response, please:\n",
        "1. **Incorporate every month’s bullet points** in chronological order, highlighting the most impactful New Features, Enhancements, Bug Fixes, and any Other Key Details (such as release dates and version numbers).\n",
        "2. **Use a narrative style** that flows naturally from month to month.\n",
        "3. **Organize the content with clear headings** or subheadings for each month or release milestone.\n",
        "4. **Provide context** on why these updates were significant for our user base (e.g., how they improve developer workflow, enhance user experience, etc.).\n",
        "5. **Keep it professional but friendly**, ensuring it’s easy to read and maintains a positive, forward-looking tone.\n",
        "6. **Conclude with a forward-thinking note**, thanking readers for their support and hinting at what to expect in the coming year.\n",
        "\n",
        "Output:\n",
        "A **polished, blog-ready “Year in Review” article**, in paragraph form with headings, that consolidates all the monthly summaries into one comprehensive reflection on the product’s evolution.\n",
        "\n",
        "Don't wrap your response on markdown. No ``` tick marks.\n",
        "\"\"\"\n",
        "\n",
        "if feed_id is not None:\n",
        "    summary_specification_id = await create_specification(enums.OpenAIModels.GPT4O_MINI_128K)\n",
        "\n",
        "    if summary_specification_id is not None:\n",
        "        print(f'Created summary specification [{summary_specification_id}]:')\n",
        "\n",
        "        publish_specification_id = await create_specification(enums.OpenAIModels.O1_MINI_128K)\n",
        "\n",
        "        if publish_specification_id is not None:\n",
        "            print(f'Created publish specification [{publish_specification_id}]:')\n",
        "\n",
        "            display(Markdown(f'### Publishing Contents...'))\n",
        "\n",
        "            published_content_id, details = await publish_contents(feed_id, summary_specification_id, publish_specification_id, summary_prompt, publish_prompt, publish_correlation_id)\n",
        "\n",
        "            if published_content_id is not None:\n",
        "                print(f'Completed publishing content [{published_content_id}].')\n",
        "\n",
        "                #if details is not None:\n",
        "                #    if details.summaries is not None and len(details.summaries) > 0:\n",
        "                #        summaries = \"\\n\".join(details.summaries)\n",
        "                #        print(f'Summaries: {summaries}')\n",
        "\n",
        "                # Need to reload content to get published markdown\n",
        "                published_content = await get_content(published_content_id)\n",
        "\n",
        "                if published_content is not None:\n",
        "                    display(Markdown(f'### Published [{published_content.name}]'))\n",
        "\n",
        "                    display(Markdown('### Timeline'))\n",
        "                    print(published_content.markdown)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ieBzAp6Z2Zew",
        "outputId": "55f98e59-40b5-4f2c-c089-293a91dea568"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created summary specification [d4947ff0-f847-4809-8d80-f8cd15d367ca]:\n",
            "Created publish specification [e4eb576b-effd-4815-812a-028ca0a432a3]:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Publishing Contents..."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed publishing content [bc3d81b2-fe62-4c30-b38b-7e069f73bc69].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Published [Published Summary]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Timeline"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Reflecting on a Year of Innovation: Graphlit's 2024 Year in Review\n",
            "\n",
            "As we look back on 2024, it's clear that Graphlit has experienced a year of remarkable growth and innovation. Our commitment to enhancing developer workflows, improving user experiences, and integrating cutting-edge technologies has driven numerous updates across our platform. From January through December, we've introduced new features, significant enhancements, and critical bug fixes to ensure our SaaS product remains a leader in the industry. Here's a comprehensive reflection on our journey through the year.\n",
            "\n",
            "## January 2024\n",
            "\n",
            "The year kicked off with substantial advancements in content management and integration capabilities. On January 18th, we introduced support for **content publishing**, enabling the summarization and repurposing of documents, audio transcripts, and image descriptions. This feature allows developers to efficiently publish conversations as content, generate bulk summaries, and integrate with external services through callback webhooks for LLM tools.\n",
            "\n",
            "Shortly after, on January 22nd, we expanded our email integration by supporting **Google and Microsoft email feeds**. This enhancement allows ingestion of both past and new emails, automatically extracting attachments and linking them to parent emails. Additionally, we introduced the **reingest content in-place** feature, offering more control over content updates and workflow management.\n",
            "\n",
            "These updates significantly improve developers' ability to manage and utilize various content types, enhancing the robustness and flexibility of applications built on our platform.\n",
            "\n",
            "## February 2024\n",
            "\n",
            "February brought enhancements focused on **semantic processing** and **model performance**. On February 2nd, we launched **Semantic Alerts**, enabling periodic LLM summarization and content publishing to generate daily reports from multiple feeds. This feature is particularly beneficial for teams looking to automate reporting and maintain up-to-date insights.\n",
            "\n",
            "We also supported the latest **OpenAI 0125 model versions** for GPT-4 and GPT-3.5 Turbo, with plans to extend support to Azure OpenAI services as they become available. Additionally, **Slack feeds** received an upgrade with the inclusion of a listing type field, allowing developers to specify whether to ingest past or new messages.\n",
            "\n",
            "Later in the month, on February 21st, we introduced support for **OneDrive and Google Drive feeds**, allowing seamless ingestion of files from shared drives. This update enhances file management capabilities and simplifies the integration process for applications relying on cloud storage solutions. We also added the ability to extract images from PDFs automatically, linking them as children of their parent files, which improves the handling of multimedia content.\n",
            "\n",
            "These enhancements empower developers with more efficient content workflows and better integration with popular cloud services, ensuring smoother operations and enhanced application functionality.\n",
            "\n",
            "## March 2024\n",
            "\n",
            "March was marked by significant expansions in **model support** and **telemetry**. On March 10th, we unveiled support for **Claude 3**, **Mistral**, and **Groq models**, broadening the range of AI models developers can leverage. Alongside this, we introduced **usage and credits telemetry**, providing detailed insights into API usage and credit consumption, which helps in monitoring and optimizing resource allocation.\n",
            "\n",
            "Mid-March, on March 13th, we added support for the **Claude 3 Haiku model** and enabled direct ingestion of **Base64 encoded files**, simplifying the process of handling various file formats and improving data ingestion workflows. Furthermore, March 23rd saw the addition of support for **Linear, GitHub Issues, and Jira issue feeds**, allowing seamless ingestion of issue tracking data as searchable content items. This integration facilitates better project management and tracking within developer workflows.\n",
            "\n",
            "These updates significantly enhance the platform's capabilities, offering developers more options for model usage and better integration with project management tools, thereby improving overall productivity and efficiency.\n",
            "\n",
            "## April 2024\n",
            "\n",
            "April was a transformative month with the introduction of robust **SDK support** and further **model integrations**. On April 7th, we launched support for **Discord feeds**, enabling ingestion of messages and file attachments from Discord channels using bot tokens. Additionally, we introduced **Cohere reranking**, allowing semantic search results to be reordered for increased relevance within RAG pipelines.\n",
            "\n",
            "Continuing this momentum, on April 23rd, we introduced native **Python and TypeScript SDKs**. These SDKs are built using Pydantic and TypeScript types and are automatically generated from our GraphQL schema, simplifying the development process by eliminating the need for GraphQL knowledge. Moreover, we expanded support for the latest **OpenAI**, **Cohere**, and **Groq models**, enhancing the versatility and performance of our platform.\n",
            "\n",
            "These enhancements streamline the development process, providing developers with powerful tools and improved model support, thereby facilitating the creation of more sophisticated and efficient applications.\n",
            "\n",
            "## May 2024\n",
            "\n",
            "May focused on **enhancing search capabilities** and **improving performance**. On May 5th, we introduced support for **Jina and Pongo rerankers**, offering developers additional options for semantic search result prioritization. Additionally, we added support for **Microsoft Teams feeds**, enabling the ingestion of messages from Teams channels, which enhances collaboration and communication integration.\n",
            "\n",
            "On May 15th, we rolled out support for **GraphRAG** and the **OpenAI GPT-4o model**. These updates include LLM revisions in RAG conversations, which allow the LLM to refine initial responses, resulting in higher quality outputs and increased token usage by 35%. We also introduced **graph visualization** in promptConversation responses, providing a visual representation of entity relationships, which aids in better data comprehension and analysis.\n",
            "\n",
            "These updates offer developers improved search functionality and enhanced data visualization capabilities, leading to more accurate and insightful application performance.\n",
            "\n",
            "## June 2024\n",
            "\n",
            "June was characterized by significant advancements in **model support** and **semantic search**. On June 9th, we added support for **Deepseek models** for prompt completion and enabled the parsing of embedded **JSON-LD** from web pages, automatically injecting them into the knowledge graph. Additionally, we enhanced our **knowledge graph generation** by making it opt-in, allowing developers to control its inclusion in responses more effectively.\n",
            "\n",
            "On June 21st, we introduced support for the **Anthropic Claude 3.5 Sonnet model** and enabled **semantic search** for observable entities within the knowledge graph using vector embeddings. This allows for more accurate and contextually relevant search results, improving the overall user experience and data retrieval processes.\n",
            "\n",
            "These enhancements provide developers with advanced model capabilities and more precise search functionalities, fostering the development of smarter and more responsive applications.\n",
            "\n",
            "## July 2024\n",
            "\n",
            "July brought a series of essential updates focusing on **model expansions** and **workflow improvements**. On July 4th, we introduced support for **webhook Alerts**, enabling HTTP POST notifications with published text results, enhancing real-time alert management. We also upgraded **Deepseek** models to support a **128k token context window**, significantly increasing the capacity for handling large datasets and complex queries.\n",
            "\n",
            "Later in the month, on July 19th and 25th, we expanded our model support to include the **OpenAI GPT-4o Mini** and **Mistral Large 2** models, respectively, offering developers even more options for deploying and managing AI models within their applications. Additionally, on July 28th, we added support for the **indexing workflow stage** and **Azure AI language detection**, enhancing our content indexing and language identification capabilities.\n",
            "\n",
            "These updates enhance the platform's flexibility and performance, providing developers with more powerful tools and greater control over their AI integrations and workflows.\n",
            "\n",
            "## August 2024\n",
            "\n",
            "August was a pivotal month with significant upgrades in **document extraction** and **entity support**. On August 8th, we rolled out support for **LLM-based document preparation** using models like OpenAI GPT-4o and Anthropic Sonnet 3.5, and introduced an open-source **.NET SDK** available on Nuget.org with comprehensive code samples on GitHub. This addition makes it easier for developers working with .NET to integrate Graphlit's capabilities into their applications.\n",
            "\n",
            "On August 11th, we enhanced our platform by supporting **Azure AI Document Intelligence by default**, improving fidelity in complex PDFs and enabling better table extraction. We also introduced **language-aware summaries and entity descriptions**, ensuring that summaries and entity metadata align with the source text's language, thereby enhancing the accuracy and relevance of content processing.\n",
            "\n",
            "Finally, on August 20th, we added support for **medical-related entities** and **Anthropic prompt caching**, expanding our platform's capabilities to handle specialized datasets and improving performance by reducing token costs during prompt processing.\n",
            "\n",
            "These enhancements provide developers with robust document processing capabilities and specialized entity support, facilitating the creation of more precise and efficient applications.\n",
            "\n",
            "## September 2024\n",
            "\n",
            "September focused on **advanced AI integrations** and **medical data handling**. On September 1st, we introduced support for **FHIR enrichment**, allowing entity enrichment from FHIR servers for medical-related entities, which is essential for healthcare applications. We also added support for the latest **Cohere models** and the **Azure AI Document Intelligence v4.0 preview API**, improving model performance and document processing.\n",
            "\n",
            "On September 26th and 30th, we expanded our model support to include the latest **Google Gemini models** and **Azure AI Inference models**, providing developers with access to a broader range of AI capabilities and improving the flexibility of model integrations. These updates ensure that Graphlit remains at the forefront of AI advancements, offering developers the tools they need to build sophisticated and intelligent applications.\n",
            "\n",
            "These updates significantly enhance the platform's AI capabilities and specialized data handling, making it easier for developers to integrate advanced AI features and manage complex datasets effectively.\n",
            "\n",
            "## October 2024\n",
            "\n",
            "October saw a blend of **tool calling enhancements** and **model support expansions**. On October 3rd, we introduced the **ingestBatch mutation**, allowing asynchronous ingestion of multiple URIs into content objects, streamlining the content ingestion process. Additionally, support for **Google Gemini Flash 1.5 8b** models was added, enhancing the range of available AI models.\n",
            "\n",
            "On October 7th and 22nd, we expanded our tool calling support to include **Anthropic and Gemini models**, improving the flexibility and integration of external tools within conversations. We also updated **Cohere image embeddings**, offering developers more robust image processing capabilities.\n",
            "\n",
            "Furthermore, on October 21st, we introduced support for configuring **image and text embedding models** at the project level, including models like **OpenAI Embedding-3-Small**, **Cohere Embed 3.0**, and **Jina CLIP image embeddings**. These updates provide developers with greater control over embedding configurations, allowing for more tailored and effective data processing.\n",
            "\n",
            "Finally, on October 22nd, we added support for the latest **Anthropic Sonnet 3.5** model and **Cohere image embeddings**, ensuring that developers have access to the most recent advancements in AI technology.\n",
            "\n",
            "These enhancements offer developers improved integration capabilities and access to the latest AI models, enabling the creation of more powerful and flexible applications.\n",
            "\n",
            "## November 2024\n",
            "\n",
            "November was a month of **advanced image analysis** and **multi-turn summarization**. On November 4th, we supported the **Anthropic Claude 3.5 Haiku model**, enhancing our AI model offerings. We also introduced an automatic disabling feature for all feeds upon reaching the free tier quota, giving developers better control over their usage limits.\n",
            "\n",
            "On November 10th, we added support for **web search mutations**, enabling the selection of search services like **Tavily or Exa.AI** without ingesting entire web pages. Additionally, we launched **multi-turn content summarization** and enhanced **Deepgram language detection**, providing more nuanced and accurate summaries and language processing capabilities.\n",
            "\n",
            "Later in the month, on November 16th and 24th, we introduced **multi-turn text summarization** and **multi-turn image analysis** capabilities. These features allow developers to engage in more interactive and detailed conversations with LLMs, facilitating deeper analysis and more refined outputs.\n",
            "\n",
            "These updates enhance the platform's capability to handle complex interactions and multimedia content, providing developers with more powerful tools for content analysis and summarization.\n",
            "\n",
            "## December 2024\n",
            "\n",
            "As the year draws to a close, December introduced several pivotal updates that enhanced **AI model support**, **document management**, and **content publishing capabilities**. On December 1st, we implemented support for a **retrieval-only RAG pipeline**, allowing developers to format LLM-ready prompts using the `formatConversation` and `completeConversation` mutations. This feature streamlines data retrieval processes and enhances the interaction between LLMs and retrieved content.\n",
            "\n",
            "Continuing on December 10th, we expanded support for **OpenAI GPT-4 Turbo 128k**, alongside **Llama 2** and **Mistral** models, providing developers with access to some of the most advanced AI models available. Additionally, we introduced **query by example** for contents and conversations, utilizing vector embeddings to improve search relevancy and user experience.\n",
            "\n",
            "On December 9th and 27th, we focused on **website mapping** and **Google Docs** integration. The December 9th update enabled **website mapping** using `mapWebmutation` to retrieve URLs from sitemap.xml files, and introduced the ability to generate web page **screenshots** with optional image processing workflows via `screenshotPagemutation`. On December 27th, we enhanced support for **LLM fallbacks** to safeguard against model provider downtimes and improved **native Google Docs** formats ingestion, allowing seamless auto-exportation to Microsoft Office formats.\n",
            "\n",
            "Finally, on December 22nd, we broadened our integration capabilities by adding support for **Dropbox, Box, Intercom, and Zendesk feeds**, enabling efficient ingestion from these popular cloud services. We also introduced support for the **OpenAI o1** and **Gemini 2.0** models, along with various bug fixes to enhance platform stability and performance.\n",
            "\n",
            "These comprehensive updates showcase Graphlit's dedication to providing developers with versatile tools and robust integrations, ensuring that applications built on our platform are both powerful and reliable.\n",
            "\n",
            "## Looking Ahead\n",
            "\n",
            "As we reflect on the past year, we're proud of the strides we've made in enhancing the Graphlit platform. Each update has been driven by our commitment to providing developers with the tools they need to build innovative and reliable applications. We're grateful for the support and feedback from our user community, which has been instrumental in guiding our development efforts.\n",
            "\n",
            "Looking forward, we are excited about the upcoming innovations and improvements slated for the next year. Stay tuned for more advanced features, enhanced integrations, and continued performance optimizations as we strive to make Graphlit an even more powerful ally in your development journey.\n",
            "\n",
            "Thank you for being a part of our community and for driving the future of Graphlit with us!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML, JSON\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "time.sleep(10) # give it some time for billing events to catch up\n",
        "\n",
        "credits = await lookup_credits(ingestion_correlation_id)\n",
        "\n",
        "if credits is not None:\n",
        "    display(Markdown(f\"### Credits used: {credits.credits:.6f} for ingestion\"))\n",
        "    print(f\"- storage [{credits.storage_ratio:.2f}%], compute [{credits.compute_ratio:.2f}%]\")\n",
        "    print(f\"- embedding [{credits.embedding_ratio:.2f}%], completion [{credits.completion_ratio:.2f}%]\")\n",
        "    print(f\"- ingestion [{credits.ingestion_ratio:.2f}%], indexing [{credits.indexing_ratio:.2f}%], preparation [{credits.preparation_ratio:.2f}%], extraction [{credits.extraction_ratio:.2f}%], enrichment [{credits.enrichment_ratio:.2f}%], publishing [{credits.publishing_ratio:.2f}%]\")\n",
        "    print(f\"- search [{credits.search_ratio:.2f}%], conversation [{credits.conversation_ratio:.2f}%]\")\n",
        "    print()\n",
        "\n",
        "usage = await lookup_usage(ingestion_correlation_id)\n",
        "\n",
        "if usage is not None:\n",
        "    display(Markdown(f\"### Usage records:\"))\n",
        "\n",
        "    for record in usage:\n",
        "        dump_usage_record(record)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s7M_W9n7ntKA",
        "outputId": "62eba7d4-af82-4a6c-8798-e173bd50c22a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Credits used: 3.458838 for ingestion"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- storage [1.38%], compute [37.37%]\n",
            "- embedding [4.58%], completion [0.00%]\n",
            "- ingestion [0.00%], indexing [0.00%], preparation [56.66%], extraction [0.00%], enrichment [0.00%], publishing [0.00%]\n",
            "- search [0.00%], conversation [0.00%]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Usage records:"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-01T20:17:40.337Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:17.420758, used credits [0.03136462]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]\n",
            "\n",
            "2025-01-01T20:17:40.219Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.349235, used credits [0.00112600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [563 tokens], throughput: 1612.095 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:40.059Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.192468, used credits [0.00095600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [478 tokens], throughput: 2483.525 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:39.687Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:16.770990, used credits [0.03019477]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]\n",
            "\n",
            "2025-01-01T20:17:39.645Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.370167, used credits [0.00112600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [563 tokens], throughput: 1520.935 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:39.598Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.368030, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1027.090 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:39.587Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.317558, used credits [0.00095600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [478 tokens], throughput: 1505.236 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:39.571Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:16.691391, used credits [0.03005146]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]\n",
            "\n",
            "2025-01-01T20:17:39.443Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.378332, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 581.499 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:39.400Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.166062, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 2276.256 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:39.239Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.177709, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 1237.982 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.995Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.188814, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 2001.975 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.956Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.537148, used credits [0.00095600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [478 tokens], throughput: 889.885 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.924Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.112029, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 3374.120 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.832Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.347835, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 632.483 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.659Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.175073, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 1256.616 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.655Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.225788, used credits [0.00112600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [563 tokens], throughput: 2493.491 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.539Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.452386, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 835.570 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.281Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.299387, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 734.834 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.215Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.124666, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 3032.097 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.104Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.123285, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 1784.480 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:38.094Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.541697, used credits [0.00112600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [563 tokens], throughput: 1039.327 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:37.776Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.311059, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1215.203 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:37.723Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.174636, used credits [0.00095600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [478 tokens], throughput: 2737.123 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:37.681Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.218337, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1731.268 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:37.579Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.216476, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 1016.278 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:37.555Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.193280, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 1138.243 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:36.871Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.456639, used credits [0.00112600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [563 tokens], throughput: 1232.922 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:36.792Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.363504, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1039.878 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:36.744Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.217073, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 1013.485 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:36.694Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.164442, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 1337.859 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:36.601Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.169392, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 2231.517 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:36.589Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.182778, used credits [0.00095600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [478 tokens], throughput: 2615.197 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:36.065Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.331984, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 662.683 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:36.052Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.314142, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1203.278 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:36.042Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.302474, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1249.693 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:35.956Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.232823, used credits [0.00112600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [563 tokens], throughput: 2418.143 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:35.913Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.183647, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 1197.950 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:35.911Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.198052, used credits [0.00095600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [478 tokens], throughput: 2413.508 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:34.651Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.712283, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 308.866 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:34.615Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.671528, used credits [0.00095600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [478 tokens], throughput: 711.809 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:34.450Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.512251, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 737.919 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:34.445Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.502492, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 752.250 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:34.443Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.503759, used credits [0.00112600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [563 tokens], throughput: 1117.599 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:34.339Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.390423, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 563.491 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:29.874Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.133857, used credits [0.00076160]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-10-support-for-web-search-multi-turn-content-summarization-deepgram-language-detection]\n",
            "- File upload [197102 bytes], throughput: 1472485.128 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:29.780Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.382167, used credits [0.00072110]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-22-support-for-latest-anthropic-sonnet-3.5-model-cohere-image-embeddings]\n",
            "- File upload [186621 bytes], throughput: 488322.657 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:29.775Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.382171, used credits [0.00081733]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2023/september-4-workflow-configuration-support-for-notion-feeds-document-ocr]\n",
            "- File upload [211525 bytes], throughput: 553482.314 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:29.369Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:06.062144, used credits [0.03800000]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:28.948Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.649253, used credits [0.03800000]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:28.815Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.514724, used credits [0.03800000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:28.141Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.252424, used credits [0.01485780]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]\n",
            "\n",
            "2025-01-01T20:17:28.034Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.300370, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 732.429 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:28.031Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.295180, used credits [0.00044000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [220 tokens], throughput: 745.307 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:27.969Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.728012, used credits [0.01571406]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]\n",
            "\n",
            "2025-01-01T20:17:27.828Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.463529, used credits [0.00112600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [563 tokens], throughput: 1214.594 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:27.793Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.433949, used credits [0.00095600]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [478 tokens], throughput: 1101.511 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:27.061Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.019815, used credits [0.01623943]\n",
            "- CONTENT [88122e0e-dc76-44ba-a14b-5f5fb698524d]\n",
            "\n",
            "2025-01-01T20:17:26.925Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.352373, used credits [0.00117000]\n",
            "- CONTENT [88122e0e-dc76-44ba-a14b-5f5fb698524d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [585 tokens], throughput: 1660.174 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:26.822Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.253943, used credits [0.00095800]\n",
            "- CONTENT [88122e0e-dc76-44ba-a14b-5f5fb698524d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [479 tokens], throughput: 1886.252 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:26.782Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.060547, used credits [0.00000616]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1595 bytes], throughput: 26343.084 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:26.460Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.594980, used credits [0.01547454]\n",
            "- CONTENT [68397ab0-4d1f-4b4a-bb0d-af89d00ba47f]\n",
            "\n",
            "2025-01-01T20:17:26.333Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.334945, used credits [0.00075200]\n",
            "- CONTENT [68397ab0-4d1f-4b4a-bb0d-af89d00ba47f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1122.573 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:26.302Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.061724, used credits [0.00001352]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3499 bytes], throughput: 56687.836 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:26.248Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.257790, used credits [0.00075200]\n",
            "- CONTENT [68397ab0-4d1f-4b4a-bb0d-af89d00ba47f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [376 tokens], throughput: 1458.552 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:25.258Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.132008, used credits [0.00001446]\n",
            "- CONTENT [88122e0e-dc76-44ba-a14b-5f5fb698524d]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3743 bytes], throughput: 28354.407 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:24.852Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.113272, used credits [0.00071832]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-22-support-for-latest-anthropic-sonnet-3.5-model-cohere-image-embeddings]\n",
            "- File upload [185901 bytes], throughput: 1641188.217 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:24.750Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.057652, used credits [0.00001299]\n",
            "- CONTENT [68397ab0-4d1f-4b4a-bb0d-af89d00ba47f]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3363 bytes], throughput: 58333.059 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:24.264Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.083261, used credits [0.00075780]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-10-support-for-web-search-multi-turn-content-summarization-deepgram-language-detection]\n",
            "- File upload [196119 bytes], throughput: 2355483.866 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:24.184Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.228912, used credits [0.03800000]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:23.636Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.325755, used credits [0.03800000]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:23.281Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.191877, used credits [0.00081866]\n",
            "- CONTENT [88122e0e-dc76-44ba-a14b-5f5fb698524d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-26-support-for-google-ai-and-cerebras-models-and-latest-groq-models]\n",
            "- File upload [211869 bytes], throughput: 1104193.474 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:22.879Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.127105, used credits [0.00078907]\n",
            "- CONTENT [68397ab0-4d1f-4b4a-bb0d-af89d00ba47f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-3-support-tool-calling-ingestbatch-mutation-gemini-flash-1.5-8b-bug-fixes]\n",
            "- File upload [204210 bytes], throughput: 1606625.708 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:22.656Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.576190, used credits [0.03800000]\n",
            "- CONTENT [88122e0e-dc76-44ba-a14b-5f5fb698524d]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:22.323Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.420132, used credits [0.03800000]\n",
            "- CONTENT [68397ab0-4d1f-4b4a-bb0d-af89d00ba47f]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:21.829Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.178678, used credits [0.01292461]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]\n",
            "\n",
            "2025-01-01T20:17:21.725Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.351132, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 1076.518 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:21.542Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.168153, used credits [0.00075600]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [378 tokens], throughput: 2247.958 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:21.150Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.280275, used credits [0.01490794]\n",
            "- CONTENT [8413f583-bc40-4957-b03b-f62d5078fe09]\n",
            "\n",
            "2025-01-01T20:17:21.035Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.558605, used credits [0.00113600]\n",
            "- CONTENT [8413f583-bc40-4957-b03b-f62d5078fe09]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [568 tokens], throughput: 1016.819 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:20.812Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.337658, used credits [0.00093600]\n",
            "- CONTENT [8413f583-bc40-4957-b03b-f62d5078fe09]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [468 tokens], throughput: 1386.016 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:20.585Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.057515, used credits [0.00001484]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3840 bytes], throughput: 66765.308 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:19.768Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:20.528693, used credits [0.03696020]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]\n",
            "\n",
            "2025-01-01T20:17:19.678Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.362677, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1080.851 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:19.625Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.304011, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1289.427 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:19.619Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.058613, used credits [0.00001700]\n",
            "- CONTENT [8413f583-bc40-4957-b03b-f62d5078fe09]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4400 bytes], throughput: 75068.671 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:19.148Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.075971, used credits [0.00081461]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2023/september-4-workflow-configuration-support-for-notion-feeds-document-ocr]\n",
            "- File upload [210820 bytes], throughput: 2775002.600 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:18.682Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.394300, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 994.166 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:18.677Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:19.436270, used credits [0.03499338]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]\n",
            "\n",
            "2025-01-01T20:17:18.674Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.395307, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 991.634 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:18.215Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.078252, used credits [0.00081722]\n",
            "- CONTENT [8413f583-bc40-4957-b03b-f62d5078fe09]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2023/september-20-paid-subscription-plans-support-for-custom-observed-entities-and-azure-openai-gpt-4]\n",
            "- File upload [211495 bytes], throughput: 2702738.968 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:18.177Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.451284, used credits [0.00109800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 1216.530 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:18.124Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.424175, used credits [0.03800000]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:18.051Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.329583, used credits [0.00090800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1377.497 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.885Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.248668, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1576.398 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.834Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.627821, used credits [0.01733409]\n",
            "- CONTENT [4983a4c2-2d1a-4640-83f7-a187a4275778]\n",
            "\n",
            "2025-01-01T20:17:17.771Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:17.859273, used credits [0.03215413]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]\n",
            "\n",
            "2025-01-01T20:17:17.763Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.124560, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 3147.065 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.760Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.866224, used credits [0.03800000]\n",
            "- CONTENT [8413f583-bc40-4957-b03b-f62d5078fe09]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:17.661Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.311264, used credits [0.00059000]\n",
            "- CONTENT [4983a4c2-2d1a-4640-83f7-a187a4275778]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [295 tokens], throughput: 947.748 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.615Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.316776, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1180.645 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.475Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.362540, used credits [0.00109800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 1514.316 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.462Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.114049, used credits [0.00059000]\n",
            "- CONTENT [4983a4c2-2d1a-4640-83f7-a187a4275778]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [295 tokens], throughput: 2586.608 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.454Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.157498, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 2374.626 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.391Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.285770, used credits [0.00090800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1588.689 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.384Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:15.879378, used credits [0.02858950]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]\n",
            "\n",
            "2025-01-01T20:17:17.312Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.505577, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 775.351 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.268Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.164820, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 2378.346 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.244Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.136900, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 2863.414 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.077Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.282946, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1321.809 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:17.029Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.221518, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1769.608 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.966Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.170056, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 2199.272 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.847Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.323684, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1211.057 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.840Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.298892, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1311.511 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.821Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.477882, used credits [0.00090800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 950.026 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.739Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.375667, used credits [0.00109800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 1461.400 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.535Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.489120, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 764.639 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.278Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.252400, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1481.777 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.214Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.395149, used credits [0.01331435]\n",
            "- CONTENT [65dc8b4e-c1dc-40e2-a653-d8d51fe0414c]\n",
            "\n",
            "2025-01-01T20:17:16.184Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.302037, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1297.855 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.167Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.272529, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1438.380 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.162Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:14.668424, used credits [0.02640927]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]\n",
            "\n",
            "2025-01-01T20:17:16.158Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.262072, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1495.772 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.095Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.328397, used credits [0.00068800]\n",
            "- CONTENT [65dc8b4e-c1dc-40e2-a653-d8d51fe0414c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [344 tokens], throughput: 1047.513 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.074Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:06.918822, used credits [0.01245676]\n",
            "- CONTENT [9819c811-85db-4a53-923e-f3a6beac4b6a]\n",
            "\n",
            "2025-01-01T20:17:16.067Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.187434, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 2091.397 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.044Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.223815, used credits [0.00085400]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 1907.826 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:16.030Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.217520, used credits [0.00085400]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 1963.033 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.985Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.341440, used credits [0.00052800]\n",
            "- CONTENT [9819c811-85db-4a53-923e-f3a6beac4b6a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [264 tokens], throughput: 773.197 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.967Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.200882, used credits [0.00068800]\n",
            "- CONTENT [65dc8b4e-c1dc-40e2-a653-d8d51fe0414c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [344 tokens], throughput: 1712.445 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.934Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.437185, used credits [0.00109800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 1255.762 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.831Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.185571, used credits [0.00052800]\n",
            "- CONTENT [9819c811-85db-4a53-923e-f3a6beac4b6a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [264 tokens], throughput: 1422.633 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.819Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.326096, used credits [0.00090800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1392.227 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.790Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.078750, used credits [0.00001020]\n",
            "- CONTENT [4983a4c2-2d1a-4640-83f7-a187a4275778]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2640 bytes], throughput: 33523.937 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:15.671Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.313164, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1194.264 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.566Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.286441, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1368.520 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.559Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.489625, used credits [0.00085400]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 872.096 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.553Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.195848, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1909.648 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.432Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.154339, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 2539.867 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.387Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.319946, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1225.207 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.364Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.282248, used credits [0.00085400]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 1512.857 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:15.236Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.155788, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 2516.237 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:14.883Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.482164, used credits [0.00109800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 1138.618 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:14.852Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.056840, used credits [0.00000856]\n",
            "- CONTENT [65dc8b4e-c1dc-40e2-a653-d8d51fe0414c]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2216 bytes], throughput: 38986.629 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:14.799Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.057417, used credits [0.00000757]\n",
            "- CONTENT [9819c811-85db-4a53-923e-f3a6beac4b6a]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1959 bytes], throughput: 34118.815 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:14.773Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.375527, used credits [0.00090800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1208.968 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:14.763Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.569101, used credits [0.00085400]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 750.306 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:14.655Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.456930, used credits [0.00085400]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 934.499 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:14.626Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:01.848332, used credits [0.00332777]\n",
            "\n",
            "2025-01-01T20:17:14.540Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.255584, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1533.742 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:14.519Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.325645, used credits [0.01318921]\n",
            "- CONTENT [8fff37ca-bf13-47f8-b452-1010061953f8]\n",
            "\n",
            "2025-01-01T20:17:14.466Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.186013, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 2107.380 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:14.424Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.312523, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1196.713 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:14.405Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.492429, used credits [0.00044200]\n",
            "- CONTENT [8fff37ca-bf13-47f8-b452-1010061953f8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [221 tokens], throughput: 448.796 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:14.258Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.142607, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 2622.596 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:14.080Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.167322, used credits [0.00044200]\n",
            "- CONTENT [8fff37ca-bf13-47f8-b452-1010061953f8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [221 tokens], throughput: 1320.807 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.906Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.462559, used credits [0.00090800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 981.496 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.906Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.446212, used credits [0.00109800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 1230.355 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.826Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.375830, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1043.024 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.746Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.737839, used credits [0.01753217]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]\n",
            "\n",
            "2025-01-01T20:17:13.744Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.320395, used credits [0.00085400]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 1332.729 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.707Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.292505, used credits [0.00085400]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 1459.802 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.677Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.234480, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1671.785 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.649Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.295427, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1265.963 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.634Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.340835, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 727.624 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.555Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.260877, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 950.640 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.528Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.172546, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 2167.537 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.505Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.102417, used credits [0.00072265]\n",
            "- CONTENT [9819c811-85db-4a53-923e-f3a6beac4b6a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-30-support-for-azure-ai-inference-models-mistral-pixtral-and-latest-google-gemini-models]\n",
            "- File upload [187021 bytes], throughput: 1826077.362 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:13.370Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.160216, used credits [0.00077536]\n",
            "- CONTENT [4983a4c2-2d1a-4640-83f7-a187a4275778]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-1-support-for-fhir-enrichment-latest-cohere-models-bug-fixes]\n",
            "- File upload [200663 bytes], throughput: 1252452.939 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:13.330Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.076981, used credits [0.00077419]\n",
            "- CONTENT [65dc8b4e-c1dc-40e2-a653-d8d51fe0414c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2024/september-3-support-for-web-search-feeds-model-deprecations]\n",
            "- File upload [200361 bytes], throughput: 2602722.999 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:13.087Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.115381, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 2149.397 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:13.085Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.111358, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 2227.041 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.981Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.060997, used credits [0.00000952]\n",
            "- CONTENT [8fff37ca-bf13-47f8-b452-1010061953f8]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2465 bytes], throughput: 40411.559 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:12.976Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.790368, used credits [0.03800000]\n",
            "- CONTENT [9819c811-85db-4a53-923e-f3a6beac4b6a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:12.832Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.969927, used credits [0.03800000]\n",
            "- CONTENT [65dc8b4e-c1dc-40e2-a653-d8d51fe0414c]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:12.805Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.642949, used credits [0.01376049]\n",
            "- CONTENT [833da803-989f-4c1c-96af-c4cf003e6e52]\n",
            "\n",
            "2025-01-01T20:17:12.720Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.367870, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 674.151 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.689Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.508883, used credits [0.00023800]\n",
            "- CONTENT [833da803-989f-4c1c-96af-c4cf003e6e52]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [119 tokens], throughput: 233.845 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.648Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:11.525326, used credits [0.02075039]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]\n",
            "\n",
            "2025-01-01T20:17:12.587Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.326550, used credits [0.03800000]\n",
            "- CONTENT [4983a4c2-2d1a-4640-83f7-a187a4275778]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:12.559Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.207993, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 1192.347 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.539Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.241567, used credits [0.00085400]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 1767.625 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.518Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.616623, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 635.720 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.506Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.209508, used credits [0.00085400]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [427 tokens], throughput: 2038.104 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.504Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.604814, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 648.134 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.503Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.582514, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 672.946 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.497Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.576592, used credits [0.00109800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 952.147 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.478Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.297420, used credits [0.00023800]\n",
            "- CONTENT [833da803-989f-4c1c-96af-c4cf003e6e52]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [119 tokens], throughput: 400.108 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.422Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.503293, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 778.871 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.396Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.475555, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 786.449 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.392Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.491092, used credits [0.00090800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 924.469 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.365Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.456842, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 818.663 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:12.046Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.334754, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 740.841 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:11.990Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.280076, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 885.475 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:11.590Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.722513, used credits [0.01390374]\n",
            "- CONTENT [119c6576-e615-476e-8ab9-27d841050c5c]\n",
            "\n",
            "2025-01-01T20:17:11.531Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.076595, used credits [0.00078939]\n",
            "- CONTENT [8fff37ca-bf13-47f8-b452-1010061953f8]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/september-2023/september-24-support-for-youtube-feeds-added-documentation-bug-fixes]\n",
            "- File upload [204293 bytes], throughput: 2667177.578 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:11.529Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.093978, used credits [0.01277212]\n",
            "- CONTENT [1243312d-9b3b-4172-88ea-ee1b5c097b2c]\n",
            "\n",
            "2025-01-01T20:17:11.472Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.298896, used credits [0.00045200]\n",
            "- CONTENT [119c6576-e615-476e-8ab9-27d841050c5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [226 tokens], throughput: 756.115 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:11.418Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.271574, used credits [0.00027600]\n",
            "- CONTENT [1243312d-9b3b-4172-88ea-ee1b5c097b2c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [138 tokens], throughput: 508.148 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:11.370Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.225814, used credits [0.00027600]\n",
            "- CONTENT [1243312d-9b3b-4172-88ea-ee1b5c097b2c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [138 tokens], throughput: 611.122 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:11.340Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.167693, used credits [0.00045200]\n",
            "- CONTENT [119c6576-e615-476e-8ab9-27d841050c5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [226 tokens], throughput: 1347.698 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:11.200Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.271826, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 912.347 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:11.144Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.215164, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 1152.612 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:11.142Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.056603, used credits [0.00000570]\n",
            "- CONTENT [833da803-989f-4c1c-96af-c4cf003e6e52]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1474 bytes], throughput: 26040.931 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:11.094Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.866674, used credits [0.03800000]\n",
            "- CONTENT [8fff37ca-bf13-47f8-b452-1010061953f8]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:10.554Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.208816, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 1187.647 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:10.536Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.192273, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 1289.833 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:10.169Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:10.263479, used credits [0.01847854]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]\n",
            "\n",
            "2025-01-01T20:17:10.142Z: Serverless compute\n",
            "- Workflow [Feed] took 0:00:47.377186, used credits [0.04264934]\n",
            "\n",
            "2025-01-01T20:17:10.137Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.058673, used credits [0.00000562]\n",
            "- CONTENT [1243312d-9b3b-4172-88ea-ee1b5c097b2c]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1455 bytes], throughput: 24798.459 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:10.071Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.338519, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1104.813 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:10.065Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.056877, used credits [0.00000794]\n",
            "- CONTENT [119c6576-e615-476e-8ab9-27d841050c5c]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2054 bytes], throughput: 36113.143 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:09.955Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.208926, used credits [0.00074800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [374 tokens], throughput: 1790.107 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:09.863Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.361994, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 685.095 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:09.755Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.247851, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 1000.602 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:09.716Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.055418, used credits [0.00001413]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3658 bytes], throughput: 66008.030 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:09.553Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.079822, used credits [0.00072158]\n",
            "- CONTENT [833da803-989f-4c1c-96af-c4cf003e6e52]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-9-support-for-github-repository-feeds-bug-fixes]\n",
            "- File upload [186744 bytes], throughput: 2339517.123 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:09.437Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.308042, used credits [0.01495794]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]\n",
            "\n",
            "2025-01-01T20:17:09.405Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.125774, used credits [0.00085133]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-10-support-for-claude-3-mistral-and-groq-models-usage-credits-telemetry-bug-fixes]\n",
            "- File upload [220324 bytes], throughput: 1751750.765 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:09.228Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.346332, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 716.075 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:09.083Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.202146, used credits [0.00049600]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [248 tokens], throughput: 1226.837 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:08.947Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.068872, used credits [0.01632775]\n",
            "- CONTENT [f6df142b-f920-4c85-b4d9-1dc09d7d2315]\n",
            "\n",
            "2025-01-01T20:17:08.834Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.285616, used credits [0.00049200]\n",
            "- CONTENT [f6df142b-f920-4c85-b4d9-1dc09d7d2315]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 861.295 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:08.787Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.172342, used credits [0.00076048]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-4-support-for-anthropic-claude-3.5-haiku-bug-fixes]\n",
            "- File upload [196812 bytes], throughput: 1141983.797 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:08.699Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.156880, used credits [0.00049200]\n",
            "- CONTENT [f6df142b-f920-4c85-b4d9-1dc09d7d2315]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [246 tokens], throughput: 1568.077 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:08.608Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.420500, used credits [0.03800000]\n",
            "- CONTENT [833da803-989f-4c1c-96af-c4cf003e6e52]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:08.572Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.087147, used credits [0.00070458]\n",
            "- CONTENT [1243312d-9b3b-4172-88ea-ee1b5c097b2c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-7-support-for-anthropic-and-gemini-tool-calling]\n",
            "- File upload [182344 bytes], throughput: 2092382.262 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:08.224Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.084634, used credits [0.00073449]\n",
            "- CONTENT [119c6576-e615-476e-8ab9-27d841050c5c]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-31-support-for-simulated-tool-calling-bug-fixes]\n",
            "- File upload [190086 bytes], throughput: 2245982.102 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:08.160Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:08.377475, used credits [0.03800000]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:08.142Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.104842, used credits [0.03800000]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:08.100Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.197673, used credits [0.00081259]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2023/october-30-optimized-conversation-responses-added-observable-aliases-bug-fixes]\n",
            "- File upload [210297 bytes], throughput: 1063863.047 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:08.063Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.403433, used credits [0.00081905]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2023/october-15-support-for-anthropic-claude-models-slack-feeds-and-entity-enrichment]\n",
            "- File upload [211969 bytes], throughput: 525413.272 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:08.024Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.556931, used credits [0.03800000]\n",
            "- CONTENT [1243312d-9b3b-4172-88ea-ee1b5c097b2c]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:08.001Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.425859, used credits [0.00081948]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2023/october-15-support-for-anthropic-claude-models-slack-feeds-and-entity-enrichment]\n",
            "- File upload [212082 bytes], throughput: 498010.256 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:07.992Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.462542, used credits [0.00077596]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-21-support-openai-cohere-jina-mistral-voyage-and-google-ai-embedding-models]\n",
            "- File upload [200819 bytes], throughput: 434163.441 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:07.946Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.665273, used credits [0.01380069]\n",
            "- CONTENT [9303dfbd-2681-4e9b-93ec-2f6a8c1c326f]\n",
            "\n",
            "2025-01-01T20:17:07.821Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.510907, used credits [0.00076800]\n",
            "- CONTENT [9303dfbd-2681-4e9b-93ec-2f6a8c1c326f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 751.605 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:07.768Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.058898, used credits [0.00000878]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2273 bytes], throughput: 38592.470 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:07.627Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.723218, used credits [0.03800000]\n",
            "- CONTENT [119c6576-e615-476e-8ab9-27d841050c5c]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:07.561Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.056147, used credits [0.00001608]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4161 bytes], throughput: 74109.431 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:07.515Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.072073, used credits [0.00000764]\n",
            "- CONTENT [f6df142b-f920-4c85-b4d9-1dc09d7d2315]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1976 bytes], throughput: 27416.647 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:07.504Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.196235, used credits [0.00076800]\n",
            "- CONTENT [9303dfbd-2681-4e9b-93ec-2f6a8c1c326f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [384 tokens], throughput: 1956.833 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:07.503Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.461125, used credits [0.03800000]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:07.284Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.689011, used credits [0.03800000]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:07.003Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.434063, used credits [0.03800000]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:06.943Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:07.181388, used credits [0.03800000]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:06.760Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.117851, used credits [0.00077486]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2024/october-21-support-openai-cohere-jina-mistral-voyage-and-google-ai-embedding-models]\n",
            "- File upload [200533 bytes], throughput: 1701576.478 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:06.351Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.055380, used credits [0.00001288]\n",
            "- CONTENT [9303dfbd-2681-4e9b-93ec-2f6a8c1c326f]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3334 bytes], throughput: 60201.696 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:06.319Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.147003, used credits [0.01466800]\n",
            "- CONTENT [cb4a2f54-a3ae-4b9b-a66e-27232fea2d14]\n",
            "\n",
            "2025-01-01T20:17:06.313Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.824615, used credits [0.01408757]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]\n",
            "\n",
            "2025-01-01T20:17:06.201Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.189867, used credits [0.00103400]\n",
            "- CONTENT [cb4a2f54-a3ae-4b9b-a66e-27232fea2d14]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [517 tokens], throughput: 2722.962 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:06.193Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.341044, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1149.413 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:06.185Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.336893, used credits [0.00078400]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [392 tokens], throughput: 1163.574 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:06.154Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.136276, used credits [0.00125200]\n",
            "- CONTENT [cb4a2f54-a3ae-4b9b-a66e-27232fea2d14]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [626 tokens], throughput: 4593.626 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:06.021Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.836953, used credits [0.03800000]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:05.987Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.111236, used credits [0.00075363]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-4-support-for-anthropic-claude-3.5-haiku-bug-fixes]\n",
            "- File upload [195038 bytes], throughput: 1753375.940 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:05.893Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.260084, used credits [0.00081530]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2023/october-30-optimized-conversation-responses-added-observable-aliases-bug-fixes]\n",
            "- File upload [210999 bytes], throughput: 811271.264 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:05.673Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.122653, used credits [0.00071882]\n",
            "- CONTENT [f6df142b-f920-4c85-b4d9-1dc09d7d2315]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-16-support-for-image-description-multi-turn-text-summarization]\n",
            "- File upload [186029 bytes], throughput: 1516704.796 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:05.390Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.208568, used credits [0.03800000]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:05.128Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.194668, used credits [0.03800000]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:05.101Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.178147, used credits [0.03800000]\n",
            "- CONTENT [f6df142b-f920-4c85-b4d9-1dc09d7d2315]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:04.701Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.067877, used credits [0.00001681]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4351 bytes], throughput: 64100.959 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:04.638Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.063612, used credits [0.00001951]\n",
            "- CONTENT [cb4a2f54-a3ae-4b9b-a66e-27232fea2d14]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5048 bytes], throughput: 79356.346 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:04.583Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.080412, used credits [0.00079287]\n",
            "- CONTENT [9303dfbd-2681-4e9b-93ec-2f6a8c1c326f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/november-2024/november-24-support-for-direct-llm-prompt-multi-turn-image-analysis-bug-fixes]\n",
            "- File upload [205195 bytes], throughput: 2551786.232 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:04.220Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.565813, used credits [0.01722245]\n",
            "- CONTENT [79510fa9-9ddb-4b7e-8436-a2b0a867ac64]\n",
            "\n",
            "2025-01-01T20:17:04.100Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.342216, used credits [0.00069600]\n",
            "- CONTENT [79510fa9-9ddb-4b7e-8436-a2b0a867ac64]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [348 tokens], throughput: 1016.900 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:04.083Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.318841, used credits [0.00069600]\n",
            "- CONTENT [79510fa9-9ddb-4b7e-8436-a2b0a867ac64]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [348 tokens], throughput: 1091.452 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:04.067Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.760504, used credits [0.03800000]\n",
            "- CONTENT [9303dfbd-2681-4e9b-93ec-2f6a8c1c326f]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:03.853Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:10.369271, used credits [0.01866901]\n",
            "- CONTENT [e0f9a487-28bc-4d08-8b71-0dded28e8378]\n",
            "\n",
            "2025-01-01T20:17:03.750Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.724405, used credits [0.00150600]\n",
            "- CONTENT [e0f9a487-28bc-4d08-8b71-0dded28e8378]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [753 tokens], throughput: 1039.473 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:03.736Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.196824, used credits [0.01655811]\n",
            "- CONTENT [3109dca5-a705-4191-a868-ab2b2485d8ca]\n",
            "\n",
            "2025-01-01T20:17:03.614Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.320461, used credits [0.00063400]\n",
            "- CONTENT [3109dca5-a705-4191-a868-ab2b2485d8ca]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 989.201 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:03.500Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.205731, used credits [0.00063400]\n",
            "- CONTENT [3109dca5-a705-4191-a868-ab2b2485d8ca]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [317 tokens], throughput: 1540.846 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:03.481Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.457880, used credits [0.00125800]\n",
            "- CONTENT [e0f9a487-28bc-4d08-8b71-0dded28e8378]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [629 tokens], throughput: 1373.722 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:02.970Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.083130, used credits [0.00081731]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/october-2023/october-15-support-for-anthropic-claude-models-slack-feeds-and-entity-enrichment]\n",
            "- File upload [211519 bytes], throughput: 2544445.607 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:02.933Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.071893, used credits [0.00084401]\n",
            "- CONTENT [cb4a2f54-a3ae-4b9b-a66e-27232fea2d14]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/june-2024/june-9-support-for-deepseek-models-json-ld-webpage-parsing-performance-improvements-and-bug-fixes]\n",
            "- File upload [218429 bytes], throughput: 3038251.290 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:02.695Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.059921, used credits [0.00001357]\n",
            "- CONTENT [79510fa9-9ddb-4b7e-8436-a2b0a867ac64]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3512 bytes], throughput: 58610.797 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:02.687Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.081978, used credits [0.01635134]\n",
            "- CONTENT [47c53407-b4d6-48e4-b91d-b9677a1028de]\n",
            "\n",
            "2025-01-01T20:17:02.576Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.372124, used credits [0.00054200]\n",
            "- CONTENT [47c53407-b4d6-48e4-b91d-b9677a1028de]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 728.252 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:02.507Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.254960, used credits [0.00054200]\n",
            "- CONTENT [47c53407-b4d6-48e4-b91d-b9677a1028de]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [271 tokens], throughput: 1062.910 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:02.469Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.942928, used credits [0.03800000]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:02.361Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.147763, used credits [0.03800000]\n",
            "- CONTENT [cb4a2f54-a3ae-4b9b-a66e-27232fea2d14]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:17:02.206Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.257839, used credits [0.00001000]\n",
            "- CONTENT [3109dca5-a705-4191-a868-ab2b2485d8ca]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2588 bytes], throughput: 10037.264 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:02.200Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.618604, used credits [0.01551708]\n",
            "- CONTENT [5d06c7c1-735d-41a3-b95e-8bce0b652102]\n",
            "\n",
            "2025-01-01T20:17:02.040Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.207713, used credits [0.00138600]\n",
            "- CONTENT [5d06c7c1-735d-41a3-b95e-8bce0b652102]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [693 tokens], throughput: 3336.339 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:02.038Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.215020, used credits [0.00111400]\n",
            "- CONTENT [5d06c7c1-735d-41a3-b95e-8bce0b652102]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [557 tokens], throughput: 2590.459 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:01.099Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.288050, used credits [0.00002022]\n",
            "- CONTENT [e0f9a487-28bc-4d08-8b71-0dded28e8378]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5234 bytes], throughput: 18170.450 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:00.636Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.178668, used credits [0.00080287]\n",
            "- CONTENT [79510fa9-9ddb-4b7e-8436-a2b0a867ac64]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-4-support-for-webhook-alerts-keywords-summarization-deepseek-128k-context-window-bug-fixes]\n",
            "- File upload [207781 bytes], throughput: 1162947.283 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:00.407Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.456717, used credits [0.01522561]\n",
            "- CONTENT [a8a2bbe9-1c58-42a7-b31c-dd4b27b74956]\n",
            "\n",
            "2025-01-01T20:17:00.319Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.054858, used credits [0.00002005]\n",
            "- CONTENT [5d06c7c1-735d-41a3-b95e-8bce0b652102]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5189 bytes], throughput: 94590.013 bytes/sec\n",
            "\n",
            "2025-01-01T20:17:00.199Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.381062, used credits [0.01508940]\n",
            "- CONTENT [8f1c79b1-beaf-481d-a7a7-078105781fbb]\n",
            "\n",
            "2025-01-01T20:17:00.100Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.333130, used credits [0.00125000]\n",
            "- CONTENT [a8a2bbe9-1c58-42a7-b31c-dd4b27b74956]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [625 tokens], throughput: 1876.144 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:00.075Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.142001, used credits [0.00063600]\n",
            "- CONTENT [8f1c79b1-beaf-481d-a7a7-078105781fbb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [318 tokens], throughput: 2239.424 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:00.045Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.115630, used credits [0.00063600]\n",
            "- CONTENT [8f1c79b1-beaf-481d-a7a7-078105781fbb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [318 tokens], throughput: 2750.158 tokens/sec\n",
            "\n",
            "2025-01-01T20:17:00.032Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.258942, used credits [0.00136800]\n",
            "- CONTENT [a8a2bbe9-1c58-42a7-b31c-dd4b27b74956]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [684 tokens], throughput: 2641.522 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:59.915Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.186948, used credits [0.03800000]\n",
            "- CONTENT [79510fa9-9ddb-4b7e-8436-a2b0a867ac64]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:59.883Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.032804, used credits [0.01626281]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]\n",
            "\n",
            "2025-01-01T20:16:59.770Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.059233, used credits [0.00001077]\n",
            "- CONTENT [47c53407-b4d6-48e4-b91d-b9677a1028de]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2786 bytes], throughput: 47034.354 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:59.763Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.263070, used credits [0.00090800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [454 tokens], throughput: 1725.779 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:59.756Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.456013, used credits [0.01522435]\n",
            "- CONTENT [7fb352e7-6eda-419f-848d-d23527d330cc]\n",
            "\n",
            "2025-01-01T20:16:59.707Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.204799, used credits [0.00109800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [549 tokens], throughput: 2680.678 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:59.646Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.311517, used credits [0.00034800]\n",
            "- CONTENT [7fb352e7-6eda-419f-848d-d23527d330cc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 558.556 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:59.632Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.298216, used credits [0.00034800]\n",
            "- CONTENT [7fb352e7-6eda-419f-848d-d23527d330cc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [174 tokens], throughput: 583.469 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:59.039Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.312225, used credits [0.00076059]\n",
            "- CONTENT [3109dca5-a705-4191-a868-ab2b2485d8ca]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-25-support-for-mistral-large-2-and-nemo-groq-llama-3.1-models-bug-fixes]\n",
            "- File upload [196841 bytes], throughput: 630446.598 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:58.543Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.058245, used credits [0.00001948]\n",
            "- CONTENT [a8a2bbe9-1c58-42a7-b31c-dd4b27b74956]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5041 bytes], throughput: 86548.499 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:58.417Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.075588, used credits [0.00001807]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4677 bytes], throughput: 61874.655 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:58.375Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.073599, used credits [0.00000663]\n",
            "- CONTENT [7fb352e7-6eda-419f-848d-d23527d330cc]: Content type [PAGE], file type [DATA]\n",
            "- File upload [1717 bytes], throughput: 23329.248 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:58.368Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:10.155208, used credits [0.01828361]\n",
            "- CONTENT [f53855bb-439c-48c9-a88d-f427da2962d1]\n",
            "\n",
            "2025-01-01T20:16:58.285Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.102315, used credits [0.00083481]\n",
            "- CONTENT [e0f9a487-28bc-4d08-8b71-0dded28e8378]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/may-2024/may-15-support-for-graphrag-openai-gpt-4o-model-performance-improvements-and-bug-fixes]\n",
            "- File upload [216047 bytes], throughput: 2111578.511 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:58.261Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.260873, used credits [0.00063000]\n",
            "- CONTENT [f53855bb-439c-48c9-a88d-f427da2962d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1207.484 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:58.256Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.257309, used credits [0.00063000]\n",
            "- CONTENT [f53855bb-439c-48c9-a88d-f427da2962d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [315 tokens], throughput: 1224.208 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:58.105Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.523447, used credits [0.03800000]\n",
            "- CONTENT [3109dca5-a705-4191-a868-ab2b2485d8ca]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:58.073Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.052468, used credits [0.00001046]\n",
            "- CONTENT [8f1c79b1-beaf-481d-a7a7-078105781fbb]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2707 bytes], throughput: 51592.860 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:58.055Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.106047, used credits [0.00077260]\n",
            "- CONTENT [47c53407-b4d6-48e4-b91d-b9677a1028de]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-28-support-for-indexing-workflow-stage-azure-ai-language-detection-bug-fixes]\n",
            "- File upload [199947 bytes], throughput: 1885454.671 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:57.877Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.089530, used credits [0.00086751]\n",
            "- CONTENT [5d06c7c1-735d-41a3-b95e-8bce0b652102]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/may-2024/may-5-support-for-jina-and-pongo-rerankers-microsoft-teams-feed-new-youtube-downloader-bug-fixes]\n",
            "- File upload [224510 bytes], throughput: 2507642.664 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:57.852Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.680167, used credits [0.01562792]\n",
            "- CONTENT [8ca8bf83-6c04-4d64-9953-5699918fbb06]\n",
            "\n",
            "2025-01-01T20:16:57.755Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.321592, used credits [0.00070200]\n",
            "- CONTENT [8ca8bf83-6c04-4d64-9953-5699918fbb06]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [351 tokens], throughput: 1091.446 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:57.651Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:10.620468, used credits [0.01912127]\n",
            "- CONTENT [931a2349-8288-4a5f-846c-9f1476cd0946]\n",
            "\n",
            "2025-01-01T20:16:57.623Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.191094, used credits [0.00070200]\n",
            "- CONTENT [8ca8bf83-6c04-4d64-9953-5699918fbb06]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [351 tokens], throughput: 1836.789 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:57.615Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.075735, used credits [0.03800000]\n",
            "- CONTENT [e0f9a487-28bc-4d08-8b71-0dded28e8378]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:57.588Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.954719, used credits [0.03800000]\n",
            "- CONTENT [47c53407-b4d6-48e4-b91d-b9677a1028de]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:57.513Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.338427, used credits [0.00214200]\n",
            "- CONTENT [931a2349-8288-4a5f-846c-9f1476cd0946]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1071 tokens], throughput: 3164.640 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:57.428Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.818532, used credits [0.03800000]\n",
            "- CONTENT [5d06c7c1-735d-41a3-b95e-8bce0b652102]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:57.333Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.174163, used credits [0.00166800]\n",
            "- CONTENT [931a2349-8288-4a5f-846c-9f1476cd0946]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [834 tokens], throughput: 4788.609 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:56.548Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.060285, used credits [0.00001167]\n",
            "- CONTENT [f53855bb-439c-48c9-a88d-f427da2962d1]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3020 bytes], throughput: 50095.214 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:56.521Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.081819, used credits [0.00076177]\n",
            "- CONTENT [8f1c79b1-beaf-481d-a7a7-078105781fbb]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-23-support-for-linear-github-issues-and-jira-issue-feeds-ingest-files-via-web-feed-sitemap]\n",
            "- File upload [197146 bytes], throughput: 2409529.292 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:56.505Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.188804, used credits [0.00083050]\n",
            "- CONTENT [a8a2bbe9-1c58-42a7-b31c-dd4b27b74956]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2024/july-19-support-for-openai-gpt-4o-mini-byo-key-for-azure-ai-similarity-by-summary-bug-fixes]\n",
            "- File upload [214932 bytes], throughput: 1138388.104 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:56.460Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.059406, used credits [0.00001223]\n",
            "- CONTENT [8ca8bf83-6c04-4d64-9953-5699918fbb06]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3165 bytes], throughput: 53277.626 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:56.332Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.099604, used credits [0.00085215]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-10-support-for-claude-3-mistral-and-groq-models-usage-credits-telemetry-bug-fixes]\n",
            "- File upload [220537 bytes], throughput: 2214133.541 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:56.326Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.121021, used credits [0.00071804]\n",
            "- CONTENT [7fb352e7-6eda-419f-848d-d23527d330cc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/march-2024/march-13-support-for-claude-3-haiku-model-direct-ingestion-of-base64-encoded-files]\n",
            "- File upload [185828 bytes], throughput: 1535507.178 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:56.045Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.117990, used credits [0.00002728]\n",
            "- CONTENT [931a2349-8288-4a5f-846c-9f1476cd0946]: Content type [PAGE], file type [DATA]\n",
            "- File upload [7060 bytes], throughput: 59835.630 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:56.029Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.717907, used credits [0.01749628]\n",
            "- CONTENT [979a18c8-361a-4786-ac5e-355c7652e5ba]\n",
            "\n",
            "2025-01-01T20:16:55.997Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.152786, used credits [0.03800000]\n",
            "- CONTENT [8f1c79b1-beaf-481d-a7a7-078105781fbb]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:55.915Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.213048, used credits [0.00166600]\n",
            "- CONTENT [979a18c8-361a-4786-ac5e-355c7652e5ba]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [833 tokens], throughput: 3909.915 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:55.892Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.898805, used credits [0.03800000]\n",
            "- CONTENT [a8a2bbe9-1c58-42a7-b31c-dd4b27b74956]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:55.884Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.174635, used credits [0.00182200]\n",
            "- CONTENT [979a18c8-361a-4786-ac5e-355c7652e5ba]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [911 tokens], throughput: 5216.604 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:55.774Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.437707, used credits [0.03800000]\n",
            "- CONTENT [7fb352e7-6eda-419f-848d-d23527d330cc]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:55.724Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.827333, used credits [0.03800000]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:54.578Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.085756, used credits [0.00077524]\n",
            "- CONTENT [f53855bb-439c-48c9-a88d-f427da2962d1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/february-2024/february-2-support-for-semantic-alerts-openai-0125-models-performance-enhancements-bug-fixes]\n",
            "- File upload [200632 bytes], throughput: 2339554.436 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:54.389Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.527764, used credits [0.01535353]\n",
            "- CONTENT [f20064c8-0d29-4243-b163-d94ebb7913d3]\n",
            "\n",
            "2025-01-01T20:16:54.197Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.347914, used credits [0.00111000]\n",
            "- CONTENT [f20064c8-0d29-4243-b163-d94ebb7913d3]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [555 tokens], throughput: 1595.221 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:54.194Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.338064, used credits [0.00139200]\n",
            "- CONTENT [f20064c8-0d29-4243-b163-d94ebb7913d3]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [696 tokens], throughput: 2058.783 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:53.949Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.072652, used credits [0.00076453]\n",
            "- CONTENT [8ca8bf83-6c04-4d64-9953-5699918fbb06]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/june-2024/june-21-support-for-the-claude-3.5-sonnet-model-knowledge-graph-semantic-search-and-bug-fixes]\n",
            "- File upload [197859 bytes], throughput: 2723398.691 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:53.680Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:09.657390, used credits [0.01738733]\n",
            "- CONTENT [42df7e18-5180-4ee2-99f5-72f3292c9872]\n",
            "\n",
            "2025-01-01T20:16:53.678Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.078023, used credits [0.00002560]\n",
            "- CONTENT [979a18c8-361a-4786-ac5e-355c7652e5ba]: Content type [PAGE], file type [DATA]\n",
            "- File upload [6624 bytes], throughput: 84897.608 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:53.527Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.255808, used credits [0.00239200]\n",
            "- CONTENT [42df7e18-5180-4ee2-99f5-72f3292c9872]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1196 tokens], throughput: 4675.376 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:53.479Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.177377, used credits [0.00310800]\n",
            "- CONTENT [42df7e18-5180-4ee2-99f5-72f3292c9872]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [1554 tokens], throughput: 8761.001 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:53.440Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.760261, used credits [0.01577212]\n",
            "- CONTENT [1149d374-504c-443f-8fb3-ca0a475776dc]\n",
            "\n",
            "2025-01-01T20:16:53.430Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.138948, used credits [0.00100493]\n",
            "- CONTENT [931a2349-8288-4a5f-846c-9f1476cd0946]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2023/december-10-support-for-openai-gpt-4-turbo-llama-2-and-mistral-models-query-by-example-bug-fixes]\n",
            "- File upload [260075 bytes], throughput: 1871748.774 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:53.334Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.347478, used credits [0.00059400]\n",
            "- CONTENT [1149d374-504c-443f-8fb3-ca0a475776dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 854.731 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:53.331Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.322212, used credits [0.00059400]\n",
            "- CONTENT [1149d374-504c-443f-8fb3-ca0a475776dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 921.753 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:53.238Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.030768, used credits [0.03800000]\n",
            "- CONTENT [8ca8bf83-6c04-4d64-9953-5699918fbb06]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:53.163Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.919426, used credits [0.03800000]\n",
            "- CONTENT [f53855bb-439c-48c9-a88d-f427da2962d1]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:53.141Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.996226, used credits [0.01619696]\n",
            "- CONTENT [523f000e-30f7-44a0-8c75-383b18aeed8f]\n",
            "\n",
            "2025-01-01T20:16:53.006Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.338354, used credits [0.01501251]\n",
            "- CONTENT [d40294bc-55cb-4f3b-ade6-42fab2f1d379]\n",
            "\n",
            "2025-01-01T20:16:53.001Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.229563, used credits [0.00059400]\n",
            "- CONTENT [523f000e-30f7-44a0-8c75-383b18aeed8f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 1293.764 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:52.973Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.204480, used credits [0.00059400]\n",
            "- CONTENT [523f000e-30f7-44a0-8c75-383b18aeed8f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [297 tokens], throughput: 1452.461 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:52.908Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.311791, used credits [0.00046400]\n",
            "- CONTENT [d40294bc-55cb-4f3b-ade6-42fab2f1d379]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 744.088 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:52.844Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.062963, used credits [0.00002159]\n",
            "- CONTENT [f20064c8-0d29-4243-b163-d94ebb7913d3]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5588 bytes], throughput: 88750.113 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:52.804Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.742634, used credits [0.03800000]\n",
            "- CONTENT [931a2349-8288-4a5f-846c-9f1476cd0946]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:52.786Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.210814, used credits [0.00046400]\n",
            "- CONTENT [d40294bc-55cb-4f3b-ade6-42fab2f1d379]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [232 tokens], throughput: 1100.495 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:51.719Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.115655, used credits [0.00003752]\n",
            "- CONTENT [42df7e18-5180-4ee2-99f5-72f3292c9872]: Content type [PAGE], file type [DATA]\n",
            "- File upload [9709 bytes], throughput: 83948.021 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:51.552Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.342308, used credits [0.01501963]\n",
            "- CONTENT [72415f65-64c2-4aca-87e0-93aa4f58cea6]\n",
            "\n",
            "2025-01-01T20:16:51.265Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.301397, used credits [0.00146200]\n",
            "- CONTENT [72415f65-64c2-4aca-87e0-93aa4f58cea6]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [731 tokens], throughput: 2425.373 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:51.182Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.057685, used credits [0.00001074]\n",
            "- CONTENT [523f000e-30f7-44a0-8c75-383b18aeed8f]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2779 bytes], throughput: 48175.770 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:51.179Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.219872, used credits [0.00130800]\n",
            "- CONTENT [72415f65-64c2-4aca-87e0-93aa4f58cea6]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [654 tokens], throughput: 2974.452 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:51.178Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.058840, used credits [0.00000906]\n",
            "- CONTENT [d40294bc-55cb-4f3b-ade6-42fab2f1d379]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2345 bytes], throughput: 39853.638 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:51.006Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.071922, used credits [0.00001084]\n",
            "- CONTENT [1149d374-504c-443f-8fb3-ca0a475776dc]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2805 bytes], throughput: 39000.313 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:50.929Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.156763, used credits [0.00096007]\n",
            "- CONTENT [979a18c8-361a-4786-ac5e-355c7652e5ba]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2024/december-22-support-for-dropbox-box-intercom-and-zendesk-feeds-openai-o1-gemini-2.0-bug-fixes]\n",
            "- File upload [248466 bytes], throughput: 1584977.587 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:50.572Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.156556, used credits [0.00093621]\n",
            "- CONTENT [f20064c8-0d29-4243-b163-d94ebb7913d3]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/july-2023/july-15-support-for-sharepoint-feeds-new-conversation-features]\n",
            "- File upload [242290 bytes], throughput: 1547621.177 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:50.347Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.010244, used credits [0.03800000]\n",
            "- CONTENT [979a18c8-361a-4786-ac5e-355c7652e5ba]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:49.884Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.058106, used credits [0.00002153]\n",
            "- CONTENT [72415f65-64c2-4aca-87e0-93aa4f58cea6]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5572 bytes], throughput: 95893.381 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:49.702Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.802834, used credits [0.03800000]\n",
            "- CONTENT [f20064c8-0d29-4243-b163-d94ebb7913d3]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:49.692Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.095128, used credits [0.00101844]\n",
            "- CONTENT [42df7e18-5180-4ee2-99f5-72f3292c9872]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/january-2024/january-18-support-for-content-publishing-llm-tools-clip-image-embeddings-bug-fixes]\n",
            "- File upload [263571 bytes], throughput: 2770692.602 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:49.450Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.126566, used credits [0.00076624]\n",
            "- CONTENT [1149d374-504c-443f-8fb3-ca0a475776dc]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/january-2024/january-22-support-for-google-and-microsoft-email-feeds-reingest-content-in-place-bug-fixes]\n",
            "- File upload [198301 bytes], throughput: 1566779.388 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:49.363Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.478062, used credits [0.01526404]\n",
            "- CONTENT [da28adff-3f2c-4062-b78c-3581c2762768]\n",
            "\n",
            "2025-01-01T20:16:49.244Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.221888, used credits [0.00097200]\n",
            "- CONTENT [da28adff-3f2c-4062-b78c-3581c2762768]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [486 tokens], throughput: 2190.291 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:49.232Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.204885, used credits [0.00114000]\n",
            "- CONTENT [da28adff-3f2c-4062-b78c-3581c2762768]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [570 tokens], throughput: 2782.054 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:49.210Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.151348, used credits [0.03800000]\n",
            "- CONTENT [42df7e18-5180-4ee2-99f5-72f3292c9872]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:49.136Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.086198, used credits [0.00074641]\n",
            "- CONTENT [d40294bc-55cb-4f3b-ade6-42fab2f1d379]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-20-support-for-medical-entities-anthropic-prompt-caching-bug-fixes]\n",
            "- File upload [193170 bytes], throughput: 2241008.471 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:49.066Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.128234, used credits [0.00075873]\n",
            "- CONTENT [523f000e-30f7-44a0-8c75-383b18aeed8f]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-9-support-direct-text-markdown-and-html-ingestion-new-specification-llm-strategy]\n",
            "- File upload [196358 bytes], throughput: 1531252.339 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:48.818Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.107733, used credits [0.03800000]\n",
            "- CONTENT [1149d374-504c-443f-8fb3-ca0a475776dc]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:48.647Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.954610, used credits [0.03800000]\n",
            "- CONTENT [d40294bc-55cb-4f3b-ade6-42fab2f1d379]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:48.305Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.136676, used credits [0.03800000]\n",
            "- CONTENT [523f000e-30f7-44a0-8c75-383b18aeed8f]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:48.086Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.093876, used credits [0.00085539]\n",
            "- CONTENT [72415f65-64c2-4aca-87e0-93aa4f58cea6]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/february-2024/february-21-support-for-onedrive-and-google-drive-feeds-extract-images-from-pdfs-bug-fixes]\n",
            "- File upload [221375 bytes], throughput: 2358151.401 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:48.084Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.067523, used credits [0.01452490]\n",
            "- CONTENT [c5dc4da1-aa86-4d89-93f2-d15ec179a24d]\n",
            "\n",
            "2025-01-01T20:16:47.928Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.305604, used credits [0.00126800]\n",
            "- CONTENT [c5dc4da1-aa86-4d89-93f2-d15ec179a24d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [634 tokens], throughput: 2074.581 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:47.844Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.225588, used credits [0.00107200]\n",
            "- CONTENT [c5dc4da1-aa86-4d89-93f2-d15ec179a24d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [536 tokens], throughput: 2376.009 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:47.843Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.060226, used credits [0.00001635]\n",
            "- CONTENT [da28adff-3f2c-4062-b78c-3581c2762768]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4231 bytes], throughput: 70252.634 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:47.466Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.230020, used credits [0.03800000]\n",
            "- CONTENT [72415f65-64c2-4aca-87e0-93aa4f58cea6]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:46.816Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.903942, used credits [0.01603081]\n",
            "- CONTENT [3ed53e18-c1a5-41f7-9a6e-ef900e439b95]\n",
            "\n",
            "2025-01-01T20:16:46.534Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.287807, used credits [0.00087000]\n",
            "- CONTENT [3ed53e18-c1a5-41f7-9a6e-ef900e439b95]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 1511.431 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:46.425Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.182500, used credits [0.00087000]\n",
            "- CONTENT [3ed53e18-c1a5-41f7-9a6e-ef900e439b95]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [435 tokens], throughput: 2383.562 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:46.197Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.502279, used credits [0.01350723]\n",
            "- CONTENT [8c9d843f-4a50-4c40-aab7-9dbeb3472396]\n",
            "\n",
            "2025-01-01T20:16:46.162Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.065704, used credits [0.00001633]\n",
            "- CONTENT [c5dc4da1-aa86-4d89-93f2-d15ec179a24d]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4226 bytes], throughput: 64318.763 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:46.076Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.360418, used credits [0.00039400]\n",
            "- CONTENT [8c9d843f-4a50-4c40-aab7-9dbeb3472396]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 546.588 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:45.830Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.112878, used credits [0.00039400]\n",
            "- CONTENT [8c9d843f-4a50-4c40-aab7-9dbeb3472396]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [197 tokens], throughput: 1745.241 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:45.738Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.308521, used credits [0.00081789]\n",
            "- CONTENT [da28adff-3f2c-4062-b78c-3581c2762768]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-3-new-data-model-for-observations-new-category-entity]\n",
            "- File upload [211669 bytes], throughput: 686075.807 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:44.843Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.921118, used credits [0.03800000]\n",
            "- CONTENT [da28adff-3f2c-4062-b78c-3581c2762768]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:44.520Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.573349, used credits [0.01543560]\n",
            "- CONTENT [ce9f8f91-d948-421a-8cda-3959e0f9c5f6]\n",
            "\n",
            "2025-01-01T20:16:44.467Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.072114, used credits [0.00001454]\n",
            "- CONTENT [3ed53e18-c1a5-41f7-9a6e-ef900e439b95]: Content type [PAGE], file type [DATA]\n",
            "- File upload [3764 bytes], throughput: 52194.846 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:44.402Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.381796, used credits [0.00052400]\n",
            "- CONTENT [ce9f8f91-d948-421a-8cda-3959e0f9c5f6]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 686.230 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:44.267Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.060226, used credits [0.00000795]\n",
            "- CONTENT [8c9d843f-4a50-4c40-aab7-9dbeb3472396]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2058 bytes], throughput: 34171.175 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:44.264Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.242718, used credits [0.00052400]\n",
            "- CONTENT [ce9f8f91-d948-421a-8cda-3959e0f9c5f6]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [262 tokens], throughput: 1079.440 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:44.097Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:12.533727, used credits [0.02256593]\n",
            "- CONTENT [c51c1b7e-b854-44a8-90c1-bc2d23043aa1]\n",
            "\n",
            "2025-01-01T20:16:44.010Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.105093, used credits [0.00081308]\n",
            "- CONTENT [c5dc4da1-aa86-4d89-93f2-d15ec179a24d]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2024/december-9-support-for-website-mapping-web-page-screenshots-groq-llama-3.3-model-bug-fixes]\n",
            "- File upload [210424 bytes], throughput: 2002262.756 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:44.002Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:11.310760, used credits [0.02036408]\n",
            "- CONTENT [79e64fc7-44b1-47b3-9b24-c8aa55dd502a]\n",
            "\n",
            "2025-01-01T20:16:43.930Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.495252, used credits [0.00124200]\n",
            "- CONTENT [c51c1b7e-b854-44a8-90c1-bc2d23043aa1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [621 tokens], throughput: 1253.908 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:43.851Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.415678, used credits [0.00143000]\n",
            "- CONTENT [c51c1b7e-b854-44a8-90c1-bc2d23043aa1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [715 tokens], throughput: 1720.081 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:43.838Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.403204, used credits [0.00155000]\n",
            "- CONTENT [79e64fc7-44b1-47b3-9b24-c8aa55dd502a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [775 tokens], throughput: 1922.106 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:43.773Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.335321, used credits [0.00137200]\n",
            "- CONTENT [79e64fc7-44b1-47b3-9b24-c8aa55dd502a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [686 tokens], throughput: 2045.801 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:43.355Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.307529, used credits [0.03800000]\n",
            "- CONTENT [c5dc4da1-aa86-4d89-93f2-d15ec179a24d]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:42.936Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.064035, used credits [0.00000946]\n",
            "- CONTENT [ce9f8f91-d948-421a-8cda-3959e0f9c5f6]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2447 bytes], throughput: 38213.417 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:42.712Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.230252, used credits [0.00080836]\n",
            "- CONTENT [3ed53e18-c1a5-41f7-9a6e-ef900e439b95]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-8-support-for-llm-based-document-extraction-.net-sdk-bug-fixes]\n",
            "- File upload [209202 bytes], throughput: 908579.216 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:42.489Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.092871, used credits [0.00074101]\n",
            "- CONTENT [8c9d843f-4a50-4c40-aab7-9dbeb3472396]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/december-2024/december-1-support-for-retrieval-only-rag-pipeline-bug-fixes]\n",
            "- File upload [191773 bytes], throughput: 2064935.093 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:42.209Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.061502, used credits [0.00002253]\n",
            "- CONTENT [79e64fc7-44b1-47b3-9b24-c8aa55dd502a]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5830 bytes], throughput: 94794.436 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:42.201Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.057962, used credits [0.00001947]\n",
            "- CONTENT [c51c1b7e-b854-44a8-90c1-bc2d23043aa1]: Content type [PAGE], file type [DATA]\n",
            "- File upload [5039 bytes], throughput: 86937.019 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:42.035Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.315251, used credits [0.03800000]\n",
            "- CONTENT [8c9d843f-4a50-4c40-aab7-9dbeb3472396]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:42.033Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:04.085348, used credits [0.03800000]\n",
            "- CONTENT [3ed53e18-c1a5-41f7-9a6e-ef900e439b95]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:40.796Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:08.355536, used credits [0.01504345]\n",
            "- CONTENT [b47614fd-41f3-491b-bbb4-d3976687e131]\n",
            "\n",
            "2025-01-01T20:16:40.658Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:07.323527, used credits [0.01318540]\n",
            "- CONTENT [fbfc6199-ba6c-4e4d-a050-4bc72d7faeaf]\n",
            "\n",
            "2025-01-01T20:16:40.558Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.494515, used credits [0.00120000]\n",
            "- CONTENT [b47614fd-41f3-491b-bbb4-d3976687e131]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [600 tokens], throughput: 1213.310 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:40.539Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.265072, used credits [0.00048400]\n",
            "- CONTENT [fbfc6199-ba6c-4e4d-a050-4bc72d7faeaf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 912.960 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:40.450Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.177436, used credits [0.00048400]\n",
            "- CONTENT [fbfc6199-ba6c-4e4d-a050-4bc72d7faeaf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [242 tokens], throughput: 1363.871 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:40.300Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.241630, used credits [0.00098000]\n",
            "- CONTENT [b47614fd-41f3-491b-bbb4-d3976687e131]: Content type [PAGE], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [490 tokens], throughput: 2027.892 tokens/sec\n",
            "\n",
            "2025-01-01T20:16:40.239Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.252825, used credits [0.00073952]\n",
            "- CONTENT [ce9f8f91-d948-421a-8cda-3959e0f9c5f6]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2024/august-11-support-for-azure-ai-document-intelligence-by-default-language-aware-summaries]\n",
            "- File upload [191388 bytes], throughput: 756997.025 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:39.499Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.506907, used credits [0.03800000]\n",
            "- CONTENT [ce9f8f91-d948-421a-8cda-3959e0f9c5f6]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:39.042Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.058443, used credits [0.00000811]\n",
            "- CONTENT [fbfc6199-ba6c-4e4d-a050-4bc72d7faeaf]: Content type [PAGE], file type [DATA]\n",
            "- File upload [2098 bytes], throughput: 35898.103 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:38.914Z: Upload Mezzanine\n",
            "- Workflow [Preparation] took 0:00:00.073416, used credits [0.00001627]\n",
            "- CONTENT [b47614fd-41f3-491b-bbb4-d3976687e131]: Content type [PAGE], file type [DATA]\n",
            "- File upload [4211 bytes], throughput: 57357.757 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:38.305Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.135807, used credits [0.00087650]\n",
            "- CONTENT [79e64fc7-44b1-47b3-9b24-c8aa55dd502a]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/april-2024/april-7-support-for-discord-feeds-cohere-reranking-section-aware-chunking-and-retrieval]\n",
            "- File upload [226838 bytes], throughput: 1670295.588 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:37.747Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.021661, used credits [0.03800000]\n",
            "- CONTENT [79e64fc7-44b1-47b3-9b24-c8aa55dd502a]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:37.637Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.306790, used credits [0.00083556]\n",
            "- CONTENT [c51c1b7e-b854-44a8-90c1-bc2d23043aa1]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/]\n",
            "- File upload [216242 bytes], throughput: 704852.334 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:36.955Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.119107, used credits [0.00071630]\n",
            "- CONTENT [fbfc6199-ba6c-4e4d-a050-4bc72d7faeaf]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/august-2023/august-17-prepare-for-usage-based-billing-append-sas-tokens-to-uris]\n",
            "- File upload [185378 bytes], throughput: 1556402.788 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:36.837Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:05.180943, used credits [0.03800000]\n",
            "- CONTENT [c51c1b7e-b854-44a8-90c1-bc2d23043aa1]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:36.700Z: Upload Master\n",
            "- Workflow [Ingestion] took 0:00:00.428233, used credits [0.00082775]\n",
            "- CONTENT [b47614fd-41f3-491b-bbb4-d3976687e131]: Content type [PAGE], file type [DOCUMENT]\n",
            "- URI [https://changelog.graphlit.dev/april-2024/april-23-support-for-python-and-typescript-sdks-latest-openai-cohere-and-groq-models-bug-fixes]\n",
            "- File upload [214221 bytes], throughput: 500243.792 bytes/sec\n",
            "\n",
            "2025-01-01T20:16:36.437Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.076634, used credits [0.03800000]\n",
            "- CONTENT [fbfc6199-ba6c-4e4d-a050-4bc72d7faeaf]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:35.793Z: Web capture\n",
            "- Workflow [Preparation] took 0:00:03.309376, used credits [0.03800000]\n",
            "- CONTENT [b47614fd-41f3-491b-bbb4-d3976687e131]\n",
            "- Processor name [Browserless], units [1]\n",
            "\n",
            "2025-01-01T20:16:11.223Z: GraphQL\n",
            "- Operation took 0:00:00.648067, used credits [0.00000000]\n",
            "- Request:\n",
            "mutation CreateFeed($feed: FeedInput!, $correlationId: String) { createFeed(feed: $feed, correlationId: $correlationId) { id name state type } }\n",
            "- Variables:\n",
            "{\"feed\":\"{ name: \\\"https:\\\\/\\\\/changelog.graphlit.dev\\\", type: WEB, web: { uri: \\\"https:\\\\/\\\\/changelog.graphlit.dev\\\", readLimit: 100 } }\",\"correlationId\":\"\\\"2025-01-01T20:16:10.555475\\\"\"}\n",
            "- Response:\n",
            "{\"data\":{\"createFeed\":{\"id\":\"a0d5de6a-9901-42e2-b016-839e5c7c8408\",\"name\":\"https://changelog.graphlit.dev\",\"state\":\"ENABLED\",\"type\":\"WEB\"}}}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "credits = await lookup_credits(publish_correlation_id)\n",
        "\n",
        "if credits is not None:\n",
        "    display(Markdown(f\"### Credits used: {credits.credits:.6f} for publishing\"))\n",
        "    print(f\"- storage [{credits.storage_ratio:.2f}%], compute [{credits.compute_ratio:.2f}%]\")\n",
        "    print(f\"- embedding [{credits.embedding_ratio:.2f}%], completion [{credits.completion_ratio:.2f}%]\")\n",
        "    print(f\"- ingestion [{credits.ingestion_ratio:.2f}%], indexing [{credits.indexing_ratio:.2f}%], preparation [{credits.preparation_ratio:.2f}%], extraction [{credits.extraction_ratio:.2f}%], enrichment [{credits.enrichment_ratio:.2f}%], publishing [{credits.publishing_ratio:.2f}%]\")\n",
        "    print(f\"- search [{credits.search_ratio:.2f}%], conversation [{credits.conversation_ratio:.2f}%]\")\n",
        "    print()\n",
        "\n",
        "usage = await lookup_usage(publish_correlation_id)\n",
        "\n",
        "if usage is not None:\n",
        "    display(Markdown(f\"### Usage records:\"))\n",
        "\n",
        "    for record in usage:\n",
        "        dump_usage_record(record)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0nqb7p9Rrr1b",
        "outputId": "1bcb8c73-2927-47e4-f52c-074b39d495ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Credits used: 22.229361 for publishing"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- storage [0.00%], compute [0.04%]\n",
            "- embedding [0.13%], completion [99.70%]\n",
            "- ingestion [0.00%], indexing [0.00%], preparation [0.00%], extraction [0.00%], enrichment [0.00%], publishing [0.00%]\n",
            "- search [0.13%], conversation [0.00%]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Usage records:"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "## October 2024\n",
            "October was a pivotal month with the support for **tool calling across Anthropic and Gemini models**, and the introduction of **GitHub repository feeds**. These updates provided developers with enhanced conversation management and deeper integrations with popular development platforms.\n",
            "\n",
            "## November 2024\n",
            "In November, we focused on expanding interactive capabilities with **multi-turn content summarization** and **direct LLM prompts**. The addition of **image description** features and support for the **Mistral Pixtral Large model** enriched our AI offerings, enabling more dynamic and versatile data interactions.\n",
            "\n",
            "## December 2024\n",
            "We concluded the year with a suite of powerful features, including **retrieval-only RAG pipelines** and **website mapping** with screenshot generation. Support for **Dropbox, Box, Intercom, and Zendesk feeds**, alongside the latest **Groq Llama 3.3 models**, ensured comprehensive data integration and improved API interactions for our users.\n",
            "\n",
            "## Looking Ahead\n",
            "As we celebrate a year of significant growth and innovation, we extend our heartfelt gratitude to our dedicated users and developers. Your support has been instrumental in driving our advancements, and we're excited to continue evolving with even more groundbreaking features and integrations in the coming year. Stay tuned for what's next as we strive to empower your projects with the best tools and technologies.\n",
            "\n",
            "Thank you for being a part of the Graphlit community!\n",
            "\n",
            "2025-01-01T20:22:31.448Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.926158, used credits [0.00665700]\n",
            "- CONTENT [b47614fd-41f3-491b-bbb4-d3976687e131]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [879 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/april-2024/april-23-support-for-python-and-typescript-sdks-latest-openai-cohere-and-groq-models-bug-fixes</name><title>April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes | Graphlit Changelog</title></metadata> 🐇\tApril 2024\n",
            "April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports a native Python SDK, using Pydantic types. The Python SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest PyPi package here.  The Streamlit sample applications have been updated to use the new Python SDK.\n",
            "💡 Graphlit now supports a native Node.js SDK, using TypeScript types. The Node.js SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest NPM package here.\n",
            "💡 Graphlit now supports the 2024-04-09 models in the OpenAI model service. GPT4_TURBO-128K will give the latest OpenAI GPT-4 model, following this model list.  We have added the GPT4_TURBO_128K_2024_04_09 enum to specify the new model.\n",
            "💡 Graphlit now supports LLaMA3 70b, LLaMA3 8b and Gemma 7b models in the Groq model service.\n",
            "💡 Graphlit now supports the Command R and Command-R+ models in the Cohere model service.\n",
            "Added support for Jina reranking, using the JINA reranking model service type in the reranking retrieval strategy.\n",
            "Updated the Cohere reranking model to use the latest v3.0 model.\n",
            "Increased the reliability of parsing LLM responses, in cases where they don't follow the JSON schema.\n",
            "⚡ Cleaned up nullability of GraphQL parameters, so parameters better reflect if they are required or optional, or allow nulls.\n",
            "⚡ Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "⚡ Split out reranking model service type as RetrievalModelServiceTypes enum.\n",
            "Bugs Fixed\n",
            "GPLA-2114: Adding content to collections not syncing search index\n",
            "GPLA-2511: Failing to render any conversation sources with section retrieval and text content\n",
            "PreviousMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "NextApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "Last updated8 months ago\n",
            "- Completion [335 tokens (includes JSON guardrails tokens)], throughput: 68.004 tokens/sec:\n",
            "- New Features:\n",
            "  - Native Python SDK introduced, using Pydantic types; code-generated from GraphQL schema, no GraphQL knowledge required.\n",
            "  - Native Node.js SDK introduced, using TypeScript types; code-generated from GraphQL schema, no GraphQL knowledge required.\n",
            "  - Support for OpenAI's 2024-04-09 models, including GPT4_TURBO-128K.\n",
            "  - Support for LLaMA3 70b, LLaMA3 8b, and Gemma 7b models in Groq model service.\n",
            "  - Support for Command R and Command-R+ models in Cohere model service.\n",
            "  - Added support for Jina reranking with the JINA reranking model service type.\n",
            "  - Updated Cohere reranking model to the latest v3.0 model.\n",
            "  - Improved reliability of parsing LLM responses that do not follow JSON schema.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Cleaned up nullability of GraphQL parameters for better clarity on required, optional, or nullable parameters.\n",
            "  - Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "  - Split reranking model service type into RetrievalModelServiceTypes enum.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where adding content to collections did not sync with the search index (GPLA-2114).\n",
            "  - Resolved rendering issues with conversation sources in section retrieval and text content (GPLA-2511).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: April 2024.\n",
            "  \n",
            "- Value:\n",
            "  - Offers developers enhanced SDK support, improved model integration, and better handling of data parsing and GraphQL parameters.\n",
            "\n",
            "2025-01-01T20:22:31.363Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:05.160947, used credits [0.00730800]\n",
            "- CONTENT [79e64fc7-44b1-47b3-9b24-c8aa55dd502a]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1068 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/april-2024/april-7-support-for-discord-feeds-cohere-reranking-section-aware-chunking-and-retrieval</name><title>April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval | Graphlit Changelog</title></metadata> 🐇\tApril 2024\n",
            "April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "New Features\n",
            "💡 Graphlit now supports Discord feeds.  By connecting to a Discord channel and providing a bot token, you can ingest all Discord messages and file attachments.\n",
            "💡  Graphlit now supports Cohere reranking after content retrieval in RAG pipeline.  You can optionally use the Cohere rerank model to semantically rerank the semantic search results, before providing as context to the LLM.\n",
            "Added support for section-aware text chunking and retrieval.  Now, when using section-aware document preparation, such as Azure AI Document Intelligence, Graphlit will store the extracted text according to the semantic chunks (i.e. sections).  The text for each section will be individually chunked and embedded into the vector index.\n",
            "Added support for retrievalStrategy in Specification type. Graphlit now supports CHUNK, SECTION and CONTENT retrieval strategies.  Chunk retrieval will use the search hit chunk, section retrieval will expand the search hit chunk to the containing section (or page, if not using section-aware preparation).  Content retrieval will expand the search hit chunk to the text of the entire document.\n",
            "Added support for rerankingStrategy in Specification type. You can now configure the reranking of content sources, using the Cohere reranking model, by assigning serviceType to COHERE.  More reranking models are planned for the future.\n",
            "Added isSynchronous flag to content ingestion mutations, such as ingestUri, so the mutation will wait for the content to complete the ingestion workflow (or error) before returning.  This is useful for utilizing the API in a Jupyter notebook or Streamlit application, in a synchronous manner without polling.\n",
            "Added includeAttachments flag to SlackFeedProperties.  When enabled, Graphlit will automatically ingest any attachments within Slack messages.\n",
            "⚡ Added ingestUri mutation to replace the now deprecated ingestPage and ingestFile mutations.  We had seen confusion on when to use one vs the other, and now for any URI, whether it is a web page or hosted PDF, you can pass it to ingestUri, and we will infer the correct content ingestion workflow.\n",
            "⚡ Removed includeSummaries from the ConversationStrategyInput type.  This will re-added in the future as part of the retrieval strategy.\n",
            "⚡ Deprecated enableExpandedRetrieval in ConversationStrategyInput type.  This is now handled by setting strategyType to SECTION or CONTENT in the RetrievalStrategyInput type.\n",
            "⚡ Moved contentLimit from ConversationStrategyInput type to RetrievalStrategyInput type. You can optionally assign the contentLimit to retrievalStrategy which limits the number of content sources leveraged in the LLM prompt context. (Default is 100.)\n",
            "Bugs Fixed\n",
            "GPLA-2469: Failed to ingest PDF hosted on GitHub\n",
            "GPLA-2390: Claude 3 Haiku not adhering to JSON schema\n",
            "GPLA-2474: Prompt rewriting should ignore formatting instructions in prompt\n",
            "GPLA-2462: Missing line break after table rows\n",
            "GPLA-2417: Not extracting images from PPTX correctly\n",
            "PreviousApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "NextMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "Last updated8 months ago\n",
            "- Completion [342 tokens (includes JSON guardrails tokens)], throughput: 66.267 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Discord feeds: Ingest messages and file attachments from a Discord channel using a bot token.\n",
            "  - Cohere reranking: Optionally rerank semantic search results in the RAG pipeline using the Cohere rerank model.\n",
            "  - Section-aware text chunking and retrieval: Store extracted text according to semantic chunks for improved indexing.\n",
            "  - Retrieval strategies: Added CHUNK, SECTION, and CONTENT retrieval strategies for more flexible content retrieval.\n",
            "  - Reranking strategy: Configure reranking of content sources using the Cohere model.\n",
            "  - Synchronous content ingestion: Added isSynchronous flag for content ingestion mutations to wait for completion.\n",
            "  - Slack attachments: Added includeAttachments flag to automatically ingest attachments in Slack messages.\n",
            "  - IngestUri mutation: Replaces deprecated ingestPage and ingestFile mutations for simplified content ingestion.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Removed includeSummaries from ConversationStrategyInput; will be re-added later.\n",
            "  - Deprecated enableExpandedRetrieval; now managed by strategyType in RetrievalStrategyInput.\n",
            "  - Moved contentLimit to RetrievalStrategyInput for better content source management.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with PDF ingestion from GitHub.\n",
            "  - Resolved JSON schema adherence for Claude 3 Haiku.\n",
            "  - Prompt rewriting now ignores formatting instructions.\n",
            "  - Corrected missing line breaks after table rows.\n",
            "  - Fixed image extraction from PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: April 7, 2024.\n",
            "  \n",
            "- Value:\n",
            "  - Offers developers enhanced content ingestion capabilities, improved retrieval strategies, and better integration with messaging platforms, streamlining workflows and increasing efficiency.\n",
            "\n",
            "2025-01-01T20:22:30.869Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.336548, used credits [0.00598200]\n",
            "- CONTENT [c51c1b7e-b854-44a8-90c1-bc2d23043aa1]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [974 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/</name><title>December 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports LLM fallbacks which can help protect your application from model provider downtime.  By assigning the fallbacksproperty when creating your conversation, you can provide an optional list of LLM specifications to be used (in order).  These fallback specifications will only be used when we failed to prompt the conversation via the main specification.  Caveat, the RAG pipeline will only use the strategies provided in the main specification for prompt rewriting, content retrieval, etc.  Content is not re-retrieved upon fallback - the formatted LLM prompt will be tried against each fallback specification in succession until one succeeds. (Colab Notebook Example)\n",
            "💡 Graphlit now supports querying of all available models, through the new modelsquery in the API.  This returns the model enum, model service type enum, description, and several other useful details about the models.\n",
            "Graphlit now supports the ingestion of native Google Docs, Google Sheets and Google Slides documents from Google Drive feeds.  These formats will be auto-exported to the corresponding Microsoft Office format (DOCX, XLSX, PPTX) prior to ingesting as content.\n",
            "Graphlit now supports unblocking of websites, such as those using Cloudflare.  You can set enableUnblockedCaptureto true on the PreparationWorkflowStageto enable unblocking - through our integration with Browserless.io headless browser service.  This does incur an additional cost per page, compared to normal web page ingestion.\n",
            "We have added support for assigning observations to contents ingested via feeds.  By assigning observationsto the IngestionWorkflowStagein workflow object, you can assign Labels, Organizations, etc. without needing to use entity extraction.\n",
            "We have added support for assigning observations when ingesting content via ingestUri, ingestText, etc. mutations. By passing observationsas a parameter, similar to `collections`, you can assign Labels, Organizations, etc. without needing to use entity extraction.\n",
            "⚡ We have changed the response type of the publishContentsmutation to return PublishContentstype.  This new PublishContentstype wraps the published Contentobject, and includes the new Detailsproperty of PublishingDetailstype. We have added an includeDetailsparameter to publishContentsmutation, which will fill in the Details property with a list of intermediate content summaries and the published text, among other publishing metrics.\n",
            "⚡ We have changed the behavior of publishContentssuch that, if no content was retrieved for publishing, the mutation returns a null content object rather than returning an error.\n",
            "Bugs Fixed\n",
            "GPLA-3645: Table headers merged together on web scrape\n",
            "GPLA-3634: Failed to extract pages from PDF with empty hyperlink text\n",
            "GPLA-3633: Not handling empty observables properly for reranking\n",
            "NextDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "Last updated3 days ago\n",
            "- Completion [255 tokens (includes JSON guardrails tokens)], throughput: 76.426 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for LLM fallbacks to protect against model provider downtime.\n",
            "  - New API query for all available models, providing details like model enum and service type.\n",
            "  - Ingestion of native Google Docs, Sheets, and Slides from Google Drive, auto-exported to Microsoft Office formats.\n",
            "  - Website unblocking support via Browserless.io integration, with additional costs per page.\n",
            "  - Ability to assign observations (Labels, Organizations) to ingested content without entity extraction.\n",
            "  - Enhanced content publishing response type with new details and metrics.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Changed response type of publishContents mutation to include new details.\n",
            "  - Updated behavior of publishContents to return null for no content instead of an error.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed merged table headers on web scrape (GPLA-3645).\n",
            "  - Resolved issues with extracting pages from PDFs with empty hyperlink text (GPLA-3634).\n",
            "  - Improved handling of empty observables for reranking (GPLA-3633).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 2024.\n",
            "  \n",
            "- Value:\n",
            "  - Offers developers improved content ingestion, enhanced model querying, and better error handling, increasing application reliability and usability.\n",
            "\n",
            "2025-01-01T20:22:29.310Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.366690, used credits [0.00565500]\n",
            "- CONTENT [da28adff-3f2c-4062-b78c-3581c2762768]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [861 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-3-new-data-model-for-observations-new-category-entity</name><title>August 3: New data model for Observations, new Category entity | Graphlit Changelog</title></metadata> 🎂\tAugust 2023\n",
            "August 3: New data model for Observations, new Category entity\n",
            "New Features\n",
            "💡 Revised data model for Observations, Occurrences and observables (i.e. Person, Organization).  Now after entity extraction, content will have one Observation for each observed entity, and a list of occurrences.  Occurrence now supports text, time and image occurrence types.  (Text: page index, time: start/end timestamp, image: bounding box)  Observations now have ObservableType and Observable fields, which specify the observed entity type and entity reference.\n",
            "💡 Added Category entity to GraphQL data model, which supports PII categories such as Phone Number or Credit Card Number.\n",
            "Added probability field to model properties, for the LLM's token probability.  (See OpenAI documentation for more detail.)\n",
            "Added error field to feeds.  If a feed fails to read from the data source, and is marked as ERRORED state, the error field will have the error description.\n",
            "Support reingestion of changed files from feeds.  For feeds, such as SharePoint or Web, where we can recognize that a file or page was updated, we will now reingest the content in-place.  Content will keep the same ID, and will restart the content workflow by re-downloading the updated content from the data source.   Existing observations will be deleted, and new observations will be created from the updated content.\n",
            "ℹ️ Ingestion of content is now idempotent, meaning if you ingest content again from the same URI, we will reingest the content in-place, while keeping the same ID.  (If we can recognize the content has not changed, such as by ETag, we will return the existing content object.)\n",
            "ℹ️ Changed GraphQL data type of SharePoint tenantId, libraryId and siteId to ID rather than String.\n",
            "✨ Performance optimization of entity extraction, and the creation of observations.\n",
            "Bugs Fixed\n",
            "GPLA-1130: Only was extracting text from first column of PDF tables.\n",
            "GPLA-1140: Text from DOCX tables was not extracted properly.\n",
            "GPLA-1154: Audio content ingested from RSS feed was not deleted when feed was deleted.\n",
            "PreviousAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "NextJuly 15: Support for SharePoint feeds, new Conversation features\n",
            "Last updated1 year ago\n",
            "- Completion [256 tokens (includes JSON guardrails tokens)], throughput: 58.626 tokens/sec:\n",
            "- New Features:\n",
            "  - Revised data model for Observations, Occurrences, and observables (e.g., Person, Organization).\n",
            "  - Each observed entity now has one Observation and a list of occurrences (supports text, time, and image types).\n",
            "  - Added Category entity to GraphQL data model for PII categories (e.g., Phone Number, Credit Card Number).\n",
            "  - Introduced probability field for LLM's token probability.\n",
            "  - Added error field to feeds for error descriptions on failed reads.\n",
            "  - Support for reingestion of changed files from feeds, maintaining the same ID.\n",
            "  - Ingestion of content is now idempotent, allowing reingestion without ID changes.\n",
            "  - Changed GraphQL data type for SharePoint identifiers from String to ID.\n",
            "  - Performance optimization for entity extraction and observation creation.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of content ingestion and updates.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed extraction issues from PDF and DOCX tables.\n",
            "  - Resolved issue where audio content from RSS feeds was not deleted upon feed deletion.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved data handling, enhanced entity extraction, and better error management.\n",
            "\n",
            "2025-01-01T20:22:27.510Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.649248, used credits [0.00332400]\n",
            "- CONTENT [fbfc6199-ba6c-4e4d-a050-4bc72d7faeaf]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [616 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-17-prepare-for-usage-based-billing-append-sas-tokens-to-uris</name><title>August 17: Prepare for usage-based billing; append SAS tokens to URIs | Graphlit Changelog</title></metadata> 🎂\tAugust 2023\n",
            "August 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "New Features\n",
            "ℹ️ Behind the scenes, Graphlit is preparing to launch usage-based billing.  This release put in place the infrastructure to track billable events.  Organizations now have a Stripe customer associated with them, and Graphlit projects are auto-subscribed to a Free/Hobby pricing plan.  In a future release, we will provide the ability to upgrade to a paid plan in the Graphlit Developer Portal.  Also, we will provide visualization of usage, on granular basis, in the Portal.\n",
            "💡 Content URIs now have Shared Access Signature (SAS) token appended, so they are accessible after query.  For example, content.transcriptUri will now be able to be downloaded or used directly in an application (until the SAS token expires).\n",
            "🧱 Added more robustness for error handling and retries, especially for LLM APIs and audio transcription APIs.\n",
            "PreviousSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "NextAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "Last updated1 year ago\n",
            "- Completion [123 tokens (includes JSON guardrails tokens)], throughput: 74.579 tokens/sec:\n",
            "- New Features:\n",
            "  - Infrastructure for usage-based billing implemented; organizations now have a Stripe customer and auto-subscribed to a Free/Hobby pricing plan.\n",
            "  - Content URIs now include Shared Access Signature (SAS) tokens for direct access after queries.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Improved error handling and retries for LLM APIs and audio transcription APIs.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 17, 2023.\n",
            "  \n",
            "- Value:\n",
            "  - Enables organizations to track billable events and access content more easily, enhancing overall user experience and operational efficiency.\n",
            "\n",
            "2025-01-01T20:22:26.356Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.863704, used credits [0.00413100]\n",
            "- CONTENT [ce9f8f91-d948-421a-8cda-3959e0f9c5f6]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [629 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-11-support-for-azure-ai-document-intelligence-by-default-language-aware-summaries</name><title>August 11: Support for Azure AI Document Intelligence by default, language-aware summaries | Graphlit Changelog</title></metadata> 🎂\tAugust 2024\n",
            "August 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "New Features\n",
            "Added support for language-aware summaries when using LLM-based document extraction.  Now the summaries for tables and sections generated by the LLM will follow the language of the source text.\n",
            "Added support for language-aware entity descriptions with using LLM-based entity extraction. Now the entity descriptions generated by the LLM will follow the language of the source text.\n",
            "⚡ We have changed the default document preparation method to use Azure AI Document Intelligence, rather than our built-in document parsers.  We have found that the fidelity of Azure AI is considerably better for complex PDFs, and provides better support for table extraction, so we have made this the default. Note: this does come with increased credit usage per-page, for PDF, DOCX and PPTX documents, but the quality of the extracted documents are noticeably higher for use in RAG pipelines.\n",
            "Bugs Fixed\n",
            "GPLA-3070: Not getting slide count assigned to metadata for PPTX files.\n",
            "PreviousAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "NextAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "Last updated4 months ago\n",
            "- Completion [187 tokens (includes JSON guardrails tokens)], throughput: 65.300 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for language-aware summaries in LLM-based document extraction, aligning summaries with the source text language.\n",
            "  - Support for language-aware entity descriptions in LLM-based entity extraction, matching descriptions to the source text language.\n",
            "  - Default document preparation method changed to Azure AI Document Intelligence for improved fidelity in complex PDFs and better table extraction.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Increased credit usage per page for PDF, DOCX, and PPTX documents due to the switch to Azure AI, but with significantly higher quality in extracted documents for RAG pipelines.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3070 where slide count was not assigned to metadata for PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 11, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced document extraction capabilities with improved accuracy and language support, facilitating better integration in applications.\n",
            "\n",
            "2025-01-01T20:22:26.117Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:07.267726, used credits [0.00834000]\n",
            "- CONTENT [931a2349-8288-4a5f-846c-9f1476cd0946]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1224 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2023/december-10-support-for-openai-gpt-4-turbo-llama-2-and-mistral-models-query-by-example-bug-fixes</name><title>December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2023\n",
            "December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the OpenAI GPT-4 Turbo 128k model, both in Azure OpenAI and native OpenAI services.  Added new model enum GPT4_TURBO_VISION_128K.\n",
            "💡 Graphlit now supports Llama 2 7b, 13b, 70b models and Mistral 7b model, via Replicate.  Developers can use their own Replicate API key, or be charged as credits for Graphlit usage.\n",
            "💡 Graphlit now supports the Anthropic Claude 2.1 model. Added new model enum CLAUDE_2_1.\n",
            "💡 Graphlit now supports the OpenAI GPT-4 Vision model for image descriptions and text extraction.  Added new model enum GPT4_TURBO_VISION_128K. See usage example in \"Multimodal RAG\" blog post.\n",
            "Added query by example to contents query.  Developers can specify one or more example contents, and query will use vector embeddings to return similar contents.\n",
            "Added query by example to conversations query.  Developers can specify one or more example conversations, and query will use vector embeddings to return similar conversations.\n",
            "Added vector search support for conversations queries.  Developers can provide search text which will use vector embeddings to return similar conversations.\n",
            "Added promptSpecifications mutation for directly prompting multiple models.  This can be used to evaluate prompts against multiple models or compare different specification parameters in parallel.\n",
            "Added promptStrategy field to Specification, which supports multiple strategy types for preprocessing the prompt before being sent to the LLM model.  For example, REWRITE prompt strategy will ask LLM to rewrite the incoming user prompt based on the previous conversation messages.\n",
            "Added suggestConversation mutation, which returns a list of suggested followup questions based on the specified conversation and related contents.  This can be used to auto-suggest questions for chatbot users.\n",
            "Added new summarization types: CHAPTERS, QUESTIONS and POSTS.   See usage examples in the \"LLMs for Podcasters\" blog post.\n",
            "Added versioned model enums such as GPT4_0613 and GPT35_TURBO_16K_1106.  Without version specified, such as GPT35_TURBO_16K, Graphlit will use the latest production model version, as defined by the LLM vendor.\n",
            "Added lookupContents query to get multiple contents by id in one query.\n",
            "⚡ In Content type, headline field was renamed to headlines and now returns an array of strings.\n",
            "⚡ Entity names are now limited to 1024 characters.  Names will be truncated if they exceed the maximum length.\n",
            "⚡ In SummarizationTypes enum, BULLET_POINTS was renamed to BULLETS.\n",
            "⚡ In ProjectStorage type, originalTotalSize was renamed to totalSize, and totalRenditionSize field was added.  totalSize is the sum of the ingested source file sizes, and totalRenditionSize is the sum of the source file sizes and any derived rendition sizes.\n",
            "⚡ In ConversationStrategy type, strategyType was renamed to type for consistency with rest of data model.\n",
            "⚡ In Specification type, optimizeSearchConversation was removed, and now is handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "Bugs Fixed\n",
            "GPLA-1725: Should ignore RSS.xml from web feed sitemap\n",
            "GPLA-1726: GPT-3.5 Turbo 16k LLM is adding \"Citation #\" to response\n",
            "GPLA-1698: Workflow not applied to link-crawled content\n",
            "GPLA-1692: Mismatched project storage total size, when some content has errored\n",
            "GPLA-1237: Add relevance threshold for semantic search\n",
            "PreviousJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "NextOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "Last updated11 months ago\n",
            "- Completion [389 tokens (includes JSON guardrails tokens)], throughput: 53.524 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4 Turbo 128k model in Azure OpenAI and native OpenAI services.\n",
            "  - Support for Llama 2 (7b, 13b, 70b) and Mistral 7b models via Replicate.\n",
            "  - Support for Anthropic Claude 2.1 model.\n",
            "  - Support for OpenAI GPT-4 Vision model for image descriptions and text extraction.\n",
            "  - Query by example added for contents and conversations, utilizing vector embeddings.\n",
            "  - Vector search support for conversations queries.\n",
            "  - PromptSpecifications mutation for prompting multiple models.\n",
            "  - New promptStrategy field for preprocessing prompts.\n",
            "  - SuggestConversation mutation for auto-suggesting follow-up questions.\n",
            "  - New summarization types: CHAPTERS, QUESTIONS, and POSTS.\n",
            "  - Versioned model enums introduced.\n",
            "  - LookupContents query for retrieving multiple contents by ID.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed headline field to headlines, returning an array of strings.\n",
            "  - Entity names limited to 1024 characters.\n",
            "  - BULLET_POINTS renamed to BULLETS in SummarizationTypes enum.\n",
            "  - ProjectStorage type fields renamed for clarity.\n",
            "  - Removed optimizeSearchConversation from Specification type, now handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Ignored RSS.xml from web feed sitemap.\n",
            "  - Resolved issue with GPT-3.5 Turbo 16k LLM adding \"Citation #\" to responses.\n",
            "  - Fixed workflow application to link-crawled content.\n",
            "  - Corrected mismatched project storage total size.\n",
            "  - Added relevance threshold for semantic search.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 10, 2023.\n",
            "  - Versioning and feature flags introduced for model enums.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support, improved querying capabilities, and better content management features.\n",
            "\n",
            "2025-01-01T20:22:25.804Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.196129, used credits [0.00422700]\n",
            "- CONTENT [523f000e-30f7-44a0-8c75-383b18aeed8f]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [673 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-9-support-direct-text-markdown-and-html-ingestion-new-specification-llm-strategy</name><title>August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy | Graphlit Changelog</title></metadata> 🎂\tAugust 2023\n",
            "August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "New Features\n",
            "💡 Added ingestText mutation which supports direct Content ingestion of plain text, Markdown and HTML.  Now, if you have pre-scraped HTML or Markdown text, you can ingest it into Graphlit without reading from a URL.\n",
            "💡 Added Specification strategy property, which allows customization of the LLM context when prompting a conversation.  ConversationStrategy now provides Windowed and Summarized message histories, as well as configuration of the weight between existing conversation messages and Content text pages (or audio transcript segments) in the LLM context.\n",
            "💡 Added auto-summarization of extracted text and audio transcripts.  There is a new Content summary property where a list of summary bullet points can be found.  These summaries can be optionally included in the Conversation prompt context for more accurate LLM responses.\n",
            "ℹ️ Added AzureOpenAIModels and OpenAIModels types to Specification model properties to make it easier to specify the desired LLM.\n",
            "ℹ️ Renamed ConversationMessage date property to timestamp\n",
            "✨ Refined the internal LLM prompts for providing content as part of Conversation context.  This provides for much clearer and accurate results from the LLM.\n",
            "PreviousAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "NextAugust 3: New data model for Observations, new Category entity\n",
            "Last updated1 year ago\n",
            "- Completion [184 tokens (includes JSON guardrails tokens)], throughput: 83.784 tokens/sec:\n",
            "- New Features:\n",
            "  - IngestText mutation added for direct ingestion of plain text, Markdown, and HTML without URL reading.\n",
            "  - Specification strategy property introduced for customizing LLM context in conversations, with options for Windowed and Summarized message histories.\n",
            "  - Auto-summarization feature for extracted text and audio transcripts, with summaries available for inclusion in conversation prompts.\n",
            "  - AzureOpenAIModels and OpenAIModels types added to Specification model properties for easier LLM specification.\n",
            "  - ConversationMessage date property renamed to timestamp.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Internal LLM prompts refined for clearer and more accurate results.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for content ingestion and LLM customization, improving the accuracy and relevance of responses in conversations.\n",
            "\n",
            "2025-01-01T20:22:24.905Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.556968, used credits [0.00432000]\n",
            "- CONTENT [d40294bc-55cb-4f3b-ade6-42fab2f1d379]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [600 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-20-support-for-medical-entities-anthropic-prompt-caching-bug-fixes</name><title>August 20: Support for medical entities, Anthropic prompt caching, bug fixes | Graphlit Changelog</title></metadata> 🎂\tAugust 2024\n",
            "August 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "💡 Graphlit now supports medical-related entities in GraphRAG, and via API for queries and mutations.\n",
            "Added support for Anthropic prompt caching. When using Anthropic Sonnet 3.5 or Haiku 3, Anthropic will now cache the entity extraction and LLM document preparation system prompts, which saves on token cost and increases performance.\n",
            "Bugs Fixed\n",
            "GPLA-3104: Should default search type to VECTOR, when performing entity similarity filter.\n",
            "GPLA-3112: Empty PDF fails entity extraction.\n",
            "PreviousSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "NextAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "Last updated4 months ago\n",
            "- Completion [210 tokens (includes JSON guardrails tokens)], throughput: 82.129 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "  - Medical-related entities supported in GraphRAG and via API for queries and mutations.\n",
            "  - Added support for Anthropic prompt caching, improving performance and reducing token costs when using Anthropic Sonnet 3.5 or Haiku 3.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved entity extraction and LLM document preparation through caching.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed default search type to VECTOR for entity similarity filter (GPLA-3104).\n",
            "  - Resolved issue where empty PDFs failed entity extraction (GPLA-3112).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 20, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for medical entity extraction and improved performance with caching, facilitating more efficient API interactions.\n",
            "\n",
            "2025-01-01T20:22:23.421Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.601668, used credits [0.00352200]\n",
            "- CONTENT [8c9d843f-4a50-4c40-aab7-9dbeb3472396]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [562 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-1-support-for-retrieval-only-rag-pipeline-bug-fixes</name><title>December 1: Support for retrieval-only RAG pipeline, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports formatting of LLM-ready prompts with our RAG pipeline, via the new formatConversation and completeConversation mutations.  This is valuable for supporting LLM streaming by directly calling the LLM from your application, and using Graphlit for RAG retrieval and conversation history. (Colab Notebook Example)\n",
            "We have added support for inline hyperlinks in extracted text from documents and web pages.\n",
            "Bugs Fixed\n",
            "GPLA-3466: Owner ID should accept any non-whitespace string\n",
            "GPLA-3458: Not getting Person-to-Organization edges from entity extraction\n",
            "PreviousDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "NextNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "Last updated1 month ago\n",
            "- Completion [153 tokens (includes JSON guardrails tokens)], throughput: 42.480 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for formatting LLM-ready prompts with RAG pipeline using formatConversation and completeConversation mutations.\n",
            "  - Inline hyperlinks support in extracted text from documents and web pages.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved LLM streaming capabilities by integrating directly with applications for RAG retrieval and conversation history.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3466: Owner ID now accepts any non-whitespace string.\n",
            "  - Fixed issue GPLA-3458: Resolved problem with missing Person-to-Organization edges from entity extraction.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 1, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances developer experience by improving LLM integration and text extraction functionalities.\n",
            "\n",
            "2025-01-01T20:22:23.396Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.645256, used credits [0.00570900]\n",
            "- CONTENT [3ed53e18-c1a5-41f7-9a6e-ef900e439b95]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [811 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-8-support-for-llm-based-document-extraction-.net-sdk-bug-fixes</name><title>August 8: Support for LLM-based document extraction, .NET SDK, bug fixes | Graphlit Changelog</title></metadata> 🎂\tAugust 2024\n",
            "August 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports LLM-based document preparation, using vision-capable models such as OpenAI GPT-4o and Anthropic Sonnet 3.5.  This is available via the MODEL_DOCUMENT preparation service type, and you can assign a customspecification object and bring your own LLM keys.\n",
            "💡 Graphlit now provides an open source .NET SDK, supporting .NET 6 and .NET 8 (and above).  SDK package can be found on Nuget.org.  Code samples can be found on GitHub.\n",
            "Added identifier property to Content object for mapping content to external database identifiers.  This is supported for content filtering as well.\n",
            "Added support for Claude 3 vision models for image-based entity extraction, using the MODEL_IMAGE entity extraction service.\n",
            "Added context augmentation to conversations, via the augmentedFilter property on the Conversation object.  Any content which matches this augmented filter will be injected into the LLM prompt content, without needing to be related by vector similarity to the user prompt.  This is useful for specifying domain knowledge which should always be referenced by the RAG pipeline.\n",
            "Added support for the latest snapshot of OpenAI GPT-4o, with the model enum GPT4O_128K_20240806.\n",
            "Added reranking of related entities, when preparing the LLM prompt context for GraphRAG.  If reranking is enabled, the metadata from the related entities will be reranked with the same reranker assigned to the conversation specification.\n",
            "⚡ We have changed the type of the duration field in the AudioMetadata and VideoMetadata types to be TimeSpan rather than string, as to be more consistent with the rest of the API data model.\n",
            "Bugs Fixed\n",
            "GPLA-2884: Support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "PreviousAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "NextJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "Last updated4 months ago\n",
            "- Completion [273 tokens (includes JSON guardrails tokens)], throughput: 74.892 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for LLM-based document preparation using models like OpenAI GPT-4o and Anthropic Sonnet 3.5 via MODEL_DOCUMENT service.\n",
            "  - Introduction of an open source .NET SDK for .NET 6 and .NET 8, available on Nuget.org with code samples on GitHub.\n",
            "  - Added identifier property to Content object for mapping to external database identifiers, aiding in content filtering.\n",
            "  - Support for Claude 3 vision models for image-based entity extraction using MODEL_IMAGE service.\n",
            "  - Context augmentation in conversations through augmentedFilter property, allowing injection of domain knowledge into LLM prompts.\n",
            "  - Support for the latest OpenAI GPT-4o snapshot (GPT4O_128K_20240806).\n",
            "  - Reranking of related entities for LLM prompt context preparation in GraphRAG.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Changed duration field type in AudioMetadata and VideoMetadata from string to TimeSpan for consistency.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2884 to support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 8, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for document extraction, improved SDK support, and better content management features.\n",
            "\n",
            "2025-01-01T20:22:22.317Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.635987, used credits [0.00819000]\n",
            "- CONTENT [979a18c8-361a-4786-ac5e-355c7652e5ba]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1230 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-22-support-for-dropbox-box-intercom-and-zendesk-feeds-openai-o1-gemini-2.0-bug-fixes</name><title>December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Dropbox feeds for ingesting files on the Dropbox cloud service. Dropbox feeds require your appKey, appSecret, redirectUriand refreshTokento be assigned. The feed also accepts an optional pathparameter to read files from a specific Dropbox folder.\n",
            "💡 Graphlit now supports Box feeds for ingesting files on the Box cloud service. Box feeds require your clientId, clientSecret, redirectUriand refreshTokento be assigned.\n",
            "💡 Graphlit now supports Intercom feeds for ingesting Intercom Articles and Tickets. We will ingest Intercom Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Intercom feeds require the accessTokenproperty to be assigned.\n",
            "💡 Graphlit now supports Zendesk feeds for ingesting Zendesk Articles and Tickets.  We will ingest Zendesk Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Zendesk feeds require the accessTokenproperty and your Zendesk subdomain to be assigned.\n",
            "Graphlit now supports the latest OpenAI o1 model, with the model enums O1_200kand O1_200k_20241217.\n",
            "Graphlit now supports the latest Gemini Flash 2.0 Experimental model, with the model enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "Graphlit now supports the latest Cohere R7B model, with the model enum COMMAND_R7B_202412.\n",
            "Graphlit now supports returning the low-level details from prompting RAG conversations, by adding the includeDetailsparameter and setting to True. This includes details on the number of sources, the exact list of messages provided to the LLM, and more.\n",
            "We have added support for filtering of observables, such as Person or Organization, by URI property.\n",
            "We have added the ability to bypass semantic search in content retrieval with conversations. You can assign NONEfor the conversation search type, and it will ignore the user prompt when retrieving content.  It will inject all contents resulting from the content filter into the RAG prompt context.\n",
            "We have added a new createdInLastproperty to all entity filters, which allows easier filtering of entities created within a recent time period. Also, we have added a new inLastproperty to the content filter, which allows easier filtering of content authored within a recent time period. For example, find all images taken in the last 3 days, or find me all emails I received yesterday.\n",
            "We have added support for the latest Azure AI Document Intelligence models, with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "We have added support for Google Drive and OneDrive feeds to ingest specific files by providing a list of file identifiers (files), in addition to the folder identifier (folderId).  If files identifiers are provided, they take precedence over the folder identifier.\n",
            "⚡ For projects upgraded to the Starter Tier after Dec 9, 2024, we have removed the content items limit. Now you can store an unlimited number of content items (i.e. files, web pages, Slack messages) on the Starter or Growth Tiers.  If you have an existing project on the Starter Tier, please reach out and we will manually remove that content item limit on the project.\n",
            "Bugs Fixed\n",
            "GPLA-3529: Can't assign collection to multitenant content\n",
            "GPLA-3579: Should decode HTML characters when parsing HTML email\n",
            "GPLA-3576: Ingesting content in-place doesn't handle isSynchronous properly\n",
            "GPLA-3457: IsFeedDone doesn't return True for finished feed with no contents\n",
            "GPLA-3572: Not handling HTTP 400 error on uploading from URI\n",
            "PreviousDecember 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes\n",
            "NextDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "Last updated7 days ago\n",
            "- Completion [375 tokens (includes JSON guardrails tokens)], throughput: 80.889 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Dropbox feeds for file ingestion; requires appKey, appSecret, redirectUri, and refreshToken.\n",
            "  - Support for Box feeds for file ingestion; requires clientId, clientSecret, redirectUri, and refreshToken.\n",
            "  - Support for Intercom feeds for Articles and Tickets; requires accessToken.\n",
            "  - Support for Zendesk feeds for Articles and Tickets; requires accessToken and Zendesk subdomain.\n",
            "  - Support for OpenAI o1 model with enums O1_200k and O1_200k_20241217.\n",
            "  - Support for Gemini Flash 2.0 Experimental model with enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "  - Support for Cohere R7B model with enum COMMAND_R7B_202412.\n",
            "  - Ability to return low-level details from RAG conversations using includeDetails parameter.\n",
            "  - Support for filtering observables by URI property.\n",
            "  - Ability to bypass semantic search in content retrieval with conversations.\n",
            "  - New createdInLast property for entity filters and inLast property for content filters for easier recent filtering.\n",
            "  - Support for Azure AI Document Intelligence models with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "  - Support for Google Drive and OneDrive feeds to ingest specific files by file identifiers.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Removal of content items limit for projects upgraded to the Starter Tier after December 9, 2024.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with multitenant content assignment, HTML character decoding in emails, synchronous content ingestion, feed completion status, and HTTP error handling during uploads.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced integration capabilities with popular cloud services and improved content management features.\n",
            "\n",
            "2025-01-01T20:22:19.698Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:05.876134, used credits [0.00976200]\n",
            "- CONTENT [42df7e18-5180-4ee2-99f5-72f3292c9872]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1586 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/january-2024/january-18-support-for-content-publishing-llm-tools-clip-image-embeddings-bug-fixes</name><title>January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes | Graphlit Changelog</title></metadata> 🎆\tJanuary 2024\n",
            "January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports content publishing, where documents, audio transcripts and even image descriptions, can be summarized, and repurposed into blog posts, emails or AI-generated podcasts.  With the new publishContents mutation, you can configure LLM prompts for summarization and publishing, and assign specifications to use different models and/or system prompts for each step in the process.  The published content will be reingested into Graphlit, and can be searched or used for conversations, like any other form of content.\n",
            "💡 Graphlit now supports publishing conversations as content with the new publishConversation mutation.  You can generate text or audio transcripts of your conversations, to be reused in other tools.\n",
            "💡 Graphlit now supports bulk summarization of contents with the summarizeContents mutation.  You can filter a set of content, by feed, by observable or by similar text, and run a set of summarizations across each content in parallel.\n",
            "💡 Graphlit now supports LLM entity extraction, with the new MODEL_TEXT entity extraction service type.  Similar to using Azure Cognitive Service Text Analytics, you can use any OpenAI or Anthropic model for extracting entities from text.  Internally the LLM returns JSON-LD entities, which we convert into Person, Organization, Place, etc. entities and assign observations to the extracted content.\n",
            "💡 Graphlit now supports LLM tools (aka function calls) with OpenAI models.  You can define the tools to be used with the LLM in the specification object.  With the new extractContents mutation, you can execute a prompt against content using a specification with tools defined.  The mutation will return the JSON arguments assigned by the LLM.\n",
            "💡 Graphlit now supports callback webhooks for LLM tools.  If you assign a URI in the ToolDefinition object, Graphlit will call your webhook the tool name and JSON arguments.  When you respond to the webhook with JSON, we will add that response to the LLM messages, and ask the LLM to complete the original prompt.\n",
            "💡 Graphlit now supports the selection of the Deepgram model (such as Meeting, Phonecall or Finance) with the preparation workflow.  Also, you can assign your own Deepgram API key, which will be used for audio transcription using that workflow.\n",
            "Added support for CLIP image embeddings using Roboflow, which can be used for similar image search.  If you search for contents by similar contents, we will now use the content's text and/or image embeddings to find similar content.\n",
            "Added support for dynamic web page ingestion.  Graphlit now navigates to and automatically scrolls web pages using Browserless.io, so we capture the fully rendered HTML before extracting text.  Also, we now support web page screenshots, if enabled with enableImageAnalysis property in preparation workflow.  These screenshots can be analyzed with multimodal modals, such as GPT-4 Vision, or can be used to create image embeddings for similar image search.\n",
            "Added table parsing when preparing documents.  We now store structured (tab-delimited) text in the JSON text mezzanine which is extracted from documents in the preparation workflow.\n",
            "Added reverse geocoding of lat/long locations found in image or other content metadata.  We now store the real-world address with the content metadata, for use in conversations.\n",
            "Added assistant messages to the conversation message history provided to the LLM.  Originally we had included only user messages, but now we are formatting both user and assistant messages into the LLM prompt for conversations.\n",
            "Added new chunking algorithm for text embeddings.  We support semantic chunking at the page or transcript segment level, and now will create embeddings from smaller sized text chunks per page or segment.\n",
            "Added content metadata to text and image embeddings.  To provide better context for the text embeddings, we now include formatted content metadata, which includes fields like title, subject, author, or description.  For emails, we include to, from, cc, and bcc fields.\n",
            "Added helper mutations isContentDone and isFeedDone which can be used for polling completion of ingested content, or all content ingested by a feed.\n",
            "Added richer image descriptions generated by the GPT-4 Vision model.  Now these provide more useful detail.\n",
            "Added validation of extracted hyperlinks.  Now we test the URIs and remove any inaccessible links during content enrichment.\n",
            "Added deleteContents,  deleteFeeds,  and deleteConversations mutations for multi-deletion of contents, feeds or conversations.\n",
            "Added deleteAllContents,  deleteAllFeeds,  and deleteAllConversations mutations for bulk, filtered deletion of entities.  You can delete all your contents, feeds, or conversations in your project, or a filtered subset of those entities.\n",
            "ℹ️ Starter tier now has a higher content limit of 100K content items.\n",
            "⚡ In the OpenAIImageExtractionProperties type, the detailMode field was renamed to detailLevel.\n",
            "⚡ Each SummarizationStrategy object now accepts the specification which is used by the summarization, rather than being assigned at the preparation workflow stage.\n",
            "⚡ addCollectionContents and removeCollectionContents mutations have been deprecated in favor of addContentsToCollections and removeContentsFromCollection mutations.\n",
            "Bugs Fixed\n",
            "GPLA-1846: Parse Markdown headings into mezzanine JSON\n",
            "GPLA-1779: Not returning SAS token with mezzanine, master URIs\n",
            "GPLA-1348: Summarize text content, not just file content\n",
            "GPLA-1297: Not assigning content error message on preparation workflow failure\n",
            "PreviousJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "NextDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes Last updated8 months ago\n",
            "- Completion [417 tokens (includes JSON guardrails tokens)], throughput: 70.965 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for content publishing, allowing documents, audio transcripts, and image descriptions to be summarized and repurposed.\n",
            "  - New mutation for publishing conversations as content, generating text or audio transcripts.\n",
            "  - Bulk summarization of contents with the summarizeContents mutation.\n",
            "  - LLM entity extraction using MODEL_TEXT service type, returning JSON-LD entities.\n",
            "  - Support for LLM tools (function calls) with OpenAI models, including the new extractContents mutation.\n",
            "  - Callback webhooks for LLM tools, enabling integration with external services.\n",
            "  - Selection of Deepgram models for audio transcription with custom API key support.\n",
            "  - CLIP image embeddings support for similar image search.\n",
            "  - Dynamic web page ingestion with automatic scrolling and screenshot capabilities.\n",
            "  - Table parsing for structured text extraction from documents.\n",
            "  - Reverse geocoding of lat/long locations in content metadata.\n",
            "  - Inclusion of assistant messages in conversation history for LLM prompts.\n",
            "  - New chunking algorithm for semantic text embeddings.\n",
            "  - Enhanced content metadata for text and image embeddings.\n",
            "  - Helper mutations for polling content ingestion completion.\n",
            "  - Richer image descriptions generated by GPT-4 Vision model.\n",
            "  - Validation of extracted hyperlinks to remove inaccessible links.\n",
            "  - Multi-deletion mutations for contents, feeds, and conversations.\n",
            "  - Bulk deletion mutations for filtered subsets of entities.\n",
            "  - Starter tier content limit increased to 100K items.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed detailMode field to detailLevel in OpenAIImageExtractionProperties.\n",
            "  - SummarizationStrategy objects now accept specifications directly.\n",
            "  - Deprecated addCollectionContents and removeCollectionContents mutations in favor of new naming conventions.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with parsing Markdown headings, returning SAS tokens, summarizing text content, and assigning error messages on workflow failures.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: January 18, 2024.\n",
            "\n",
            "- Value:\n",
            "  - These updates enhance content management, improve integration with LLMs, and streamline workflows for developers.\n",
            "\n",
            "2025-01-01T20:22:19.654Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.076129, used credits [0.00606300]\n",
            "- CONTENT [c5dc4da1-aa86-4d89-93f2-d15ec179a24d]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [921 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-9-support-for-website-mapping-web-page-screenshots-groq-llama-3.3-model-bug-fixes</name><title>December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports mapping a website with the mapWebmutation. You can provide a URL to a website, and the query will return a list of URLs based on the sitemap.xml (or sitemap-index.xml) file, at or underneath the provided URL.\n",
            "💡 Graphlit now supports the generation of web page screenshots with the screenshotPagemutation. By providing the URL of a web page, and optionally, the maximum desired height of the screenshot, we will screenshot the webpage and ingest it automatically as content.  You can provide an optional workflow, which will be applied to the ingested image content, for operations like generating image descriptions with a vision LLM.\n",
            "💡 Graphlit now supports the direct summarization of text with the summarizeTextmutation. By providing the desired summarization strategy, we will summarize the text (i.e. bullet points, social media posts) and return the summarization.\n",
            "💡 Graphlit now supports the direct extraction of text with the extractTextmutation. By providing the LLM tool definitions and an optional LLM specification, we will prompt the desired LLM (or OpenAI GPT-4o, by default) to invoke the provided tools, and return the JSON responses from the LLM tool calling.\n",
            "Graphlit now supports the latest Groq Llama 3.3 model, with the model enum LLAMA_3_3_70B.\n",
            "We have updated Cohere reranking to use the latest Cohere rerank-v3.5model by default.\n",
            "⚡ We have added a new flattenCitationsfield to the ConversationStrategyInputtype.  By assigning this field to True, when calling promptConversation,we will combine multiple citations from the same content into a single citation.\n",
            "⚡ For Microsoft email, Microsoft Teams and OneDrive feeds, we have added the clientIdand clientSecretfields as required feed properties. These properties must be assigned, in addition to the refreshTokenfield for proper authentication to the Microsoft Graph API used by these feeds. (Colab Notebook Example)\n",
            "Bugs Fixed\n",
            "GPLA-3492: Not finding sitemap at parent web path\n",
            "GPLA-3500: Failed to handle mismatch of Deepgram model/language\n",
            "PreviousDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "NextDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "Last updated22 days ago\n",
            "- Completion [275 tokens (includes JSON guardrails tokens)], throughput: 67.466 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for website mapping using mapWebmutation to return URLs from sitemap.xml.\n",
            "  - Generation of web page screenshots with screenshotPagemutation, including optional image processing workflows.\n",
            "  - Direct summarization of text with summarizeTextmutation, allowing various summarization strategies.\n",
            "  - Direct extraction of text with extractTextmutation, utilizing LLM tools for JSON responses.\n",
            "  - Support for Groq Llama 3.3 model (LLAMA_3_3_70B).\n",
            "  - Updated Cohere reranking to use rerank-v3.5 model by default.\n",
            "  - New flattenCitations field in ConversationStrategyInput to combine multiple citations.\n",
            "  - Required clientId and clientSecret fields added for Microsoft email, Teams, and OneDrive feeds.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of citations in conversation prompts.\n",
            "  - Enhanced authentication requirements for Microsoft Graph API feeds.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with not finding sitemap at parent web path (GPLA-3492).\n",
            "  - Resolved mismatch handling for Deepgram model/language (GPLA-3500).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for web mapping, content ingestion, and improved API interactions, streamlining workflows and increasing efficiency.\n",
            "\n",
            "2025-01-01T20:22:18.819Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.242335, used credits [0.00461400]\n",
            "- CONTENT [f53855bb-439c-48c9-a88d-f427da2962d1]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [690 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-2-support-for-semantic-alerts-openai-0125-models-performance-enhancements-bug-fixes</name><title>February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes | Graphlit Changelog</title></metadata> 🌧️\tFebruary 2024\n",
            "February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Semantic Alerts, which allows for LLM summarization and publishing of content, on a periodic basis.  This is useful for generating daily reports from email, Slack or other time-based feeds.  Alerts support the same publishing options, i.e. audio and text, as the publishContents mutation.\n",
            "💡 Graphlit now supports the latest OpenAI 0125 model versions, for GPT-4 and GPT-3.5 Turbo.  We will add support for Azure OpenAI when Microsoft releases support for these.\n",
            "Slack feeds now support a listing type field, where you can specify if you want PAST or NEW Slack messages in the feed.\n",
            "🔥 This release provides many performance enhancements, which will speed up the content workflows for ingested content.\n",
            "Bugs Fixed\n",
            "GPLA-2114: Collections not being added to text embedding index documents.\n",
            "GPLA-2063: Not handling hallucinated citations.\n",
            "GPLA-1916: Collections not inherited from project-scope into tenant-scope.\n",
            "GPLA-2105: Should error on add/remove of contents to/from collections if content does not exist.\n",
            "PreviousFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "NextJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "Last updated10 months ago\n",
            "- Completion [212 tokens (includes JSON guardrails tokens)], throughput: 65.385 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Semantic Alerts for LLM summarization and content publishing on a periodic basis, useful for generating daily reports from various feeds.\n",
            "  - Support for OpenAI 0125 model versions for GPT-4 and GPT-3.5 Turbo, with plans to add Azure OpenAI support when available.\n",
            "  - Slack feeds now include a listing type field to specify PAST or NEW messages.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Performance enhancements to speed up content workflows for ingested content.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with collections not being added to text embedding index documents.\n",
            "  - Resolved handling of hallucinated citations.\n",
            "  - Addressed inheritance of collections from project-scope to tenant-scope.\n",
            "  - Implemented error handling for adding/removing contents to/from collections if content does not exist.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 2, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved content management capabilities, enhanced performance, and better error handling in workflows.\n",
            "\n",
            "2025-01-01T20:22:17.536Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.668337, used credits [0.00685800]\n",
            "- CONTENT [72415f65-64c2-4aca-87e0-93aa4f58cea6]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1042 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-21-support-for-onedrive-and-google-drive-feeds-extract-images-from-pdfs-bug-fixes</name><title>February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes | Graphlit Changelog</title></metadata> 🌧️\tFebruary 2024\n",
            "February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports OneDrive and Google Drive feeds.  Files can be ingested from OneDrive or Google Drive, including shared drives where the authenticated user has access.  Both OneDrive and Google Drive support the reading of existing files, and tracking new files added to storage with recurrent feeds.\n",
            "💡 Graphlit now supports email backup files, such as EML or MSG, which will be assigned the EMAIL file type.  During email file preparation, we will automatically extract and ingest any file attachments.\n",
            "💡 Graphlit now automatically extracts embedded images in PDF files, ingests them as content objects, and links them as children of the parent PDF.\n",
            "💡 Graphlit now supports recursive Notion feeds.  When the isRecursive flag is true in the Notion feed properties, we will crawl child pages and databases, and recursively ingest them in addition to the specified pages and databases.\n",
            "Added support for assigning collections to content ingested with the ingestPage, ingestFile or ingestText mutations.  This saves a step where the content will automatically be added to the collection(s) without requiring another mutation call.\n",
            "Added support for the CODE file type for a wide variety of source code formats, i.e. Python .py, Javascript .js.  Code files use optimized text splitting for enhanced search and retrieval.\n",
            "Added support for customGuidance in Specification object, which can be used for injecting a guidance prompt during the RAG process.  For example, you can instruct the LLM to return a default response string if no content sources are found via semantic search.\n",
            "Added tenants field to Project object, which returns a list of all tenant IDs which have been used to create an entity in Graphlit.\n",
            "Added email metadata, separate from document metadata.  Now emails will contain indexed metadata such as to, from, or subject.\n",
            "⚡ The contents field for content objects has been replaced with children and parent fields.  For example, when a ZIP file is unpacked, the unpacked files will be added as children of the ZIP file, and the ZIP file will be the parent of each of the unpacked files.\n",
            "⚡ Removed enableImageAnalysis field from image preparation properties in workflow object.  Now is enabled by default.\n",
            "⚡ Moved disableSmartCapture field to preparation workflow stage from page preparation properties.  This is used to disable the use of headless Chrome browser to capture HTML from web pages.  It is enabled by default, and if disabled, Graphlit will simply download the HTML from the web page rather than rendering on headless Chrome browser.\n",
            "Bugs Fixed\n",
            "GPLA-2099: Failed to ingest ArXiV PDF.  Fixed PDF parsing error.\n",
            "GPLA-2174: LLM response is incorrect with conversation history, but no content sources.\n",
            "GPLA-2199: ZIP package left in Indexed state after content workflow.\n",
            "PreviousMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "NextFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "Last updated9 months ago\n",
            "- Completion [311 tokens (includes JSON guardrails tokens)], throughput: 84.780 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OneDrive and Google Drive feeds, allowing ingestion of files from shared drives.\n",
            "  - Support for email backup files (EML, MSG) with automatic extraction of attachments.\n",
            "  - Automatic extraction of embedded images from PDF files, linking them as children of the parent PDF.\n",
            "  - Support for recursive Notion feeds with the isRecursive flag for crawling child pages and databases.\n",
            "  - Collections can now be assigned to ingested content automatically.\n",
            "  - Introduction of CODE file type for various source code formats with optimized text splitting.\n",
            "  - Custom guidance support in Specification object for RAG process.\n",
            "  - Added tenants field to Project object for listing tenant IDs.\n",
            "  - Indexed email metadata (to, from, subject) added separately from document metadata.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Contents field in content objects replaced with children and parent fields for better structure.\n",
            "  - Default enabling of image analysis in workflow object.\n",
            "  - Moved disableSmartCapture field to preparation workflow stage, enabled by default.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed PDF parsing error for ArXiV PDFs (GPLA-2099).\n",
            "  - Corrected LLM response issues with conversation history (GPLA-2174).\n",
            "  - Resolved issue of ZIP package remaining in Indexed state after content workflow (GPLA-2199).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve file ingestion capabilities and metadata handling, streamlining workflows for developers.\n",
            "\n",
            "2025-01-01T20:22:15.483Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.931969, used credits [0.00635100]\n",
            "- CONTENT [f20064c8-0d29-4243-b163-d94ebb7913d3]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [913 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2023/july-15-support-for-sharepoint-feeds-new-conversation-features</name><title>July 15: Support for SharePoint feeds, new Conversation features | Graphlit Changelog</title></metadata> 🎇\tJuly 2023\n",
            "July 15: Support for SharePoint feeds, new Conversation features\n",
            "New Features\n",
            "💡 Added support for SharePoint feeds: now can create feed to ingest files from SharePoint document library (and optionally, folder within document library)\n",
            "💡 Added support for PII detection during entity extraction from text documents and audio transcripts: now we will create labels such as PII: Social Security Number automatically when PII is detected\n",
            "💡 Added support for developer's own OpenAI API keys and Azure OpenAI deployments in Specifications\n",
            "ℹ️  Changed semantics of deleteFeed to delete the contents ingested by the feed; since contents are linked to feeds, now feeds can be disabled, while keeping the lineage to the feed, and if feeds are deleted, they will delete the linked contents, so we never lose the feed-to-content lineage\n",
            "Added GraphQL query for SharePoint consent URI, for registered Graphlit Platform Azure AD application\n",
            "Better handling of web sitemap indexes: now if a sitemap.xml contains a sitemapindex element, we will load all linked sitemaps for evaluating web pages to ingest from Web feed\n",
            "Added new GraphQL mutations for openConversation, closeConversation and undoConversation\n",
            "Added timestamps to Conversation messages\n",
            "Added new GraphQL mutations for openCollection and closeCollection\n",
            "Added more configuration for content search: now can specify searchType (KEYWORD, VECTOR, HYBRID) and queryType (SIMPLE, FULL - aka Lucene syntax)\n",
            "Better parsing of iTunes podcast metadata\n",
            "⚡ Renamed listingLimit field on feeds to readLimit\n",
            "⚡ Renamed topK to numberSimilar for content vector search type\n",
            "⚡ Changed GraphQL feed properties: split out azure into azureBlob and azureFile properties\n",
            "⚡ Changed GraphQL specification properties: split out openAI into openAI and azureOpenAI properties\n",
            "⚡ Removed count fields on query results, and replaced with explicit count{Entity} queries, which support search and filtering.\n",
            "Bugs Fixed\n",
            "GPLA-1043: Reddit readLimit not taking effect: now the specified limit of Reddit posts will be leveraged for Reddit feeds\n",
            "GPLA-1064: Performance on entity extraction and observation creation for large PDFs was under expectations: now able to build knowledge graph from large PDFs much faster (4x speed improvement)\n",
            "GPLA-1053: If rendition generation errored during content workflow, the content was not properly marked as errored\n",
            "GPLA-1102: Large Web sitemaps were slow to load; rewrote sitemap index handling, and now can process sitemaps with 150K+ entries in seconds.\n",
            "PreviousAugust 3: New data model for Observations, new Category entity\n",
            "Last updated6 months ago\n",
            "- Completion [301 tokens (includes JSON guardrails tokens)], throughput: 76.552 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for SharePoint feeds to ingest files from SharePoint document libraries.\n",
            "  - PII detection during entity extraction from text documents and audio transcripts, automatically labeling detected PII.\n",
            "  - Support for custom OpenAI API keys and Azure OpenAI deployments in Specifications.\n",
            "  - GraphQL query for SharePoint consent URI for registered Azure AD applications.\n",
            "  - Improved handling of web sitemap indexes to load linked sitemaps.\n",
            "  - New GraphQL mutations for managing conversations and collections.\n",
            "  - Timestamps added to Conversation messages.\n",
            "  - Enhanced content search configuration with searchType and queryType options.\n",
            "  - Better parsing of iTunes podcast metadata.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed fields for clarity: listingLimit to readLimit, topK to numberSimilar.\n",
            "  - Split GraphQL properties for Azure and OpenAI for better organization.\n",
            "  - Removed count fields on query results, replaced with explicit count queries.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed Reddit readLimit issue to ensure specified limits are applied.\n",
            "  - Improved performance for entity extraction from large PDFs (4x speed increase).\n",
            "  - Corrected error marking for content workflow rendition generation.\n",
            "  - Enhanced sitemap index handling for faster processing of large web sitemaps (150K+ entries).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 15, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced data ingestion capabilities, improved performance, and better management of content and conversations.\n",
            "\n",
            "2025-01-01T20:22:15.413Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.717697, used credits [0.00469500]\n",
            "- CONTENT [1149d374-504c-443f-8fb3-ca0a475776dc]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [673 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/january-2024/january-22-support-for-google-and-microsoft-email-feeds-reingest-content-in-place-bug-fixes</name><title>January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes | Graphlit Changelog</title></metadata> 🎆\tJanuary 2024\n",
            "January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Google and Microsoft email feeds.  Email feeds can be created to ingest past emails, or poll for new emails.  Emails create an EMAIL content type. Attachment files can optionally be extracted from emails, and will be linked to their parent email content. If assigning a workflow to the feed, the workflow will be applied both to the email content and the extracted attachment files.\n",
            "💡 Graphlit now supports reingesting content in-place.  The ingestText, ingestPage and ingestFile mutations now take an optional id parameter for an existing content object.  If this id is provided, the existing content will be updated from the provided text or URI source, and will restart the assigned workflow.\n",
            "Added restartAllContents mutation to restart workflow on all partially-ingested contents in project.\n",
            "Added text field to ConversationCitation type, which returns the relevant text from the content source with the citation.\n",
            "Bugs Fixed\n",
            "GPLA-1313: Not extracting links from HTML\n",
            "GPLA-2030: No text extracted from shapes in PPTX files\n",
            "PreviousFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "NextJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [223 tokens (includes JSON guardrails tokens)], throughput: 82.055 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Google and Microsoft email feeds, allowing ingestion of past and new emails, creating an EMAIL content type. Attachments can be extracted and linked to parent emails.\n",
            "  - Support for reingesting content in-place with optional id parameter for existing content objects, updating them from provided text or URI source, and restarting assigned workflows.\n",
            "  - Added restartAllContents mutation to restart workflows on all partially-ingested contents in a project.\n",
            "  - Added text field to ConversationCitation type to return relevant text from the content source with the citation.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved content ingestion capabilities with new mutations and features.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with links not being extracted from HTML (GPLA-1313).\n",
            "  - Resolved problem of no text being extracted from shapes in PPTX files (GPLA-2030).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: January 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced content ingestion options and improved workflow management, increasing efficiency in handling email and existing content updates.\n",
            "\n",
            "2025-01-01T20:22:13.779Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.836339, used credits [0.00610800]\n",
            "- CONTENT [a8a2bbe9-1c58-42a7-b31c-dd4b27b74956]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1016 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-19-support-for-openai-gpt-4o-mini-byo-key-for-azure-ai-similarity-by-summary-bug-fixes</name><title>July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the OpenAI GPT-4o Mini model, with 16k output tokens.\n",
            "💡 Graphlit now supports 'bring-your-own-key' for Azure AI Document Intelligence models.  We have added a custom endpoint and key property, which can be assigned to use your own Azure AI resource.\n",
            "Updated to use Jina reranker v2 (jina-reranker-v2-base-multilingual) by default.\n",
            "Updated to assign the summary, bullets, etc properties when calling summarizeContents mutation.  Now when summarizing contents, we will store the resulting summary in the content itself, in addition to returning the summarized results.\n",
            "Added relevance property to all entity types, which will be assigned when searching for these entities.  Entity results will be sorted (descending) by this search relevance score.\n",
            "Added the ability to manually update summary, bullets, etc. properties when calling the updateContent mutation.\n",
            "Added offset property to AtlassianJiraFeedProperties, so the timezone offset can be properly assigned for paging of the Jira feed.  (Defaults to zero offset, i.e. UTC.)  Jira does not store dates in UTC format, and the timestamps are based on the server timezone of the hosted Jira instance.  By assigning the timezone offset with the Jira feed, we can reliably page the updated date timestamps from the Jira API.\n",
            "⚡ We have changed the content similarity search behavior to find similar content by summary, rather than text of the document, when a summary has been previously generated.  For long documents, this will provide a more accurate similarity, rather than comparing on the first few pages of text in a document.\n",
            "⚡ We have changed the behavior of assigning offset in the entity filter objects for paging through entities.  If using vector or hybrid search, this offset will be ignored (i.e. zero offset).  Paging will not be supported through vector or hybrid search results. For keyword search, the offset will continue to be used, along with the limit property, to provide paging through the search results.  We have made this change because we have found that index-based paging is not reliable with our vector/hybrid search approach.  We are investigating ways to support this reliably with vector/hybrid search in the future.\n",
            "Bugs Fixed\n",
            "GPLA-2915: Add retry on OpenAI API HTTP 524 error (gateway timeout).\n",
            "GPLA-2908: Not paging through Jira feed correctly.\n",
            "GPLA-2917: Search by similar content is not giving expected results on long documents.\n",
            "GPLA-2244: Keyword search not finding text in latter part of long PDF.\n",
            "PreviousJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "NextJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [255 tokens (includes JSON guardrails tokens)], throughput: 66.470 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4o Mini model with 16k output tokens.\n",
            "  - 'Bring-your-own-key' support for Azure AI Document Intelligence models with custom endpoint and key property.\n",
            "  - Updated to Jina reranker v2 by default.\n",
            "  - Enhanced summarization capabilities, storing summaries in content.\n",
            "  - Added relevance property for entity types, sorting results by search relevance score.\n",
            "  - Ability to manually update summary and bullet properties in updateContent mutation.\n",
            "  - Offset property added to AtlassianJiraFeedProperties for accurate paging of Jira feed.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Content similarity search now finds similar content by summary for better accuracy.\n",
            "  - Changes in entity filter objects for paging; zero offset for vector/hybrid search.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Added retry on OpenAI API HTTP 524 error.\n",
            "  - Fixed paging issues with Jira feed.\n",
            "  - Improved search results for similar content in long documents.\n",
            "  - Resolved keyword search issues in long PDFs.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 19, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved model support, enhanced search capabilities, and better handling of content summaries and entity relevance.\n",
            "\n",
            "2025-01-01T20:22:13.704Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.012726, used credits [0.00456600]\n",
            "- CONTENT [3109dca5-a705-4191-a868-ab2b2485d8ca]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [710 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-25-support-for-mistral-large-2-and-nemo-groq-llama-3.1-models-bug-fixes</name><title>July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the Mistral Large 2 and Mistral Nemo models. The existing MISTRAL_LARGE model enum now will use Mistral Large 2.\n",
            "💡 Graphlit now supports the Llama 3.1 8b, 70b and 405b models on Groq.  (Note, these are rate-limited according to Groq's platform constraints.)\n",
            "Added support for revision strategy on data extraction specifications.  Now you can prompt the LLM to revise its previous data extraction response, similar to the existing completion revision strategy.\n",
            "Added version property for AzureDocumentPreparationProperties type for assigning the API version used by Azure AI Document Intelligence.   By default, Graphlit will continue to use the v4.0 (Preview) API version, but you can override this to assign version to V2023_10_31 to use the v3.1 (GA) API version instead.  For some documents, the General Availability (GA) version of the API can provide better results.\n",
            "Bugs Fixed\n",
            "GPLA-2988: Not extracting hyperlinks from Office documents.\n",
            "PreviousJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "NextJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [203 tokens (includes JSON guardrails tokens)], throughput: 67.381 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Mistral Large 2 and Mistral Nemo models; existing MISTRAL_LARGE model enum updated.\n",
            "  - Support for Llama 3.1 models (8b, 70b, 405b) on Groq, with rate limits per Groq's platform.\n",
            "  - Added revision strategy for data extraction specifications, allowing LLM to revise previous responses.\n",
            "  - Version property added for AzureDocumentPreparationProperties to assign API version for Azure AI Document Intelligence; default is v4.0 (Preview), can override to V2023_10_31 for v3.1 (GA) API.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with hyperlinks not being extracted from Office documents (GPLA-2988).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 25, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve model support and data extraction capabilities, offering developers more flexibility and better results with Azure AI.\n",
            "\n",
            "2025-01-01T20:22:12.545Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:05.325587, used credits [0.00687000]\n",
            "- CONTENT [cb4a2f54-a3ae-4b9b-a66e-27232fea2d14]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [894 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/june-2024/june-9-support-for-deepseek-models-json-ld-webpage-parsing-performance-improvements-and-bug-fixes</name><title>June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes | Graphlit Changelog</title></metadata> 🎓\tJune 2024\n",
            "June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Deepseek LLMs for prompt completion.  We offer the deepseek-chat and deepseek-coder models.\n",
            "💡 Graphlit now supports parsing embedded JSON-LD from web pages.  If a web page contains 'script' tags with JSON-LD, we will automatically parse and inject into the knowledge graph.\n",
            "⚡ We have changed the default model for entity extraction and image completions to be OpenAI GPT-4o.  This provides faster performance and better quality output.\n",
            "⚡ We have changed the behavior of knowledge graph generation, from a prompted conversation, to be opt-in.  In order to receive the graph's nodes and edges with the response, you will now need to set generateGraph to True in the specification's graphStrategy object.  This provides improved performance when the graph is not needed for visualization.\n",
            "Added thing property for observable entities, which returns the JSON-LD metadata associated with the entity.\n",
            "Added regex-based filtering for URI paths during feed ingestion, link crawling, and workflow filtering.  You can assign regex patterns in allowedPaths and excludedPaths.\n",
            "Added observableLimit to configure the limit of how many observed entities will be added to the GraphRAG context, defaults to 1000.\n",
            "Added prompt to suggestConversation mutation, which allows customization of the followup question generation.\n",
            "Updated suggestConversation to incorporate the past conversation message history, in addition to the filtered set of content sources.\n",
            "🔥  We have improved performance in knowledge graph retrieval and generation, via better parallelization and batching.\n",
            "Bugs Fixed\n",
            "GPLA-2748: Optimize the retrieval performance of observed entities during GraphRAG\n",
            "GPLA-2732: Invalid user-provided URI causing parsing exception\n",
            "GPLA-2666: Shouldn't require tenant ID for Microsoft email or Teams\n",
            "GPLA-2772: Not returning labels or categories from graph in API\n",
            "GPLA-2762: Failed to extract spreadsheet images\n",
            "GPLA-2687: Email to/from not getting added as observations on emails\n",
            "GPLA-2738: API is returning 'audio' metadata from podcast HTML document\n",
            "PreviousJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "NextMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [349 tokens (includes JSON guardrails tokens)], throughput: 65.533 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Deepseek LLMs for prompt completion (deepseek-chat and deepseek-coder models).\n",
            "  - Parsing of embedded JSON-LD from web pages, automatically injecting into the knowledge graph.\n",
            "  - Default model for entity extraction and image completions changed to OpenAI GPT-4o for improved performance and quality.\n",
            "  - Knowledge graph generation behavior changed to opt-in; requires setting generateGraph to True for response inclusion.\n",
            "  - Added thing property for observable entities to return associated JSON-LD metadata.\n",
            "  - Regex-based filtering for URI paths during feed ingestion, link crawling, and workflow filtering.\n",
            "  - Added observableLimit to configure the number of observed entities in GraphRAG context (default 1000).\n",
            "  - Customization of follow-up question generation via prompt in suggestConversation mutation.\n",
            "  - Updated suggestConversation to include past conversation message history.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved performance in knowledge graph retrieval and generation through better parallelization and batching.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Optimized retrieval performance of observed entities during GraphRAG.\n",
            "  - Fixed parsing exception caused by invalid user-provided URI.\n",
            "  - Removed requirement for tenant ID for Microsoft email or Teams.\n",
            "  - Resolved issue of not returning labels or categories from graph in API.\n",
            "  - Fixed failure to extract spreadsheet images.\n",
            "  - Addressed issue of email to/from not being added as observations.\n",
            "  - Corrected API returning 'audio' metadata from podcast HTML documents.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: June 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for model support, improved performance, and better handling of data through new features and bug fixes.\n",
            "\n",
            "2025-01-01T20:22:11.425Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.022561, used credits [0.00406800]\n",
            "- CONTENT [47c53407-b4d6-48e4-b91d-b9677a1028de]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [640 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-28-support-for-indexing-workflow-stage-azure-ai-language-detection-bug-fixes</name><title>July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "New Features\n",
            "Added indexing workflow stage. This provides for configuration of indexing services, which may infer metadata from the content.\n",
            "Added AZURE_AI_LANGUAGE content indexing service, which supports inferring the language of extracted text or transcript.\n",
            "Added support for language content metadata.  This returns a list of languages in ISO 639-1 format, which may have been inferred from the extracted text or transcript.\n",
            "Added support for MODEL_IMAGE extraction service.  This provides integration with vision models beyond those provided by OpenAI.  You can assign a custom specification and bring-your-own API key for image extraction models.\n",
            "⚡ We have deprecated the OPENAI_IMAGE service type, and developers should now use the LLM image service instead.\n",
            "⚡ We have removed the language field from AudioMetadata type, which has been replaced by the new LanguageMetadata type.\n",
            "Bugs Fixed\n",
            "GPLA-2987: Extracting text with Azure Doc Intelligence does not extract hyperlinks\n",
            "PreviousAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "NextJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [179 tokens (includes JSON guardrails tokens)], throughput: 59.221 tokens/sec:\n",
            "- New Features:\n",
            "  - Added indexing workflow stage for configuring indexing services to infer metadata.\n",
            "  - Introduced AZURE_AI_LANGUAGE content indexing service for inferring language from text or transcripts.\n",
            "  - Support for language content metadata in ISO 639-1 format.\n",
            "  - Added MODEL_IMAGE extraction service for integration with custom vision models using a personal API key.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Deprecated OPENAI_IMAGE service type; developers should now use LLM image service.\n",
            "  - Removed language field from AudioMetadata type, replaced with new LanguageMetadata type.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2987 where Azure Doc Intelligence did not extract hyperlinks.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 28, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for language detection, custom image extraction, and improved metadata handling.\n",
            "\n",
            "2025-01-01T20:22:10.609Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.966464, used credits [0.00525000]\n",
            "- CONTENT [79510fa9-9ddb-4b7e-8436-a2b0a867ac64]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [730 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-4-support-for-webhook-alerts-keywords-summarization-deepseek-128k-context-window-bug-fixes</name><title>July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports webhook Alerts.  In addition to Slack notifications, you can now receive an HTTP POST webhook with the results of the published text (or text and audio URI) from a prompted alert.\n",
            "Updated the Deepseek chat and coder models to support a 128k token context window.\n",
            "Added customSummary property to Content object, which returns the custom summary generated via preparation workflow.\n",
            "Added keywords summarization type, which is now stored in keywords property in Content object.\n",
            "Added slackChannels query, which returns the list of Slack channels from the workspace authenticated by the Slack bot token.\n",
            "⚡ We have changed the response from the credits query to return a single ProjectCredits object, rather than the list of correlated objects previously returned.  The credits response now covers all credit usage over the time period specified.\n",
            "Bugs Fixed\n",
            "GPLA-2874: Processing entities is taking longer than 30min for 300+ page PDF\n",
            "GPLA-2875: Messages in queue expiring too early\n",
            "GPLA-2881: Feed read count increasing, after hitting read limit\n",
            "GPLA-2884: Need to handle Anthropic 'overloaded' API response\n",
            "GPLA-2906: JIRA issue identifier not assigned to issue metadata\n",
            "PreviousJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "NextJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [255 tokens (includes JSON guardrails tokens)], throughput: 85.961 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for webhook Alerts, allowing HTTP POST notifications with published text results.\n",
            "  - Updated Deepseek models to support a 128k token context window.\n",
            "  - Added customSummary property to Content object for custom summaries.\n",
            "  - Introduced keywords summarization type, stored in keywords property of Content object.\n",
            "  - Added slackChannels query to list Slack channels from the authenticated workspace.\n",
            "  - Changed credits query response to return a single ProjectCredits object covering all credit usage.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved response structure for credits query.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - GPLA-2874: Reduced processing time for large PDFs.\n",
            "  - GPLA-2875: Fixed issue with messages in queue expiring too early.\n",
            "  - GPLA-2881: Resolved feed read count issue after hitting read limit.\n",
            "  - GPLA-2884: Handled Anthropic 'overloaded' API response.\n",
            "  - GPLA-2906: Assigned JIRA issue identifier to issue metadata.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 4, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve notification capabilities and processing efficiency, offering developers more robust tools for managing alerts and content summarization.\n",
            "\n",
            "2025-01-01T20:22:09.866Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.180422, used credits [0.00510300]\n",
            "- CONTENT [8ca8bf83-6c04-4d64-9953-5699918fbb06]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [737 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/june-2024/june-21-support-for-the-claude-3.5-sonnet-model-knowledge-graph-semantic-search-and-bug-fixes</name><title>June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes | Graphlit Changelog</title></metadata> 🎓\tJune 2024\n",
            "June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the Anthropic Claude 3.5 Sonnet model, which can be assigned with the CLAUDE_3_5_SONNET model enum.\n",
            "💡 Graphlit now supports semantic search of observable entities in the knowledge graph, such as Person, Organization and Place.  These entity types will now have vector embeddings created from their enriched metadata, and support searching by similar text, and searching by similar entities.\n",
            "⚡ We have changed the Google Drive and Google Email feed properties to require the Google OAuth client ID and client secret, along with the existing refresh token, for proper authentication against Google APIs.\n",
            "⚡ We have added a credits quota on the Free Tier.  Once 1000 credits have been used on the Free Tier, no more content can be ingested, and an upgrade to a paid tier is required.  Customers will receive an email when the credits, storage or contents quota has been reached.\n",
            "Bugs Fixed\n",
            "GPLA-2837: Failed to ingest LinkedIn page as Web feed\n",
            "GPLA-2831: Zero-byte file was left in Indexed state\n",
            "GPLA-2834: Not reading any files from Azure blob feed with space in prefix\n",
            "GPLA-2828: Better handling for files with unknown (or missing) file extensions\n",
            "PreviousJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "NextJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [241 tokens (includes JSON guardrails tokens)], throughput: 75.776 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for the Anthropic Claude 3.5 Sonnet model (model enum: CLAUDE_3_5_SONNET).\n",
            "  - Semantic search for observable entities in the knowledge graph (Person, Organization, Place) with vector embeddings for enriched metadata.\n",
            "  - Google Drive and Google Email feed properties now require Google OAuth client ID, client secret, and refresh token for authentication.\n",
            "  - Introduction of a credits quota on the Free Tier; ingestion stops after 1000 credits, requiring an upgrade to a paid tier.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved authentication process for Google APIs.\n",
            "  - Notification system for Free Tier users when credits, storage, or content quotas are reached.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Resolved issues with ingesting LinkedIn pages, handling zero-byte files, reading files from Azure blob feeds with spaces, and better handling of files with unknown or missing extensions.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: June 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with improved model support, better search capabilities, and clearer usage limits on the Free Tier, facilitating more efficient application development.\n",
            "\n",
            "2025-01-01T20:22:08.331Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.982376, used credits [0.00616800]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [836 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/march-2024/march-10-support-for-claude-3-mistral-and-groq-models-usage-credits-telemetry-bug-fixes</name><title>March 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes | Graphlit Changelog</title></metadata> 🍀\tMarch 2024\n",
            "March 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports a Command-Line Interface (CLI) for directly accessing the Graphlit Data API without writing code.  See the documentation here.\n",
            "💡 Graphlit now supports the Groq Platform, and models such as Mixtral 8x7b.\n",
            "💡 Graphlit now supports Claude 3 Opus and Sonnet models.\n",
            "💡 Graphlit now supports Mistral La Plateforme, and models such as Mistral Small, Medium, and Large and Mixtral 8x7b.\n",
            "💡 Graphlit now supports the latest v4 of Azure Document Intelligence, including their new models such as Credit Card, Marriage Certificate, and Mortgage documents.\n",
            "Added support for detailed usage and credits telemetry via API, with the usage, credits, lookupUsage and lookupCredits queries.\n",
            "Added support for correlated telemetry, where an optional correlationId can be provided with GraphQL queries and mutations, so credits and usage can be tracked across requests.\n",
            "Added support for project webhook, which will be called when credits have been consumed by the project.\n",
            "Added support for image extraction during DOCX, XLSX, and PPTX document preparation.\n",
            "Added text and markdown properties to Content object, which provide formatted output of extracted text from any content.\n",
            "Added more accurate extraction of tables into mezzanine JSON format, across all content types.\n",
            "Added throughput property to Conversation messages, which returns the tokens/second throughput of LLM.\n",
            "⚡ Deprecated mezzanineUri property in Content object, which has been replaced by textUri and audioUri.\n",
            "Bugs Fixed\n",
            "GPLA-2281: Not extracting table from PPTX file.\n",
            "GPLA-2282: Not extracting Markdown tables.\n",
            "GPLA-2247: Not extracting relative HTML links properly.\n",
            "GPLA-2241: Failed to post Alert to Slack with Markdown format.\n",
            "PreviousMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "NextFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [305 tokens (includes JSON guardrails tokens)], throughput: 76.587 tokens/sec:\n",
            "- New Features:\n",
            "  - Command-Line Interface (CLI) for direct access to the Graphlit Data API.\n",
            "  - Support for Groq Platform and models like Mixtral 8x7b.\n",
            "  - Support for Claude 3 Opus and Sonnet models.\n",
            "  - Support for Mistral La Plateforme and models such as Mistral Small, Medium, and Large.\n",
            "  - Support for Azure Document Intelligence v4, including new models for Credit Card, Marriage Certificate, and Mortgage documents.\n",
            "  - Detailed usage and credits telemetry via API.\n",
            "  - Correlated telemetry with optional correlationId for tracking credits and usage.\n",
            "  - Project webhook for notifications when credits are consumed.\n",
            "  - Image extraction during DOCX, XLSX, and PPTX document preparation.\n",
            "  - Text and markdown properties added to Content object for formatted output.\n",
            "  - More accurate extraction of tables into mezzanine JSON format.\n",
            "  - Throughput property added to Conversation messages for tokens/second throughput.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Deprecated mezzanineUri property replaced by textUri and audioUri.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with table extraction from PPTX and Markdown files.\n",
            "  - Improved extraction of relative HTML links.\n",
            "  - Resolved failure to post alerts to Slack with Markdown format.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: March 10, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for data extraction, improved API access, and better tracking of usage and credits.\n",
            "\n",
            "2025-01-01T20:22:07.541Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.692224, used credits [0.00318300]\n",
            "- CONTENT [7fb352e7-6eda-419f-848d-d23527d330cc]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [549 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/march-2024/march-13-support-for-claude-3-haiku-model-direct-ingestion-of-base64-encoded-files</name><title>March 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files | Graphlit Changelog</title></metadata> 🍀\tMarch 2024\n",
            "March 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "New Features\n",
            "💡 Graphlit now supports the Claude 3 Haiku model.\n",
            "Added support for direct ingestion of Base64 encoded files with the ingestEncodedFile mutation.  You can pass a Base64 encoded string and MIME type of the file, and it will be ingested into the Graphlit Platform.\n",
            "Added modelService and model properties to ConversationMessage type, which return the model service and model which was used for the LLM completion.\n",
            "PreviousMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "NextMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [128 tokens (includes JSON guardrails tokens)], throughput: 75.640 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Claude 3 Haiku model.\n",
            "  - Direct ingestion of Base64 encoded files via the ingestEncodedFile mutation, allowing the passing of a Base64 encoded string and MIME type.\n",
            "  - Added modelService and model properties to ConversationMessage type for LLM completion details.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - None specified.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: March 13, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support and file ingestion capabilities, improving integration and usability within the Graphlit Platform.\n",
            "\n",
            "2025-01-01T20:22:07.124Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.977006, used credits [0.00719700]\n",
            "- CONTENT [5d06c7c1-735d-41a3-b95e-8bce0b652102]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [947 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/may-2024/may-5-support-for-jina-and-pongo-rerankers-microsoft-teams-feed-new-youtube-downloader-bug-fixes</name><title>May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes | Graphlit Changelog</title></metadata> 💐\tMay 2024\n",
            "May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the Jina reranker and Pongo semantic filtering (reranking), in the Specification object.  Now you can choose between COHERE, PONGO and JINA for your reranking serviceType.\n",
            "💡 Graphlit now supports Microsoft Teams feeds for reading messages from Teams channels.\n",
            "Given changes in YouTube video player HTML, we have rewritten the YouTube downloader to support the new page format.\n",
            "Added better handling of HTTP errors when validating URIs.  Previously some websites were returning HTTP 403 (Forbidden) errors when validating their URI, or downloading content.  Now Graphlit is able to scrape these sites, which previously returned errors.\n",
            "Added support for updating content metadata in updateContent mutation.  Now the video, audio, document, etc. metadata can be updated after the content workflow has finished.\n",
            "Added query_contents_graph (and queryContentsGraph) functions to SDKs, which can be used to return nodes and edges from knowledge graph for visualization.\n",
            "⚡ Citation indices have been changed to be one-based from zero-based.  For example, you will now see \"This is a citation. [1]\" as the first citation in the list.\n",
            "⚡ Added isSynchronous flag to deleteAll and multiple delete mutations.  By default, bulk delete operations are now asynchronous (and completed after the mutation returns), unless the isSynchronous flag is set to true.\n",
            "⚡ Added missing count mutations, such as countAlerts, countFeeds, etc.\n",
            "⚡ Renamed query_content_facets to query_contents_facets in Python SDK\n",
            "⚡ Renamed queryContentFacets to queryContentsFacets in Node.js SDK\n",
            "Bugs Fixed\n",
            "GPLA-2544: Page relevance not filled-in in all situations\n",
            "GPLA-2546: Not extracting links from PDF with Azure AI Doc Intelligence\n",
            "GPLA-2557: Sporadically returning HTTP 500 from GraphQL API\n",
            "GPLA-2573: Failed to re-ingest content which was deleted immediately after initial ingestion\n",
            "GPLA-2575: Not validating for empty (non-null) parameters in mutations\n",
            "GPLA-2578: Need to handle invalid JSON from LLMs; improper escaping or formatting\n",
            "GPLA-2585: Failed to ingest encoded file with colon (:) in name\n",
            "PreviousMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "NextApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [363 tokens (includes JSON guardrails tokens)], throughput: 72.935 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Jina and Pongo rerankers in the Specification object; options for COHERE, PONGO, and JINA as reranking service types.\n",
            "  - Microsoft Teams feeds support for reading messages from Teams channels.\n",
            "  - Rewritten YouTube downloader to accommodate changes in YouTube video player HTML.\n",
            "  - Improved handling of HTTP errors for URI validation and content downloading.\n",
            "  - Support for updating content metadata in updateContent mutation post-workflow.\n",
            "  - Added query_contents_graph functions to SDKs for knowledge graph visualization.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Citation indices changed from zero-based to one-based.\n",
            "  - Added isSynchronous flag for deleteAll and multiple delete mutations; bulk deletes are asynchronous by default.\n",
            "  - Added missing count mutations (e.g., countAlerts, countFeeds).\n",
            "  - Renamed query_content_facets to query_contents_facets in Python SDK.\n",
            "  - Renamed queryContentFacets to queryContentsFacets in Node.js SDK.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed page relevance not being filled in all situations.\n",
            "  - Resolved issues with link extraction from PDFs using Azure AI Doc Intelligence.\n",
            "  - Addressed sporadic HTTP 500 errors from GraphQL API.\n",
            "  - Fixed failures in re-ingesting content deleted immediately after initial ingestion.\n",
            "  - Ensured validation for non-null parameters in mutations.\n",
            "  - Handled invalid JSON from LLMs with proper escaping or formatting.\n",
            "  - Fixed ingestion issues for encoded files with colons in names.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: May 5, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced flexibility with reranking options, improved integration with Microsoft Teams, and better error handling, leading to a more robust and user-friendly platform.\n",
            "\n",
            "2025-01-01T20:22:06.605Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.407434, used credits [0.00655200]\n",
            "- CONTENT [e0f9a487-28bc-4d08-8b71-0dded28e8378]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1012 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/may-2024/may-15-support-for-graphrag-openai-gpt-4o-model-performance-improvements-and-bug-fixes</name><title>May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes | Graphlit Changelog</title></metadata> 💐\tMay 2024\n",
            "May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports GraphRAG, where the extracted entities in the knowledge graph can be added as additional context to your RAG con,versation.  Also, with GraphRAG, entities can be extracted from the user prompt, and used as additional content filters - or can be used to query related content sources, which are combined with the vector search results.  This can be configured by specifying your graphStrategy in the Specification object.\n",
            "💡 Graphlit now supports LLM revisions within RAG conversations, where the LLM can be prompted to revise its initial completion response. From our testing, this has been shown to provide 35% more output tokens with higher quality responses.  This can be configured by specifying your revisionStrategy, and you can use our built-in revision prompt, or provide a custom one, and specify how many revisions you want the LLM to make.\n",
            "💡 Graphlit now supports the new OpenAI GPT-4o model for RAG conversations.\n",
            "⚡ We have changed the default model for Conversations to be OpenAI GPT-4o, from Azure OpenAI GPT-3.5 16k.  This provides faster performance and better quality output.\n",
            "Added graph to promptConversation response, so you can visualize or leverage the nodes and edges of the knowledge graph, resulting from the content retrieval.  For example, if a Person and Organization were observed in the cited content sources used by the RAG pipeline, you will get back those entities and their relationship (such as Person 'works-for' Organization).\n",
            "Expanded the enriched data from WIkipedia to include the long description of an entity.\n",
            "Added getSharePointLibraries, getSharePointFolders, and getOneDriveFolders queries to the API, which can be used to enumerate the storage services.  This makes locating the SharePoint libraryId easier, for example.\n",
            "Added getTeams and getTeamsChannels queries to the API for enumerating Microsoft Teams workspaces.\n",
            "Added extractedCount to the entity extraction connector to limit the number of extracted entities, per entity type.  I.e. if extracted count is 10, it will extract at most ten each of Persons, Organizations, etc.\n",
            "🔥  We have improved performance in several areas: creation of observations after entity extraction, access to cloud storage, rendering the RAG context.\n",
            "🔥  We have optimized the LLM entity extraction process to identify more properties, as well as entity-to-entity relationships.\n",
            "Bugs Fixed\n",
            "GPLA-2652: Not extracting text from HTML in RSS post\n",
            "GPLA-2627: Limit filter only returning half the results\n",
            "GPLA-2613: Not properly extracting structured text from JSON/XML formats\n",
            "PreviousJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "NextMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [293 tokens (includes JSON guardrails tokens)], throughput: 85.988 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for GraphRAG, allowing extracted entities to be added as context in RAG conversations and used as content filters.\n",
            "  - LLM revisions within RAG conversations, improving output quality by 35% with configurable revision strategies.\n",
            "  - Support for OpenAI GPT-4o model in RAG conversations.\n",
            "  - Default model changed to OpenAI GPT-4o for faster performance and better quality.\n",
            "  - Added graph visualization in promptConversation responses to show relationships in the knowledge graph.\n",
            "  - Expanded enriched data from Wikipedia to include long descriptions of entities.\n",
            "  - New API queries: getSharePointLibraries, getSharePointFolders, getOneDriveFolders for easier storage service enumeration.\n",
            "  - New API queries: getTeams and getTeamsChannels for enumerating Microsoft Teams workspaces.\n",
            "  - Added extractedCount to limit the number of extracted entities per type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Performance improvements in entity extraction, cloud storage access, and RAG context rendering.\n",
            "  - Optimized LLM entity extraction to identify more properties and relationships.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with text extraction from HTML in RSS posts.\n",
            "  - Resolved limit filter returning incomplete results.\n",
            "  - Corrected structured text extraction from JSON/XML formats.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: May 15, 2024.\n",
            "  - New feature flags and performance enhancements improve developer experience and output quality.\n",
            "\n",
            "2025-01-01T20:22:05.748Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.262976, used credits [0.00450000]\n",
            "- CONTENT [8f1c79b1-beaf-481d-a7a7-078105781fbb]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [700 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/march-2024/march-23-support-for-linear-github-issues-and-jira-issue-feeds-ingest-files-via-web-feed-sitemap</name><title>March 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap | Graphlit Changelog</title></metadata> 🍀\tMarch 2024\n",
            "March 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "New Features\n",
            "💡 Graphlit now supports Linear, GitHub Issues and Atlassian Jira feeds.  Graphlit will ingest issues (aka tasks, stories) from these issue-tracking services as individual content items, which will be made searchable and conversational.\n",
            "💡 Added support for ISSUEcontent type, which includes metadata such as title, authors, commenters, status, type, project and team.\n",
            "💡 Added support for default feed read limit.  Now, if you don't specify the readLimit property on feeds, it will default to reading 100 content items.  You can override this default by assigning a custom read limit, which has no upper bounds.  However, one-shot feeds much complete within 15 minutes, or they will be stopped automatically.\n",
            "Added support for ingesting files referenced in a Web sitemap.  Previously any files (i.e. PDF, MP3) referenced in a sitemap.xml would be ignored.  Now you can optionally enable includeFiles in the WebFeedPropertiesInput object to have Graphlit ingest non-HTML pages as part of the Web feed.\n",
            "Bugs Fixed\n",
            "GPLA-2374: Failed to ingest MP4 with large XMP metadata.\n",
            "PreviousApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "NextMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "Last updated8 months ago\n",
            "- Completion [200 tokens (includes JSON guardrails tokens)], throughput: 88.379 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Linear, GitHub Issues, and Atlassian Jira feeds for ingesting issues as searchable content items.\n",
            "  - Introduction of ISSUE content type with metadata including title, authors, commenters, status, type, project, and team.\n",
            "  - Default feed read limit set to 100 content items, customizable with no upper bounds; one-shot feeds must complete within 15 minutes.\n",
            "  - Ability to ingest files referenced in a Web sitemap, including non-HTML pages like PDFs and MP3s.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of files in Web sitemaps by enabling ingestion of non-HTML content.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2374 related to failing to ingest MP4 files with large XMP metadata.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: March 2024.\n",
            "  \n",
            "- Value:\n",
            "  - Offers developers enhanced integration with popular issue-tracking services and improved content ingestion capabilities.\n",
            "\n",
            "2025-01-01T20:22:04.309Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.038406, used credits [0.00537300]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [855 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-10-support-for-web-search-multi-turn-content-summarization-deepgram-language-detection</name><title>November 10: Support for web search, multi-turn content summarization, Deepgram language detection | Graphlit Changelog</title></metadata> 🦃\tNovember 2024\n",
            "November 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "New Features\n",
            "💡 Graphlit now supports web search with the searchWeb mutation.  You can select the search service, either Tavily or Exa.AI, and provide the search query and number of search results to be returned.  This is different than the web search feed, in that searchWeb returns the relevant text from the web page and the web page URL from each search hit, but does not ingest each of the web pages. This new mutation is optimized to be used from within an LLM tool.\n",
            "💡 Graphlit now supports multi-turn summarization of content with the reviseContent mutation.  You can provide an LLM prompt and a content reference, along with an optional specification.  This can be used for summarizing any content (documents, web pages, audio transcripts, etc.), and having a multi-turn conversation with the LLM to revise the output from the LLM.  Internally, this creates a conversation locked to a single piece of content.  This works especially well with the OpenAI o1-preview and o1-mini models, because they provide a longer LLM output from each turn.\n",
            "Graphlit now supports the configuration of the Deepgram transcription language, and whether detectLanguage is enabled in DeepgramAudioPreparationPropertiesInput.  Language detection is now enabled by default, and can be disabled by setting detectLanguage to false.\n",
            "⚡ We have added a requireTool option to promptConversation mutation, so you can control whether the LLM must call one of the provided tool, or if tool calling is optional.\n",
            "⚡ For accounts created after Nov 8, 2024, we have lowered the credits quota on the Free tier from 1000 credits to 100 credits, and now offer unlimited feeds on the Hobby Tier.\n",
            "⚡ The Graphlit Data API will now return HTTP 402 (Payment Required) when you have exceeded the credits quota on the free tier.  You must upgrade to the Hobby Tier (or higher) to continue using the API, once the credits quota has been reached.\n",
            "PreviousNovember 16: Support for image description, multi-turn text summarization\n",
            "NextNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "Last updated1 month ago\n",
            "- Completion [234 tokens (includes JSON guardrails tokens)], throughput: 77.014 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for web search via the searchWeb mutation, allowing selection of search service (Tavily or Exa.AI) and retrieval of relevant text and URLs without ingesting web pages.\n",
            "  - Multi-turn content summarization with the reviseContent mutation, enabling conversation with LLM for summarizing various content types.\n",
            "  - Configuration of Deepgram transcription language and default language detection enabled in DeepgramAudioPreparationPropertiesInput.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Added requireTool option to promptConversation mutation for controlling tool calling.\n",
            "  - Reduced credits quota on Free tier from 1000 to 100 for accounts created after Nov 8, 2024, with unlimited feeds on the Hobby Tier.\n",
            "  - Graphlit Data API now returns HTTP 402 (Payment Required) when exceeding credits quota on the Free tier.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 10, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for web search and content summarization, improved control over tool usage, and clearer API usage guidelines regarding credits.\n",
            "\n",
            "2025-01-01T20:22:03.425Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.313494, used credits [0.00547500]\n",
            "- CONTENT [9303dfbd-2681-4e9b-93ec-2f6a8c1c326f]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [761 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-24-support-for-direct-llm-prompt-multi-turn-image-analysis-bug-fixes</name><title>November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes | Graphlit Changelog</title></metadata> 🦃\tNovember 2024\n",
            "November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports multi-turn analysis of images with the reviseImage and reviseEncodedImage mutations.  You can provide an LLM prompt and either a URI or Base-64 encoded image and MIME type, along with an optional LLM specification.  This can be used for analyzing any image and having a multi-turn conversation with the LLM to revise the output from the LLM. (Colab Notebook Example)\n",
            "💡 Graphlit now supports directly prompting an LLM with the prompt mutation, bypassing any RAG content retrieval, while providing an optional list of previous conversation messages.  This also accepts an optional LLM specification. (Colab Notebook Example)\n",
            "We have added support for the new Mistral Pixtral Large model, with PIXTRAL_LARGE model enum, which can be used with LLM completion or entity extraction LLM specifications.\n",
            "We have added support for the OpenAI 2024-11-20 version of GPT-4o, with GPT4O_128K_20241120 model enum.\n",
            "⚡ We have added Microsoft Entra ID (fka Azure Active Directory) clientId and clientSecret properties to the SharePointFeedPropertiesInput type, which are now required when creating a SharePoint feed using user authentication with refreshToken property. (Colab Notebook Example)\n",
            "Bugs Fixed\n",
            "GPLA-3438: Not filtering on desktop presentation when scraping web pages\n",
            "GPLA-3340: Failed to parse invalid JSON from extracted PDF page\n",
            "GPLA-3427: Not formatting extracted tables properly from Sonnet 3.5\n",
            "PreviousDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "NextNovember 16: Support for image description, multi-turn text summarization\n",
            "Last updated1 month ago\n",
            "- Completion [266 tokens (includes JSON guardrails tokens)], throughput: 80.278 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for multi-turn analysis of images using reviseImage and reviseEncodedImage mutations, allowing LLM prompts with image URIs or Base-64 encoded images.\n",
            "  - Direct prompting of an LLM with the prompt mutation, bypassing RAG content retrieval, with optional previous conversation messages.\n",
            "  - Support for Mistral Pixtral Large model (PIXTRAL_LARGE) for LLM completion or entity extraction.\n",
            "  - Support for OpenAI GPT-4o (2024-11-20 version) with GPT4O_128K_20241120 model enum.\n",
            "  - Added Microsoft Entra ID clientId and clientSecret properties for SharePoint feed creation with user authentication.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Required properties for SharePoint feed creation have been updated.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed filtering issue on desktop presentation when scraping web pages (GPLA-3438).\n",
            "  - Resolved JSON parsing error from extracted PDF pages (GPLA-3340).\n",
            "  - Corrected formatting of extracted tables from Sonnet 3.5 (GPLA-3427).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve image analysis capabilities and LLM interactions, streamlining workflows for developers.\n",
            "\n",
            "2025-01-01T20:22:03.088Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.345251, used credits [0.00607500]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [765 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2023/october-15-support-for-anthropic-claude-models-slack-feeds-and-entity-enrichment</name><title>October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment | Graphlit Changelog</title></metadata> 🎃\tOctober 2023\n",
            "October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "New Features\n",
            "🔥 Graphlit now supports Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "🔥 Graphlit now supports Slack feeds, and will ingest Slack messages and linked file attachments from a Slack channel.  Note, this requires the creation of a Slack bot which has been added to the appropriate Slack channel.\n",
            "💡 Added support for entity enrichment to workflow object, which offers Diffbot, Wikipedia and Crunchbase enrichment of observed entities, such as Person, Organization and Place.\n",
            "💡 Added support for text extraction from images.  When using Azure Image Analytics for entity extraction, Graphlit will extract and store any identified text which then becomes searchable.\n",
            "Added embedFacets property to conversation strategy in specification object.\n",
            "Added embedCitations property to conversation strategy in specification object.  This makes content citations optional with the completed conversation message.\n",
            "Added GraphQL mutations for multi-delete of entities, such as deleteCollections, deleteLabels, or deleteConversations.\n",
            "Added GraphQL deleteAllConversations mutation to delete all conversations.\n",
            "Added support for automatically adding ingested content to one or more collections, via ingestion stage of workflow object.\n",
            "Added specification property to preparation workflow stage, which will be used to select the LLM for text summarization.\n",
            "Expanded the properties for observed entities, such as Person, Organization or Product.  Now supports a wider range of properties for entity enrichment.\n",
            "Bugs Fixed\n",
            "GPLA-1520: Unlimited conversation quota not assigned when upgrading project tier\n",
            "GPLA-1285: Entity enrichment not firing event, which can be sent to actions\n",
            "GPLA-1361: Web page left in ingested state, when URL not accessible.\n",
            "PreviousOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "NextSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "Last updated1 year ago\n",
            "- Completion [315 tokens (includes JSON guardrails tokens)], throughput: 72.493 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "  - Ingestion of Slack messages and linked file attachments from Slack channels (requires a Slack bot).\n",
            "  - Entity enrichment for observed entities (Person, Organization, Place) using Diffbot, Wikipedia, and Crunchbase.\n",
            "  - Text extraction from images using Azure Image Analytics, making identified text searchable.\n",
            "  - Added embedFacets and embedCitations properties to conversation strategy in specification object.\n",
            "  - GraphQL mutations for multi-delete of entities (deleteCollections, deleteLabels, deleteConversations).\n",
            "  - GraphQL deleteAllConversations mutation to delete all conversations.\n",
            "  - Automatic addition of ingested content to collections during the ingestion stage of the workflow.\n",
            "  - Specification property added to preparation workflow stage for selecting LLM for text summarization.\n",
            "  - Expanded properties for observed entities for enhanced entity enrichment.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved support for a wider range of properties for entity enrichment.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed unlimited conversation quota not assigned when upgrading project tier (GPLA-1520).\n",
            "  - Resolved issue with entity enrichment not firing events (GPLA-1285).\n",
            "  - Fixed web page remaining in ingested state when URL is not accessible (GPLA-1361).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 15, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for integrating and enriching data from various sources, improving the overall functionality and usability of the Graphlit Platform.\n",
            "\n",
            "2025-01-01T20:22:02.055Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.921770, used credits [0.00363900]\n",
            "- CONTENT [f6df142b-f920-4c85-b4d9-1dc09d7d2315]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [613 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-16-support-for-image-description-multi-turn-text-summarization</name><title>November 16: Support for image description, multi-turn text summarization | Graphlit Changelog</title></metadata> 🦃\tNovember 2024\n",
            "November 16: Support for image description, multi-turn text summarization\n",
            "New Features\n",
            "💡 Graphlit now supports multi-turn summarization of text with the reviseText mutation.  You can provide an LLM prompt and text string, along with an optional specification.  This can be used for summarizing any raw text and having a multi-turn conversation with the LLM to revise the output from the LLM.  (Colab Notebook Example)\n",
            "💡 Graphlit now supports image descriptions using vision LLMs, without needing to ingest the image first.  With the new describeImage mutation, which takes a URI, and describeEncodedImage mutation, which takes a Base-64 encoded image and MIME type, you can use any vision LLM to prompt an image description.  These mutations accept an optional specification, where you can select your vision LLM.  If not provided, OpenAI GPT-4o will be used. (Colab Notebook Example)\n",
            "PreviousNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "NextNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "Last updated1 month ago\n",
            "- Completion [150 tokens (includes JSON guardrails tokens)], throughput: 78.053 tokens/sec:\n",
            "- New Features:\n",
            "  - Multi-turn summarization of text with the reviseText mutation, allowing for LLM prompts and text revisions.\n",
            "  - Image descriptions supported via describeImage mutation (URI input) and describeEncodedImage mutation (Base-64 input), utilizing vision LLMs.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Optional specifications for selecting vision LLMs; defaults to OpenAI GPT-4o if not specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned in this release.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 16, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for text summarization and image description without prior image ingestion, improving interaction with LLMs.\n",
            "\n",
            "2025-01-01T20:22:01.169Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.615696, used credits [0.00386100]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [619 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-4-support-for-anthropic-claude-3.5-haiku-bug-fixes</name><title>November 4: Support for Anthropic Claude 3.5 Haiku, bug fixes | Graphlit Changelog</title></metadata> 🦃\tNovember 2024\n",
            "November 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "New Features\n",
            "Graphlit now supports the latest Anthropic Haiku 3.5 model, with the model enum CLAUDE_3_5_HAIKU_20241022.\n",
            "⚡ Once a project has hit the free tier quota, we will now automatically disable all feeds.  Once the project has been upgraded to a paid tier, you can use the enableFeed mutation to re-enable your existing feeds to continue ingestion.\n",
            "⚡ We have added the disableFallback flag to the RetrievalStrategyInput type, so you can disable the default behavior of falling back to the previous conversation's contents, or worst-case, falling back to the most recently uploaded content.  By setting disableFallback to true, conversations will only attempt to retrieve contents based on the provided filter and/or augmentedFilter properties.\n",
            "Bugs Fixed\n",
            "GPLA-3367: Not extracting text from HTML button element\n",
            "PreviousNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "NextOctober 31: Support for simulated tool calling, bug fixes\n",
            "Last updated1 month ago\n",
            "- Completion [167 tokens (includes JSON guardrails tokens)], throughput: 63.845 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Anthropic Haiku 3.5 model (model enum: CLAUDE_3_5_HAIKU_20241022).\n",
            "  - Automatic disabling of all feeds upon reaching the free tier quota; re-enable feeds with enableFeed mutation after upgrading to a paid tier.\n",
            "  - Addition of disableFallback flag in RetrievalStrategyInput type to control fallback behavior in conversations.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved control over conversation content retrieval with the new disableFallback flag.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3367: Text extraction from HTML button elements.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 4, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support, better feed management, and improved content retrieval control.\n",
            "\n",
            "2025-01-01T20:22:00.028Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.568185, used credits [0.00577200]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [812 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-21-support-openai-cohere-jina-mistral-voyage-and-google-ai-embedding-models</name><title>October 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "New Features\n",
            "💡 Graphlit now supports the configuration of image and text embedding models, at the Project level.  You can create an embedding specification for a text or image embedding model, and then assign that to the Project, and all further embedding requests will use that embedding model.  See this Colab notebook for an example of how to configure the project.\n",
            "💡 Graphlit now supports the OpenAI Embedding-3-Small and Embedding-3-Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0 and 3.0 text embedding models.  Graphlit also now supports Jina CLIP image embeddings, which are used by default for image search.\n",
            "Graphlit now supports the chunkTokenLimit property in Specifications, which specifies the number of tokens for each embedded text chunk.  If this is not configured, Graphlit uses 600 tokens for each embedded text chunk.\n",
            "Graphlit now supports the Voyage reranking model.\n",
            "Graphlit now supports the ingestTextBatch mutation, which accepts an array of text and name pairs, and will asynchronously ingest these into content objects.\n",
            "⚡ We have moved the chunkTokenLimit property from the Workflow storage embeddings strategy to the Specification object.  The Workflow storage property has now been deprecated.\n",
            "⚡ We have deprecated the openAIImage property from Workflow entity extraction properties. Use the modelImage property instead.\n",
            "Once a text embedding model has been updated at the project level, any content, conversations or observed entities will no longer be semantically searchable.\n",
            "Text embeddings are not compatible across models, so you will need to delete and reingest any content, or recreate conversations or knowledge graph entities, with the new embedding model to become searchable.\n",
            "PreviousOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "NextOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "Last updated2 months ago\n",
            "- Completion [278 tokens (includes JSON guardrails tokens)], throughput: 77.911 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for configuration of image and text embedding models at the Project level.\n",
            "  - Support for OpenAI Embedding-3-Small and Embedding-3-Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0 and 3.0 text embedding models.\n",
            "  - Support for Jina CLIP image embeddings for image search.\n",
            "  - Introduction of chunkTokenLimit property in Specifications for token count per embedded text chunk.\n",
            "  - Support for Voyage reranking model.\n",
            "  - New ingestTextBatch mutation for asynchronous ingestion of text and name pairs.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Moved chunkTokenLimit property from Workflow storage embeddings strategy to Specification object; Workflow storage property deprecated.\n",
            "  - Deprecated openAIImage property from Workflow entity extraction properties; use modelImage property instead.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Updating text embedding models at the project level will affect semantic searchability of content, conversations, or observed entities.\n",
            "  - Text embeddings are incompatible across models; requires deletion and reingestion of content for new embedding model searchability.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 2024.\n",
            "  \n",
            "- Value:\n",
            "  - Offers developers enhanced flexibility in embedding model configuration and improved ingestion capabilities, while ensuring compatibility and searchability of content.\n",
            "\n",
            "2025-01-01T20:21:59.951Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.010855, used credits [0.00559200]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [740 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2023/october-30-optimized-conversation-responses-added-observable-aliases-bug-fixes</name><title>October 30: Optimized conversation responses; added observable aliases; bug fixes | Graphlit Changelog</title></metadata> 🎃\tOctober 2023\n",
            "October 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports 'aliases' of observable names, as the alternateNames property.  When an observed entity, such as Organization, is enriched, we store the original name and the enriched name as an alias.  For example, \"OpenAI\" may be enriched to \"OpenAI, Inc.\", and we store \"OpenAI\" as an alias, and update the name to \"OpenAI, Inc.\".\n",
            "💡 Added workflows filter to ContentCriteriaInput type, for filtering content by workflow(s) when creating conversation.\n",
            "Optimized formatting of content sources into prompt context, for more accurate conversation responses.\n",
            "Optimized formatting of extracted text from Slack messages, for better knowledge retrieval.\n",
            "Updated text tokenizer for more accurate token counting.\n",
            "Upgraded Azure Text Analytics to latest preview API version.\n",
            "Authors found in RSS feeds are now stored as observations of Person entities.\n",
            "Added rate limiting for Reddit feeds.\n",
            "Added rate limiting for Wikipedia enrichment.\n",
            "Added support for reading Reddit post comments when reading Reddit feed.\n",
            "⚡ EmbedFacets has been renamed to EnableFacets in the conversation strategy.\n",
            "⚡ Removed extra content level in IngestionWorkflowStage type.  Now, the if property is of type IngestionContentFilter.\n",
            "Bugs Fixed\n",
            "GPLA-1556: Better handling of very long user prompts.\n",
            "GPLA-1627: Optimized token budget for more accurate prompt completion.\n",
            "GPLA-1585: More accurate entity matching in Wikipedia entity enrichment.\n",
            "PreviousDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "NextOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "Last updated1 year ago\n",
            "- Completion [281 tokens (includes JSON guardrails tokens)], throughput: 93.329 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for 'aliases' of observable names via the alternateNames property.\n",
            "  - Workflows filter added to ContentCriteriaInput type for filtering content by workflow(s).\n",
            "  - Optimized formatting of content sources for improved conversation responses.\n",
            "  - Enhanced formatting of extracted text from Slack messages for better knowledge retrieval.\n",
            "  - Updated text tokenizer for accurate token counting.\n",
            "  - Upgraded Azure Text Analytics to the latest preview API version.\n",
            "  - Authors in RSS feeds are now stored as observations of Person entities.\n",
            "  - Rate limiting added for Reddit feeds and Wikipedia enrichment.\n",
            "  - Support for reading Reddit post comments.\n",
            "  - EmbedFacets renamed to EnableFacets in conversation strategy.\n",
            "  - Removed extra content level in IngestionWorkflowStage type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Better handling of long user prompts.\n",
            "  - Optimized token budget for prompt completion.\n",
            "  - More accurate entity matching in Wikipedia entity enrichment.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Improved handling of long user prompts (GPLA-1556).\n",
            "  - Optimized token budget for accurate prompt completion (GPLA-1627).\n",
            "  - Enhanced entity matching in Wikipedia enrichment (GPLA-1585).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved accuracy in conversation responses, better content filtering, and enhanced entity recognition capabilities.\n",
            "\n",
            "2025-01-01T20:21:58.462Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.590128, used credits [0.00431100]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [597 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-22-support-for-latest-anthropic-sonnet-3.5-model-cohere-image-embeddings</name><title>October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "New Features\n",
            "Graphlit now supports the latest Anthropic Sonnet 3.5 model (released 10/22/2024).  We have added date-versions model enums for the Anthropic models: CLAUDE_3_5_SONNET_20240620, CLAUDE_3_5_SONNET_20241022, CLAUDE_3_HAIKU_20240307, CLAUDE_3_OPUS_20240229, CLAUDE_3_SONNET_20240229. The existing model enums will target the latest released models, as specified by Anthropic.\n",
            "Graphlit now supports image embeddings using the Cohere Embed 3.0 models.\n",
            "PreviousOctober 31: Support for simulated tool calling, bug fixes\n",
            "NextOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "Last updated2 months ago\n",
            "- Completion [210 tokens (includes JSON guardrails tokens)], throughput: 81.077 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for the latest Anthropic Sonnet 3.5 model (released 10/22/2024).\n",
            "  - Added date-version model enums for Anthropic models: CLAUDE_3_5_SONNET_20240620, CLAUDE_3_5_SONNET_20241022, CLAUDE_3_HAIKU_20240307, CLAUDE_3_OPUS_20240229, CLAUDE_3_SONNET_20240229.\n",
            "  - Support for image embeddings using Cohere Embed 3.0 models.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Existing model enums will now target the latest released models as specified by Anthropic.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned in this release.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers access to the latest AI models and enhanced image embedding capabilities, improving functionality and integration options.\n",
            "\n",
            "2025-01-01T20:21:58.376Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.810422, used credits [0.00530700]\n",
            "- CONTENT [68397ab0-4d1f-4b4a-bb0d-af89d00ba47f]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [761 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-3-support-tool-calling-ingestbatch-mutation-gemini-flash-1.5-8b-bug-fixes</name><title>October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the ingestBatch mutation, which accepts an array of URIs to files or web pages, and will asynchronously ingest these into content objects.\n",
            "💡 Graphlit now supports the continueConversation mutation, which accepts an array of called tool responses. Also, promptConversation now accepts an array of tool definitions. When tools are called by the LLM, the assistant message returned from promptConversation will have a list of toolCalls which need to responded to from your calling code.  These responses are to be provided back to the LLM via the continueConversation mutation.\n",
            "💡 Graphlit now supports tool calling with OpenAI, Mistral, Deepseek, Groq, and Cerebras model services.  Anthropic, Google Gemini and Cohere support will come later.\n",
            "Added support for prefilled user and assistant messages with createConversation mutation. Now you can send an array of messages when creating a new conversation, which will bootstrap the conversation with the LLM.  These must be provided in user/assistant pairs.\n",
            "Added support for Google Gemini Flash 1.5 8b model.\n",
            "⚡ We have deprecated the tools property in the Specification object. These will be removed at a later date.  Tools are now to be sent directly to the extractContents and promptConversation mutations.\n",
            "Bugs Fixed\n",
            "GPLA-3207: Models shouldn't be required on update specification call\n",
            "GPLA-3220: Don't send system prompt with OpenAI o1 models\n",
            "PreviousOctober 7: Support for Anthropic and Gemini tool calling\n",
            "NextSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "Last updated2 months ago\n",
            "- Completion [252 tokens (includes JSON guardrails tokens)], throughput: 89.666 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for ingestBatch mutation to asynchronously ingest an array of URIs into content objects.\n",
            "  - Support for continueConversation mutation, allowing responses to tool calls to be sent back to the LLM.\n",
            "  - Tool calling support added for OpenAI, Mistral, Deepseek, Groq, and Cerebras model services; Anthropic, Google Gemini, and Cohere support coming later.\n",
            "  - Prefilled user and assistant messages can now be sent with createConversation mutation.\n",
            "  - Added support for Google Gemini Flash 1.5 8b model.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Deprecated tools property in the Specification object; tools must now be sent directly to extractContents and promptConversation mutations.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where models were incorrectly required on update specification call (GPLA-3207).\n",
            "  - Resolved issue of sending system prompt with OpenAI o1 models (GPLA-3220).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 2024.\n",
            "  - Version: Gemini Flash 1.5 8b.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for conversation management and model integration, improving the overall functionality of the Graphlit Platform.\n",
            "\n",
            "2025-01-01T20:21:56.824Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.964748, used credits [0.00363300]\n",
            "- CONTENT [119c6576-e615-476e-8ab9-27d841050c5c]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [587 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-31-support-for-simulated-tool-calling-bug-fixes</name><title>October 31: Support for simulated tool calling, bug fixes | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 31: Support for simulated tool calling, bug fixes\n",
            "New Features\n",
            "Graphlit now supports simulated tool calling for LLMs which don't natively support it, such as OpenAI o1-preview and o1-mini.  Tool schema will be formatted into the LLM prompt context, and tool responses are parsed out of the JSON formatted response.\n",
            "⚡ Given customer feedback, we have lowered the vector and hybrid thresholds used by the semantic search.  Previously, some content at a low relevance was being excluded from the semantic search results.  Now, more low-relevance content will be included in the results, used by the RAG pipeline.  Reranking can be used to sort the search results for relevance.\n",
            "Bugs Fixed\n",
            "GPLA-3357: Not extracting all images from PDF, and should filter out single-color images.\n",
            "PreviousNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "NextOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "Last updated2 months ago\n",
            "- Completion [156 tokens (includes JSON guardrails tokens)], throughput: 79.399 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for simulated tool calling for LLMs like OpenAI o1-preview and o1-mini.\n",
            "  - Tool schema formatted into LLM prompt context; tool responses parsed from JSON.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Lowered vector and hybrid thresholds for semantic search based on customer feedback, allowing more low-relevance content in results.\n",
            "  - Reranking feature for sorting search results by relevance.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3357: now extracts all images from PDF and filters out single-color images.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve search result relevance and broaden content inclusion, benefiting developers in optimizing LLM interactions.\n",
            "\n",
            "2025-01-01T20:21:56.326Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.389897, used credits [0.00678300]\n",
            "- CONTENT [8413f583-bc40-4957-b03b-f62d5078fe09]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [845 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2023/september-20-paid-subscription-plans-support-for-custom-observed-entities-and-azure-openai-gpt-4</name><title>September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4 | Graphlit Changelog</title></metadata> 🛠️\tSeptember 2023\n",
            "September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "New Features\n",
            "🔥 Graphlit now supports paid Hobby, Starter and Growth tiers for projects, in addition to the existing Free tier.  Starting at $49/mo, plus $0.10/credit for usage, we now support higher quota based on your subscribed tier.   By providing a payment method for your organization in the Developer Portal, you can upgrade each project individually to the tier that fits your application's needs.\n",
            "💡 Added GraphQL mutations for the creation, update and deletion of observed entities (i.e. Person, Organization, Place, Product, Event, Label, Category).\n",
            "💡 Added new observed entity types to knowledge graph: Repo (i.e. Git repo), Software.\n",
            "💡 Added searchType and numberSimilar fields to Specification object for configuring semantic search in conversations.   In situations where the user prompt is limited in length, HYBRID search type can provide better semantic search results for the prompt context.\n",
            "💡 Added support for the Azure OpenAI GPT-4 model.\n",
            "Added support for project quota field.  Project quotas are based on the subscribed pricing tier.   Quota limits are now applied as content is ingested, and as feeds and conversations are created.\n",
            "Added contentLimit to conversation strategy object to limit the number of semantic search content results which are formatted into prompt context.\n",
            "Better relevance ranking on semantic search results when formatting prompt context in conversations.\n",
            "ℹ️ Free tier has updated quota: 1GB storage, 100 contents, 3 feeds and 10 conversations.\n",
            "⚡ Now using the Deepgram Nova-2 audio transcription model, which is 18% more accurate, and 5-40x faster.\n",
            "Bugs Fixed\n",
            "GPLA-1373: Failed to extract multiple text pages from DOCX without page breaks.  Now we support token-aware page chunking.\n",
            "GPLA-1377: Failed during semantic search with no content results, when prompting conversation.\n",
            "GPLA-1415: Failed when user prompt couldn't generate text embeddings.\n",
            "PreviousSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "NextSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "Last updated11 months ago\n",
            "- Completion [354 tokens (includes JSON guardrails tokens)], throughput: 80.640 tokens/sec:\n",
            "- New Features:\n",
            "  - Introduction of paid Hobby, Starter, and Growth subscription tiers starting at $49/month, with usage fees of $0.10/credit.\n",
            "  - GraphQL mutations for creating, updating, and deleting observed entities (Person, Organization, Place, Product, Event, Label, Category).\n",
            "  - New observed entity types added: Repo (Git repo), Software.\n",
            "  - Added searchType and numberSimilar fields to Specification object for enhanced semantic search in conversations.\n",
            "  - Support for Azure OpenAI GPT-4 model.\n",
            "  - Project quota field introduced, with limits based on the subscribed pricing tier.\n",
            "  - ContentLimit added to conversation strategy object to manage semantic search content results.\n",
            "  - Improved relevance ranking for semantic search results in conversations.\n",
            "  - Updated Free tier quota: 1GB storage, 100 contents, 3 feeds, and 10 conversations.\n",
            "  - Implementation of Deepgram Nova-2 audio transcription model, offering 18% more accuracy and 5-40x faster performance.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Better relevance ranking on semantic search results.\n",
            "  - Quota limits applied during content ingestion and conversation creation.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Resolved issue with extracting multiple text pages from DOCX without page breaks (GPLA-1373).\n",
            "  - Fixed semantic search failure when no content results were available (GPLA-1377).\n",
            "  - Addressed failure in generating text embeddings from user prompts (GPLA-1415).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 20, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers flexible subscription options, enhanced search capabilities, and improved performance for audio transcription and semantic search, facilitating better project management and user experience.\n",
            "\n",
            "2025-01-01T20:21:55.766Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.521796, used credits [0.00295800]\n",
            "- CONTENT [1243312d-9b3b-4172-88ea-ee1b5c097b2c]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [498 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-7-support-for-anthropic-and-gemini-tool-calling</name><title>October 7: Support for Anthropic and Gemini tool calling | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 7: Support for Anthropic and Gemini tool calling\n",
            "New Features\n",
            "💡 Graphlit now supports tool calling with Anthropic and Google Gemini models.\n",
            "⚡ We have removed the uri property for tools from ToolDefinitionInput, such that inline webhook tools are no longer supported.  Now you can define any external tools to be called, and those can support sync or async data access to fulfill the tool call.\n",
            "PreviousOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "NextOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "Last updated2 months ago\n",
            "- Completion [122 tokens (includes JSON guardrails tokens)], throughput: 80.168 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for tool calling with Anthropic and Google Gemini models.\n",
            "  - Removal of the uri property for tools from ToolDefinitionInput, eliminating support for inline webhook tools. External tools can now be defined for sync or async data access.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved flexibility in defining external tools for tool calling.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 7, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for integrating external tools and improved data access options.\n",
            "\n",
            "2025-01-01T20:21:55.493Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.761517, used credits [0.00252000]\n",
            "- CONTENT [833da803-989f-4c1c-96af-c4cf003e6e52]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [484 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-9-support-for-github-repository-feeds-bug-fixes</name><title>October 9: Support for GitHub repository feeds, bug fixes | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 9: Support for GitHub repository feeds, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports GitHub feeds, by providing the repository owner and name similar to GitHub Issues feeds, and will ingest code files from any GitHub repository.\n",
            "Bugs Fixed\n",
            "GPLA-3262: Missing row separator in table markdown formatting\n",
            "PreviousOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "NextOctober 7: Support for Anthropic and Gemini tool calling\n",
            "Last updated2 months ago\n",
            "- Completion [89 tokens (includes JSON guardrails tokens)], throughput: 50.525 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for GitHub repository feeds, allowing ingestion of code files by providing the repository owner and name.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed missing row separator in table markdown formatting (GPLA-3262).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances integration with GitHub, improving developers' ability to work with code repositories.\n",
            "\n",
            "2025-01-01T20:21:54.809Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.201810, used credits [0.00522900]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [739 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2023/september-4-workflow-configuration-support-for-notion-feeds-document-ocr</name><title>September 4: Workflow configuration; support for Notion feeds; document OCR | Graphlit Changelog</title></metadata> 🛠️\tSeptember 2023\n",
            "September 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "New Features\n",
            "🔥 Added Workflow entity to data model for configuring stages of content workflow; can be assigned to Feed or with ingestPage, ingestFile, or ingestText mutations to control how content is ingested, prepared, extracted and enriched into the knowledge graph.\n",
            "💡 Added support for Notion feeds: now can create feed to ingest files from Notion pages or databases (i.e. wikis).\n",
            "💡 Added support for API-created Observation entities, which allow for custom observations of observable entities (i.e. Person, Label) on Content.\n",
            "💡 Added support for Azure AI Document Intelligence as an optional method for preparing PDF files, using OCR and advanced layout analysis.\n",
            "💡 Added summarization strategies, where content can be summarized into paragraphs, bullet points or headline.\n",
            "Added ability to assign default Workflow and Specification to project.\n",
            "Added more well-known link types, during link crawling, such as Discord, Airtable and TypeForm.\n",
            "ℹ️ Free/Hobby plan now has 5GB storage quota; any content ingested past that limit will be auto-deleted.\n",
            "⚡ Actions have been moved into Workflow entity.\n",
            "⚡ Link enrichment for Feeds has been moved into the Workflow enrichment stage, now called link crawling.  ExcludeContentDomain property has been reversed and is now called IncludeContentDomain.\n",
            "Bugs Fixed\n",
            "GPLA-1204: Failed to ingest content with backslash in name.\n",
            "GPLA-1276: Failed to ingest RSS posts which contained enclosure URI, but no post URI.\n",
            "PreviousSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "NextAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "Last updated1 year ago\n",
            "- Completion [251 tokens (includes JSON guardrails tokens)], throughput: 78.393 tokens/sec:\n",
            "- New Features:\n",
            "  - Added Workflow entity for configuring content workflow stages.\n",
            "  - Support for Notion feeds to ingest files from Notion pages or databases.\n",
            "  - API-created Observation entities for custom observations on Content.\n",
            "  - Support for Azure AI Document Intelligence for PDF preparation using OCR.\n",
            "  - Summarization strategies for content into paragraphs, bullet points, or headlines.\n",
            "  - Ability to assign default Workflow and Specification to projects.\n",
            "  - Added support for more link types during link crawling (e.g., Discord, Airtable, TypeForm).\n",
            "  - Free/Hobby plan now includes a 5GB storage quota.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Actions moved into Workflow entity.\n",
            "  - Link enrichment for Feeds moved to Workflow enrichment stage, renamed to link crawling.\n",
            "  - ExcludeContentDomain property renamed to IncludeContentDomain.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with content ingestion failure due to backslash in name.\n",
            "  - Resolved failure to ingest RSS posts with enclosure URI but no post URI.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 4, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced workflow configuration, improved content ingestion from Notion, and advanced document processing capabilities.\n",
            "\n",
            "2025-01-01T20:21:54.125Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.523540, used credits [0.00399300]\n",
            "- CONTENT [8fff37ca-bf13-47f8-b452-1010061953f8]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [587 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2023/september-24-support-for-youtube-feeds-added-documentation-bug-fixes</name><title>September 24: Support for YouTube feeds; added documentation; bug fixes | Graphlit Changelog</title></metadata> 🛠️\tSeptember 2023\n",
            "September 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "New Features\n",
            "🔥 Graphlit now supports YouTube feeds, where you can ingest a set of YouTube videos, or an entire YouTube playlist or channel.   Note, we currently support only the ingestion of audio from YouTube videos, which gets transcribed and added to your conversational knowledge graph.\n",
            "New Documentation\n",
            "Added documentation for observable entities mutations and queries (Label, Category, Person, Organization, Place, Event, Product, Repo, Software).\n",
            "Added documentation for using custom Azure OpenAI and OpenAI models with Specifications\n",
            "Bugs Fixed\n",
            "GPLA-1459: LLM prompt formatting was exceeding the token budget with long user prompts.\n",
            "GPLA-1445: Failed to ingest PDF from URL where filename in Content-Disposition header contained a backslash.\n",
            "PreviousOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "NextSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "Last updated7 months ago\n",
            "- Completion [186 tokens (includes JSON guardrails tokens)], throughput: 73.706 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for YouTube feeds, allowing ingestion of YouTube videos, playlists, or channels (currently supports audio ingestion and transcription).\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Added documentation for observable entities mutations and queries (Label, Category, Person, Organization, Place, Event, Product, Repo, Software).\n",
            "  - Added documentation for using custom Azure OpenAI and OpenAI models with Specifications.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed LLM prompt formatting issue (GPLA-1459) that exceeded token budget with long user prompts.\n",
            "  - Resolved issue (GPLA-1445) with PDF ingestion from URL when filename in Content-Disposition header contained a backslash.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 2023.\n",
            "\n",
            "- Value:\n",
            "  - Enhances developers' ability to integrate YouTube content and improves documentation for better usage of observable entities and custom models.\n",
            "\n",
            "2025-01-01T20:21:53.620Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.496565, used credits [0.00676500]\n",
            "- CONTENT [88122e0e-dc76-44ba-a14b-5f5fb698524d]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [851 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-26-support-for-google-ai-and-cerebras-models-and-latest-groq-models</name><title>September 26: Support for Google AI and Cerebras models, and latest Groq models | Graphlit Changelog</title></metadata> 🎒\tSeptember 2024\n",
            "September 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "New Features\n",
            "💡 Graphlit now supports the Cerebras model service which offers the LLAMA_3_1_70B and LLAMA_3_1_8B models.\n",
            "💡 Graphlit now supports the Google AI model service which offers the GEMINI_1_5_PRO and GEMINI_1_5_FLASH models.\n",
            "We have added support for the latest Groq Llama 3.2 preview models, including LLAMA_3_2_1B_PREVIEW, LLAMA_3_2_3B_PREVIEW, LLAMA_3_2_11B_TEXT_PREVIEW, and LLAMA_3_2_90B_TEXT_PREVIEW.  We have also added support for the Llama 3.2 multimodal model LLAMA_3_2_11B_VISION_PREVIEW.\n",
            "We have added a new specification parameter to the promptConversation mutation. Now you can specify your initial specification for a new conversation, or update an existing conversation, without requiring additional API calls.\n",
            "⚡ We have changed the retrieval behavior of the promptConversation mutation. Now, if no relevant content was found via vector-based semantic search (given the user prompt), we will fallback to any relevant content from the message in the conversation. If there was no content from the conversation to fallback to, we will fallback to the last ingested content in the project. This solves an issue where a first prompt like 'Summarize this' would find no relevant content.  Now it will fallback to retrieve the last ingested content.\n",
            "⚡ We have renamed the Groq model enum from LLAVA_1_5_7B to LLAVA_1_5_7B_PREVIEW.\n",
            "Bugs Fixed\n",
            "GPLA-3083: Not sending custom instructions/guidance with extraction prompt\n",
            "GPLA-3146: Filtering Persons by email not working\n",
            "GPLA-3171: Not failing on deprecated OpenAI model\n",
            "GPLA-3158: Summarization not using revision strategy\n",
            "PreviousSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "NextSeptember 3: Support for web search feeds, model deprecations\n",
            "Last updated3 months ago\n",
            "- Completion [351 tokens (includes JSON guardrails tokens)], throughput: 78.060 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Cerebras model service: LLAMA_3_1_70B and LLAMA_3_1_8B models.\n",
            "  - Support for Google AI model service: GEMINI_1_5_PRO and GEMINI_1_5_FLASH models.\n",
            "  - Support for latest Groq Llama 3.2 preview models: LLAMA_3_2_1B_PREVIEW, LLAMA_3_2_3B_PREVIEW, LLAMA_3_2_11B_TEXT_PREVIEW, LLAMA_3_2_90B_TEXT_PREVIEW, and multimodal model LLAMA_3_2_11B_VISION_PREVIEW.\n",
            "  - New specification parameter added to promptConversation mutation for initial or updated conversation specifications.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Changed retrieval behavior of promptConversation mutation to fallback to relevant content from the conversation or last ingested content if no relevant content is found.\n",
            "  - Renamed Groq model enum from LLAVA_1_5_7B to LLAVA_1_5_7B_PREVIEW.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with not sending custom instructions with extraction prompt (GPLA-3083).\n",
            "  - Resolved filtering persons by email not working (GPLA-3146).\n",
            "  - Addressed issue of not failing on deprecated OpenAI model (GPLA-3171).\n",
            "  - Fixed summarization not using revision strategy (GPLA-3158).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 26, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support and improved conversation handling, increasing the efficiency and effectiveness of interactions with the platform.\n",
            "\n",
            "2025-01-01T20:21:51.673Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.551386, used credits [0.00443100]\n",
            "- CONTENT [4983a4c2-2d1a-4640-83f7-a187a4275778]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [661 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-1-support-for-fhir-enrichment-latest-cohere-models-bug-fixes</name><title>September 1: Support for FHIR enrichment, latest Cohere models, bug fixes | Graphlit Changelog</title></metadata> 🎒\tSeptember 2024\n",
            "September 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports entity enrichment from Fast Healthcare Interoperability Resources (FHIR) servers. You can provide the endpoint for a FHIR server, and Graphlit will enrich medical-related entities from the data found in the FHIR server.\n",
            "Added support for latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408) and added datestamped model enums for the previous versions (COMMAND_R_202403, COMMAND_R_PLUS_202404).  The latest model enums (COMMAND_R and COMMAND_R_PLUS) currently point to the models (COMMAND_R_202403 and COMMAND_R_PLUS_202404) as specified by the Cohere API.\n",
            "Added support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31), now used by default.\n",
            "⚡ We have changed the name of the LinkReferenceType to LinkReference to follow the existing data model standard.\n",
            "Bugs Fixed\n",
            "GPLA-3120: LLM is adding source tags to end of completed messages\n",
            "GPLA-3133: Failed to load sitemap on child page of website.\n",
            "PreviousSeptember 3: Support for web search feeds, model deprecations\n",
            "NextAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "Last updated4 months ago\n",
            "- Completion [204 tokens (includes JSON guardrails tokens)], throughput: 79.957 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for entity enrichment from FHIR servers for medical-related entities.\n",
            "  - Added support for latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408) with datestamped model enums for previous versions.\n",
            "  - Support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31) now used by default.\n",
            "  - Renamed LinkReferenceType to LinkReference to align with data model standards.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Updated naming conventions for consistency with existing data models.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where LLM added source tags to the end of completed messages (GPLA-3120).\n",
            "  - Resolved failure to load sitemap on child page of website (GPLA-3133).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 1, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances data enrichment capabilities for healthcare applications and improves model support for developers.\n",
            "\n",
            "2025-01-01T20:21:51.497Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.376576, used credits [0.00416700]\n",
            "- CONTENT [9819c811-85db-4a53-923e-f3a6beac4b6a]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [641 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-30-support-for-azure-ai-inference-models-mistral-pixtral-and-latest-google-gemini-models</name><title>September 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models | Graphlit Changelog</title></metadata> 🎒\tSeptember 2024\n",
            "September 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "New Features\n",
            "💡 Graphlit now supports the Azure AI Model Inference API (aka Models as a Service) model service which offers serverless hosting to many models such as Meta Llama 3.2, Cohere Command-R, and many more.  For Azure AI, all models are 'custom', and you will need to provide the serverless endpoint, API key and number of tokens accepted in context window, after provisioning the model of your choice.\n",
            "We have added support for the multimodal Mistral Pixtral model, under the model enum PIXTRAL_12B_2409.\n",
            "We have added versioned model enums for Google Gemini, so you can access GEMINI_1_5_FLASH_001, GEMINI_1_5_FLASH_002, GEMINI_1_5_PRO_001 and GEMINI_1_5_PRO_002.\n",
            "PreviousOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "NextSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "Last updated3 months ago\n",
            "- Completion [187 tokens (includes JSON guardrails tokens)], throughput: 78.685 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Azure AI Model Inference API, enabling serverless hosting for models like Meta Llama 3.2 and Cohere Command-R.\n",
            "  - Added support for the multimodal Mistral Pixtral model (PIXTRAL_12B_2409).\n",
            "  - Introduced versioned model enums for Google Gemini: GEMINI_1_5_FLASH_001, GEMINI_1_5_FLASH_002, GEMINI_1_5_PRO_001, GEMINI_1_5_PRO_002.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Enhanced model access and management through versioned enums.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned in this release.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 30, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers expanded access to advanced AI models and improved integration capabilities with Azure services.\n",
            "\n",
            "2025-01-01T20:21:51.339Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.222726, used credits [0.00411900]\n",
            "- CONTENT [65dc8b4e-c1dc-40e2-a653-d8d51fe0414c]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [689 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-3-support-for-web-search-feeds-model-deprecations</name><title>September 3: Support for web search feeds, model deprecations | Graphlit Changelog</title></metadata> 🎒\tSeptember 2024\n",
            "September 3: Support for web search feeds, model deprecations\n",
            "New Features\n",
            "💡 Graphlit now supports web search feeds, using the Tavily and Exa.AI web search APIs. You can choose the SEARCH feed type, and assign your search text property, and we will ingest the referenced web pages from the search results.  Optionally, you can select the search service via the serviceType property under search feed properties.  By default, Graphlit will use the Tavily API.\n",
            "⚡ We have deprecated these OpenAI models, according to the future support OpenAI is providing to these legacy models: GPT35_TURBO, GPT35_TURBO_0613, GPT35_TURBO_16K, GPT35_TURBO_16K_0125, GPT35_TURBO_16K_0613, GPT35_TURBO_16K_1106, GPT4, GPT4_0613, GPT4_32K, GPT4_32K_0613, GPT4_TURBO_VISION_128K, and GPT4_TURBO_VISION_128K_1106.  We suggest using GPT-4o or GPT-4o Mini instead.\n",
            "Bugs Fixed\n",
            "GPLA-2523: Can't ingest from same feed URI multiple times and wait on isFeedDone\n",
            "PreviousSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "NextSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "Last updated3 months ago\n",
            "- Completion [171 tokens (includes JSON guardrails tokens)], throughput: 76.933 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for web search feeds using Tavily and Exa.AI APIs.\n",
            "  - Ability to choose SEARCH feed type and assign search text property for ingesting web pages from search results.\n",
            "  - Option to select search service via serviceType property, defaulting to Tavily API.\n",
            "  - Deprecation of several OpenAI models, recommending GPT-4o or GPT-4o Mini as alternatives.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2523: Ingestion from the same feed URI multiple times and waiting on isFeedDone.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 3, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for web search integration and guidance on model usage with OpenAI.\n",
            "\n",
            "2025-01-01T20:21:47.588Z: Search entities\n",
            "- Workflow [Semantic search] took 0:00:00.089619, used credits [0.00960000]\n",
            "- Processor name [Azure AI Search], units [48]\n",
            "\n",
            "2025-01-01T20:21:41.628Z: GraphQL\n",
            "- Operation took 0:00:00.372833, used credits [0.00000000]\n",
            "- Request:\n",
            "mutation PublishContents($summaryPrompt: String, $publishPrompt: String!, $connector: ContentPublishingConnectorInput!, $filter: ContentFilter, $includeDetails: Boolean, $isSynchronous: Boolean, $correlationId: String, $name: String, $summarySpecification: EntityReferenceInput, $publishSpecification: EntityReferenceInput, $workflow: EntityReferenceInput) { publishContents(summaryPrompt: $summaryPrompt, publishPrompt: $publishPrompt, connector: $connector, filter: $filter, includeDetails: $includeDetails, isSynchronous: $isSynchronous, correlationId: $correlationId, name: $name, summarySpecification: $summarySpecification, publishSpecification: $publishSpecification, workflow: $workflow) { content { id name state originalDate identifier markdown uri type fileType mimeType format formatName fileExtension fileName fileSize masterUri imageUri textUri audioUri transcriptUri summary customSummary keywords bullets headlines posts chapters questions video { width height duration make model software title description keywords author } audio { keywords author series episode episodeType season publisher copyright genre title description bitrate channels sampleRate bitsPerSample duration } image { width height resolutionX resolutionY bitsPerComponent components projectionType orientation description make model software lens focalLength exposureTime fNumber iso heading pitch } document { title subject summary author publisher description keywords pageCount worksheetCount slideCount wordCount lineCount paragraphCount isEncrypted hasDigitalSignature } } details { contents { id } summaries text textType summarySpecification publishSpecification summaryTime publishTime } } }\n",
            "- Variables:\n",
            "{\"summaryPrompt\":\"\\\"\\\\nYou are an AI assistant that extracts the most important information from product changelog pages.\\\\n\\\\nYou are being provided a changelog web page for one of many releases of the Graphlit Platform in 2024.\\\\n\\\\nYour task is to produce a concise summary that covers:\\\\n\\\\nNew Features – Briefly list or describe each new capability.\\\\nEnhancements\\\\/Improvements – Any notable improvements or changes.\\\\nBug Fixes – Summaries of what was fixed and why it matters.\\\\nOther Key Details – Any version numbers, feature flags, or breaking changes.\\\\nDates - When a feature was released, include both the month and year\\\\nValue - What this offers to developers.\\\\nKeep it succinct, accurate, and organized. Use short sentences or bullet points so it’s easy to incorporate into a map\\\\/reduce pipeline. Omit any superfluous text.\\\\n\\\\nOutput:\\\\nA concise summary in bullet points highlighting the essential updates from the changelog.\\\\n\\\"\",\"publishPrompt\":\"\\\"\\\\n**Prompt:**  \\\\nYou are an AI writing assistant. You have been provided a collection of **concise bullet-point summaries** representing each monthly changelog from our SaaS product throughout the past year. Using those summaries as source material, your task is to craft a **complete and cohesive “Year in Review” blog post** that covers all important updates from January through December.  \\\\n\\\\nIn your response, please:  \\\\n1. **Incorporate every month’s bullet points** in chronological order, highlighting the most impactful New Features, Enhancements, Bug Fixes, and any Other Key Details (such as release dates and version numbers).  \\\\n2. **Use a narrative style** that flows naturally from month to month.  \\\\n3. **Organize the content with clear headings** or subheadings for each month or release milestone.  \\\\n4. **Provide context** on why these updates were significant for our user base (e.g., how they improve developer workflow, enhance user experience, etc.).  \\\\n5. **Keep it professional but friendly**, ensuring it’s easy to read and maintains a positive, forward-looking tone.  \\\\n6. **Conclude with a forward-thinking note**, thanking readers for their support and hinting at what to expect in the coming year.\\\\n\\\\nOutput:  \\\\nA **polished, blog-ready “Year in Review” article**, in paragraph form with headings, that consolidates all the monthly summaries into one comprehensive reflection on the product’s evolution.\\\\n\\\\nDon't wrap your response on markdown. No ``` tick marks.\\\\n\\\"\",\"connector\":\"{ type: TEXT, format: MARKDOWN }\",\"filter\":\"{ feeds: [ { id: \\\"a0d5de6a-9901-42e2-b016-839e5c7c8408\\\" } ] }\",\"includeDetails\":\"true\",\"isSynchronous\":\"true\",\"correlationId\":\"\\\"2025-01-01T20:16:10.555554\\\"\",\"name\":\"\\\"Published Summary\\\"\",\"summarySpecification\":\"{ id: \\\"7c4463e3-2138-4db4-ad99-301952606edd\\\" }\",\"publishSpecification\":\"{ id: \\\"17910639-27ed-4f4b-9fb1-441378694398\\\" }\"}\n",
            "\n",
            "2025-01-01T20:21:37.749Z: Serverless compute\n",
            "- Workflow [Entity Event] took 0:00:01.265673, used credits [0.00227874]\n",
            "- CONTENT [1873bf7a-4489-4315-b6d0-effdcc599873]\n",
            "\n",
            "2025-01-01T20:21:37.598Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.398428, used credits [0.00679600]\n",
            "- CONTENT [1873bf7a-4489-4315-b6d0-effdcc599873]: Content type [TEXT], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [3398 tokens], throughput: 8528.526 tokens/sec\n",
            "\n",
            "2025-01-01T20:21:37.489Z: Text embedding\n",
            "- Workflow [Preparation] took 0:00:00.293892, used credits [0.00679800]\n",
            "- CONTENT [1873bf7a-4489-4315-b6d0-effdcc599873]: Content type [TEXT], file type [DOCUMENT]\n",
            "- Model service [OpenAI], model name [Ada_002]\n",
            "- Text embedding [3399 tokens], throughput: 11565.457 tokens/sec\n",
            "\n",
            "2025-01-01T20:21:36.226Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:03:17.545572, used credits [16.05510000]\n",
            "- Model service [OpenAI], model name [O1_200k]\n",
            "- Prompt [16449 tokens (includes RAG context tokens)]:\n",
            "<source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-3-new-data-model-for-observations-new-category-entity</name><title>August 3: New data model for Observations, new Category entity | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Revised data model for Observations, Occurrences, and observables (e.g., Person, Organization).\n",
            "  - Each observed entity now has one Observation and a list of occurrences (supports text, time, and image types).\n",
            "  - Added Category entity to GraphQL data model for PII categories (e.g., Phone Number, Credit Card Number).\n",
            "  - Introduced probability field for LLM's token probability.\n",
            "  - Added error field to feeds for error descriptions on failed reads.\n",
            "  - Support for reingestion of changed files from feeds, maintaining the same ID.\n",
            "  - Ingestion of content is now idempotent, allowing reingestion without ID change if content is unchanged.\n",
            "  - Changed GraphQL data type for SharePoint identifiers from String to ID.\n",
            "  - Performance optimization for entity extraction and observation creation.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of content ingestion and updates.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed extraction issues from PDF and DOCX tables.\n",
            "  - Resolved issue where audio content from RSS feeds was not deleted upon feed deletion.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved data handling, enhanced performance, and better error management in the Graphlit Platform.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-20-support-for-medical-entities-anthropic-prompt-caching-bug-fixes</name><title>August 20: Support for medical entities, Anthropic prompt caching, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "  - Medical-related entities supported in GraphRAG and via API for queries and mutations.\n",
            "  - Added support for Anthropic prompt caching, improving performance and reducing token costs when using Anthropic Sonnet 3.5 or Haiku 3.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved entity similarity filter defaults to VECTOR.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where empty PDFs failed entity extraction.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 20, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances capabilities for developers working with medical data and improves efficiency in using Anthropic models.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-11-support-for-azure-ai-document-intelligence-by-default-language-aware-summaries</name><title>August 11: Support for Azure AI Document Intelligence by default, language-aware summaries | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for language-aware summaries in LLM-based document extraction, aligning summaries with the source text language.\n",
            "  - Support for language-aware entity descriptions in LLM-based entity extraction, matching descriptions to the source text language.\n",
            "  - Default document preparation method changed to Azure AI Document Intelligence for improved fidelity in complex PDFs and better table extraction.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Increased credit usage per page for PDF, DOCX, and PPTX documents due to the switch to Azure AI, but with significantly higher quality in extracted documents for RAG pipelines.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3070: Slide count not assigned to metadata for PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 11, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced document extraction capabilities with improved accuracy and language support, facilitating better integration in applications.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2023/december-10-support-for-openai-gpt-4-turbo-llama-2-and-mistral-models-query-by-example-bug-fixes</name><title>December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4 Turbo 128k model in Azure OpenAI and native OpenAI services.\n",
            "  - Support for Llama 2 (7b, 13b, 70b) and Mistral 7b models via Replicate.\n",
            "  - Support for Anthropic Claude 2.1 model.\n",
            "  - Support for OpenAI GPT-4 Vision model for image descriptions and text extraction.\n",
            "  - Query by example added for contents and conversations, utilizing vector embeddings.\n",
            "  - Vector search support for conversations queries.\n",
            "  - PromptSpecifications mutation for prompting multiple models.\n",
            "  - New promptStrategy field for preprocessing prompts.\n",
            "  - SuggestConversation mutation for auto-suggesting follow-up questions.\n",
            "  - New summarization types: CHAPTERS, QUESTIONS, and POSTS.\n",
            "  - Versioned model enums introduced.\n",
            "  - LookupContents query for retrieving multiple contents by ID.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed headline field to headlines, returning an array of strings.\n",
            "  - Entity names limited to 1024 characters.\n",
            "  - BULLET_POINTS renamed to BULLETS in SummarizationTypes enum.\n",
            "  - ProjectStorage type fields renamed for clarity.\n",
            "  - Removed optimizeSearchConversation from Specification type, now handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Ignored RSS.xml from web feed sitemap.\n",
            "  - Resolved issue with GPT-3.5 Turbo 16k LLM adding \"Citation #\" to responses.\n",
            "  - Fixed workflow application to link-crawled content.\n",
            "  - Corrected mismatched project storage total size.\n",
            "  - Added relevance threshold for semantic search.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 10, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support, improved querying capabilities, and streamlined prompt management, facilitating more efficient and effective application development.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-21-support-for-onedrive-and-google-drive-feeds-extract-images-from-pdfs-bug-fixes</name><title>February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for OneDrive and Google Drive feeds, allowing ingestion of files from shared drives.\n",
            "  - Support for email backup files (EML, MSG) with automatic extraction of attachments.\n",
            "  - Automatic extraction of embedded images from PDF files, linking them as children of the parent PDF.\n",
            "  - Support for recursive Notion feeds with the isRecursive flag for crawling child pages and databases.\n",
            "  - Collections can now be assigned to ingested content automatically.\n",
            "  - Introduction of CODE file type for various source code formats with optimized text splitting.\n",
            "  - Custom guidance support in Specification object for RAG process.\n",
            "  - Added tenants field to Project object for listing tenant IDs.\n",
            "  - Indexed email metadata (to, from, subject) separate from document metadata.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Contents field replaced with children and parent fields for content objects.\n",
            "  - EnableImageAnalysis field removed; now enabled by default.\n",
            "  - DisableSmartCapture field moved to preparation workflow stage, enabled by default.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed PDF parsing error for ArXiV PDFs (GPLA-2099).\n",
            "  - Corrected LLM response issue with conversation history (GPLA-2174).\n",
            "  - Resolved issue of ZIP package remaining in Indexed state after content workflow (GPLA-2199).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced file ingestion capabilities, improved metadata handling, and streamlined content management processes.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-25-support-for-mistral-large-2-and-nemo-groq-llama-3.1-models-bug-fixes</name><title>July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for Mistral Large 2 and Mistral Nemo models; existing MISTRAL_LARGE model enum updated.\n",
            "  - Support for Llama 3.1 models (8b, 70b, 405b) on Groq, with rate limits per Groq's platform.\n",
            "  - Added revision strategy for data extraction specifications, allowing LLM to revise previous responses.\n",
            "  - Version property added for AzureDocumentPreparationProperties to assign API version for Azure AI Document Intelligence; default is v4.0 (Preview), can override to V2023_10_31 for v3.1 (GA) API version.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with hyperlinks not being extracted from Office documents (GPLA-2988).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 25, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve model support and data extraction capabilities, offering developers more flexibility and better results with Azure AI.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/may-2024/may-5-support-for-jina-and-pongo-rerankers-microsoft-teams-feed-new-youtube-downloader-bug-fixes</name><title>May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for Jina and Pongo rerankers in the Specification object; options for COHERE, PONGO, and JINA as reranking service types.\n",
            "  - Microsoft Teams feeds support for reading messages from Teams channels.\n",
            "  - Rewritten YouTube downloader to accommodate changes in YouTube video player HTML.\n",
            "  - Improved handling of HTTP errors for URI validation and content downloading.\n",
            "  - Support for updating content metadata in updateContent mutation post-workflow.\n",
            "  - Added query_contents_graph functions for knowledge graph visualization.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Citation indices changed from zero-based to one-based.\n",
            "  - Added isSynchronous flag for deleteAll and multiple delete mutations; bulk deletes are asynchronous by default.\n",
            "  - Added missing count mutations (e.g., countAlerts, countFeeds).\n",
            "  - Renamed query_content_facets to query_contents_facets in Python SDK.\n",
            "  - Renamed queryContentFacets to queryContentsFacets in Node.js SDK.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed page relevance not being filled in all situations.\n",
            "  - Resolved issues with link extraction from PDFs using Azure AI Doc Intelligence.\n",
            "  - Addressed sporadic HTTP 500 errors from GraphQL API.\n",
            "  - Fixed failure to re-ingest content deleted immediately after initial ingestion.\n",
            "  - Ensured validation for non-null parameters in mutations.\n",
            "  - Handled invalid JSON from LLMs with proper escaping or formatting.\n",
            "  - Fixed ingestion failure for encoded files with colons in names.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: May 5, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced flexibility with reranking options, improved integration with Microsoft Teams, and better error handling, leading to a more robust and user-friendly platform.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/may-2024/may-15-support-for-graphrag-openai-gpt-4o-model-performance-improvements-and-bug-fixes</name><title>May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for GraphRAG, allowing extracted entities to be added as context in RAG conversations and used as content filters.\n",
            "  - LLM revisions within RAG conversations, improving output by 35% with higher quality responses.\n",
            "  - Support for OpenAI GPT-4o model in RAG conversations.\n",
            "  - Default model changed to OpenAI GPT-4o for faster performance and better quality.\n",
            "  - Added graph visualization in promptConversation responses to show relationships in the knowledge graph.\n",
            "  - Expanded enriched data from Wikipedia to include long descriptions of entities.\n",
            "  - New API queries: getSharePointLibraries, getSharePointFolders, getOneDriveFolders, getTeams, and getTeamsChannels for easier storage service enumeration.\n",
            "  - Added extractedCount to limit the number of extracted entities per type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved performance in entity extraction, cloud storage access, and RAG context rendering.\n",
            "  - Optimized LLM entity extraction to identify more properties and relationships.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with text extraction from HTML in RSS posts, limit filter returning incomplete results, and structured text extraction from JSON/XML formats.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: May 15, 2024.\n",
            "  - Version updates include support for new models and enhanced API functionalities.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved performance, enhanced data extraction capabilities, and better integration with Microsoft services.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-1-support-for-retrieval-only-rag-pipeline-bug-fixes</name><title>December 1: Support for retrieval-only RAG pipeline, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for formatting LLM-ready prompts with RAG pipeline using formatConversation and completeConversation mutations.\n",
            "  - Inline hyperlinks can now be included in extracted text from documents and web pages.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved support for LLM streaming by enabling direct calls to the LLM from applications.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3466: Owner ID now accepts any non-whitespace string.\n",
            "  - Fixed issue GPLA-3458: Resolved problem with missing Person-to-Organization edges from entity extraction.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 1, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with better integration for LLM interactions and improved text extraction capabilities.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-9-support-for-website-mapping-web-page-screenshots-groq-llama-3.3-model-bug-fixes</name><title>December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for website mapping using mapWebmutation to return URLs from sitemap.xml.\n",
            "  - Generation of web page screenshots with screenshotPagemutation, including optional image processing workflows.\n",
            "  - Direct summarization of text with summarizeTextmutation, allowing various summarization strategies.\n",
            "  - Direct extraction of text with extractTextmutation, utilizing LLM tools for JSON responses.\n",
            "  - Support for Groq Llama 3.3 model (LLAMA_3_3_70B).\n",
            "  - Updated Cohere reranking to use rerank-v3.5 model by default.\n",
            "  - New flattenCitations field in ConversationStrategyInput to combine multiple citations.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Required clientId and clientSecret fields added for Microsoft email, Teams, and OneDrive feeds for proper authentication.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with not finding sitemap at parent web path (GPLA-3492).\n",
            "  - Resolved mismatch handling of Deepgram model/language (GPLA-3500).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for web mapping, content ingestion, and improved integration with Microsoft services.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-22-support-for-dropbox-box-intercom-and-zendesk-feeds-openai-o1-gemini-2.0-bug-fixes</name><title>December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for Dropbox feeds for file ingestion, requiring appKey, appSecret, redirectUri, and refreshToken.\n",
            "  - Support for Box feeds for file ingestion, requiring clientId, clientSecret, redirectUri, and refreshToken.\n",
            "  - Support for Intercom feeds to ingest Articles and Tickets, requiring accessToken.\n",
            "  - Support for Zendesk feeds to ingest Articles and Tickets, requiring accessToken and Zendesk subdomain.\n",
            "  - Support for OpenAI o1 model with enums O1_200k and O1_200k_20241217.\n",
            "  - Support for Gemini Flash 2.0 Experimental model with enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "  - Support for Cohere R7B model with enum COMMAND_R7B_202412.\n",
            "  - Ability to return low-level details from RAG conversations using includeDetails parameter.\n",
            "  - Support for filtering observables by URI property.\n",
            "  - Ability to bypass semantic search in content retrieval by assigning NONE for conversation search type.\n",
            "  - New createdInLast property for entity filters and inLast property for content filters for recent time period filtering.\n",
            "  - Support for Azure AI Document Intelligence models with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "  - Support for Google Drive and OneDrive feeds to ingest specific files by providing file identifiers.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Removal of content items limit for projects upgraded to the Starter Tier after December 9, 2024, allowing unlimited storage of content items.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with collection assignment to multitenant content.\n",
            "  - Resolved HTML character decoding when parsing HTML email.\n",
            "  - Corrected handling of isSynchronous during content ingestion.\n",
            "  - Fixed IsFeedDone not returning True for finished feeds with no contents.\n",
            "  - Addressed HTTP 400 error handling on URI uploads.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced integration capabilities with popular services, improved content management, and expanded model support for AI applications.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2023/july-15-support-for-sharepoint-feeds-new-conversation-features</name><title>July 15: Support for SharePoint feeds, new Conversation features | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for SharePoint feeds to ingest files from SharePoint document libraries.\n",
            "  - PII detection during entity extraction from text documents and audio transcripts, automatically labeling detected PII.\n",
            "  - Support for custom OpenAI API keys and Azure OpenAI deployments in Specifications.\n",
            "  - GraphQL query for SharePoint consent URI for registered Azure AD applications.\n",
            "  - Improved handling of web sitemap indexes to load linked sitemaps.\n",
            "  - New GraphQL mutations for managing conversations and collections.\n",
            "  - Timestamps added to Conversation messages.\n",
            "  - Enhanced content search configuration with new search and query types.\n",
            "  - Better parsing of iTunes podcast metadata.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed fields for clarity: listingLimit to readLimit, topK to numberSimilar.\n",
            "  - Split GraphQL properties for Azure and OpenAI for better organization.\n",
            "  - Removed count fields on query results, replaced with explicit count queries.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed Reddit readLimit issue to ensure specified limits are applied.\n",
            "  - Improved performance for entity extraction from large PDFs (4x speed increase).\n",
            "  - Corrected error handling for rendition generation in content workflows.\n",
            "  - Enhanced loading speed for large web sitemaps, now processing 150K+ entries quickly.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 15, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for data ingestion, improved performance, and better management of content and conversations.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-19-support-for-openai-gpt-4o-mini-byo-key-for-azure-ai-similarity-by-summary-bug-fixes</name><title>July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4o Mini model with 16k output tokens.\n",
            "  - 'Bring-your-own-key' support for Azure AI Document Intelligence models with custom endpoint and key property.\n",
            "  - Updated to Jina reranker v2 by default.\n",
            "  - Enhanced summarization capabilities with properties stored in content.\n",
            "  - Added relevance property for entity types, sorting results by search relevance score.\n",
            "  - Ability to manually update summary and bullet properties in updateContent mutation.\n",
            "  - Offset property added to AtlassianJiraFeedProperties for proper timezone assignment in Jira feed.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Content similarity search now finds similar content by summary for better accuracy.\n",
            "  - Changes in entity filter objects for paging; zero offset for vector/hybrid search.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Added retry on OpenAI API HTTP 524 error.\n",
            "  - Fixed paging issues in Jira feed.\n",
            "  - Improved search results for similar content in long documents.\n",
            "  - Resolved keyword search issues in long PDFs.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 19, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved model support, enhanced search capabilities, and better handling of content summaries and entity relevance.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/june-2024/june-9-support-for-deepseek-models-json-ld-webpage-parsing-performance-improvements-and-bug-fixes</name><title>June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for Deepseek LLMs for prompt completion (deepseek-chat and deepseek-coder models).\n",
            "  - Parsing of embedded JSON-LD from web pages, automatically injecting into the knowledge graph.\n",
            "  - Default model for entity extraction and image completions changed to OpenAI GPT-4o for improved performance and quality.\n",
            "  - Knowledge graph generation behavior changed to opt-in; requires setting generateGraph to True for response inclusion.\n",
            "  - Added thing property for observable entities to return associated JSON-LD metadata.\n",
            "  - Regex-based filtering for URI paths during feed ingestion and workflow filtering.\n",
            "  - Added observableLimit to configure the number of observed entities in GraphRAG context (default 1000).\n",
            "  - Customization of follow-up question generation via prompt in suggestConversation mutation.\n",
            "  - Updated suggestConversation to include past conversation message history.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved performance in knowledge graph retrieval and generation through better parallelization and batching.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Optimized retrieval performance of observed entities during GraphRAG.\n",
            "  - Fixed parsing exception caused by invalid user-provided URI.\n",
            "  - Removed requirement for tenant ID for Microsoft email or Teams.\n",
            "  - Resolved issue of not returning labels or categories from graph in API.\n",
            "  - Fixed failure to extract spreadsheet images.\n",
            "  - Addressed issue of email to/from not being added as observations.\n",
            "  - Corrected API returning 'audio' metadata from podcast HTML documents.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: June 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for entity extraction, improved performance, and better integration of JSON-LD data, facilitating more efficient knowledge graph management.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-28-support-for-indexing-workflow-stage-azure-ai-language-detection-bug-fixes</name><title>July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Added indexing workflow stage for configuring indexing services to infer metadata from content.\n",
            "  - Introduced AZURE_AI_LANGUAGE content indexing service for inferring language from extracted text or transcripts.\n",
            "  - Supported language content metadata returning a list of languages in ISO 639-1 format.\n",
            "  - Added MODEL_IMAGE extraction service for integration with various vision models, allowing custom specifications and API keys.\n",
            "  - Deprecated OPENAI_IMAGE service type; developers should use LLM image service instead.\n",
            "  - Removed language field from AudioMetadata type, replaced with new LanguageMetadata type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2987 where Azure Doc Intelligence did not extract hyperlinks.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 28, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for content indexing, language detection, and image extraction, improving overall functionality and flexibility.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2023/october-15-support-for-anthropic-claude-models-slack-feeds-and-entity-enrichment</name><title>October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "  - Ingestion of Slack messages and linked file attachments from Slack channels (requires a Slack bot).\n",
            "  - Entity enrichment for observed entities (Person, Organization, Place) using Diffbot, Wikipedia, and Crunchbase.\n",
            "  - Text extraction from images using Azure Image Analytics, making identified text searchable.\n",
            "  - Added embedFacets and embedCitations properties to conversation strategy in specification object.\n",
            "  - GraphQL mutations for multi-delete of entities (deleteCollections, deleteLabels, deleteConversations).\n",
            "  - GraphQL deleteAllConversations mutation to delete all conversations.\n",
            "  - Automatic addition of ingested content to collections during the ingestion stage of the workflow.\n",
            "  - Specification property added to preparation workflow stage for selecting LLM for text summarization.\n",
            "  - Expanded properties for observed entities for enhanced entity enrichment.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved capabilities for entity enrichment and text extraction.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed unlimited conversation quota not assigned when upgrading project tier (GPLA-1520).\n",
            "  - Resolved issue with entity enrichment not firing events (GPLA-1285).\n",
            "  - Fixed web page remaining in ingested state when URL is not accessible (GPLA-1361).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 15, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced support for LLMs, improved data ingestion from Slack, and advanced entity enrichment capabilities.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-16-support-for-image-description-multi-turn-text-summarization</name><title>November 16: Support for image description, multi-turn text summarization | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Multi-turn summarization of text with the reviseText mutation, allowing for LLM prompts and text revisions.\n",
            "  - Image descriptions supported via describeImage mutation (URI input) and describeEncodedImage mutation (Base-64 encoded image), utilizing vision LLMs.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Optional specifications for selecting vision LLMs; defaults to OpenAI GPT-4o if not specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned in this release.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 16, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for text summarization and image description without prior image ingestion, improving interaction with LLMs.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2023/october-30-optimized-conversation-responses-added-observable-aliases-bug-fixes</name><title>October 30: Optimized conversation responses; added observable aliases; bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for 'aliases' of observable names via the alternateNames property.\n",
            "  - Workflows filter added to ContentCriteriaInput type for filtering content by workflow(s).\n",
            "  - Optimized formatting of content sources for improved conversation responses.\n",
            "  - Enhanced formatting of extracted text from Slack messages for better knowledge retrieval.\n",
            "  - Updated text tokenizer for accurate token counting.\n",
            "  - Upgraded Azure Text Analytics to the latest preview API version.\n",
            "  - Authors in RSS feeds are now stored as observations of Person entities.\n",
            "  - Rate limiting added for Reddit feeds and Wikipedia enrichment.\n",
            "  - Support for reading Reddit post comments.\n",
            "  - EmbedFacets renamed to EnableFacets in conversation strategy.\n",
            "  - Removed extra content level in IngestionWorkflowStage type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Better handling of long user prompts.\n",
            "  - Optimized token budget for prompt completion.\n",
            "  - More accurate entity matching in Wikipedia entity enrichment.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Improved handling of long user prompts (GPLA-1556).\n",
            "  - Optimized token budget for accurate prompt completion (GPLA-1627).\n",
            "  - Enhanced entity matching in Wikipedia enrichment (GPLA-1585).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved conversation accuracy, better content filtering, and enhanced entity recognition capabilities.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-10-support-for-web-search-multi-turn-content-summarization-deepgram-language-detection</name><title>November 10: Support for web search, multi-turn content summarization, Deepgram language detection | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for web search via the searchWeb mutation, allowing selection of search service (Tavily or Exa.AI) and retrieval of relevant text and URLs without ingesting web pages.\n",
            "  - Multi-turn content summarization with the reviseContent mutation, enabling conversation with LLM for summarizing various content types.\n",
            "  - Configuration of Deepgram transcription language and default language detection enabled in DeepgramAudioPreparationPropertiesInput.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Added requireTool option to promptConversation mutation for controlling tool calling.\n",
            "  - Reduced credits quota on Free tier from 1000 to 100 for accounts created after Nov 8, 2024, with unlimited feeds on the Hobby Tier.\n",
            "  - Graphlit Data API now returns HTTP 402 (Payment Required) when exceeding free tier credits quota.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 10, 2024.\n",
            "\n",
            "- Value:\n",
            "  - These updates enhance the platform's capabilities for developers, providing more flexible search and summarization options, improved language detection, and clearer usage policies regarding credits.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-24-support-for-direct-llm-prompt-multi-turn-image-analysis-bug-fixes</name><title>November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for multi-turn analysis of images using reviseImage and reviseEncodedImage mutations, allowing LLM prompts with image URIs or Base-64 encoded images.\n",
            "  - Direct prompting of an LLM with the prompt mutation, bypassing RAG content retrieval, with optional previous conversation messages.\n",
            "  - Support for Mistral Pixtral Large model (PIXTRAL_LARGE).\n",
            "  - Support for OpenAI GPT-4o (GPT4O_128K_20241120).\n",
            "  - Added Microsoft Entra ID clientId and clientSecret properties for SharePoint feed creation with user authentication.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved user authentication process for SharePoint feeds.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed filtering issue on desktop presentation when scraping web pages (GPLA-3438).\n",
            "  - Resolved JSON parsing error from extracted PDF pages (GPLA-3340).\n",
            "  - Corrected table formatting from Sonnet 3.5 (GPLA-3427).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 2024.\n",
            "  \n",
            "- Value:\n",
            "  - Enhancements provide developers with advanced image analysis capabilities and improved LLM interactions, streamlining workflows and increasing efficiency.\n",
            "</summary>\n",
            "</source>\n",
            "\n",
            " <source>\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-22-support-for-latest-anthropic-sonnet-3.5-model-cohere-image-embeddings</name><title>October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings | Graphlit Changelog</title></metadata>\n",
            "<summary>\n",
            "- New Features:\n",
            "  - Support for the latest Anthropic Sonnet 3.5 model (released 10/22/2024).\n",
            "  - Added date-version model enums for Anthropic models: CL\n",
            "- Completion [9267 tokens (includes JSON guardrails tokens)], throughput: 46.911 tokens/sec:\n",
            "# Year in Review: January–December 2024\n",
            "\n",
            "## January 2024\n",
            "\n",
            "### January 18 Release\n",
            "In January, we unveiled robust capabilities for publishing and repurposing content. This included bulk summarization, generating transcripts for audio or conversation data, and extracting JSON-LD entities using large language models (LLMs). We also introduced function-calling (LLM tools) to enable callback webhooks with external services, CLIP image embeddings for more accurate image-based searches, and a new chunking algorithm to improve semantic text embeddings. Other highlights included dynamic web page ingestion with automated scrolling, table parsing for structured data, reverse geocoding to enrich location metadata, and expanded capacity on the Starter tier (now supporting up to 100K content items). Various bug fixes addressed issues with Markdown heading parsing, summary token budgeting, SAS token returns, and error handling in workflow failures.\n",
            "\n",
            "### January 22 Release\n",
            "Later in January, we added support for ingesting Google and Microsoft emails—including attachments—into a dedicated EMAIL content type. Content could also be reingested in place using an existing ID, speeding up updates without creating duplicate entries. A new restartAllContents mutation offered the ability to restart workflows on partially ingested files. On the bug-fix side, we resolved link-extraction gaps from HTML and improved text extraction from PPTX shapes.\n",
            "\n",
            "## February 2024\n",
            "\n",
            "### February 2 Release\n",
            "February kicked off with Semantic Alerts, letting teams schedule regular LLM-generated summaries or content publications for daily or weekly reports. We rolled out OpenAI 0125 model versions for GPT-4 and GPT-3.5 Turbo, plus a performance boost to content workflows for faster ingestion. We also tackled hiccups such as collections not syncing to text embedding indexes and improved handling of “hallucinated” citations.\n",
            "\n",
            "### February 21 Release\n",
            "Support for OneDrive and Google Drive feeds enabled direct ingestion of files from shared drives. Email backup files (EML, MSG) also became ingestible, with attachments automatically linked. Notion feeds gained a recursive crawling feature, and we introduced a CODE file type to better handle source code. We improved PDF parsing error handling, restructured the children/parent fields in content objects, and removed flags around image analysis to make it the default.\n",
            "\n",
            "## March 2024\n",
            "\n",
            "### March 10 Release\n",
            "March began with the debut of our Command-Line Interface (CLI) for direct access to the Graphlit Data API. We expanded model support with Groq, Mistral, and Anthropic Claude 3, alongside deeper Azure Document Intelligence v4 compatibility. Detailed usage and credits telemetry now helps teams track consumption, supported by correlation IDs and project-level webhooks. Additional enhancements included image extraction from Office documents and the introduction of text and markdown properties in Content for clearer formatting.\n",
            "\n",
            "### March 13 Release\n",
            "We introduced the Claude 3 Haiku model and allowed direct ingestion of Base64-encoded files through the new ingestEncodedFile mutation. Developers can now see modelService and model details in conversation messages, providing better transparency for LLM completions.\n",
            "\n",
            "### March 23 Release\n",
            "Later in March, we expanded to feed types for Linear, GitHub Issues, and Atlassian Jira. An ISSUE content type was added, capturing essential metadata like title, authors, and status for each tracked item. This release also improved ingestion from web sitemaps (including PDFs or MP3s) and fixed issues with very large MP4 metadata.\n",
            "\n",
            "## April 2024\n",
            "\n",
            "### April 7 Release\n",
            "In April, our new Discord feed feature allowed ingestion of channel messages and attachments. We introduced section-aware text chunking (CHUNK, SECTION, CONTENT retrieval strategies) and a reranking strategy powered by Cohere. Synchronous content ingestion let you wait on completion before the mutation returned. We consolidated Slack attachments via an includeAttachments flag, and replaced older ingestPage and ingestFile mutations with a single ingestUri approach. Various bug fixes targeted JSON schema adherence for Claude 3 Haiku and improved table-row formatting.\n",
            "\n",
            "### April 23 Release\n",
            "We launched native Python and Node.js SDKs with typed interfaces—both code-generated from the GraphQL schema—so developers can now integrate without needing direct GraphQL knowledge. This release also added support for new OpenAI models (including GPT4_TURBO-128K), LLaMA3 on Groq, and updated Cohere reranking to v3.0. Notable enhancements included clarifications on nullability of GraphQL parameters and newly added deleteWorkflows/deleteAllCollections mutations. We also fixed issues with content-to-collection syncing and conversation-source rendering.\n",
            "\n",
            "## May 2024\n",
            "\n",
            "### May 5 Release\n",
            "May’s first update introduced support for Jina and Pongo rerankers, enabling more flexible ranking services. Microsoft Teams feeds let you capture channel messages, and an updated YouTube downloader addressed platform changes. We introduced an updateContent mutation for post-workflow metadata updates and a new query_contents_graph function for knowledge graph visualization. Bug fixes included improved page-relevance fields, better link extraction from PDFs, and safer handling of invalid JSON from LLMs.\n",
            "\n",
            "### May 15 Release\n",
            "Our GraphRAG feature debuted next, enabling extracted entities to be added as context in retrieval-augmented generation (RAG) dialogs. We introduced LLM revisions to answers within a RAG conversation, boosting quality by 35%. The default LLM changed to OpenAI GPT-4o, and new graph visualizations in promptConversation provided a better view of relationships. We improved performance in entity extraction, cloud storage connections, and bug fixes around limit filters and structured text ingestion from JSON/XML files.\n",
            "\n",
            "## June 2024\n",
            "\n",
            "### June 9 Release\n",
            "June brought support for Deepseek LLMs and the parsing of embedded JSON-LD from web pages, strengthening the knowledge graph. We made knowledge graph generation an opt-in feature (generateGraph must be set to true). A new thing property for observable entities returned their JSON-LD metadata, while regex-based URI filters offered more refined feed ingestion. We addressed retrieval performance bottlenecks and fixed issues with spreadsheet image extraction, among other improvements.\n",
            "\n",
            "### June 21 Release\n",
            "Later in June, we introduced the Claude 3.5 Sonnet model and semantic search for observable entities (like Person or Organization) using vector embeddings. Google authentication was streamlined, and the free tier now includes a 1,000-credit limit before requiring an upgrade. Bug fixes centered on reliably ingesting LinkedIn pages, handling zero-byte files, and better managing filenames with missing or unknown extensions.\n",
            "\n",
            "## July 2024\n",
            "\n",
            "### July 4 Release\n",
            "We introduced webhook Alerts to send HTTP POST notifications with published text results and upgraded Deepseek models to handle a 128k token context window. A new keywords summarization type also arrived, while Slack channel listings became accessible via slackChannels. Bug fixes shortened processing time on large PDFs and addressed queue expiration before messages could be fully processed.\n",
            "\n",
            "### July 19 Release\n",
            "Next in July, we launched the OpenAI GPT-4o Mini model with 16k tokens, plus a bring-your-own-key feature for custom Azure AI Document Intelligence endpoints. Content similarity search became more effective by examining content summaries, and we added a relevance property to filter entity results. Several search and paging bugs were squashed, including a retry on OpenAI errors and improved keyword searches in lengthy PDFs.\n",
            "\n",
            "### July 25 Release\n",
            "Support for Mistral Large 2 and Nemo models arrived, along with Llama 3.1 on Groq. A new revision strategy let LLMs refine previous extraction responses. We also added a version property to AzureDocumentPreparationProperties to let you assign an API version. Among the fixes, hyperlinks in Office documents now extract properly.\n",
            "\n",
            "### July 28 Release\n",
            "To close out July, we introduced an indexing workflow stage for configuring indexing services, including AZURE_AI_LANGUAGE for automatic language detection. Language metadata now returns an ISO 639-1 formatted list of languages. A new MODEL_IMAGE service type streamlines integration with various vision models. We deprecated the OPENAI_IMAGE service type in favor of a more unified modelImage approach. Finally, a bug was fixed around hyperlink extraction in Azure Document Intelligence.\n",
            "\n",
            "## August 2024\n",
            "\n",
            "### August 8 Release\n",
            "In August, we added LLM-based document preparation using OpenAI GPT-4o or Anthropic Sonnet 3.5, plus a new .NET SDK on NuGet for easier .NET integration. Claude 3 Vision is now supported for advanced image-based entity detection, and we introduced an identifier property for Content to map objects back to external databases. Reranking of related entities further improved GraphRAG context. We also resolved issues with HTTP 529 errors from Anthropic calls.\n",
            "\n",
            "### August 11 Release\n",
            "This release focused on language-aware features: summaries and entity descriptions now align with the original text language. We made Azure AI Document Intelligence the default document preparation method, boosting PDF fidelity and table extraction. A small bug fix addressed missing slide counts in PPTX metadata.\n",
            "\n",
            "### August 20 Release\n",
            "For teams handling medical data, we added support for extracting and managing MedicalStudy, MedicalCondition, MedicalDrug, and related entities. These medical entities can be integrated into GraphRAG or accessed via the API. Anthropic prompt caching was also introduced for improved performance and reduced token usage. An empty-PDF bug in entity extraction was resolved, and we switched the similarity filter default to VECTOR for more consistent results.\n",
            "\n",
            "## September 2024\n",
            "\n",
            "### September 1 Release\n",
            "We introduced FHIR enrichment for medical-related entities, plus the latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408). The default Azure AI Document Intelligence API moved to v4.0 preview. We renamed LinkReferenceType to LinkReference for better consistency. Bug fixes included removing stray “source tags” in LLM completions and loading child sitemaps properly.\n",
            "\n",
            "### September 3 Release\n",
            "Teams can now create “web search” feeds that harness Tavily or Exa.AI services to discover relevant pages for ingestion. We also deprecated older OpenAI GPT-3.5 entries, encouraging use of GPT-4o and GPT-4o Mini. A feed-ingestion issue was fixed to handle multiple identical URIs more gracefully.\n",
            "\n",
            "### September 26 Release\n",
            "Support arrived for Cerebras and Google AI model services—expanding coverage to Llama 3.1, Gemini 1.5, and a preview of Groq Llama 3.2. PromptConversation got a new parameter to specify or update conversation specifications on the fly. Bug fixes tackled custom instructions, email filtering for Person entities, deprecated model references, and summation flows not incorporating revision strategy.\n",
            "\n",
            "### September 30 Release\n",
            "Azure AI Model Inference was introduced, letting teams host select models (like Meta Llama 3.2) serverlessly. We added the Mistral Pixtral model and versioned Google Gemini models (e.g., GEMINI_1_5_FLASH_002). No major bugs were reported this cycle.\n",
            "\n",
            "## October 2024\n",
            "\n",
            "### October 3 Release\n",
            "With the ingestBatch mutation, multiple URIs can now be ingested asynchronously in a single call. We also enhanced tool calling for OpenAI, Mistral, Deepseek, Groq, and Cerebras, with an eye on Anthropic and Gemini integration. Meanwhile, createConversation gained the ability to bootstrap an entire conversation with user and assistant messages. We resolved model requirements in specification updates and fixed system prompt issues with OpenAI o1.\n",
            "\n",
            "### October 7 Release\n",
            "We introduced tool-calling support for Anthropic and Google Gemini models. Inline webhook tools were removed, streamlining how external tools are defined and accessed. No bug fixes were listed here—only a focus on better external tool flexibility.\n",
            "\n",
            "### October 9 Release\n",
            "GitHub repository feeds are now available, allowing ingestion of code files directly from specified owners and repositories. A minor markdown table formatting error was resolved to keep table rows properly separated.\n",
            "\n",
            "### October 21 Release\n",
            "New project-level controls for image and text embedding models launched, supporting OpenAI’s Embedding-3-Small/Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0/3.0 text embeddings. We introduced chunkTokenLimit in Specifications for more precise chunk sizing and rolled out an ingestTextBatch mutation for asynchronous text ingestion. Adjusting a project embedding model may require content reingestion to ensure consistent semantic search results.\n",
            "\n",
            "### October 22 Release\n",
            "Anthropic Sonnet 3.5 became available, with newly versioned Anthropic model enums for easy tracking. We also added Cohere image embeddings to enrich image-based analysis further.\n",
            "\n",
            "### October 31 Release\n",
            "October ended with simulated tool calling for specific OpenAI o1-preview and o1-mini models, providing tool schemas right in the LLM prompt context. We lowered vector and hybrid search thresholds to return more results, then reranked them by relevance. A bug fix made sure PDF image extraction included all images (while filtering single-color placeholders).\n",
            "\n",
            "## November 2024\n",
            "\n",
            "### November 4 Release\n",
            "Anthropic Claude 3.5 Haiku became available, and any free-tier project hitting its quota automatically disables all feeds (which can be re-enabled post-upgrade). A new disableFallback flag in RetrievalStrategyInput offers more control over conversation content retrieval. We also fixed an HTML button text-extraction issue.\n",
            "\n",
            "### November 10 Release\n",
            "We introduced searchWeb for direct web searching via Tavily or Exa.AI—no need to ingest pages beforehand—alongside multi-turn summation with reviseContent. Deepgram audio transcription includes default language detection, and we made usage policies clearer by returning HTTP 402 when free-tier credits run out.\n",
            "\n",
            "### November 16 Release\n",
            "This update allowed multi-turn text summarization with the reviseText mutation, letting developers refine and revise text with LLM prompts. Image descriptions also got a boost through describeImage (URI-based) and describeEncodedImage (Base64-based), defaulting to OpenAI GPT-4o if no other vision model is specified.\n",
            "\n",
            "### November 24 Release\n",
            "Rounding out November, we added multi-turn image analysis via reviseImage or reviseEncodedImage, letting teams iterate on image-based insights. A new direct LLM prompt mutation bypassed retrieval-augmented generation for straightforward instructions. Fresh support for Mistral Pixtral Large and an updated OpenAI GPT-4o were also included. Bug fixes targeted JSON parsing in PDF pages and table formatting for Sonnet 3.5 outputs.\n",
            "\n",
            "## December 2024\n",
            "\n",
            "### December 1 Release\n",
            "Early in December, formatConversation and completeConversation empowered retrieval-only RAG pipelines, with inline hyperlinks placed directly in extracted text. The LLM streaming experience improved, and we fixed issues such as an invalid Owner ID acceptance rule and missing Person-to-Organization edges in entity extraction.\n",
            "\n",
            "### December 9 Release\n",
            "Website mapping through mapWeb now exposes all URLs from a sitemap, while screenshotPage can capture screenshots (with optional image processing) before ingestion. Summaries can be generated quickly with summarizeText, and advanced text extraction is available via extractText. Groq Llama 3.3 joined the model lineup, with new reranking defaults for Cohere. We fixed sitemap detection and language mismatch issues in Deepgram setup.\n",
            "\n",
            "### December 22 Release\n",
            "We added connections to Dropbox, Box, Intercom, and Zendesk for ingesting files, articles, and tickets. OpenAI o1 and Gemini Flash 2.0 expansions provided more model choices. Content retrieval can now skip semantic search by setting the conversation’s search type to NONE, and new filter properties (createdInLast or inLast) help isolate recent data. No more content item limits on the Starter tier after December 9, so storage can keep growing. We also resolved multi-tenant collection assignment hiccups and improved HTML decoding.\n",
            "\n",
            "### December 27 Release\n",
            "To wrap up the year, we introduced LLM fallbacks—key for resiliency if a provider goes down—and launched a new API query listing all available models in detail. Native Google Docs, Sheets, and Slides now ingest seamlessly, while Browserless.io integration enables unblocking for otherwise restricted websites. Observations like labels or organizations can be assigned without a full-blown entity extraction step. Publishing improvements returned more content details, and bug fixes addressed table header merges, hyperlink text issues, and empty observables in reranking.\n",
            "\n",
            "---\n",
            "\n",
            "## Moving Forward\n",
            "As this year closes, we want to thank you for embracing all the advancements—from multi-turn image analysis and robust new feed types to expanded LLM capabilities and improved workflow management. We’re excited to continue enhancing developer workflows and user experiences in the coming months. Stay tuned for faster performance, deeper enterprise integrations, and more cutting-edge features as we head into the next year and beyond!\n",
            "\n",
            "2025-01-01T20:18:18.599Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:06.414519, used credits [0.00733200]\n",
            "- CONTENT [79e64fc7-44b1-47b3-9b24-c8aa55dd502a]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1068 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/april-2024/april-7-support-for-discord-feeds-cohere-reranking-section-aware-chunking-and-retrieval</name><title>April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval | Graphlit Changelog</title></metadata> 🐇\tApril 2024\n",
            "April 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "New Features\n",
            "💡 Graphlit now supports Discord feeds.  By connecting to a Discord channel and providing a bot token, you can ingest all Discord messages and file attachments.\n",
            "💡  Graphlit now supports Cohere reranking after content retrieval in RAG pipeline.  You can optionally use the Cohere rerank model to semantically rerank the semantic search results, before providing as context to the LLM.\n",
            "Added support for section-aware text chunking and retrieval.  Now, when using section-aware document preparation, such as Azure AI Document Intelligence, Graphlit will store the extracted text according to the semantic chunks (i.e. sections).  The text for each section will be individually chunked and embedded into the vector index.\n",
            "Added support for retrievalStrategy in Specification type. Graphlit now supports CHUNK, SECTION and CONTENT retrieval strategies.  Chunk retrieval will use the search hit chunk, section retrieval will expand the search hit chunk to the containing section (or page, if not using section-aware preparation).  Content retrieval will expand the search hit chunk to the text of the entire document.\n",
            "Added support for rerankingStrategy in Specification type. You can now configure the reranking of content sources, using the Cohere reranking model, by assigning serviceType to COHERE.  More reranking models are planned for the future.\n",
            "Added isSynchronous flag to content ingestion mutations, such as ingestUri, so the mutation will wait for the content to complete the ingestion workflow (or error) before returning.  This is useful for utilizing the API in a Jupyter notebook or Streamlit application, in a synchronous manner without polling.\n",
            "Added includeAttachments flag to SlackFeedProperties.  When enabled, Graphlit will automatically ingest any attachments within Slack messages.\n",
            "⚡ Added ingestUri mutation to replace the now deprecated ingestPage and ingestFile mutations.  We had seen confusion on when to use one vs the other, and now for any URI, whether it is a web page or hosted PDF, you can pass it to ingestUri, and we will infer the correct content ingestion workflow.\n",
            "⚡ Removed includeSummaries from the ConversationStrategyInput type.  This will re-added in the future as part of the retrieval strategy.\n",
            "⚡ Deprecated enableExpandedRetrieval in ConversationStrategyInput type.  This is now handled by setting strategyType to SECTION or CONTENT in the RetrievalStrategyInput type.\n",
            "⚡ Moved contentLimit from ConversationStrategyInput type to RetrievalStrategyInput type. You can optionally assign the contentLimit to retrievalStrategy which limits the number of content sources leveraged in the LLM prompt context. (Default is 100.)\n",
            "Bugs Fixed\n",
            "GPLA-2469: Failed to ingest PDF hosted on GitHub\n",
            "GPLA-2390: Claude 3 Haiku not adhering to JSON schema\n",
            "GPLA-2474: Prompt rewriting should ignore formatting instructions in prompt\n",
            "GPLA-2462: Missing line break after table rows\n",
            "GPLA-2417: Not extracting images from PPTX correctly\n",
            "PreviousApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "NextMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "Last updated8 months ago\n",
            "- Completion [344 tokens (includes JSON guardrails tokens)], throughput: 53.628 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Discord feeds: Ingest messages and file attachments from a Discord channel using a bot token.\n",
            "  - Cohere reranking: Optionally rerank semantic search results in the RAG pipeline using the Cohere rerank model.\n",
            "  - Section-aware text chunking and retrieval: Store extracted text according to semantic chunks for improved indexing.\n",
            "  - Retrieval strategies: Added CHUNK, SECTION, and CONTENT retrieval strategies for more flexible content retrieval.\n",
            "  - Reranking strategy: Configure reranking of content sources using the Cohere model by assigning serviceType to COHERE.\n",
            "  - Synchronous content ingestion: Added isSynchronous flag for content ingestion mutations to wait for completion before returning.\n",
            "  - Slack attachments: Added includeAttachments flag to automatically ingest attachments in Slack messages.\n",
            "  - IngestUri mutation: Replaces deprecated ingestPage and ingestFile mutations for simplified content ingestion.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Removed includeSummaries from ConversationStrategyInput; will be re-added in the future.\n",
            "  - Deprecated enableExpandedRetrieval; now managed by strategyType in RetrievalStrategyInput.\n",
            "  - Moved contentLimit to RetrievalStrategyInput for better content source management.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with PDF ingestion from GitHub, JSON schema adherence for Claude 3 Haiku, prompt rewriting ignoring formatting instructions, line breaks after table rows, and image extraction from PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: April 2024\n",
            "  - Version updates include new flags and strategies for improved functionality.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for content ingestion, retrieval, and processing, improving integration with various platforms and models.\n",
            "\n",
            "2025-01-01T20:18:18.389Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.460436, used credits [0.00591000]\n",
            "- CONTENT [c51c1b7e-b854-44a8-90c1-bc2d23043aa1]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [974 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/</name><title>December 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports LLM fallbacks which can help protect your application from model provider downtime.  By assigning the fallbacksproperty when creating your conversation, you can provide an optional list of LLM specifications to be used (in order).  These fallback specifications will only be used when we failed to prompt the conversation via the main specification.  Caveat, the RAG pipeline will only use the strategies provided in the main specification for prompt rewriting, content retrieval, etc.  Content is not re-retrieved upon fallback - the formatted LLM prompt will be tried against each fallback specification in succession until one succeeds. (Colab Notebook Example)\n",
            "💡 Graphlit now supports querying of all available models, through the new modelsquery in the API.  This returns the model enum, model service type enum, description, and several other useful details about the models.\n",
            "Graphlit now supports the ingestion of native Google Docs, Google Sheets and Google Slides documents from Google Drive feeds.  These formats will be auto-exported to the corresponding Microsoft Office format (DOCX, XLSX, PPTX) prior to ingesting as content.\n",
            "Graphlit now supports unblocking of websites, such as those using Cloudflare.  You can set enableUnblockedCaptureto true on the PreparationWorkflowStageto enable unblocking - through our integration with Browserless.io headless browser service.  This does incur an additional cost per page, compared to normal web page ingestion.\n",
            "We have added support for assigning observations to contents ingested via feeds.  By assigning observationsto the IngestionWorkflowStagein workflow object, you can assign Labels, Organizations, etc. without needing to use entity extraction.\n",
            "We have added support for assigning observations when ingesting content via ingestUri, ingestText, etc. mutations. By passing observationsas a parameter, similar to `collections`, you can assign Labels, Organizations, etc. without needing to use entity extraction.\n",
            "⚡ We have changed the response type of the publishContentsmutation to return PublishContentstype.  This new PublishContentstype wraps the published Contentobject, and includes the new Detailsproperty of PublishingDetailstype. We have added an includeDetailsparameter to publishContentsmutation, which will fill in the Details property with a list of intermediate content summaries and the published text, among other publishing metrics.\n",
            "⚡ We have changed the behavior of publishContentssuch that, if no content was retrieved for publishing, the mutation returns a null content object rather than returning an error.\n",
            "Bugs Fixed\n",
            "GPLA-3645: Table headers merged together on web scrape\n",
            "GPLA-3634: Failed to extract pages from PDF with empty hyperlink text\n",
            "GPLA-3633: Not handling empty observables properly for reranking\n",
            "NextDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "Last updated3 days ago\n",
            "- Completion [249 tokens (includes JSON guardrails tokens)], throughput: 55.824 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for LLM fallbacks to protect against model provider downtime.\n",
            "  - New API query for all available models, providing model details.\n",
            "  - Ingestion of native Google Docs, Sheets, and Slides from Google Drive, auto-exported to Microsoft formats.\n",
            "  - Website unblocking support via Browserless.io integration, with additional costs per page.\n",
            "  - Ability to assign observations (Labels, Organizations) to ingested content without entity extraction.\n",
            "  - Enhanced content publishing response type with new details and metrics.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Changed response type of publishContents mutation to include new details.\n",
            "  - Updated behavior of publishContents to return null for no content instead of an error.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed merged table headers on web scrape (GPLA-3645).\n",
            "  - Resolved issue with extracting pages from PDFs with empty hyperlink text (GPLA-3634).\n",
            "  - Improved handling of empty observables for reranking (GPLA-3633).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 2024.\n",
            "  \n",
            "- Value:\n",
            "  - Offers developers improved content ingestion, enhanced model querying, and better error handling, increasing application reliability and functionality.\n",
            "\n",
            "2025-01-01T20:18:17.900Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.226293, used credits [0.00669300]\n",
            "- CONTENT [b47614fd-41f3-491b-bbb4-d3976687e131]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [879 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/april-2024/april-23-support-for-python-and-typescript-sdks-latest-openai-cohere-and-groq-models-bug-fixes</name><title>April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes | Graphlit Changelog</title></metadata> 🐇\tApril 2024\n",
            "April 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports a native Python SDK, using Pydantic types. The Python SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest PyPi package here.  The Streamlit sample applications have been updated to use the new Python SDK.\n",
            "💡 Graphlit now supports a native Node.js SDK, using TypeScript types. The Node.js SDK is code-generated from the current GraphQL schema, but does not require GraphQL knowledge. You can find the latest NPM package here.\n",
            "💡 Graphlit now supports the 2024-04-09 models in the OpenAI model service. GPT4_TURBO-128K will give the latest OpenAI GPT-4 model, following this model list.  We have added the GPT4_TURBO_128K_2024_04_09 enum to specify the new model.\n",
            "💡 Graphlit now supports LLaMA3 70b, LLaMA3 8b and Gemma 7b models in the Groq model service.\n",
            "💡 Graphlit now supports the Command R and Command-R+ models in the Cohere model service.\n",
            "Added support for Jina reranking, using the JINA reranking model service type in the reranking retrieval strategy.\n",
            "Updated the Cohere reranking model to use the latest v3.0 model.\n",
            "Increased the reliability of parsing LLM responses, in cases where they don't follow the JSON schema.\n",
            "⚡ Cleaned up nullability of GraphQL parameters, so parameters better reflect if they are required or optional, or allow nulls.\n",
            "⚡ Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "⚡ Split out reranking model service type as RetrievalModelServiceTypes enum.\n",
            "Bugs Fixed\n",
            "GPLA-2114: Adding content to collections not syncing search index\n",
            "GPLA-2511: Failing to render any conversation sources with section retrieval and text content\n",
            "PreviousMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "NextApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "Last updated8 months ago\n",
            "- Completion [338 tokens (includes JSON guardrails tokens)], throughput: 79.976 tokens/sec:\n",
            "- New Features:\n",
            "  - Native Python SDK introduced, using Pydantic types; code-generated from GraphQL schema, no GraphQL knowledge required.\n",
            "  - Native Node.js SDK introduced, using TypeScript types; code-generated from GraphQL schema, no GraphQL knowledge required.\n",
            "  - Support for OpenAI's 2024-04-09 models, including GPT4_TURBO-128K.\n",
            "  - Support for LLaMA3 70b, LLaMA3 8b, and Gemma 7b models in Groq model service.\n",
            "  - Support for Command R and Command-R+ models in Cohere model service.\n",
            "  - Added support for Jina reranking with the JINA reranking model service type.\n",
            "  - Updated Cohere reranking model to v3.0.\n",
            "  - Improved reliability of parsing LLM responses not following JSON schema.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Cleaned up nullability of GraphQL parameters for better clarity on required, optional, or nullable parameters.\n",
            "  - Added missing deleteWorkflows and deleteAllCollections mutations.\n",
            "  - Split reranking model service type into RetrievalModelServiceTypes enum.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where adding content to collections did not sync with the search index (GPLA-2114).\n",
            "  - Resolved rendering issues with conversation sources in section retrieval and text content (GPLA-2511).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: April 2024.\n",
            "  \n",
            "- Value:\n",
            "  - Offers developers enhanced SDKs for Python and TypeScript, improved model support, and better handling of LLM responses, increasing development efficiency and capabilities.\n",
            "\n",
            "2025-01-01T20:18:14.187Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.126803, used credits [0.00575100]\n",
            "- CONTENT [da28adff-3f2c-4062-b78c-3581c2762768]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [861 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-3-new-data-model-for-observations-new-category-entity</name><title>August 3: New data model for Observations, new Category entity | Graphlit Changelog</title></metadata> 🎂\tAugust 2023\n",
            "August 3: New data model for Observations, new Category entity\n",
            "New Features\n",
            "💡 Revised data model for Observations, Occurrences and observables (i.e. Person, Organization).  Now after entity extraction, content will have one Observation for each observed entity, and a list of occurrences.  Occurrence now supports text, time and image occurrence types.  (Text: page index, time: start/end timestamp, image: bounding box)  Observations now have ObservableType and Observable fields, which specify the observed entity type and entity reference.\n",
            "💡 Added Category entity to GraphQL data model, which supports PII categories such as Phone Number or Credit Card Number.\n",
            "Added probability field to model properties, for the LLM's token probability.  (See OpenAI documentation for more detail.)\n",
            "Added error field to feeds.  If a feed fails to read from the data source, and is marked as ERRORED state, the error field will have the error description.\n",
            "Support reingestion of changed files from feeds.  For feeds, such as SharePoint or Web, where we can recognize that a file or page was updated, we will now reingest the content in-place.  Content will keep the same ID, and will restart the content workflow by re-downloading the updated content from the data source.   Existing observations will be deleted, and new observations will be created from the updated content.\n",
            "ℹ️ Ingestion of content is now idempotent, meaning if you ingest content again from the same URI, we will reingest the content in-place, while keeping the same ID.  (If we can recognize the content has not changed, such as by ETag, we will return the existing content object.)\n",
            "ℹ️ Changed GraphQL data type of SharePoint tenantId, libraryId and siteId to ID rather than String.\n",
            "✨ Performance optimization of entity extraction, and the creation of observations.\n",
            "Bugs Fixed\n",
            "GPLA-1130: Only was extracting text from first column of PDF tables.\n",
            "GPLA-1140: Text from DOCX tables was not extracted properly.\n",
            "GPLA-1154: Audio content ingested from RSS feed was not deleted when feed was deleted.\n",
            "PreviousAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "NextJuly 15: Support for SharePoint feeds, new Conversation features\n",
            "Last updated1 year ago\n",
            "- Completion [264 tokens (includes JSON guardrails tokens)], throughput: 84.431 tokens/sec:\n",
            "- New Features:\n",
            "  - Revised data model for Observations, Occurrences, and observables (e.g., Person, Organization).\n",
            "  - Each observed entity now has one Observation and a list of occurrences (supports text, time, and image types).\n",
            "  - Added Category entity to GraphQL data model for PII categories (e.g., Phone Number, Credit Card Number).\n",
            "  - Introduced probability field for LLM's token probability.\n",
            "  - Added error field to feeds for error descriptions on failed reads.\n",
            "  - Support for reingestion of changed files from feeds, maintaining the same ID.\n",
            "  - Ingestion of content is now idempotent, allowing reingestion without ID change if content is unchanged.\n",
            "  - Changed GraphQL data type for SharePoint identifiers from String to ID.\n",
            "  - Performance optimization for entity extraction and observation creation.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of content ingestion and updates.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed extraction issues from PDF and DOCX tables.\n",
            "  - Resolved issue where audio content from RSS feeds was not deleted upon feed deletion.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved data handling, enhanced performance, and better error management in the Graphlit Platform.\n",
            "\n",
            "2025-01-01T20:18:13.891Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.129033, used credits [0.00325200]\n",
            "- CONTENT [fbfc6199-ba6c-4e4d-a050-4bc72d7faeaf]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [616 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-17-prepare-for-usage-based-billing-append-sas-tokens-to-uris</name><title>August 17: Prepare for usage-based billing; append SAS tokens to URIs | Graphlit Changelog</title></metadata> 🎂\tAugust 2023\n",
            "August 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "New Features\n",
            "ℹ️ Behind the scenes, Graphlit is preparing to launch usage-based billing.  This release put in place the infrastructure to track billable events.  Organizations now have a Stripe customer associated with them, and Graphlit projects are auto-subscribed to a Free/Hobby pricing plan.  In a future release, we will provide the ability to upgrade to a paid plan in the Graphlit Developer Portal.  Also, we will provide visualization of usage, on granular basis, in the Portal.\n",
            "💡 Content URIs now have Shared Access Signature (SAS) token appended, so they are accessible after query.  For example, content.transcriptUri will now be able to be downloaded or used directly in an application (until the SAS token expires).\n",
            "🧱 Added more robustness for error handling and retries, especially for LLM APIs and audio transcription APIs.\n",
            "PreviousSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "NextAugust 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "Last updated1 year ago\n",
            "- Completion [117 tokens (includes JSON guardrails tokens)], throughput: 54.955 tokens/sec:\n",
            "- New Features:\n",
            "  - Infrastructure for usage-based billing implemented; organizations now have a Stripe customer and auto-subscribed to a Free/Hobby pricing plan.\n",
            "  - Content URIs now include Shared Access Signature (SAS) tokens for direct access after queries.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Improved error handling and retries for LLM APIs and audio transcription APIs.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 17, 2023.\n",
            "  \n",
            "- Value:\n",
            "  - Prepares developers for future billing options and enhances content accessibility and API reliability.\n",
            "\n",
            "2025-01-01T20:18:13.619Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.054730, used credits [0.00420300]\n",
            "- CONTENT [523f000e-30f7-44a0-8c75-383b18aeed8f]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [673 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2023/august-9-support-direct-text-markdown-and-html-ingestion-new-specification-llm-strategy</name><title>August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy | Graphlit Changelog</title></metadata> 🎂\tAugust 2023\n",
            "August 9: Support direct text, Markdown and HTML ingestion; new Specification LLM strategy\n",
            "New Features\n",
            "💡 Added ingestText mutation which supports direct Content ingestion of plain text, Markdown and HTML.  Now, if you have pre-scraped HTML or Markdown text, you can ingest it into Graphlit without reading from a URL.\n",
            "💡 Added Specification strategy property, which allows customization of the LLM context when prompting a conversation.  ConversationStrategy now provides Windowed and Summarized message histories, as well as configuration of the weight between existing conversation messages and Content text pages (or audio transcript segments) in the LLM context.\n",
            "💡 Added auto-summarization of extracted text and audio transcripts.  There is a new Content summary property where a list of summary bullet points can be found.  These summaries can be optionally included in the Conversation prompt context for more accurate LLM responses.\n",
            "ℹ️ Added AzureOpenAIModels and OpenAIModels types to Specification model properties to make it easier to specify the desired LLM.\n",
            "ℹ️ Renamed ConversationMessage date property to timestamp\n",
            "✨ Refined the internal LLM prompts for providing content as part of Conversation context.  This provides for much clearer and accurate results from the LLM.\n",
            "PreviousAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "NextAugust 3: New data model for Observations, new Category entity\n",
            "Last updated1 year ago\n",
            "- Completion [182 tokens (includes JSON guardrails tokens)], throughput: 44.886 tokens/sec:\n",
            "- New Features:\n",
            "  - IngestText mutation added for direct ingestion of plain text, Markdown, and HTML without URL reading.\n",
            "  - Specification strategy property introduced for customizing LLM context in conversations, with options for Windowed and Summarized message histories.\n",
            "  - Auto-summarization feature for extracted text and audio transcripts, with summaries available for inclusion in conversation prompts.\n",
            "  - AzureOpenAIModels and OpenAIModels types added to Specification model properties for easier LLM specification.\n",
            "  - ConversationMessage date property renamed to timestamp.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Internal LLM prompts refined for clearer and more accurate results.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for content ingestion and LLM customization, improving the accuracy and relevance of responses.\n",
            "\n",
            "2025-01-01T20:18:12.033Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.106926, used credits [0.00394800]\n",
            "- CONTENT [d40294bc-55cb-4f3b-ade6-42fab2f1d379]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [600 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-20-support-for-medical-entities-anthropic-prompt-caching-bug-fixes</name><title>August 20: Support for medical entities, Anthropic prompt caching, bug fixes | Graphlit Changelog</title></metadata> 🎂\tAugust 2024\n",
            "August 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "💡 Graphlit now supports medical-related entities in GraphRAG, and via API for queries and mutations.\n",
            "Added support for Anthropic prompt caching. When using Anthropic Sonnet 3.5 or Haiku 3, Anthropic will now cache the entity extraction and LLM document preparation system prompts, which saves on token cost and increases performance.\n",
            "Bugs Fixed\n",
            "GPLA-3104: Should default search type to VECTOR, when performing entity similarity filter.\n",
            "GPLA-3112: Empty PDF fails entity extraction.\n",
            "PreviousSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "NextAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "Last updated4 months ago\n",
            "- Completion [179 tokens (includes JSON guardrails tokens)], throughput: 57.613 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for extraction of medical-related entities: MedicalStudy, MedicalCondition, MedicalGuideline, MedicalDrug, MedicalDrugClass, MedicalIndication, MedicalContraindication, MedicalTest, MedicalDevice, MedicalTherapy, and MedicalProcedure.\n",
            "  - Medical-related entities supported in GraphRAG and via API for queries and mutations.\n",
            "  - Added support for Anthropic prompt caching, improving performance and reducing token costs when using Anthropic Sonnet 3.5 or Haiku 3.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved entity similarity filter defaults to VECTOR.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where empty PDFs failed entity extraction.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 20, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances capabilities for developers working with medical data and improves efficiency in using Anthropic models.\n",
            "\n",
            "2025-01-01T20:18:11.693Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.711863, used credits [0.00411900]\n",
            "- CONTENT [ce9f8f91-d948-421a-8cda-3959e0f9c5f6]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [629 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-11-support-for-azure-ai-document-intelligence-by-default-language-aware-summaries</name><title>August 11: Support for Azure AI Document Intelligence by default, language-aware summaries | Graphlit Changelog</title></metadata> 🎂\tAugust 2024\n",
            "August 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "New Features\n",
            "Added support for language-aware summaries when using LLM-based document extraction.  Now the summaries for tables and sections generated by the LLM will follow the language of the source text.\n",
            "Added support for language-aware entity descriptions with using LLM-based entity extraction. Now the entity descriptions generated by the LLM will follow the language of the source text.\n",
            "⚡ We have changed the default document preparation method to use Azure AI Document Intelligence, rather than our built-in document parsers.  We have found that the fidelity of Azure AI is considerably better for complex PDFs, and provides better support for table extraction, so we have made this the default. Note: this does come with increased credit usage per-page, for PDF, DOCX and PPTX documents, but the quality of the extracted documents are noticeably higher for use in RAG pipelines.\n",
            "Bugs Fixed\n",
            "GPLA-3070: Not getting slide count assigned to metadata for PPTX files.\n",
            "PreviousAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "NextAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "Last updated4 months ago\n",
            "- Completion [186 tokens (includes JSON guardrails tokens)], throughput: 68.588 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for language-aware summaries in LLM-based document extraction, aligning summaries with the source text language.\n",
            "  - Support for language-aware entity descriptions in LLM-based entity extraction, matching descriptions to the source text language.\n",
            "  - Default document preparation method changed to Azure AI Document Intelligence for improved fidelity in complex PDFs and better table extraction.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Increased credit usage per page for PDF, DOCX, and PPTX documents due to the switch to Azure AI, but with significantly higher quality in extracted documents for RAG pipelines.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3070: Slide count not assigned to metadata for PPTX files.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 11, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced document extraction capabilities with improved accuracy and language support, facilitating better integration in applications.\n",
            "\n",
            "2025-01-01T20:18:10.987Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.924963, used credits [0.00357000]\n",
            "- CONTENT [8c9d843f-4a50-4c40-aab7-9dbeb3472396]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [562 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-1-support-for-retrieval-only-rag-pipeline-bug-fixes</name><title>December 1: Support for retrieval-only RAG pipeline, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports formatting of LLM-ready prompts with our RAG pipeline, via the new formatConversation and completeConversation mutations.  This is valuable for supporting LLM streaming by directly calling the LLM from your application, and using Graphlit for RAG retrieval and conversation history. (Colab Notebook Example)\n",
            "We have added support for inline hyperlinks in extracted text from documents and web pages.\n",
            "Bugs Fixed\n",
            "GPLA-3466: Owner ID should accept any non-whitespace string\n",
            "GPLA-3458: Not getting Person-to-Organization edges from entity extraction\n",
            "PreviousDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "NextNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "Last updated1 month ago\n",
            "- Completion [157 tokens (includes JSON guardrails tokens)], throughput: 53.676 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for formatting LLM-ready prompts with RAG pipeline using formatConversation and completeConversation mutations.\n",
            "  - Inline hyperlinks can now be included in extracted text from documents and web pages.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved support for LLM streaming by enabling direct calls to the LLM from applications.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3466: Owner ID now accepts any non-whitespace string.\n",
            "  - Fixed issue GPLA-3458: Resolved problem with missing Person-to-Organization edges from entity extraction.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 1, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements provide developers with better integration for LLM interactions and improved text extraction capabilities.\n",
            "\n",
            "2025-01-01T20:18:09.480Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:06.829118, used credits [0.00578700]\n",
            "- CONTENT [c5dc4da1-aa86-4d89-93f2-d15ec179a24d]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [921 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-9-support-for-website-mapping-web-page-screenshots-groq-llama-3.3-model-bug-fixes</name><title>December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports mapping a website with the mapWebmutation. You can provide a URL to a website, and the query will return a list of URLs based on the sitemap.xml (or sitemap-index.xml) file, at or underneath the provided URL.\n",
            "💡 Graphlit now supports the generation of web page screenshots with the screenshotPagemutation. By providing the URL of a web page, and optionally, the maximum desired height of the screenshot, we will screenshot the webpage and ingest it automatically as content.  You can provide an optional workflow, which will be applied to the ingested image content, for operations like generating image descriptions with a vision LLM.\n",
            "💡 Graphlit now supports the direct summarization of text with the summarizeTextmutation. By providing the desired summarization strategy, we will summarize the text (i.e. bullet points, social media posts) and return the summarization.\n",
            "💡 Graphlit now supports the direct extraction of text with the extractTextmutation. By providing the LLM tool definitions and an optional LLM specification, we will prompt the desired LLM (or OpenAI GPT-4o, by default) to invoke the provided tools, and return the JSON responses from the LLM tool calling.\n",
            "Graphlit now supports the latest Groq Llama 3.3 model, with the model enum LLAMA_3_3_70B.\n",
            "We have updated Cohere reranking to use the latest Cohere rerank-v3.5model by default.\n",
            "⚡ We have added a new flattenCitationsfield to the ConversationStrategyInputtype.  By assigning this field to True, when calling promptConversation,we will combine multiple citations from the same content into a single citation.\n",
            "⚡ For Microsoft email, Microsoft Teams and OneDrive feeds, we have added the clientIdand clientSecretfields as required feed properties. These properties must be assigned, in addition to the refreshTokenfield for proper authentication to the Microsoft Graph API used by these feeds. (Colab Notebook Example)\n",
            "Bugs Fixed\n",
            "GPLA-3492: Not finding sitemap at parent web path\n",
            "GPLA-3500: Failed to handle mismatch of Deepgram model/language\n",
            "PreviousDecember 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "NextDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "Last updated22 days ago\n",
            "- Completion [252 tokens (includes JSON guardrails tokens)], throughput: 36.901 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for website mapping using mapWebmutation to return URLs from sitemap.xml.\n",
            "  - Generation of web page screenshots with screenshotPagemutation, including optional image processing workflows.\n",
            "  - Direct summarization of text with summarizeTextmutation, allowing various summarization strategies.\n",
            "  - Direct extraction of text with extractTextmutation, utilizing LLM tools for JSON responses.\n",
            "  - Support for Groq Llama 3.3 model (LLAMA_3_3_70B).\n",
            "  - Updated Cohere reranking to use rerank-v3.5 model by default.\n",
            "  - New flattenCitations field in ConversationStrategyInput to combine multiple citations.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Required clientId and clientSecret fields added for Microsoft email, Teams, and OneDrive feeds for proper authentication.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with not finding sitemap at parent web path (GPLA-3492).\n",
            "  - Resolved mismatch handling of Deepgram model/language (GPLA-3500).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for web mapping, content ingestion, and improved integration with Microsoft services.\n",
            "\n",
            "2025-01-01T20:18:08.845Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:05.673722, used credits [0.00887400]\n",
            "- CONTENT [979a18c8-361a-4786-ac5e-355c7652e5ba]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1230 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2024/december-22-support-for-dropbox-box-intercom-and-zendesk-feeds-openai-o1-gemini-2.0-bug-fixes</name><title>December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2024\n",
            "December 22: Support for Dropbox, Box, Intercom and Zendesk feeds, OpenAI o1, Gemini 2.0, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Dropbox feeds for ingesting files on the Dropbox cloud service. Dropbox feeds require your appKey, appSecret, redirectUriand refreshTokento be assigned. The feed also accepts an optional pathparameter to read files from a specific Dropbox folder.\n",
            "💡 Graphlit now supports Box feeds for ingesting files on the Box cloud service. Box feeds require your clientId, clientSecret, redirectUriand refreshTokento be assigned.\n",
            "💡 Graphlit now supports Intercom feeds for ingesting Intercom Articles and Tickets. We will ingest Intercom Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Intercom feeds require the accessTokenproperty to be assigned.\n",
            "💡 Graphlit now supports Zendesk feeds for ingesting Zendesk Articles and Tickets.  We will ingest Zendesk Articles as PAGEcontent type, and Tickets as ISSUEcontent type. Zendesk feeds require the accessTokenproperty and your Zendesk subdomain to be assigned.\n",
            "Graphlit now supports the latest OpenAI o1 model, with the model enums O1_200kand O1_200k_20241217.\n",
            "Graphlit now supports the latest Gemini Flash 2.0 Experimental model, with the model enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "Graphlit now supports the latest Cohere R7B model, with the model enum COMMAND_R7B_202412.\n",
            "Graphlit now supports returning the low-level details from prompting RAG conversations, by adding the includeDetailsparameter and setting to True. This includes details on the number of sources, the exact list of messages provided to the LLM, and more.\n",
            "We have added support for filtering of observables, such as Person or Organization, by URI property.\n",
            "We have added the ability to bypass semantic search in content retrieval with conversations. You can assign NONEfor the conversation search type, and it will ignore the user prompt when retrieving content.  It will inject all contents resulting from the content filter into the RAG prompt context.\n",
            "We have added a new createdInLastproperty to all entity filters, which allows easier filtering of entities created within a recent time period. Also, we have added a new inLastproperty to the content filter, which allows easier filtering of content authored within a recent time period. For example, find all images taken in the last 3 days, or find me all emails I received yesterday.\n",
            "We have added support for the latest Azure AI Document Intelligence models, with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "We have added support for Google Drive and OneDrive feeds to ingest specific files by providing a list of file identifiers (files), in addition to the folder identifier (folderId).  If files identifiers are provided, they take precedence over the folder identifier.\n",
            "⚡ For projects upgraded to the Starter Tier after Dec 9, 2024, we have removed the content items limit. Now you can store an unlimited number of content items (i.e. files, web pages, Slack messages) on the Starter or Growth Tiers.  If you have an existing project on the Starter Tier, please reach out and we will manually remove that content item limit on the project.\n",
            "Bugs Fixed\n",
            "GPLA-3529: Can't assign collection to multitenant content\n",
            "GPLA-3579: Should decode HTML characters when parsing HTML email\n",
            "GPLA-3576: Ingesting content in-place doesn't handle isSynchronous properly\n",
            "GPLA-3457: IsFeedDone doesn't return True for finished feed with no contents\n",
            "GPLA-3572: Not handling HTTP 400 error on uploading from URI\n",
            "PreviousDecember 27: Support for LLM fallbacks, native Google Docs formats, website unblocking, bug fixes\n",
            "NextDecember 9: Support for website mapping, web page screenshots, Groq Llama 3.3 model, bug fixes\n",
            "Last updated7 days ago\n",
            "- Completion [432 tokens (includes JSON guardrails tokens)], throughput: 76.140 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Dropbox feeds for file ingestion, requiring appKey, appSecret, redirectUri, and refreshToken.\n",
            "  - Support for Box feeds for file ingestion, requiring clientId, clientSecret, redirectUri, and refreshToken.\n",
            "  - Support for Intercom feeds to ingest Articles and Tickets, requiring accessToken.\n",
            "  - Support for Zendesk feeds to ingest Articles and Tickets, requiring accessToken and Zendesk subdomain.\n",
            "  - Support for OpenAI o1 model with enums O1_200k and O1_200k_20241217.\n",
            "  - Support for Gemini Flash 2.0 Experimental model with enum GEMINI_2_0_FLASH_EXPERIMENTAL.\n",
            "  - Support for Cohere R7B model with enum COMMAND_R7B_202412.\n",
            "  - Ability to return low-level details from RAG conversations using includeDetails parameter.\n",
            "  - Support for filtering observables by URI property.\n",
            "  - Ability to bypass semantic search in content retrieval by assigning NONE for conversation search type.\n",
            "  - New createdInLast property for entity filters and inLast property for content filters for recent time period filtering.\n",
            "  - Support for Azure AI Document Intelligence models with enums US_PAY_STUB, US_BANK_STATEMENT, and US_BANK_CHECK.\n",
            "  - Support for Google Drive and OneDrive feeds to ingest specific files by providing file identifiers.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Removal of content items limit for projects upgraded to the Starter Tier after December 9, 2024, allowing unlimited storage of content items.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with collection assignment to multitenant content.\n",
            "  - Resolved HTML character decoding when parsing HTML email.\n",
            "  - Corrected handling of isSynchronous during content ingestion.\n",
            "  - Fixed IsFeedDone not returning True for finished feeds with no contents.\n",
            "  - Addressed HTTP 400 error handling on URI uploads.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced integration capabilities with popular services, improved content management, and expanded model support for AI applications.\n",
            "\n",
            "2025-01-01T20:18:08.817Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.778265, used credits [0.00578100]\n",
            "- CONTENT [3ed53e18-c1a5-41f7-9a6e-ef900e439b95]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [811 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/august-2024/august-8-support-for-llm-based-document-extraction-.net-sdk-bug-fixes</name><title>August 8: Support for LLM-based document extraction, .NET SDK, bug fixes | Graphlit Changelog</title></metadata> 🎂\tAugust 2024\n",
            "August 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports LLM-based document preparation, using vision-capable models such as OpenAI GPT-4o and Anthropic Sonnet 3.5.  This is available via the MODEL_DOCUMENT preparation service type, and you can assign a customspecification object and bring your own LLM keys.\n",
            "💡 Graphlit now provides an open source .NET SDK, supporting .NET 6 and .NET 8 (and above).  SDK package can be found on Nuget.org.  Code samples can be found on GitHub.\n",
            "Added identifier property to Content object for mapping content to external database identifiers.  This is supported for content filtering as well.\n",
            "Added support for Claude 3 vision models for image-based entity extraction, using the MODEL_IMAGE entity extraction service.\n",
            "Added context augmentation to conversations, via the augmentedFilter property on the Conversation object.  Any content which matches this augmented filter will be injected into the LLM prompt content, without needing to be related by vector similarity to the user prompt.  This is useful for specifying domain knowledge which should always be referenced by the RAG pipeline.\n",
            "Added support for the latest snapshot of OpenAI GPT-4o, with the model enum GPT4O_128K_20240806.\n",
            "Added reranking of related entities, when preparing the LLM prompt context for GraphRAG.  If reranking is enabled, the metadata from the related entities will be reranked with the same reranker assigned to the conversation specification.\n",
            "⚡ We have changed the type of the duration field in the AudioMetadata and VideoMetadata types to be TimeSpan rather than string, as to be more consistent with the rest of the API data model.\n",
            "Bugs Fixed\n",
            "GPLA-2884: Support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "PreviousAugust 11: Support for Azure AI Document Intelligence by default, language-aware summaries\n",
            "NextJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "Last updated4 months ago\n",
            "- Completion [279 tokens (includes JSON guardrails tokens)], throughput: 73.843 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for LLM-based document preparation using models like OpenAI GPT-4o and Anthropic Sonnet 3.5 via MODEL_DOCUMENT service.\n",
            "  - Introduction of an open source .NET SDK for .NET 6 and .NET 8, available on Nuget.org with code samples on GitHub.\n",
            "  - Added identifier property to Content object for mapping to external database identifiers, aiding in content filtering.\n",
            "  - Support for Claude 3 vision models for image-based entity extraction using MODEL_IMAGE service.\n",
            "  - Context augmentation in conversations through augmentedFilter property, allowing injection of domain knowledge into LLM prompts.\n",
            "  - Support for the latest OpenAI GPT-4o snapshot (GPT4O_128K_20240806).\n",
            "  - Reranking of related entities for LLM prompt context preparation in GraphRAG.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Changed duration field type in AudioMetadata and VideoMetadata from string to TimeSpan for consistency.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2884 to support retry on HTTP 529 (Overloaded) error from Anthropic API.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: August 8, 2024.\n",
            "\n",
            "- Value:\n",
            "  - These updates enhance document extraction capabilities, improve SDK accessibility for .NET developers, and streamline content management and entity extraction processes.\n",
            "\n",
            "2025-01-01T20:18:07.735Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.084445, used credits [0.00828000]\n",
            "- CONTENT [931a2349-8288-4a5f-846c-9f1476cd0946]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1224 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/december-2023/december-10-support-for-openai-gpt-4-turbo-llama-2-and-mistral-models-query-by-example-bug-fixes</name><title>December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes | Graphlit Changelog</title></metadata> 🎄\tDecember 2023\n",
            "December 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the OpenAI GPT-4 Turbo 128k model, both in Azure OpenAI and native OpenAI services.  Added new model enum GPT4_TURBO_VISION_128K.\n",
            "💡 Graphlit now supports Llama 2 7b, 13b, 70b models and Mistral 7b model, via Replicate.  Developers can use their own Replicate API key, or be charged as credits for Graphlit usage.\n",
            "💡 Graphlit now supports the Anthropic Claude 2.1 model. Added new model enum CLAUDE_2_1.\n",
            "💡 Graphlit now supports the OpenAI GPT-4 Vision model for image descriptions and text extraction.  Added new model enum GPT4_TURBO_VISION_128K. See usage example in \"Multimodal RAG\" blog post.\n",
            "Added query by example to contents query.  Developers can specify one or more example contents, and query will use vector embeddings to return similar contents.\n",
            "Added query by example to conversations query.  Developers can specify one or more example conversations, and query will use vector embeddings to return similar conversations.\n",
            "Added vector search support for conversations queries.  Developers can provide search text which will use vector embeddings to return similar conversations.\n",
            "Added promptSpecifications mutation for directly prompting multiple models.  This can be used to evaluate prompts against multiple models or compare different specification parameters in parallel.\n",
            "Added promptStrategy field to Specification, which supports multiple strategy types for preprocessing the prompt before being sent to the LLM model.  For example, REWRITE prompt strategy will ask LLM to rewrite the incoming user prompt based on the previous conversation messages.\n",
            "Added suggestConversation mutation, which returns a list of suggested followup questions based on the specified conversation and related contents.  This can be used to auto-suggest questions for chatbot users.\n",
            "Added new summarization types: CHAPTERS, QUESTIONS and POSTS.   See usage examples in the \"LLMs for Podcasters\" blog post.\n",
            "Added versioned model enums such as GPT4_0613 and GPT35_TURBO_16K_1106.  Without version specified, such as GPT35_TURBO_16K, Graphlit will use the latest production model version, as defined by the LLM vendor.\n",
            "Added lookupContents query to get multiple contents by id in one query.\n",
            "⚡ In Content type, headline field was renamed to headlines and now returns an array of strings.\n",
            "⚡ Entity names are now limited to 1024 characters.  Names will be truncated if they exceed the maximum length.\n",
            "⚡ In SummarizationTypes enum, BULLET_POINTS was renamed to BULLETS.\n",
            "⚡ In ProjectStorage type, originalTotalSize was renamed to totalSize, and totalRenditionSize field was added.  totalSize is the sum of the ingested source file sizes, and totalRenditionSize is the sum of the source file sizes and any derived rendition sizes.\n",
            "⚡ In ConversationStrategy type, strategyType was renamed to type for consistency with rest of data model.\n",
            "⚡ In Specification type, optimizeSearchConversation was removed, and now is handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "Bugs Fixed\n",
            "GPLA-1725: Should ignore RSS.xml from web feed sitemap\n",
            "GPLA-1726: GPT-3.5 Turbo 16k LLM is adding \"Citation #\" to response\n",
            "GPLA-1698: Workflow not applied to link-crawled content\n",
            "GPLA-1692: Mismatched project storage total size, when some content has errored\n",
            "GPLA-1237: Add relevance threshold for semantic search\n",
            "PreviousJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "NextOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "Last updated11 months ago\n",
            "- Completion [384 tokens (includes JSON guardrails tokens)], throughput: 94.015 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4 Turbo 128k model in Azure OpenAI and native OpenAI services.\n",
            "  - Support for Llama 2 (7b, 13b, 70b) and Mistral 7b models via Replicate.\n",
            "  - Support for Anthropic Claude 2.1 model.\n",
            "  - Support for OpenAI GPT-4 Vision model for image descriptions and text extraction.\n",
            "  - Query by example added for contents and conversations, utilizing vector embeddings.\n",
            "  - Vector search support for conversations queries.\n",
            "  - PromptSpecifications mutation for prompting multiple models.\n",
            "  - New promptStrategy field for preprocessing prompts.\n",
            "  - SuggestConversation mutation for auto-suggesting follow-up questions.\n",
            "  - New summarization types: CHAPTERS, QUESTIONS, and POSTS.\n",
            "  - Versioned model enums introduced.\n",
            "  - LookupContents query for retrieving multiple contents by ID.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed headline field to headlines, returning an array of strings.\n",
            "  - Entity names limited to 1024 characters.\n",
            "  - BULLET_POINTS renamed to BULLETS in SummarizationTypes enum.\n",
            "  - ProjectStorage type fields renamed for clarity.\n",
            "  - Removed optimizeSearchConversation from Specification type, now handled by OPTIMIZE_SEARCH prompt strategy.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Ignored RSS.xml from web feed sitemap.\n",
            "  - Resolved issue with GPT-3.5 Turbo 16k LLM adding \"Citation #\" to responses.\n",
            "  - Fixed workflow application to link-crawled content.\n",
            "  - Corrected mismatched project storage total size.\n",
            "  - Added relevance threshold for semantic search.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: December 10, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support, improved querying capabilities, and streamlined prompt management, facilitating more efficient and effective application development.\n",
            "\n",
            "2025-01-01T20:18:04.940Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:07.000231, used credits [0.00970200]\n",
            "- CONTENT [42df7e18-5180-4ee2-99f5-72f3292c9872]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1586 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/january-2024/january-18-support-for-content-publishing-llm-tools-clip-image-embeddings-bug-fixes</name><title>January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes | Graphlit Changelog</title></metadata> 🎆\tJanuary 2024\n",
            "January 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports content publishing, where documents, audio transcripts and even image descriptions, can be summarized, and repurposed into blog posts, emails or AI-generated podcasts.  With the new publishContents mutation, you can configure LLM prompts for summarization and publishing, and assign specifications to use different models and/or system prompts for each step in the process.  The published content will be reingested into Graphlit, and can be searched or used for conversations, like any other form of content.\n",
            "💡 Graphlit now supports publishing conversations as content with the new publishConversation mutation.  You can generate text or audio transcripts of your conversations, to be reused in other tools.\n",
            "💡 Graphlit now supports bulk summarization of contents with the summarizeContents mutation.  You can filter a set of content, by feed, by observable or by similar text, and run a set of summarizations across each content in parallel.\n",
            "💡 Graphlit now supports LLM entity extraction, with the new MODEL_TEXT entity extraction service type.  Similar to using Azure Cognitive Service Text Analytics, you can use any OpenAI or Anthropic model for extracting entities from text.  Internally the LLM returns JSON-LD entities, which we convert into Person, Organization, Place, etc. entities and assign observations to the extracted content.\n",
            "💡 Graphlit now supports LLM tools (aka function calls) with OpenAI models.  You can define the tools to be used with the LLM in the specification object.  With the new extractContents mutation, you can execute a prompt against content using a specification with tools defined.  The mutation will return the JSON arguments assigned by the LLM.\n",
            "💡 Graphlit now supports callback webhooks for LLM tools.  If you assign a URI in the ToolDefinition object, Graphlit will call your webhook the tool name and JSON arguments.  When you respond to the webhook with JSON, we will add that response to the LLM messages, and ask the LLM to complete the original prompt.\n",
            "💡 Graphlit now supports the selection of the Deepgram model (such as Meeting, Phonecall or Finance) with the preparation workflow.  Also, you can assign your own Deepgram API key, which will be used for audio transcription using that workflow.\n",
            "Added support for CLIP image embeddings using Roboflow, which can be used for similar image search.  If you search for contents by similar contents, we will now use the content's text and/or image embeddings to find similar content.\n",
            "Added support for dynamic web page ingestion.  Graphlit now navigates to and automatically scrolls web pages using Browserless.io, so we capture the fully rendered HTML before extracting text.  Also, we now support web page screenshots, if enabled with enableImageAnalysis property in preparation workflow.  These screenshots can be analyzed with multimodal modals, such as GPT-4 Vision, or can be used to create image embeddings for similar image search.\n",
            "Added table parsing when preparing documents.  We now store structured (tab-delimited) text in the JSON text mezzanine which is extracted from documents in the preparation workflow.\n",
            "Added reverse geocoding of lat/long locations found in image or other content metadata.  We now store the real-world address with the content metadata, for use in conversations.\n",
            "Added assistant messages to the conversation message history provided to the LLM.  Originally we had included only user messages, but now we are formatting both user and assistant messages into the LLM prompt for conversations.\n",
            "Added new chunking algorithm for text embeddings.  We support semantic chunking at the page or transcript segment level, and now will create embeddings from smaller sized text chunks per page or segment.\n",
            "Added content metadata to text and image embeddings.  To provide better context for the text embeddings, we now include formatted content metadata, which includes fields like title, subject, author, or description.  For emails, we include to, from, cc, and bcc fields.\n",
            "Added helper mutations isContentDone and isFeedDone which can be used for polling completion of ingested content, or all content ingested by a feed.\n",
            "Added richer image descriptions generated by the GPT-4 Vision model.  Now these provide more useful detail.\n",
            "Added validation of extracted hyperlinks.  Now we test the URIs and remove any inaccessible links during content enrichment.\n",
            "Added deleteContents,  deleteFeeds,  and deleteConversations mutations for multi-deletion of contents, feeds or conversations.\n",
            "Added deleteAllContents,  deleteAllFeeds,  and deleteAllConversations mutations for bulk, filtered deletion of entities.  You can delete all your contents, feeds, or conversations in your project, or a filtered subset of those entities.\n",
            "ℹ️ Starter tier now has a higher content limit of 100K content items.\n",
            "⚡ In the OpenAIImageExtractionProperties type, the detailMode field was renamed to detailLevel.\n",
            "⚡ Each SummarizationStrategy object now accepts the specification which is used by the summarization, rather than being assigned at the preparation workflow stage.\n",
            "⚡ addCollectionContents and removeCollectionContents mutations have been deprecated in favor of addContentsToCollections and removeContentsFromCollection mutations.\n",
            "Bugs Fixed\n",
            "GPLA-1846: Parse Markdown headings into mezzanine JSON\n",
            "GPLA-1779: Not returning SAS token with mezzanine, master URIs\n",
            "GPLA-1348: Summarize text content, not just file content\n",
            "GPLA-1297: Not assigning content error message on preparation workflow failure\n",
            "PreviousJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "NextDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes Last updated8 months ago\n",
            "- Completion [412 tokens (includes JSON guardrails tokens)], throughput: 58.855 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for content publishing, allowing documents, audio transcripts, and image descriptions to be summarized and repurposed.\n",
            "  - New mutation for publishing conversations as content, generating text or audio transcripts.\n",
            "  - Bulk summarization of contents with the summarizeContents mutation.\n",
            "  - LLM entity extraction using MODEL_TEXT service type, returning JSON-LD entities.\n",
            "  - Support for LLM tools (function calls) with OpenAI models, including extractContents mutation.\n",
            "  - Callback webhooks for LLM tools, enabling interaction with external services.\n",
            "  - Selection of Deepgram models for audio transcription with custom API key.\n",
            "  - CLIP image embeddings support for similar image search.\n",
            "  - Dynamic web page ingestion with automatic scrolling and screenshot capabilities.\n",
            "  - Table parsing for structured text extraction from documents.\n",
            "  - Reverse geocoding of lat/long locations in content metadata.\n",
            "  - Inclusion of assistant messages in conversation history for LLM prompts.\n",
            "  - New chunking algorithm for semantic text embeddings.\n",
            "  - Enhanced content metadata for text and image embeddings.\n",
            "  - Helper mutations for polling content ingestion completion.\n",
            "  - Richer image descriptions generated by GPT-4 Vision model.\n",
            "  - Validation of extracted hyperlinks to remove inaccessible links.\n",
            "  - Mutations for multi-deletion and bulk deletion of contents, feeds, or conversations.\n",
            "  - Starter tier content limit increased to 100K items.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed detailMode field to detailLevel in OpenAIImageExtractionProperties.\n",
            "  - SummarizationStrategy objects now accept specifications directly.\n",
            "  - Deprecated addCollectionContents and removeCollectionContents mutations in favor of new mutations.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with parsing Markdown headings, returning SAS tokens, summarizing text content, and assigning error messages on workflow failures.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: January 18, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced content management capabilities, improved integration with LLM tools, and better data handling for various content types.\n",
            "\n",
            "2025-01-01T20:18:03.616Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:06.153016, used credits [0.00636300]\n",
            "- CONTENT [f20064c8-0d29-4243-b163-d94ebb7913d3]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [913 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2023/july-15-support-for-sharepoint-feeds-new-conversation-features</name><title>July 15: Support for SharePoint feeds, new Conversation features | Graphlit Changelog</title></metadata> 🎇\tJuly 2023\n",
            "July 15: Support for SharePoint feeds, new Conversation features\n",
            "New Features\n",
            "💡 Added support for SharePoint feeds: now can create feed to ingest files from SharePoint document library (and optionally, folder within document library)\n",
            "💡 Added support for PII detection during entity extraction from text documents and audio transcripts: now we will create labels such as PII: Social Security Number automatically when PII is detected\n",
            "💡 Added support for developer's own OpenAI API keys and Azure OpenAI deployments in Specifications\n",
            "ℹ️  Changed semantics of deleteFeed to delete the contents ingested by the feed; since contents are linked to feeds, now feeds can be disabled, while keeping the lineage to the feed, and if feeds are deleted, they will delete the linked contents, so we never lose the feed-to-content lineage\n",
            "Added GraphQL query for SharePoint consent URI, for registered Graphlit Platform Azure AD application\n",
            "Better handling of web sitemap indexes: now if a sitemap.xml contains a sitemapindex element, we will load all linked sitemaps for evaluating web pages to ingest from Web feed\n",
            "Added new GraphQL mutations for openConversation, closeConversation and undoConversation\n",
            "Added timestamps to Conversation messages\n",
            "Added new GraphQL mutations for openCollection and closeCollection\n",
            "Added more configuration for content search: now can specify searchType (KEYWORD, VECTOR, HYBRID) and queryType (SIMPLE, FULL - aka Lucene syntax)\n",
            "Better parsing of iTunes podcast metadata\n",
            "⚡ Renamed listingLimit field on feeds to readLimit\n",
            "⚡ Renamed topK to numberSimilar for content vector search type\n",
            "⚡ Changed GraphQL feed properties: split out azure into azureBlob and azureFile properties\n",
            "⚡ Changed GraphQL specification properties: split out openAI into openAI and azureOpenAI properties\n",
            "⚡ Removed count fields on query results, and replaced with explicit count{Entity} queries, which support search and filtering.\n",
            "Bugs Fixed\n",
            "GPLA-1043: Reddit readLimit not taking effect: now the specified limit of Reddit posts will be leveraged for Reddit feeds\n",
            "GPLA-1064: Performance on entity extraction and observation creation for large PDFs was under expectations: now able to build knowledge graph from large PDFs much faster (4x speed improvement)\n",
            "GPLA-1053: If rendition generation errored during content workflow, the content was not properly marked as errored\n",
            "GPLA-1102: Large Web sitemaps were slow to load; rewrote sitemap index handling, and now can process sitemaps with 150K+ entries in seconds.\n",
            "PreviousAugust 3: New data model for Observations, new Category entity\n",
            "Last updated6 months ago\n",
            "- Completion [302 tokens (includes JSON guardrails tokens)], throughput: 49.082 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for SharePoint feeds to ingest files from SharePoint document libraries.\n",
            "  - PII detection during entity extraction from text documents and audio transcripts, automatically labeling detected PII.\n",
            "  - Support for custom OpenAI API keys and Azure OpenAI deployments in Specifications.\n",
            "  - GraphQL query for SharePoint consent URI for registered Azure AD applications.\n",
            "  - Improved handling of web sitemap indexes to load linked sitemaps.\n",
            "  - New GraphQL mutations for managing conversations and collections.\n",
            "  - Timestamps added to Conversation messages.\n",
            "  - Enhanced content search configuration with new search and query types.\n",
            "  - Better parsing of iTunes podcast metadata.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Renamed fields for clarity: listingLimit to readLimit, topK to numberSimilar.\n",
            "  - Split GraphQL properties for Azure and OpenAI for better organization.\n",
            "  - Removed count fields on query results, replaced with explicit count queries.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed Reddit readLimit issue to ensure specified limits are applied.\n",
            "  - Improved performance for entity extraction from large PDFs (4x speed increase).\n",
            "  - Corrected error handling for rendition generation in content workflows.\n",
            "  - Enhanced loading speed for large web sitemaps, now processing 150K+ entries quickly.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 15, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for data ingestion, improved performance, and better management of content and conversations.\n",
            "\n",
            "2025-01-01T20:18:03.028Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.495010, used credits [0.00459000]\n",
            "- CONTENT [f53855bb-439c-48c9-a88d-f427da2962d1]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [690 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-2-support-for-semantic-alerts-openai-0125-models-performance-enhancements-bug-fixes</name><title>February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes | Graphlit Changelog</title></metadata> 🌧️\tFebruary 2024\n",
            "February 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Semantic Alerts, which allows for LLM summarization and publishing of content, on a periodic basis.  This is useful for generating daily reports from email, Slack or other time-based feeds.  Alerts support the same publishing options, i.e. audio and text, as the publishContents mutation.\n",
            "💡 Graphlit now supports the latest OpenAI 0125 model versions, for GPT-4 and GPT-3.5 Turbo.  We will add support for Azure OpenAI when Microsoft releases support for these.\n",
            "Slack feeds now support a listing type field, where you can specify if you want PAST or NEW Slack messages in the feed.\n",
            "🔥 This release provides many performance enhancements, which will speed up the content workflows for ingested content.\n",
            "Bugs Fixed\n",
            "GPLA-2114: Collections not being added to text embedding index documents.\n",
            "GPLA-2063: Not handling hallucinated citations.\n",
            "GPLA-1916: Collections not inherited from project-scope into tenant-scope.\n",
            "GPLA-2105: Should error on add/remove of contents to/from collections if content does not exist.\n",
            "PreviousFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "NextJanuary 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "Last updated10 months ago\n",
            "- Completion [210 tokens (includes JSON guardrails tokens)], throughput: 84.168 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Semantic Alerts for LLM summarization and content publishing on a periodic basis, useful for generating daily reports from various feeds.\n",
            "  - Support for OpenAI 0125 model versions for GPT-4 and GPT-3.5 Turbo, with plans to add Azure OpenAI support when available.\n",
            "  - Slack feeds now include a listing type field to specify PAST or NEW messages.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Performance enhancements to speed up content workflows for ingested content.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with collections not being added to text embedding index documents.\n",
            "  - Resolved handling of hallucinated citations.\n",
            "  - Addressed inheritance of collections from project-scope to tenant-scope.\n",
            "  - Implemented error handling for adding/removing contents to/from collections if content does not exist.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 2, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved content management capabilities, enhanced performance, and better error handling.\n",
            "\n",
            "2025-01-01T20:18:02.592Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.845666, used credits [0.00683400]\n",
            "- CONTENT [72415f65-64c2-4aca-87e0-93aa4f58cea6]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1042 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/february-2024/february-21-support-for-onedrive-and-google-drive-feeds-extract-images-from-pdfs-bug-fixes</name><title>February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes | Graphlit Changelog</title></metadata> 🌧️\tFebruary 2024\n",
            "February 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports OneDrive and Google Drive feeds.  Files can be ingested from OneDrive or Google Drive, including shared drives where the authenticated user has access.  Both OneDrive and Google Drive support the reading of existing files, and tracking new files added to storage with recurrent feeds.\n",
            "💡 Graphlit now supports email backup files, such as EML or MSG, which will be assigned the EMAIL file type.  During email file preparation, we will automatically extract and ingest any file attachments.\n",
            "💡 Graphlit now automatically extracts embedded images in PDF files, ingests them as content objects, and links them as children of the parent PDF.\n",
            "💡 Graphlit now supports recursive Notion feeds.  When the isRecursive flag is true in the Notion feed properties, we will crawl child pages and databases, and recursively ingest them in addition to the specified pages and databases.\n",
            "Added support for assigning collections to content ingested with the ingestPage, ingestFile or ingestText mutations.  This saves a step where the content will automatically be added to the collection(s) without requiring another mutation call.\n",
            "Added support for the CODE file type for a wide variety of source code formats, i.e. Python .py, Javascript .js.  Code files use optimized text splitting for enhanced search and retrieval.\n",
            "Added support for customGuidance in Specification object, which can be used for injecting a guidance prompt during the RAG process.  For example, you can instruct the LLM to return a default response string if no content sources are found via semantic search.\n",
            "Added tenants field to Project object, which returns a list of all tenant IDs which have been used to create an entity in Graphlit.\n",
            "Added email metadata, separate from document metadata.  Now emails will contain indexed metadata such as to, from, or subject.\n",
            "⚡ The contents field for content objects has been replaced with children and parent fields.  For example, when a ZIP file is unpacked, the unpacked files will be added as children of the ZIP file, and the ZIP file will be the parent of each of the unpacked files.\n",
            "⚡ Removed enableImageAnalysis field from image preparation properties in workflow object.  Now is enabled by default.\n",
            "⚡ Moved disableSmartCapture field to preparation workflow stage from page preparation properties.  This is used to disable the use of headless Chrome browser to capture HTML from web pages.  It is enabled by default, and if disabled, Graphlit will simply download the HTML from the web page rather than rendering on headless Chrome browser.\n",
            "Bugs Fixed\n",
            "GPLA-2099: Failed to ingest ArXiV PDF.  Fixed PDF parsing error.\n",
            "GPLA-2174: LLM response is incorrect with conversation history, but no content sources.\n",
            "GPLA-2199: ZIP package left in Indexed state after content workflow.\n",
            "PreviousMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "NextFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "Last updated9 months ago\n",
            "- Completion [309 tokens (includes JSON guardrails tokens)], throughput: 80.350 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OneDrive and Google Drive feeds, allowing ingestion of files from shared drives.\n",
            "  - Support for email backup files (EML, MSG) with automatic extraction of attachments.\n",
            "  - Automatic extraction of embedded images from PDF files, linking them as children of the parent PDF.\n",
            "  - Support for recursive Notion feeds with the isRecursive flag for crawling child pages and databases.\n",
            "  - Collections can now be assigned to ingested content automatically.\n",
            "  - Introduction of CODE file type for various source code formats with optimized text splitting.\n",
            "  - Custom guidance support in Specification object for RAG process.\n",
            "  - Added tenants field to Project object for listing tenant IDs.\n",
            "  - Indexed email metadata (to, from, subject) separate from document metadata.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Contents field replaced with children and parent fields for content objects.\n",
            "  - EnableImageAnalysis field removed; now enabled by default.\n",
            "  - DisableSmartCapture field moved to preparation workflow stage, enabled by default.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed PDF parsing error for ArXiV PDFs (GPLA-2099).\n",
            "  - Corrected LLM response issue with conversation history (GPLA-2174).\n",
            "  - Resolved issue of ZIP package remaining in Indexed state after content workflow (GPLA-2199).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: February 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced file ingestion capabilities, improved metadata handling, and streamlined content management processes.\n",
            "\n",
            "2025-01-01T20:18:00.492Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.715874, used credits [0.00469500]\n",
            "- CONTENT [1149d374-504c-443f-8fb3-ca0a475776dc]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [673 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/january-2024/january-22-support-for-google-and-microsoft-email-feeds-reingest-content-in-place-bug-fixes</name><title>January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes | Graphlit Changelog</title></metadata> 🎆\tJanuary 2024\n",
            "January 22: Support for Google and Microsoft email feeds, reingest content in-place, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Google and Microsoft email feeds.  Email feeds can be created to ingest past emails, or poll for new emails.  Emails create an EMAIL content type. Attachment files can optionally be extracted from emails, and will be linked to their parent email content. If assigning a workflow to the feed, the workflow will be applied both to the email content and the extracted attachment files.\n",
            "💡 Graphlit now supports reingesting content in-place.  The ingestText, ingestPage and ingestFile mutations now take an optional id parameter for an existing content object.  If this id is provided, the existing content will be updated from the provided text or URI source, and will restart the assigned workflow.\n",
            "Added restartAllContents mutation to restart workflow on all partially-ingested contents in project.\n",
            "Added text field to ConversationCitation type, which returns the relevant text from the content source with the citation.\n",
            "Bugs Fixed\n",
            "GPLA-1313: Not extracting links from HTML\n",
            "GPLA-2030: No text extracted from shapes in PPTX files\n",
            "PreviousFebruary 2: Support for Semantic Alerts, OpenAI 0125 models, performance enhancements, bug fixes\n",
            "NextJanuary 18: Support for content publishing, LLM tools, CLIP image embeddings, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [223 tokens (includes JSON guardrails tokens)], throughput: 82.110 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Google and Microsoft email feeds, allowing ingestion of past and new emails, creating an EMAIL content type. Attachments can be extracted and linked to parent emails.\n",
            "  - Support for reingesting content in-place with optional id parameter for existing content objects, updating them from provided text or URI source, and restarting assigned workflows.\n",
            "  - Added restartAllContents mutation to restart workflows on all partially-ingested contents in a project.\n",
            "  - Added text field to ConversationCitation type to return relevant text from the content source with the citation.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved content ingestion capabilities with new mutations and features.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with links not being extracted from HTML (GPLA-1313).\n",
            "  - Resolved problem of no text being extracted from shapes in PPTX files (GPLA-2030).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: January 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced content ingestion options and improved workflow management, increasing efficiency in handling email and existing content updates.\n",
            "\n",
            "2025-01-01T20:17:58.664Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.407618, used credits [0.00457800]\n",
            "- CONTENT [3109dca5-a705-4191-a868-ab2b2485d8ca]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [710 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-25-support-for-mistral-large-2-and-nemo-groq-llama-3.1-models-bug-fixes</name><title>July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the Mistral Large 2 and Mistral Nemo models. The existing MISTRAL_LARGE model enum now will use Mistral Large 2.\n",
            "💡 Graphlit now supports the Llama 3.1 8b, 70b and 405b models on Groq.  (Note, these are rate-limited according to Groq's platform constraints.)\n",
            "Added support for revision strategy on data extraction specifications.  Now you can prompt the LLM to revise its previous data extraction response, similar to the existing completion revision strategy.\n",
            "Added version property for AzureDocumentPreparationProperties type for assigning the API version used by Azure AI Document Intelligence.   By default, Graphlit will continue to use the v4.0 (Preview) API version, but you can override this to assign version to V2023_10_31 to use the v3.1 (GA) API version instead.  For some documents, the General Availability (GA) version of the API can provide better results.\n",
            "Bugs Fixed\n",
            "GPLA-2988: Not extracting hyperlinks from Office documents.\n",
            "PreviousJuly 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "NextJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [204 tokens (includes JSON guardrails tokens)], throughput: 59.866 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Mistral Large 2 and Mistral Nemo models; existing MISTRAL_LARGE model enum updated.\n",
            "  - Support for Llama 3.1 models (8b, 70b, 405b) on Groq, with rate limits per Groq's platform.\n",
            "  - Added revision strategy for data extraction specifications, allowing LLM to revise previous responses.\n",
            "  - Version property added for AzureDocumentPreparationProperties to assign API version for Azure AI Document Intelligence; default is v4.0 (Preview), can override to V2023_10_31 for v3.1 (GA) API version.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with hyperlinks not being extracted from Office documents (GPLA-2988).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 25, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve model support and data extraction capabilities, offering developers more flexibility and better results with Azure AI.\n",
            "\n",
            "2025-01-01T20:17:57.646Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:05.316186, used credits [0.00687000]\n",
            "- CONTENT [cb4a2f54-a3ae-4b9b-a66e-27232fea2d14]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [894 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/june-2024/june-9-support-for-deepseek-models-json-ld-webpage-parsing-performance-improvements-and-bug-fixes</name><title>June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes | Graphlit Changelog</title></metadata> 🎓\tJune 2024\n",
            "June 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports Deepseek LLMs for prompt completion.  We offer the deepseek-chat and deepseek-coder models.\n",
            "💡 Graphlit now supports parsing embedded JSON-LD from web pages.  If a web page contains 'script' tags with JSON-LD, we will automatically parse and inject into the knowledge graph.\n",
            "⚡ We have changed the default model for entity extraction and image completions to be OpenAI GPT-4o.  This provides faster performance and better quality output.\n",
            "⚡ We have changed the behavior of knowledge graph generation, from a prompted conversation, to be opt-in.  In order to receive the graph's nodes and edges with the response, you will now need to set generateGraph to True in the specification's graphStrategy object.  This provides improved performance when the graph is not needed for visualization.\n",
            "Added thing property for observable entities, which returns the JSON-LD metadata associated with the entity.\n",
            "Added regex-based filtering for URI paths during feed ingestion, link crawling, and workflow filtering.  You can assign regex patterns in allowedPaths and excludedPaths.\n",
            "Added observableLimit to configure the limit of how many observed entities will be added to the GraphRAG context, defaults to 1000.\n",
            "Added prompt to suggestConversation mutation, which allows customization of the followup question generation.\n",
            "Updated suggestConversation to incorporate the past conversation message history, in addition to the filtered set of content sources.\n",
            "🔥  We have improved performance in knowledge graph retrieval and generation, via better parallelization and batching.\n",
            "Bugs Fixed\n",
            "GPLA-2748: Optimize the retrieval performance of observed entities during GraphRAG\n",
            "GPLA-2732: Invalid user-provided URI causing parsing exception\n",
            "GPLA-2666: Shouldn't require tenant ID for Microsoft email or Teams\n",
            "GPLA-2772: Not returning labels or categories from graph in API\n",
            "GPLA-2762: Failed to extract spreadsheet images\n",
            "GPLA-2687: Email to/from not getting added as observations on emails\n",
            "GPLA-2738: API is returning 'audio' metadata from podcast HTML document\n",
            "PreviousJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "NextMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [349 tokens (includes JSON guardrails tokens)], throughput: 65.649 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Deepseek LLMs for prompt completion (deepseek-chat and deepseek-coder models).\n",
            "  - Parsing of embedded JSON-LD from web pages, automatically injecting into the knowledge graph.\n",
            "  - Default model for entity extraction and image completions changed to OpenAI GPT-4o for improved performance and quality.\n",
            "  - Knowledge graph generation behavior changed to opt-in; requires setting generateGraph to True for response inclusion.\n",
            "  - Added thing property for observable entities to return associated JSON-LD metadata.\n",
            "  - Regex-based filtering for URI paths during feed ingestion and workflow filtering.\n",
            "  - Added observableLimit to configure the number of observed entities in GraphRAG context (default 1000).\n",
            "  - Customization of follow-up question generation via prompt in suggestConversation mutation.\n",
            "  - Updated suggestConversation to include past conversation message history.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved performance in knowledge graph retrieval and generation through better parallelization and batching.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Optimized retrieval performance of observed entities during GraphRAG.\n",
            "  - Fixed parsing exception caused by invalid user-provided URI.\n",
            "  - Removed requirement for tenant ID for Microsoft email or Teams.\n",
            "  - Resolved issue of not returning labels or categories from graph in API.\n",
            "  - Fixed failure to extract spreadsheet images.\n",
            "  - Addressed issue of email to/from not being added as observations.\n",
            "  - Corrected API returning 'audio' metadata from podcast HTML documents.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: June 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for entity extraction, improved performance, and better integration of JSON-LD data, facilitating more efficient knowledge graph management.\n",
            "\n",
            "2025-01-01T20:17:57.644Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.775744, used credits [0.00612000]\n",
            "- CONTENT [a8a2bbe9-1c58-42a7-b31c-dd4b27b74956]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1016 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-19-support-for-openai-gpt-4o-mini-byo-key-for-azure-ai-similarity-by-summary-bug-fixes</name><title>July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the OpenAI GPT-4o Mini model, with 16k output tokens.\n",
            "💡 Graphlit now supports 'bring-your-own-key' for Azure AI Document Intelligence models.  We have added a custom endpoint and key property, which can be assigned to use your own Azure AI resource.\n",
            "Updated to use Jina reranker v2 (jina-reranker-v2-base-multilingual) by default.\n",
            "Updated to assign the summary, bullets, etc properties when calling summarizeContents mutation.  Now when summarizing contents, we will store the resulting summary in the content itself, in addition to returning the summarized results.\n",
            "Added relevance property to all entity types, which will be assigned when searching for these entities.  Entity results will be sorted (descending) by this search relevance score.\n",
            "Added the ability to manually update summary, bullets, etc. properties when calling the updateContent mutation.\n",
            "Added offset property to AtlassianJiraFeedProperties, so the timezone offset can be properly assigned for paging of the Jira feed.  (Defaults to zero offset, i.e. UTC.)  Jira does not store dates in UTC format, and the timestamps are based on the server timezone of the hosted Jira instance.  By assigning the timezone offset with the Jira feed, we can reliably page the updated date timestamps from the Jira API.\n",
            "⚡ We have changed the content similarity search behavior to find similar content by summary, rather than text of the document, when a summary has been previously generated.  For long documents, this will provide a more accurate similarity, rather than comparing on the first few pages of text in a document.\n",
            "⚡ We have changed the behavior of assigning offset in the entity filter objects for paging through entities.  If using vector or hybrid search, this offset will be ignored (i.e. zero offset).  Paging will not be supported through vector or hybrid search results. For keyword search, the offset will continue to be used, along with the limit property, to provide paging through the search results.  We have made this change because we have found that index-based paging is not reliable with our vector/hybrid search approach.  We are investigating ways to support this reliably with vector/hybrid search in the future.\n",
            "Bugs Fixed\n",
            "GPLA-2915: Add retry on OpenAI API HTTP 524 error (gateway timeout).\n",
            "GPLA-2908: Not paging through Jira feed correctly.\n",
            "GPLA-2917: Search by similar content is not giving expected results on long documents.\n",
            "GPLA-2244: Keyword search not finding text in latter part of long PDF.\n",
            "PreviousJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "NextJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [256 tokens (includes JSON guardrails tokens)], throughput: 92.228 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for OpenAI GPT-4o Mini model with 16k output tokens.\n",
            "  - 'Bring-your-own-key' support for Azure AI Document Intelligence models with custom endpoint and key property.\n",
            "  - Updated to Jina reranker v2 by default.\n",
            "  - Enhanced summarization capabilities with properties stored in content.\n",
            "  - Added relevance property for entity types, sorting results by search relevance score.\n",
            "  - Ability to manually update summary and bullet properties in updateContent mutation.\n",
            "  - Offset property added to AtlassianJiraFeedProperties for proper timezone assignment in Jira feed.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Content similarity search now finds similar content by summary for better accuracy.\n",
            "  - Changes in entity filter objects for paging; zero offset for vector/hybrid search.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Added retry on OpenAI API HTTP 524 error.\n",
            "  - Fixed paging issues in Jira feed.\n",
            "  - Improved search results for similar content in long documents.\n",
            "  - Resolved keyword search issues in long PDFs.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 19, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved model support, enhanced search capabilities, and better handling of content summaries and entity relevance.\n",
            "\n",
            "2025-01-01T20:17:57.386Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.852439, used credits [0.00427200]\n",
            "- CONTENT [47c53407-b4d6-48e4-b91d-b9677a1028de]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [640 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-28-support-for-indexing-workflow-stage-azure-ai-language-detection-bug-fixes</name><title>July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 28: Support for indexing workflow stage, Azure AI language detection, bug fixes\n",
            "New Features\n",
            "Added indexing workflow stage. This provides for configuration of indexing services, which may infer metadata from the content.\n",
            "Added AZURE_AI_LANGUAGE content indexing service, which supports inferring the language of extracted text or transcript.\n",
            "Added support for language content metadata.  This returns a list of languages in ISO 639-1 format, which may have been inferred from the extracted text or transcript.\n",
            "Added support for MODEL_IMAGE extraction service.  This provides integration with vision models beyond those provided by OpenAI.  You can assign a custom specification and bring-your-own API key for image extraction models.\n",
            "⚡ We have deprecated the OPENAI_IMAGE service type, and developers should now use the LLM image service instead.\n",
            "⚡ We have removed the language field from AudioMetadata type, which has been replaced by the new LanguageMetadata type.\n",
            "Bugs Fixed\n",
            "GPLA-2987: Extracting text with Azure Doc Intelligence does not extract hyperlinks\n",
            "PreviousAugust 8: Support for LLM-based document extraction, .NET SDK, bug fixes\n",
            "NextJuly 25: Support for Mistral Large 2 & Nemo, Groq Llama 3.1 models, bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [196 tokens (includes JSON guardrails tokens)], throughput: 68.713 tokens/sec:\n",
            "- New Features:\n",
            "  - Added indexing workflow stage for configuring indexing services to infer metadata from content.\n",
            "  - Introduced AZURE_AI_LANGUAGE content indexing service for inferring language from extracted text or transcripts.\n",
            "  - Supported language content metadata returning a list of languages in ISO 639-1 format.\n",
            "  - Added MODEL_IMAGE extraction service for integration with various vision models, allowing custom specifications and API keys.\n",
            "  - Deprecated OPENAI_IMAGE service type; developers should use LLM image service instead.\n",
            "  - Removed language field from AudioMetadata type, replaced with new LanguageMetadata type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2987 where Azure Doc Intelligence did not extract hyperlinks.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 28, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for content indexing, language detection, and image extraction, improving overall functionality and flexibility.\n",
            "\n",
            "2025-01-01T20:17:55.175Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.624064, used credits [0.00525000]\n",
            "- CONTENT [79510fa9-9ddb-4b7e-8436-a2b0a867ac64]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [730 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/july-2024/july-4-support-for-webhook-alerts-keywords-summarization-deepseek-128k-context-window-bug-fixes</name><title>July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes | Graphlit Changelog</title></metadata> ☀️\tJuly 2024\n",
            "July 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports webhook Alerts.  In addition to Slack notifications, you can now receive an HTTP POST webhook with the results of the published text (or text and audio URI) from a prompted alert.\n",
            "Updated the Deepseek chat and coder models to support a 128k token context window.\n",
            "Added customSummary property to Content object, which returns the custom summary generated via preparation workflow.\n",
            "Added keywords summarization type, which is now stored in keywords property in Content object.\n",
            "Added slackChannels query, which returns the list of Slack channels from the workspace authenticated by the Slack bot token.\n",
            "⚡ We have changed the response from the credits query to return a single ProjectCredits object, rather than the list of correlated objects previously returned.  The credits response now covers all credit usage over the time period specified.\n",
            "Bugs Fixed\n",
            "GPLA-2874: Processing entities is taking longer than 30min for 300+ page PDF\n",
            "GPLA-2875: Messages in queue expiring too early\n",
            "GPLA-2881: Feed read count increasing, after hitting read limit\n",
            "GPLA-2884: Need to handle Anthropic 'overloaded' API response\n",
            "GPLA-2906: JIRA issue identifier not assigned to issue metadata\n",
            "PreviousJuly 19: Support for OpenAI GPT-4o Mini, BYO-key for Azure AI, similarity by summary, bug fixes\n",
            "NextJune 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "Last updated5 months ago\n",
            "- Completion [255 tokens (includes JSON guardrails tokens)], throughput: 97.177 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for webhook Alerts, allowing HTTP POST notifications with published text results.\n",
            "  - Updated Deepseek models to support a 128k token context window.\n",
            "  - Added customSummary property to Content object for custom summaries.\n",
            "  - Introduced keywords summarization type, stored in keywords property of Content object.\n",
            "  - Added slackChannels query to list Slack channels from the authenticated workspace.\n",
            "  - Changed credits query response to return a single ProjectCredits object covering all credit usage.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved response structure for credits query.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - GPLA-2874: Reduced processing time for large PDFs.\n",
            "  - GPLA-2875: Fixed issue with messages in queue expiring too early.\n",
            "  - GPLA-2881: Resolved feed read count issue after hitting read limit.\n",
            "  - GPLA-2884: Handled Anthropic 'overloaded' API response.\n",
            "  - GPLA-2906: Assigned JIRA issue identifier to issue metadata.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: July 4, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve notification capabilities and processing efficiency, offering developers more robust tools for managing alerts and content summarization.\n",
            "\n",
            "2025-01-01T20:17:54.708Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.233493, used credits [0.00497100]\n",
            "- CONTENT [8ca8bf83-6c04-4d64-9953-5699918fbb06]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [737 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/june-2024/june-21-support-for-the-claude-3.5-sonnet-model-knowledge-graph-semantic-search-and-bug-fixes</name><title>June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes | Graphlit Changelog</title></metadata> 🎓\tJune 2024\n",
            "June 21: Support for the Claude 3.5 Sonnet model, knowledge graph semantic search, and bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the Anthropic Claude 3.5 Sonnet model, which can be assigned with the CLAUDE_3_5_SONNET model enum.\n",
            "💡 Graphlit now supports semantic search of observable entities in the knowledge graph, such as Person, Organization and Place.  These entity types will now have vector embeddings created from their enriched metadata, and support searching by similar text, and searching by similar entities.\n",
            "⚡ We have changed the Google Drive and Google Email feed properties to require the Google OAuth client ID and client secret, along with the existing refresh token, for proper authentication against Google APIs.\n",
            "⚡ We have added a credits quota on the Free Tier.  Once 1000 credits have been used on the Free Tier, no more content can be ingested, and an upgrade to a paid tier is required.  Customers will receive an email when the credits, storage or contents quota has been reached.\n",
            "Bugs Fixed\n",
            "GPLA-2837: Failed to ingest LinkedIn page as Web feed\n",
            "GPLA-2831: Zero-byte file was left in Indexed state\n",
            "GPLA-2834: Not reading any files from Azure blob feed with space in prefix\n",
            "GPLA-2828: Better handling for files with unknown (or missing) file extensions\n",
            "PreviousJuly 4: Support for webhook Alerts, keywords summarization, Deepseek 128k context window, bug fixes\n",
            "NextJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [230 tokens (includes JSON guardrails tokens)], throughput: 71.130 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for the Anthropic Claude 3.5 Sonnet model (model enum: CLAUDE_3_5_SONNET).\n",
            "  - Semantic search for observable entities in the knowledge graph (Person, Organization, Place) with vector embeddings for enriched metadata.\n",
            "  - Google Drive and Google Email feed properties now require Google OAuth client ID, client secret, and refresh token for authentication.\n",
            "  - Introduction of a credits quota on the Free Tier; ingestion stops after 1000 credits, requiring an upgrade to a paid tier.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved authentication process for Google APIs.\n",
            "  - Notification system for Free Tier users when credits, storage, or content quotas are reached.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Resolved issues with ingesting LinkedIn pages, handling zero-byte files, reading files from Azure blob feeds with spaces, and better handling of files with unknown or missing extensions.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: June 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support, improved search capabilities, and better management of resource quotas.\n",
            "\n",
            "2025-01-01T20:17:54.409Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:06.248312, used credits [0.00458400]\n",
            "- CONTENT [8f1c79b1-beaf-481d-a7a7-078105781fbb]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [700 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/march-2024/march-23-support-for-linear-github-issues-and-jira-issue-feeds-ingest-files-via-web-feed-sitemap</name><title>March 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap | Graphlit Changelog</title></metadata> 🍀\tMarch 2024\n",
            "March 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "New Features\n",
            "💡 Graphlit now supports Linear, GitHub Issues and Atlassian Jira feeds.  Graphlit will ingest issues (aka tasks, stories) from these issue-tracking services as individual content items, which will be made searchable and conversational.\n",
            "💡 Added support for ISSUEcontent type, which includes metadata such as title, authors, commenters, status, type, project and team.\n",
            "💡 Added support for default feed read limit.  Now, if you don't specify the readLimit property on feeds, it will default to reading 100 content items.  You can override this default by assigning a custom read limit, which has no upper bounds.  However, one-shot feeds much complete within 15 minutes, or they will be stopped automatically.\n",
            "Added support for ingesting files referenced in a Web sitemap.  Previously any files (i.e. PDF, MP3) referenced in a sitemap.xml would be ignored.  Now you can optionally enable includeFiles in the WebFeedPropertiesInput object to have Graphlit ingest non-HTML pages as part of the Web feed.\n",
            "Bugs Fixed\n",
            "GPLA-2374: Failed to ingest MP4 with large XMP metadata.\n",
            "PreviousApril 7: Support for Discord feeds, Cohere reranking, section-aware chunking and retrieval\n",
            "NextMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "Last updated8 months ago\n",
            "- Completion [207 tokens (includes JSON guardrails tokens)], throughput: 33.129 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Linear, GitHub Issues, and Atlassian Jira feeds for ingesting issues as searchable content items.\n",
            "  - Introduction of ISSUE content type with metadata including title, authors, commenters, status, type, project, and team.\n",
            "  - Default feed read limit set to 100 content items, customizable with no upper bounds; one-shot feeds must complete within 15 minutes.\n",
            "  - Ability to ingest files referenced in a Web sitemap, including non-HTML pages like PDFs and MP3s.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved handling of files in Web sitemaps by enabling ingestion of non-HTML content.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2374 related to failing to ingest MP4 files with large XMP metadata.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: March 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances integration with popular issue-tracking services, improves content ingestion capabilities, and provides flexibility in feed management for developers.\n",
            "\n",
            "2025-01-01T20:17:52.507Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.067114, used credits [0.00621600]\n",
            "- CONTENT [b57cc140-8c7d-4608-8a89-7dfa783797ec]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [836 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/march-2024/march-10-support-for-claude-3-mistral-and-groq-models-usage-credits-telemetry-bug-fixes</name><title>March 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes | Graphlit Changelog</title></metadata> 🍀\tMarch 2024\n",
            "March 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports a Command-Line Interface (CLI) for directly accessing the Graphlit Data API without writing code.  See the documentation here.\n",
            "💡 Graphlit now supports the Groq Platform, and models such as Mixtral 8x7b.\n",
            "💡 Graphlit now supports Claude 3 Opus and Sonnet models.\n",
            "💡 Graphlit now supports Mistral La Plateforme, and models such as Mistral Small, Medium, and Large and Mixtral 8x7b.\n",
            "💡 Graphlit now supports the latest v4 of Azure Document Intelligence, including their new models such as Credit Card, Marriage Certificate, and Mortgage documents.\n",
            "Added support for detailed usage and credits telemetry via API, with the usage, credits, lookupUsage and lookupCredits queries.\n",
            "Added support for correlated telemetry, where an optional correlationId can be provided with GraphQL queries and mutations, so credits and usage can be tracked across requests.\n",
            "Added support for project webhook, which will be called when credits have been consumed by the project.\n",
            "Added support for image extraction during DOCX, XLSX, and PPTX document preparation.\n",
            "Added text and markdown properties to Content object, which provide formatted output of extracted text from any content.\n",
            "Added more accurate extraction of tables into mezzanine JSON format, across all content types.\n",
            "Added throughput property to Conversation messages, which returns the tokens/second throughput of LLM.\n",
            "⚡ Deprecated mezzanineUri property in Content object, which has been replaced by textUri and audioUri.\n",
            "Bugs Fixed\n",
            "GPLA-2281: Not extracting table from PPTX file.\n",
            "GPLA-2282: Not extracting Markdown tables.\n",
            "GPLA-2247: Not extracting relative HTML links properly.\n",
            "GPLA-2241: Failed to post Alert to Slack with Markdown format.\n",
            "PreviousMarch 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "NextFebruary 21: Support for OneDrive and Google Drive feeds, extract images from PDFs, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [309 tokens (includes JSON guardrails tokens)], throughput: 100.746 tokens/sec:\n",
            "- New Features:\n",
            "  - Command-Line Interface (CLI) for direct access to the Graphlit Data API.\n",
            "  - Support for Groq Platform and models like Mixtral 8x7b.\n",
            "  - Support for Claude 3 Opus and Sonnet models.\n",
            "  - Support for Mistral La Plateforme and models such as Mistral Small, Medium, and Large.\n",
            "  - Support for Azure Document Intelligence v4, including new models for Credit Card, Marriage Certificate, and Mortgage documents.\n",
            "  - Detailed usage and credits telemetry via API.\n",
            "  - Correlated telemetry with optional correlationId for tracking credits and usage.\n",
            "  - Project webhook for notifications when credits are consumed.\n",
            "  - Image extraction during DOCX, XLSX, and PPTX document preparation.\n",
            "  - Text and markdown properties added to Content object for formatted output.\n",
            "  - More accurate extraction of tables into mezzanine JSON format.\n",
            "  - Throughput property added to Conversation messages for tokens/second throughput.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Deprecated mezzanineUri property in Content object, replaced by textUri and audioUri.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with table extraction from PPTX and Markdown files.\n",
            "  - Improved extraction of relative HTML links.\n",
            "  - Resolved failure to post alerts to Slack with Markdown format.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: March 10, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for data extraction, improved API access, and better tracking of usage and credits.\n",
            "\n",
            "2025-01-01T20:17:52.305Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.322618, used credits [0.00306300]\n",
            "- CONTENT [7fb352e7-6eda-419f-848d-d23527d330cc]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [549 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/march-2024/march-13-support-for-claude-3-haiku-model-direct-ingestion-of-base64-encoded-files</name><title>March 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files | Graphlit Changelog</title></metadata> 🍀\tMarch 2024\n",
            "March 13: Support for Claude 3 Haiku model, direct ingestion of Base64 encoded files\n",
            "New Features\n",
            "💡 Graphlit now supports the Claude 3 Haiku model.\n",
            "Added support for direct ingestion of Base64 encoded files with the ingestEncodedFile mutation.  You can pass a Base64 encoded string and MIME type of the file, and it will be ingested into the Graphlit Platform.\n",
            "Added modelService and model properties to ConversationMessage type, which return the model service and model which was used for the LLM completion.\n",
            "PreviousMarch 23: Support for Linear, GitHub Issues and Jira issue feeds, ingest files via Web feed sitemap\n",
            "NextMarch 10: Support for Claude 3, Mistral and Groq models, usage/credits telemetry, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [118 tokens (includes JSON guardrails tokens)], throughput: 89.217 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Claude 3 Haiku model.\n",
            "  - Direct ingestion of Base64 encoded files via the ingestEncodedFile mutation, allowing the input of a Base64 string and MIME type.\n",
            "  - Added modelService and model properties to ConversationMessage type for LLM completion details.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - None specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - None specified.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: March 13, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support and improved file ingestion capabilities.\n",
            "\n",
            "2025-01-01T20:17:51.427Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.078551, used credits [0.00716100]\n",
            "- CONTENT [5d06c7c1-735d-41a3-b95e-8bce0b652102]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [947 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/may-2024/may-5-support-for-jina-and-pongo-rerankers-microsoft-teams-feed-new-youtube-downloader-bug-fixes</name><title>May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes | Graphlit Changelog</title></metadata> 💐\tMay 2024\n",
            "May 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the Jina reranker and Pongo semantic filtering (reranking), in the Specification object.  Now you can choose between COHERE, PONGO and JINA for your reranking serviceType.\n",
            "💡 Graphlit now supports Microsoft Teams feeds for reading messages from Teams channels.\n",
            "Given changes in YouTube video player HTML, we have rewritten the YouTube downloader to support the new page format.\n",
            "Added better handling of HTTP errors when validating URIs.  Previously some websites were returning HTTP 403 (Forbidden) errors when validating their URI, or downloading content.  Now Graphlit is able to scrape these sites, which previously returned errors.\n",
            "Added support for updating content metadata in updateContent mutation.  Now the video, audio, document, etc. metadata can be updated after the content workflow has finished.\n",
            "Added query_contents_graph (and queryContentsGraph) functions to SDKs, which can be used to return nodes and edges from knowledge graph for visualization.\n",
            "⚡ Citation indices have been changed to be one-based from zero-based.  For example, you will now see \"This is a citation. [1]\" as the first citation in the list.\n",
            "⚡ Added isSynchronous flag to deleteAll and multiple delete mutations.  By default, bulk delete operations are now asynchronous (and completed after the mutation returns), unless the isSynchronous flag is set to true.\n",
            "⚡ Added missing count mutations, such as countAlerts, countFeeds, etc.\n",
            "⚡ Renamed query_content_facets to query_contents_facets in Python SDK\n",
            "⚡ Renamed queryContentFacets to queryContentsFacets in Node.js SDK\n",
            "Bugs Fixed\n",
            "GPLA-2544: Page relevance not filled-in in all situations\n",
            "GPLA-2546: Not extracting links from PDF with Azure AI Doc Intelligence\n",
            "GPLA-2557: Sporadically returning HTTP 500 from GraphQL API\n",
            "GPLA-2573: Failed to re-ingest content which was deleted immediately after initial ingestion\n",
            "GPLA-2575: Not validating for empty (non-null) parameters in mutations\n",
            "GPLA-2578: Need to handle invalid JSON from LLMs; improper escaping or formatting\n",
            "GPLA-2585: Failed to ingest encoded file with colon (:) in name\n",
            "PreviousMay 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "NextApril 23: Support for Python and TypeScript SDKs, latest OpenAI, Cohere & Groq models, bug fixes\n",
            "Last updated8 months ago\n",
            "- Completion [360 tokens (includes JSON guardrails tokens)], throughput: 88.267 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Jina and Pongo rerankers in the Specification object; options for COHERE, PONGO, and JINA as reranking service types.\n",
            "  - Microsoft Teams feeds support for reading messages from Teams channels.\n",
            "  - Rewritten YouTube downloader to accommodate changes in YouTube video player HTML.\n",
            "  - Improved handling of HTTP errors for URI validation and content downloading.\n",
            "  - Support for updating content metadata in updateContent mutation post-workflow.\n",
            "  - Added query_contents_graph functions for knowledge graph visualization.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Citation indices changed from zero-based to one-based.\n",
            "  - Added isSynchronous flag for deleteAll and multiple delete mutations; bulk deletes are asynchronous by default.\n",
            "  - Added missing count mutations (e.g., countAlerts, countFeeds).\n",
            "  - Renamed query_content_facets to query_contents_facets in Python SDK.\n",
            "  - Renamed queryContentFacets to queryContentsFacets in Node.js SDK.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed page relevance not being filled in all situations.\n",
            "  - Resolved issues with link extraction from PDFs using Azure AI Doc Intelligence.\n",
            "  - Addressed sporadic HTTP 500 errors from GraphQL API.\n",
            "  - Fixed failure to re-ingest content deleted immediately after initial ingestion.\n",
            "  - Ensured validation for non-null parameters in mutations.\n",
            "  - Handled invalid JSON from LLMs with proper escaping or formatting.\n",
            "  - Fixed ingestion failure for encoded files with colons in names.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: May 5, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced flexibility with reranking options, improved integration with Microsoft Teams, and better error handling, leading to a more robust and user-friendly platform.\n",
            "\n",
            "2025-01-01T20:17:50.870Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.021227, used credits [0.00656400]\n",
            "- CONTENT [e0f9a487-28bc-4d08-8b71-0dded28e8378]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [1012 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/may-2024/may-15-support-for-graphrag-openai-gpt-4o-model-performance-improvements-and-bug-fixes</name><title>May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes | Graphlit Changelog</title></metadata> 💐\tMay 2024\n",
            "May 15: Support for GraphRAG, OpenAI GPT-4o model, performance improvements and bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports GraphRAG, where the extracted entities in the knowledge graph can be added as additional context to your RAG con,versation.  Also, with GraphRAG, entities can be extracted from the user prompt, and used as additional content filters - or can be used to query related content sources, which are combined with the vector search results.  This can be configured by specifying your graphStrategy in the Specification object.\n",
            "💡 Graphlit now supports LLM revisions within RAG conversations, where the LLM can be prompted to revise its initial completion response. From our testing, this has been shown to provide 35% more output tokens with higher quality responses.  This can be configured by specifying your revisionStrategy, and you can use our built-in revision prompt, or provide a custom one, and specify how many revisions you want the LLM to make.\n",
            "💡 Graphlit now supports the new OpenAI GPT-4o model for RAG conversations.\n",
            "⚡ We have changed the default model for Conversations to be OpenAI GPT-4o, from Azure OpenAI GPT-3.5 16k.  This provides faster performance and better quality output.\n",
            "Added graph to promptConversation response, so you can visualize or leverage the nodes and edges of the knowledge graph, resulting from the content retrieval.  For example, if a Person and Organization were observed in the cited content sources used by the RAG pipeline, you will get back those entities and their relationship (such as Person 'works-for' Organization).\n",
            "Expanded the enriched data from WIkipedia to include the long description of an entity.\n",
            "Added getSharePointLibraries, getSharePointFolders, and getOneDriveFolders queries to the API, which can be used to enumerate the storage services.  This makes locating the SharePoint libraryId easier, for example.\n",
            "Added getTeams and getTeamsChannels queries to the API for enumerating Microsoft Teams workspaces.\n",
            "Added extractedCount to the entity extraction connector to limit the number of extracted entities, per entity type.  I.e. if extracted count is 10, it will extract at most ten each of Persons, Organizations, etc.\n",
            "🔥  We have improved performance in several areas: creation of observations after entity extraction, access to cloud storage, rendering the RAG context.\n",
            "🔥  We have optimized the LLM entity extraction process to identify more properties, as well as entity-to-entity relationships.\n",
            "Bugs Fixed\n",
            "GPLA-2652: Not extracting text from HTML in RSS post\n",
            "GPLA-2627: Limit filter only returning half the results\n",
            "GPLA-2613: Not properly extracting structured text from JSON/XML formats\n",
            "PreviousJune 9: Support for Deepseek models, JSON-LD webpage parsing, performance improvements and bug fixes\n",
            "NextMay 5: Support for Jina and Pongo rerankers, Microsoft Teams feed, new YouTube downloader, bug fixes\n",
            "Last updated6 months ago\n",
            "- Completion [294 tokens (includes JSON guardrails tokens)], throughput: 97.311 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for GraphRAG, allowing extracted entities to be added as context in RAG conversations and used as content filters.\n",
            "  - LLM revisions within RAG conversations, improving output by 35% with higher quality responses.\n",
            "  - Support for OpenAI GPT-4o model in RAG conversations.\n",
            "  - Default model changed to OpenAI GPT-4o for faster performance and better quality.\n",
            "  - Added graph visualization in promptConversation responses to show relationships in the knowledge graph.\n",
            "  - Expanded enriched data from Wikipedia to include long descriptions of entities.\n",
            "  - New API queries: getSharePointLibraries, getSharePointFolders, getOneDriveFolders, getTeams, and getTeamsChannels for easier storage service enumeration.\n",
            "  - Added extractedCount to limit the number of extracted entities per type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved performance in entity extraction, cloud storage access, and RAG context rendering.\n",
            "  - Optimized LLM entity extraction to identify more properties and relationships.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issues with text extraction from HTML in RSS posts, limit filter returning incomplete results, and structured text extraction from JSON/XML formats.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: May 15, 2024.\n",
            "  - Version updates include support for new models and enhanced API functionalities.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved performance, enhanced data extraction capabilities, and better integration with Microsoft services.\n",
            "\n",
            "2025-01-01T20:17:49.376Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.925142, used credits [0.00539700]\n",
            "- CONTENT [03103626-0933-40e5-9eb3-31d1d9694e1a]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [855 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-10-support-for-web-search-multi-turn-content-summarization-deepgram-language-detection</name><title>November 10: Support for web search, multi-turn content summarization, Deepgram language detection | Graphlit Changelog</title></metadata> 🦃\tNovember 2024\n",
            "November 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "New Features\n",
            "💡 Graphlit now supports web search with the searchWeb mutation.  You can select the search service, either Tavily or Exa.AI, and provide the search query and number of search results to be returned.  This is different than the web search feed, in that searchWeb returns the relevant text from the web page and the web page URL from each search hit, but does not ingest each of the web pages. This new mutation is optimized to be used from within an LLM tool.\n",
            "💡 Graphlit now supports multi-turn summarization of content with the reviseContent mutation.  You can provide an LLM prompt and a content reference, along with an optional specification.  This can be used for summarizing any content (documents, web pages, audio transcripts, etc.), and having a multi-turn conversation with the LLM to revise the output from the LLM.  Internally, this creates a conversation locked to a single piece of content.  This works especially well with the OpenAI o1-preview and o1-mini models, because they provide a longer LLM output from each turn.\n",
            "Graphlit now supports the configuration of the Deepgram transcription language, and whether detectLanguage is enabled in DeepgramAudioPreparationPropertiesInput.  Language detection is now enabled by default, and can be disabled by setting detectLanguage to false.\n",
            "⚡ We have added a requireTool option to promptConversation mutation, so you can control whether the LLM must call one of the provided tool, or if tool calling is optional.\n",
            "⚡ For accounts created after Nov 8, 2024, we have lowered the credits quota on the Free tier from 1000 credits to 100 credits, and now offer unlimited feeds on the Hobby Tier.\n",
            "⚡ The Graphlit Data API will now return HTTP 402 (Payment Required) when you have exceeded the credits quota on the free tier.  You must upgrade to the Hobby Tier (or higher) to continue using the API, once the credits quota has been reached.\n",
            "PreviousNovember 16: Support for image description, multi-turn text summarization\n",
            "NextNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "Last updated1 month ago\n",
            "- Completion [236 tokens (includes JSON guardrails tokens)], throughput: 80.680 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for web search via the searchWeb mutation, allowing selection of search service (Tavily or Exa.AI) and retrieval of relevant text and URLs without ingesting web pages.\n",
            "  - Multi-turn content summarization with the reviseContent mutation, enabling conversation with LLM for summarizing various content types.\n",
            "  - Configuration of Deepgram transcription language and default language detection enabled in DeepgramAudioPreparationPropertiesInput.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Added requireTool option to promptConversation mutation for controlling tool calling.\n",
            "  - Reduced credits quota on Free tier from 1000 to 100 for accounts created after Nov 8, 2024, with unlimited feeds on the Hobby Tier.\n",
            "  - Graphlit Data API now returns HTTP 402 (Payment Required) when exceeding free tier credits quota.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 10, 2024.\n",
            "\n",
            "- Value:\n",
            "  - These updates enhance the platform's capabilities for developers, providing more flexible search and summarization options, improved language detection, and clearer usage policies regarding credits.\n",
            "\n",
            "2025-01-01T20:17:48.101Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.642801, used credits [0.00527100]\n",
            "- CONTENT [9303dfbd-2681-4e9b-93ec-2f6a8c1c326f]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [761 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-24-support-for-direct-llm-prompt-multi-turn-image-analysis-bug-fixes</name><title>November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes | Graphlit Changelog</title></metadata> 🦃\tNovember 2024\n",
            "November 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports multi-turn analysis of images with the reviseImage and reviseEncodedImage mutations.  You can provide an LLM prompt and either a URI or Base-64 encoded image and MIME type, along with an optional LLM specification.  This can be used for analyzing any image and having a multi-turn conversation with the LLM to revise the output from the LLM. (Colab Notebook Example)\n",
            "💡 Graphlit now supports directly prompting an LLM with the prompt mutation, bypassing any RAG content retrieval, while providing an optional list of previous conversation messages.  This also accepts an optional LLM specification. (Colab Notebook Example)\n",
            "We have added support for the new Mistral Pixtral Large model, with PIXTRAL_LARGE model enum, which can be used with LLM completion or entity extraction LLM specifications.\n",
            "We have added support for the OpenAI 2024-11-20 version of GPT-4o, with GPT4O_128K_20241120 model enum.\n",
            "⚡ We have added Microsoft Entra ID (fka Azure Active Directory) clientId and clientSecret properties to the SharePointFeedPropertiesInput type, which are now required when creating a SharePoint feed using user authentication with refreshToken property. (Colab Notebook Example)\n",
            "Bugs Fixed\n",
            "GPLA-3438: Not filtering on desktop presentation when scraping web pages\n",
            "GPLA-3340: Failed to parse invalid JSON from extracted PDF page\n",
            "GPLA-3427: Not formatting extracted tables properly from Sonnet 3.5\n",
            "PreviousDecember 1: Support for retrieval-only RAG pipeline, bug fixes\n",
            "NextNovember 16: Support for image description, multi-turn text summarization\n",
            "Last updated1 month ago\n",
            "- Completion [249 tokens (includes JSON guardrails tokens)], throughput: 94.218 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for multi-turn analysis of images using reviseImage and reviseEncodedImage mutations, allowing LLM prompts with image URIs or Base-64 encoded images.\n",
            "  - Direct prompting of an LLM with the prompt mutation, bypassing RAG content retrieval, with optional previous conversation messages.\n",
            "  - Support for Mistral Pixtral Large model (PIXTRAL_LARGE).\n",
            "  - Support for OpenAI GPT-4o (GPT4O_128K_20241120).\n",
            "  - Added Microsoft Entra ID clientId and clientSecret properties for SharePoint feed creation with user authentication.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved user authentication process for SharePoint feeds.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed filtering issue on desktop presentation when scraping web pages (GPLA-3438).\n",
            "  - Resolved JSON parsing error from extracted PDF pages (GPLA-3340).\n",
            "  - Corrected table formatting from Sonnet 3.5 (GPLA-3427).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 2024.\n",
            "  \n",
            "- Value:\n",
            "  - Enhancements provide developers with advanced image analysis capabilities and improved LLM interactions, streamlining workflows and increasing efficiency.\n",
            "\n",
            "2025-01-01T20:17:47.784Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.454916, used credits [0.00599100]\n",
            "- CONTENT [f1678834-af32-4b3d-b6a2-54a2abc0aac5]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [765 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2023/october-15-support-for-anthropic-claude-models-slack-feeds-and-entity-enrichment</name><title>October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment | Graphlit Changelog</title></metadata> 🎃\tOctober 2023\n",
            "October 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "New Features\n",
            "🔥 Graphlit now supports Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "🔥 Graphlit now supports Slack feeds, and will ingest Slack messages and linked file attachments from a Slack channel.  Note, this requires the creation of a Slack bot which has been added to the appropriate Slack channel.\n",
            "💡 Added support for entity enrichment to workflow object, which offers Diffbot, Wikipedia and Crunchbase enrichment of observed entities, such as Person, Organization and Place.\n",
            "💡 Added support for text extraction from images.  When using Azure Image Analytics for entity extraction, Graphlit will extract and store any identified text which then becomes searchable.\n",
            "Added embedFacets property to conversation strategy in specification object.\n",
            "Added embedCitations property to conversation strategy in specification object.  This makes content citations optional with the completed conversation message.\n",
            "Added GraphQL mutations for multi-delete of entities, such as deleteCollections, deleteLabels, or deleteConversations.\n",
            "Added GraphQL deleteAllConversations mutation to delete all conversations.\n",
            "Added support for automatically adding ingested content to one or more collections, via ingestion stage of workflow object.\n",
            "Added specification property to preparation workflow stage, which will be used to select the LLM for text summarization.\n",
            "Expanded the properties for observed entities, such as Person, Organization or Product.  Now supports a wider range of properties for entity enrichment.\n",
            "Bugs Fixed\n",
            "GPLA-1520: Unlimited conversation quota not assigned when upgrading project tier\n",
            "GPLA-1285: Entity enrichment not firing event, which can be sent to actions\n",
            "GPLA-1361: Web page left in ingested state, when URL not accessible.\n",
            "PreviousOctober 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "NextSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "Last updated1 year ago\n",
            "- Completion [308 tokens (includes JSON guardrails tokens)], throughput: 89.148 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Anthropic Claude and Anthropic Claude Instant large language models.\n",
            "  - Ingestion of Slack messages and linked file attachments from Slack channels (requires a Slack bot).\n",
            "  - Entity enrichment for observed entities (Person, Organization, Place) using Diffbot, Wikipedia, and Crunchbase.\n",
            "  - Text extraction from images using Azure Image Analytics, making identified text searchable.\n",
            "  - Added embedFacets and embedCitations properties to conversation strategy in specification object.\n",
            "  - GraphQL mutations for multi-delete of entities (deleteCollections, deleteLabels, deleteConversations).\n",
            "  - GraphQL deleteAllConversations mutation to delete all conversations.\n",
            "  - Automatic addition of ingested content to collections during the ingestion stage of the workflow.\n",
            "  - Specification property added to preparation workflow stage for selecting LLM for text summarization.\n",
            "  - Expanded properties for observed entities for enhanced entity enrichment.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved capabilities for entity enrichment and text extraction.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed unlimited conversation quota not assigned when upgrading project tier (GPLA-1520).\n",
            "  - Resolved issue with entity enrichment not firing events (GPLA-1285).\n",
            "  - Fixed web page remaining in ingested state when URL is not accessible (GPLA-1361).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 15, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced support for LLMs, improved data ingestion from Slack, and advanced entity enrichment capabilities.\n",
            "\n",
            "2025-01-01T20:17:47.266Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.718888, used credits [0.00365100]\n",
            "- CONTENT [f6df142b-f920-4c85-b4d9-1dc09d7d2315]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [613 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-16-support-for-image-description-multi-turn-text-summarization</name><title>November 16: Support for image description, multi-turn text summarization | Graphlit Changelog</title></metadata> 🦃\tNovember 2024\n",
            "November 16: Support for image description, multi-turn text summarization\n",
            "New Features\n",
            "💡 Graphlit now supports multi-turn summarization of text with the reviseText mutation.  You can provide an LLM prompt and text string, along with an optional specification.  This can be used for summarizing any raw text and having a multi-turn conversation with the LLM to revise the output from the LLM.  (Colab Notebook Example)\n",
            "💡 Graphlit now supports image descriptions using vision LLMs, without needing to ingest the image first.  With the new describeImage mutation, which takes a URI, and describeEncodedImage mutation, which takes a Base-64 encoded image and MIME type, you can use any vision LLM to prompt an image description.  These mutations accept an optional specification, where you can select your vision LLM.  If not provided, OpenAI GPT-4o will be used. (Colab Notebook Example)\n",
            "PreviousNovember 24: Support for direct LLM prompt, multi-turn image analysis, bug fixes\n",
            "NextNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "Last updated1 month ago\n",
            "- Completion [151 tokens (includes JSON guardrails tokens)], throughput: 87.847 tokens/sec:\n",
            "- New Features:\n",
            "  - Multi-turn summarization of text with the reviseText mutation, allowing for LLM prompts and text revisions.\n",
            "  - Image descriptions supported via describeImage mutation (URI input) and describeEncodedImage mutation (Base-64 encoded image), utilizing vision LLMs.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Optional specifications for selecting vision LLMs; defaults to OpenAI GPT-4o if not specified.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned in this release.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 16, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for text summarization and image description without prior image ingestion, improving interaction with LLMs.\n",
            "\n",
            "2025-01-01T20:17:46.334Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.004898, used credits [0.00386100]\n",
            "- CONTENT [3d6fee17-3911-4a12-9354-aaaed930f9d1]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [619 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/november-2024/november-4-support-for-anthropic-claude-3.5-haiku-bug-fixes</name><title>November 4: Support for Anthropic Claude 3.5 Haiku, bug fixes | Graphlit Changelog</title></metadata> 🦃\tNovember 2024\n",
            "November 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "New Features\n",
            "Graphlit now supports the latest Anthropic Haiku 3.5 model, with the model enum CLAUDE_3_5_HAIKU_20241022.\n",
            "⚡ Once a project has hit the free tier quota, we will now automatically disable all feeds.  Once the project has been upgraded to a paid tier, you can use the enableFeed mutation to re-enable your existing feeds to continue ingestion.\n",
            "⚡ We have added the disableFallback flag to the RetrievalStrategyInput type, so you can disable the default behavior of falling back to the previous conversation's contents, or worst-case, falling back to the most recently uploaded content.  By setting disableFallback to true, conversations will only attempt to retrieve contents based on the provided filter and/or augmentedFilter properties.\n",
            "Bugs Fixed\n",
            "GPLA-3367: Not extracting text from HTML button element\n",
            "PreviousNovember 10: Support for web search, multi-turn content summarization, Deepgram language detection\n",
            "NextOctober 31: Support for simulated tool calling, bug fixes\n",
            "Last updated1 month ago\n",
            "- Completion [167 tokens (includes JSON guardrails tokens)], throughput: 83.296 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Anthropic Haiku 3.5 model (model enum: CLAUDE_3_5_HAIKU_20241022).\n",
            "  - Automatic disabling of all feeds upon reaching the free tier quota; re-enable feeds with enableFeed mutation after upgrading to a paid tier.\n",
            "  - Addition of disableFallback flag in RetrievalStrategyInput type to control fallback behavior in conversations.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved control over conversation content retrieval with the new disableFallback feature.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3367: Text extraction from HTML button elements.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: November 4, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support, better feed management, and improved content retrieval control.\n",
            "\n",
            "2025-01-01T20:17:45.483Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.420148, used credits [0.00556800]\n",
            "- CONTENT [2791558e-fd4b-4ab1-9328-fe6ac0c05786]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [740 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2023/october-30-optimized-conversation-responses-added-observable-aliases-bug-fixes</name><title>October 30: Optimized conversation responses; added observable aliases; bug fixes | Graphlit Changelog</title></metadata> 🎃\tOctober 2023\n",
            "October 30: Optimized conversation responses; added observable aliases; bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports 'aliases' of observable names, as the alternateNames property.  When an observed entity, such as Organization, is enriched, we store the original name and the enriched name as an alias.  For example, \"OpenAI\" may be enriched to \"OpenAI, Inc.\", and we store \"OpenAI\" as an alias, and update the name to \"OpenAI, Inc.\".\n",
            "💡 Added workflows filter to ContentCriteriaInput type, for filtering content by workflow(s) when creating conversation.\n",
            "Optimized formatting of content sources into prompt context, for more accurate conversation responses.\n",
            "Optimized formatting of extracted text from Slack messages, for better knowledge retrieval.\n",
            "Updated text tokenizer for more accurate token counting.\n",
            "Upgraded Azure Text Analytics to latest preview API version.\n",
            "Authors found in RSS feeds are now stored as observations of Person entities.\n",
            "Added rate limiting for Reddit feeds.\n",
            "Added rate limiting for Wikipedia enrichment.\n",
            "Added support for reading Reddit post comments when reading Reddit feed.\n",
            "⚡ EmbedFacets has been renamed to EnableFacets in the conversation strategy.\n",
            "⚡ Removed extra content level in IngestionWorkflowStage type.  Now, the if property is of type IngestionContentFilter.\n",
            "Bugs Fixed\n",
            "GPLA-1556: Better handling of very long user prompts.\n",
            "GPLA-1627: Optimized token budget for more accurate prompt completion.\n",
            "GPLA-1585: More accurate entity matching in Wikipedia entity enrichment.\n",
            "PreviousDecember 10: Support for OpenAI GPT-4 Turbo, Llama 2 and Mistral models; query by example, bug fixes\n",
            "NextOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "Last updated1 year ago\n",
            "- Completion [279 tokens (includes JSON guardrails tokens)], throughput: 81.575 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for 'aliases' of observable names via the alternateNames property.\n",
            "  - Workflows filter added to ContentCriteriaInput type for filtering content by workflow(s).\n",
            "  - Optimized formatting of content sources for improved conversation responses.\n",
            "  - Enhanced formatting of extracted text from Slack messages for better knowledge retrieval.\n",
            "  - Updated text tokenizer for accurate token counting.\n",
            "  - Upgraded Azure Text Analytics to the latest preview API version.\n",
            "  - Authors in RSS feeds are now stored as observations of Person entities.\n",
            "  - Rate limiting added for Reddit feeds and Wikipedia enrichment.\n",
            "  - Support for reading Reddit post comments.\n",
            "  - EmbedFacets renamed to EnableFacets in conversation strategy.\n",
            "  - Removed extra content level in IngestionWorkflowStage type.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Better handling of long user prompts.\n",
            "  - Optimized token budget for prompt completion.\n",
            "  - More accurate entity matching in Wikipedia entity enrichment.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Improved handling of long user prompts (GPLA-1556).\n",
            "  - Optimized token budget for accurate prompt completion (GPLA-1627).\n",
            "  - Enhanced entity matching in Wikipedia enrichment (GPLA-1585).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers improved conversation accuracy, better content filtering, and enhanced entity recognition capabilities.\n",
            "\n",
            "2025-01-01T20:17:45.419Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.689532, used credits [0.00558000]\n",
            "- CONTENT [9e472441-d42b-4b2e-9115-fc9a123975dc]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [812 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-21-support-openai-cohere-jina-mistral-voyage-and-google-ai-embedding-models</name><title>October 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "New Features\n",
            "💡 Graphlit now supports the configuration of image and text embedding models, at the Project level.  You can create an embedding specification for a text or image embedding model, and then assign that to the Project, and all further embedding requests will use that embedding model.  See this Colab notebook for an example of how to configure the project.\n",
            "💡 Graphlit now supports the OpenAI Embedding-3-Small and Embedding-3-Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0 and 3.0 text embedding models.  Graphlit also now supports Jina CLIP image embeddings, which are used by default for image search.\n",
            "Graphlit now supports the chunkTokenLimit property in Specifications, which specifies the number of tokens for each embedded text chunk.  If this is not configured, Graphlit uses 600 tokens for each embedded text chunk.\n",
            "Graphlit now supports the Voyage reranking model.\n",
            "Graphlit now supports the ingestTextBatch mutation, which accepts an array of text and name pairs, and will asynchronously ingest these into content objects.\n",
            "⚡ We have moved the chunkTokenLimit property from the Workflow storage embeddings strategy to the Specification object.  The Workflow storage property has now been deprecated.\n",
            "⚡ We have deprecated the openAIImage property from Workflow entity extraction properties. Use the modelImage property instead.\n",
            "Once a text embedding model has been updated at the project level, any content, conversations or observed entities will no longer be semantically searchable.\n",
            "Text embeddings are not compatible across models, so you will need to delete and reingest any content, or recreate conversations or knowledge graph entities, with the new embedding model to become searchable.\n",
            "PreviousOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "NextOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "Last updated2 months ago\n",
            "- Completion [262 tokens (includes JSON guardrails tokens)], throughput: 71.012 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for configuring image and text embedding models at the Project level.\n",
            "  - Support for OpenAI Embedding-3-Small and Embedding-3-Large, Cohere Embed 3.0, Jina Embed 3.0, Mistral Embed, and Voyage 2.0 and 3.0 text embedding models.\n",
            "  - Support for Jina CLIP image embeddings for image search.\n",
            "  - Introduction of chunkTokenLimit property in Specifications for token count per embedded text chunk.\n",
            "  - Support for the Voyage reranking model.\n",
            "  - New ingestTextBatch mutation for asynchronous ingestion of text and name pairs.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Moved chunkTokenLimit property from Workflow storage to Specification object; Workflow storage property deprecated.\n",
            "  - Deprecated openAIImage property; use modelImage property instead.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Updating text embedding models at the project level will affect semantic searchability of content, conversations, and observed entities. Requires deletion and reingestion of content for new model compatibility.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 21, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced flexibility in embedding model configuration and improved ingestion capabilities, while ensuring compatibility and searchability of content.\n",
            "\n",
            "2025-01-01T20:17:44.263Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.623351, used credits [0.00435900]\n",
            "- CONTENT [706fcc2b-e7bf-4d03-9660-c94e2dc27296]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [597 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-22-support-for-latest-anthropic-sonnet-3.5-model-cohere-image-embeddings</name><title>October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "New Features\n",
            "Graphlit now supports the latest Anthropic Sonnet 3.5 model (released 10/22/2024).  We have added date-versions model enums for the Anthropic models: CLAUDE_3_5_SONNET_20240620, CLAUDE_3_5_SONNET_20241022, CLAUDE_3_HAIKU_20240307, CLAUDE_3_OPUS_20240229, CLAUDE_3_SONNET_20240229. The existing model enums will target the latest released models, as specified by Anthropic.\n",
            "Graphlit now supports image embeddings using the Cohere Embed 3.0 models.\n",
            "PreviousOctober 31: Support for simulated tool calling, bug fixes\n",
            "NextOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "Last updated2 months ago\n",
            "- Completion [214 tokens (includes JSON guardrails tokens)], throughput: 81.575 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for the latest Anthropic Sonnet 3.5 model (released 10/22/2024).\n",
            "  - Added date-version model enums for Anthropic models: CLAUDE_3_5_SONNET_20240620, CLAUDE_3_5_SONNET_20241022, CLAUDE_3_HAIKU_20240307, CLAUDE_3_OPUS_20240229, CLAUDE_3_SONNET_20240229.\n",
            "  - Support for image embeddings using Cohere Embed 3.0 models.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Existing model enums now target the latest released models as specified by Anthropic.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned in this release.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 22, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers access to the latest Anthropic models and enhanced image embedding capabilities, improving integration and functionality within the Graphlit Platform.\n",
            "\n",
            "2025-01-01T20:17:44.255Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.401704, used credits [0.00527100]\n",
            "- CONTENT [68397ab0-4d1f-4b4a-bb0d-af89d00ba47f]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [761 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-3-support-tool-calling-ingestbatch-mutation-gemini-flash-1.5-8b-bug-fixes</name><title>October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports the ingestBatch mutation, which accepts an array of URIs to files or web pages, and will asynchronously ingest these into content objects.\n",
            "💡 Graphlit now supports the continueConversation mutation, which accepts an array of called tool responses. Also, promptConversation now accepts an array of tool definitions. When tools are called by the LLM, the assistant message returned from promptConversation will have a list of toolCalls which need to responded to from your calling code.  These responses are to be provided back to the LLM via the continueConversation mutation.\n",
            "💡 Graphlit now supports tool calling with OpenAI, Mistral, Deepseek, Groq, and Cerebras model services.  Anthropic, Google Gemini and Cohere support will come later.\n",
            "Added support for prefilled user and assistant messages with createConversation mutation. Now you can send an array of messages when creating a new conversation, which will bootstrap the conversation with the LLM.  These must be provided in user/assistant pairs.\n",
            "Added support for Google Gemini Flash 1.5 8b model.\n",
            "⚡ We have deprecated the tools property in the Specification object. These will be removed at a later date.  Tools are now to be sent directly to the extractContents and promptConversation mutations.\n",
            "Bugs Fixed\n",
            "GPLA-3207: Models shouldn't be required on update specification call\n",
            "GPLA-3220: Don't send system prompt with OpenAI o1 models\n",
            "PreviousOctober 7: Support for Anthropic and Gemini tool calling\n",
            "NextSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "Last updated2 months ago\n",
            "- Completion [249 tokens (includes JSON guardrails tokens)], throughput: 73.199 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for ingestBatch mutation to asynchronously ingest an array of URIs into content objects.\n",
            "  - Introduction of continueConversation mutation for handling tool responses and promptConversation now accepts an array of tool definitions.\n",
            "  - Tool calling support added for OpenAI, Mistral, Deepseek, Groq, and Cerebras model services; support for Anthropic, Google Gemini, and Cohere coming later.\n",
            "  - Prefilled user and assistant messages supported with createConversation mutation, allowing an array of messages to bootstrap conversations.\n",
            "  - Added support for Google Gemini Flash 1.5 8b model.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Deprecated tools property in the Specification object; tools now sent directly to extractContents and promptConversation mutations.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where models were incorrectly required on update specification call (GPLA-3207).\n",
            "  - Resolved issue of sending system prompt with OpenAI o1 models (GPLA-3220).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 2024.\n",
            "  - Notable deprecation of tools property indicates future changes.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve developer experience by streamlining tool interactions and conversation management.\n",
            "\n",
            "2025-01-01T20:17:42.001Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.930127, used credits [0.00368100]\n",
            "- CONTENT [119c6576-e615-476e-8ab9-27d841050c5c]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [587 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-31-support-for-simulated-tool-calling-bug-fixes</name><title>October 31: Support for simulated tool calling, bug fixes | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 31: Support for simulated tool calling, bug fixes\n",
            "New Features\n",
            "Graphlit now supports simulated tool calling for LLMs which don't natively support it, such as OpenAI o1-preview and o1-mini.  Tool schema will be formatted into the LLM prompt context, and tool responses are parsed out of the JSON formatted response.\n",
            "⚡ Given customer feedback, we have lowered the vector and hybrid thresholds used by the semantic search.  Previously, some content at a low relevance was being excluded from the semantic search results.  Now, more low-relevance content will be included in the results, used by the RAG pipeline.  Reranking can be used to sort the search results for relevance.\n",
            "Bugs Fixed\n",
            "GPLA-3357: Not extracting all images from PDF, and should filter out single-color images.\n",
            "PreviousNovember 4: Support for Anthropic Claude 3.5 Haiku, bug fixes\n",
            "NextOctober 22: Support for latest Anthropic Sonnet 3.5 model, Cohere image embeddings\n",
            "Last updated2 months ago\n",
            "- Completion [160 tokens (includes JSON guardrails tokens)], throughput: 82.896 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for simulated tool calling for LLMs like OpenAI o1-preview and o1-mini.\n",
            "  - Tool schema formatted into LLM prompt context; tool responses parsed from JSON.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Lowered vector and hybrid thresholds for semantic search based on customer feedback, allowing more low-relevance content in results.\n",
            "  - Reranking feature added to sort search results for relevance.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-3357: Ensured all images are extracted from PDFs and single-color images are filtered out.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhancements improve search result relevance and broaden content inclusion, benefiting developers in creating more effective applications.\n",
            "\n",
            "2025-01-01T20:17:41.596Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.530283, used credits [0.00295800]\n",
            "- CONTENT [1243312d-9b3b-4172-88ea-ee1b5c097b2c]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [498 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-7-support-for-anthropic-and-gemini-tool-calling</name><title>October 7: Support for Anthropic and Gemini tool calling | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 7: Support for Anthropic and Gemini tool calling\n",
            "New Features\n",
            "💡 Graphlit now supports tool calling with Anthropic and Google Gemini models.\n",
            "⚡ We have removed the uri property for tools from ToolDefinitionInput, such that inline webhook tools are no longer supported.  Now you can define any external tools to be called, and those can support sync or async data access to fulfill the tool call.\n",
            "PreviousOctober 9: Support for GitHub repository feeds, bug fixes\n",
            "NextOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "Last updated2 months ago\n",
            "- Completion [122 tokens (includes JSON guardrails tokens)], throughput: 79.724 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for tool calling with Anthropic and Google Gemini models.\n",
            "  - Removal of the uri property for tools from ToolDefinitionInput, eliminating support for inline webhook tools. External tools can now be defined for sync or async data access.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Improved flexibility in defining external tools for tool calling.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 7, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for integrating external tools and improved data access options.\n",
            "\n",
            "2025-01-01T20:17:41.582Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.844317, used credits [0.00648300]\n",
            "- CONTENT [8413f583-bc40-4957-b03b-f62d5078fe09]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [845 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2023/september-20-paid-subscription-plans-support-for-custom-observed-entities-and-azure-openai-gpt-4</name><title>September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4 | Graphlit Changelog</title></metadata> 🛠️\tSeptember 2023\n",
            "September 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "New Features\n",
            "🔥 Graphlit now supports paid Hobby, Starter and Growth tiers for projects, in addition to the existing Free tier.  Starting at $49/mo, plus $0.10/credit for usage, we now support higher quota based on your subscribed tier.   By providing a payment method for your organization in the Developer Portal, you can upgrade each project individually to the tier that fits your application's needs.\n",
            "💡 Added GraphQL mutations for the creation, update and deletion of observed entities (i.e. Person, Organization, Place, Product, Event, Label, Category).\n",
            "💡 Added new observed entity types to knowledge graph: Repo (i.e. Git repo), Software.\n",
            "💡 Added searchType and numberSimilar fields to Specification object for configuring semantic search in conversations.   In situations where the user prompt is limited in length, HYBRID search type can provide better semantic search results for the prompt context.\n",
            "💡 Added support for the Azure OpenAI GPT-4 model.\n",
            "Added support for project quota field.  Project quotas are based on the subscribed pricing tier.   Quota limits are now applied as content is ingested, and as feeds and conversations are created.\n",
            "Added contentLimit to conversation strategy object to limit the number of semantic search content results which are formatted into prompt context.\n",
            "Better relevance ranking on semantic search results when formatting prompt context in conversations.\n",
            "ℹ️ Free tier has updated quota: 1GB storage, 100 contents, 3 feeds and 10 conversations.\n",
            "⚡ Now using the Deepgram Nova-2 audio transcription model, which is 18% more accurate, and 5-40x faster.\n",
            "Bugs Fixed\n",
            "GPLA-1373: Failed to extract multiple text pages from DOCX without page breaks.  Now we support token-aware page chunking.\n",
            "GPLA-1377: Failed during semantic search with no content results, when prompting conversation.\n",
            "GPLA-1415: Failed when user prompt couldn't generate text embeddings.\n",
            "PreviousSeptember 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "NextSeptember 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "Last updated11 months ago\n",
            "- Completion [329 tokens (includes JSON guardrails tokens)], throughput: 85.581 tokens/sec:\n",
            "- New Features:\n",
            "  - Introduced paid Hobby, Starter, and Growth subscription tiers starting at $49/month, with usage fees of $0.10/credit.\n",
            "  - Added GraphQL mutations for creating, updating, and deleting observed entities (Person, Organization, Place, Product, Event, Label, Category).\n",
            "  - New observed entity types: Repo (Git repo), Software.\n",
            "  - Enhanced Specification object with searchType and numberSimilar fields for improved semantic search.\n",
            "  - Support for Azure OpenAI GPT-4 model.\n",
            "  - Project quota field added, with limits based on subscription tier.\n",
            "  - ContentLimit added to conversation strategy object for limiting semantic search results.\n",
            "  - Improved relevance ranking for semantic search results.\n",
            "  - Updated Free tier quota: 1GB storage, 100 contents, 3 feeds, and 10 conversations.\n",
            "  - Implemented Deepgram Nova-2 audio transcription model, offering 18% more accuracy and 5-40x faster processing.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Better semantic search results with new configuration options.\n",
            "  - Improved audio transcription accuracy and speed.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Resolved issue with extracting multiple text pages from DOCX files using token-aware page chunking.\n",
            "  - Fixed semantic search failure when no content results were available.\n",
            "  - Addressed failure in generating text embeddings from user prompts.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 20, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers flexible subscription options, enhanced entity management, improved search capabilities, and better audio transcription, facilitating more efficient project development and management.\n",
            "\n",
            "2025-01-01T20:17:40.774Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.089689, used credits [0.00522900]\n",
            "- CONTENT [811852e3-3743-4d28-af22-15c2423faa4a]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [739 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2023/september-4-workflow-configuration-support-for-notion-feeds-document-ocr</name><title>September 4: Workflow configuration; support for Notion feeds; document OCR | Graphlit Changelog</title></metadata> 🛠️\tSeptember 2023\n",
            "September 4: Workflow configuration; support for Notion feeds; document OCR\n",
            "New Features\n",
            "🔥 Added Workflow entity to data model for configuring stages of content workflow; can be assigned to Feed or with ingestPage, ingestFile, or ingestText mutations to control how content is ingested, prepared, extracted and enriched into the knowledge graph.\n",
            "💡 Added support for Notion feeds: now can create feed to ingest files from Notion pages or databases (i.e. wikis).\n",
            "💡 Added support for API-created Observation entities, which allow for custom observations of observable entities (i.e. Person, Label) on Content.\n",
            "💡 Added support for Azure AI Document Intelligence as an optional method for preparing PDF files, using OCR and advanced layout analysis.\n",
            "💡 Added summarization strategies, where content can be summarized into paragraphs, bullet points or headline.\n",
            "Added ability to assign default Workflow and Specification to project.\n",
            "Added more well-known link types, during link crawling, such as Discord, Airtable and TypeForm.\n",
            "ℹ️ Free/Hobby plan now has 5GB storage quota; any content ingested past that limit will be auto-deleted.\n",
            "⚡ Actions have been moved into Workflow entity.\n",
            "⚡ Link enrichment for Feeds has been moved into the Workflow enrichment stage, now called link crawling.  ExcludeContentDomain property has been reversed and is now called IncludeContentDomain.\n",
            "Bugs Fixed\n",
            "GPLA-1204: Failed to ingest content with backslash in name.\n",
            "GPLA-1276: Failed to ingest RSS posts which contained enclosure URI, but no post URI.\n",
            "PreviousSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "NextAugust 17: Prepare for usage-based billing; append SAS tokens to URIs\n",
            "Last updated1 year ago\n",
            "- Completion [251 tokens (includes JSON guardrails tokens)], throughput: 81.238 tokens/sec:\n",
            "- New Features:\n",
            "  - Added Workflow entity for configuring content workflow stages.\n",
            "  - Support for Notion feeds to ingest files from Notion pages or databases.\n",
            "  - API-created Observation entities for custom observations on Content.\n",
            "  - Support for Azure AI Document Intelligence for PDF preparation using OCR.\n",
            "  - Summarization strategies for content into paragraphs, bullet points, or headlines.\n",
            "  - Ability to assign default Workflow and Specification to projects.\n",
            "  - Added support for more link types during link crawling (e.g., Discord, Airtable, TypeForm).\n",
            "  - Free/Hobby plan now includes a 5GB storage quota.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Actions moved into Workflow entity.\n",
            "  - Link enrichment for Feeds moved to Workflow enrichment stage, renamed to link crawling.\n",
            "  - ExcludeContentDomain property renamed to IncludeContentDomain.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with content ingestion failure due to backslash in name.\n",
            "  - Resolved failure to ingest RSS posts with enclosure URI but no post URI.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 4, 2023.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced workflow configuration, improved content ingestion from Notion, and advanced document processing capabilities.\n",
            "\n",
            "2025-01-01T20:17:40.021Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.053877, used credits [0.00398100]\n",
            "- CONTENT [8fff37ca-bf13-47f8-b452-1010061953f8]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [587 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2023/september-24-support-for-youtube-feeds-added-documentation-bug-fixes</name><title>September 24: Support for YouTube feeds; added documentation; bug fixes | Graphlit Changelog</title></metadata> 🛠️\tSeptember 2023\n",
            "September 24: Support for YouTube feeds; added documentation; bug fixes\n",
            "New Features\n",
            "🔥 Graphlit now supports YouTube feeds, where you can ingest a set of YouTube videos, or an entire YouTube playlist or channel.   Note, we currently support only the ingestion of audio from YouTube videos, which gets transcribed and added to your conversational knowledge graph.\n",
            "New Documentation\n",
            "Added documentation for observable entities mutations and queries (Label, Category, Person, Organization, Place, Event, Product, Repo, Software).\n",
            "Added documentation for using custom Azure OpenAI and OpenAI models with Specifications\n",
            "Bugs Fixed\n",
            "GPLA-1459: LLM prompt formatting was exceeding the token budget with long user prompts.\n",
            "GPLA-1445: Failed to ingest PDF from URL where filename in Content-Disposition header contained a backslash.\n",
            "PreviousOctober 15: Support for Anthropic Claude models, Slack feeds and entity enrichment\n",
            "NextSeptember 20: Paid subscription plans; support for custom observed entities & Azure OpenAI GPT-4\n",
            "Last updated7 months ago\n",
            "- Completion [185 tokens (includes JSON guardrails tokens)], throughput: 60.579 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for YouTube feeds, allowing ingestion of YouTube videos, playlists, or channels (currently supports audio ingestion and transcription).\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Added documentation for observable entities mutations and queries (Label, Category, Person, Organization, Place, Event, Product, Repo, Software).\n",
            "  - Added documentation for using custom Azure OpenAI and OpenAI models with Specifications.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed LLM prompt formatting issue (GPLA-1459) that exceeded token budget with long user prompts.\n",
            "  - Resolved issue (GPLA-1445) with PDF ingestion from URL when filename in Content-Disposition header contained a backslash.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 2023.\n",
            "\n",
            "- Value:\n",
            "  - Enhances developers' ability to integrate YouTube content and improves documentation for better usage of observable entities and custom models.\n",
            "\n",
            "2025-01-01T20:17:39.982Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:01.513159, used credits [0.00252000]\n",
            "- CONTENT [833da803-989f-4c1c-96af-c4cf003e6e52]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [484 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/october-2024/october-9-support-for-github-repository-feeds-bug-fixes</name><title>October 9: Support for GitHub repository feeds, bug fixes | Graphlit Changelog</title></metadata> 🎃\tOctober 2024\n",
            "October 9: Support for GitHub repository feeds, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports GitHub feeds, by providing the repository owner and name similar to GitHub Issues feeds, and will ingest code files from any GitHub repository.\n",
            "Bugs Fixed\n",
            "GPLA-3262: Missing row separator in table markdown formatting\n",
            "PreviousOctober 21: Support OpenAI, Cohere, Jina, Mistral, Voyage and Google AI embedding models\n",
            "NextOctober 7: Support for Anthropic and Gemini tool calling\n",
            "Last updated2 months ago\n",
            "- Completion [89 tokens (includes JSON guardrails tokens)], throughput: 58.817 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for GitHub repository feeds, allowing ingestion of code files by providing the repository owner and name.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed missing row separator in table markdown formatting (GPLA-3262).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: October 9, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances integration with GitHub, improving developers' ability to work with code repositories.\n",
            "\n",
            "2025-01-01T20:17:38.426Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:04.272459, used credits [0.00674100]\n",
            "- CONTENT [88122e0e-dc76-44ba-a14b-5f5fb698524d]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [851 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-26-support-for-google-ai-and-cerebras-models-and-latest-groq-models</name><title>September 26: Support for Google AI and Cerebras models, and latest Groq models | Graphlit Changelog</title></metadata> 🎒\tSeptember 2024\n",
            "September 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "New Features\n",
            "💡 Graphlit now supports the Cerebras model service which offers the LLAMA_3_1_70B and LLAMA_3_1_8B models.\n",
            "💡 Graphlit now supports the Google AI model service which offers the GEMINI_1_5_PRO and GEMINI_1_5_FLASH models.\n",
            "We have added support for the latest Groq Llama 3.2 preview models, including LLAMA_3_2_1B_PREVIEW, LLAMA_3_2_3B_PREVIEW, LLAMA_3_2_11B_TEXT_PREVIEW, and LLAMA_3_2_90B_TEXT_PREVIEW.  We have also added support for the Llama 3.2 multimodal model LLAMA_3_2_11B_VISION_PREVIEW.\n",
            "We have added a new specification parameter to the promptConversation mutation. Now you can specify your initial specification for a new conversation, or update an existing conversation, without requiring additional API calls.\n",
            "⚡ We have changed the retrieval behavior of the promptConversation mutation. Now, if no relevant content was found via vector-based semantic search (given the user prompt), we will fallback to any relevant content from the message in the conversation. If there was no content from the conversation to fallback to, we will fallback to the last ingested content in the project. This solves an issue where a first prompt like 'Summarize this' would find no relevant content.  Now it will fallback to retrieve the last ingested content.\n",
            "⚡ We have renamed the Groq model enum from LLAVA_1_5_7B to LLAVA_1_5_7B_PREVIEW.\n",
            "Bugs Fixed\n",
            "GPLA-3083: Not sending custom instructions/guidance with extraction prompt\n",
            "GPLA-3146: Filtering Persons by email not working\n",
            "GPLA-3171: Not failing on deprecated OpenAI model\n",
            "GPLA-3158: Summarization not using revision strategy\n",
            "PreviousSeptember 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "NextSeptember 3: Support for web search feeds, model deprecations\n",
            "Last updated3 months ago\n",
            "- Completion [349 tokens (includes JSON guardrails tokens)], throughput: 81.686 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Cerebras model service: LLAMA_3_1_70B and LLAMA_3_1_8B models.\n",
            "  - Support for Google AI model service: GEMINI_1_5_PRO and GEMINI_1_5_FLASH models.\n",
            "  - Added support for Groq Llama 3.2 preview models: LLAMA_3_2_1B_PREVIEW, LLAMA_3_2_3B_PREVIEW, LLAMA_3_2_11B_TEXT_PREVIEW, LLAMA_3_2_90B_TEXT_PREVIEW, and multimodal model LLAMA_3_2_11B_VISION_PREVIEW.\n",
            "  - New specification parameter for promptConversation mutation to specify or update conversation specifications without extra API calls.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Changed retrieval behavior of promptConversation mutation to fallback to relevant content from the conversation or last ingested content if no relevant content is found.\n",
            "  - Renamed Groq model enum from LLAVA_1_5_7B to LLAVA_1_5_7B_PREVIEW.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue with not sending custom instructions with extraction prompt (GPLA-3083).\n",
            "  - Resolved filtering persons by email not working (GPLA-3146).\n",
            "  - Addressed failure on deprecated OpenAI model (GPLA-3171).\n",
            "  - Fixed summarization not using revision strategy (GPLA-3158).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 26, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced model support and improved conversation handling, leading to more efficient interactions and better content retrieval.\n",
            "\n",
            "2025-01-01T20:17:37.632Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.477985, used credits [0.00425100]\n",
            "- CONTENT [9819c811-85db-4a53-923e-f3a6beac4b6a]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [641 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-30-support-for-azure-ai-inference-models-mistral-pixtral-and-latest-google-gemini-models</name><title>September 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models | Graphlit Changelog</title></metadata> 🎒\tSeptember 2024\n",
            "September 30: Support for Azure AI Inference models, Mistral Pixtral and latest Google Gemini models\n",
            "New Features\n",
            "💡 Graphlit now supports the Azure AI Model Inference API (aka Models as a Service) model service which offers serverless hosting to many models such as Meta Llama 3.2, Cohere Command-R, and many more.  For Azure AI, all models are 'custom', and you will need to provide the serverless endpoint, API key and number of tokens accepted in context window, after provisioning the model of your choice.\n",
            "We have added support for the multimodal Mistral Pixtral model, under the model enum PIXTRAL_12B_2409.\n",
            "We have added versioned model enums for Google Gemini, so you can access GEMINI_1_5_FLASH_001, GEMINI_1_5_FLASH_002, GEMINI_1_5_PRO_001 and GEMINI_1_5_PRO_002.\n",
            "PreviousOctober 3: Support tool calling, ingestBatch mutation, Gemini Flash 1.5 8b, bug fixes\n",
            "NextSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "Last updated3 months ago\n",
            "- Completion [194 tokens (includes JSON guardrails tokens)], throughput: 55.779 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for Azure AI Model Inference API, enabling serverless hosting for models like Meta Llama 3.2 and Cohere Command-R.\n",
            "  - Added support for the multimodal Mistral Pixtral model (PIXTRAL_12B_2409).\n",
            "  - Introduced versioned model enums for Google Gemini: GEMINI_1_5_FLASH_001, GEMINI_1_5_FLASH_002, GEMINI_1_5_PRO_001, GEMINI_1_5_PRO_002.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Enhanced model accessibility through versioned enums for Google Gemini.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - No specific bug fixes mentioned in this release.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 30, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers expanded model support and flexibility with serverless hosting options, enhancing the capabilities of the Graphlit Platform.\n",
            "\n",
            "2025-01-01T20:17:37.533Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:03.381089, used credits [0.00428700]\n",
            "- CONTENT [65dc8b4e-c1dc-40e2-a653-d8d51fe0414c]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [689 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-3-support-for-web-search-feeds-model-deprecations</name><title>September 3: Support for web search feeds, model deprecations | Graphlit Changelog</title></metadata> 🎒\tSeptember 2024\n",
            "September 3: Support for web search feeds, model deprecations\n",
            "New Features\n",
            "💡 Graphlit now supports web search feeds, using the Tavily and Exa.AI web search APIs. You can choose the SEARCH feed type, and assign your search text property, and we will ingest the referenced web pages from the search results.  Optionally, you can select the search service via the serviceType property under search feed properties.  By default, Graphlit will use the Tavily API.\n",
            "⚡ We have deprecated these OpenAI models, according to the future support OpenAI is providing to these legacy models: GPT35_TURBO, GPT35_TURBO_0613, GPT35_TURBO_16K, GPT35_TURBO_16K_0125, GPT35_TURBO_16K_0613, GPT35_TURBO_16K_1106, GPT4, GPT4_0613, GPT4_32K, GPT4_32K_0613, GPT4_TURBO_VISION_128K, and GPT4_TURBO_VISION_128K_1106.  We suggest using GPT-4o or GPT-4o Mini instead.\n",
            "Bugs Fixed\n",
            "GPLA-2523: Can't ingest from same feed URI multiple times and wait on isFeedDone\n",
            "PreviousSeptember 26: Support for Google AI and Cerebras models, and latest Groq models\n",
            "NextSeptember 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "Last updated3 months ago\n",
            "- Completion [185 tokens (includes JSON guardrails tokens)], throughput: 54.716 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for web search feeds using Tavily and Exa.AI APIs.\n",
            "  - Ability to choose SEARCH feed type and assign search text property for ingesting web pages from search results.\n",
            "  - Option to select search service via serviceType property, defaulting to Tavily API.\n",
            "  \n",
            "- Enhancements/Improvements:\n",
            "  - Deprecation of several OpenAI models (e.g., GPT35_TURBO, GPT4) due to reduced future support; recommended alternatives are GPT-4o and GPT-4o Mini.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue GPLA-2523: Ingestion from the same feed URI multiple times and waiting on isFeedDone.\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 3, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Offers developers enhanced capabilities for web search integration and guidance on model usage with improved support.\n",
            "\n",
            "2025-01-01T20:17:36.900Z: Prompt completion\n",
            "- Workflow [Enrichment] took 0:00:02.751593, used credits [0.00455100]\n",
            "- CONTENT [4983a4c2-2d1a-4640-83f7-a187a4275778]\n",
            "- Model service [OpenAI], model name [GPT4o_Mini_128k]\n",
            "- Prompt [661 tokens (includes RAG context tokens)]:\n",
            "<metadata type='Page'><name>https://changelog.graphlit.dev/september-2024/september-1-support-for-fhir-enrichment-latest-cohere-models-bug-fixes</name><title>September 1: Support for FHIR enrichment, latest Cohere models, bug fixes | Graphlit Changelog</title></metadata> 🎒\tSeptember 2024\n",
            "September 1: Support for FHIR enrichment, latest Cohere models, bug fixes\n",
            "New Features\n",
            "💡 Graphlit now supports entity enrichment from Fast Healthcare Interoperability Resources (FHIR) servers. You can provide the endpoint for a FHIR server, and Graphlit will enrich medical-related entities from the data found in the FHIR server.\n",
            "Added support for latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408) and added datestamped model enums for the previous versions (COMMAND_R_202403, COMMAND_R_PLUS_202404).  The latest model enums (COMMAND_R and COMMAND_R_PLUS) currently point to the models (COMMAND_R_202403 and COMMAND_R_PLUS_202404) as specified by the Cohere API.\n",
            "Added support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31), now used by default.\n",
            "⚡ We have changed the name of the LinkReferenceType to LinkReference to follow the existing data model standard.\n",
            "Bugs Fixed\n",
            "GPLA-3120: LLM is adding source tags to end of completed messages\n",
            "GPLA-3133: Failed to load sitemap on child page of website.\n",
            "PreviousSeptember 3: Support for web search feeds, model deprecations\n",
            "NextAugust 20: Support for medical entities, Anthropic prompt caching, bug fixes\n",
            "Last updated4 months ago\n",
            "- Completion [214 tokens (includes JSON guardrails tokens)], throughput: 77.773 tokens/sec:\n",
            "- New Features:\n",
            "  - Support for entity enrichment from FHIR servers for medical-related entities.\n",
            "  - Added support for latest Cohere models (COMMAND_R_202408, COMMAND_R_PLUS_202408) with datestamped model enums for previous versions.\n",
            "  - Support for the latest Azure AI Document Intelligence v4.0 preview API (2024-07-31) now used by default.\n",
            "  - Renamed LinkReferenceType to LinkReference to align with data model standards.\n",
            "\n",
            "- Enhancements/Improvements:\n",
            "  - Updated naming conventions for consistency with existing data models.\n",
            "\n",
            "- Bug Fixes:\n",
            "  - Fixed issue where LLM was adding source tags to the end of completed messages (GPLA-3120).\n",
            "  - Resolved failure to load sitemap on child page of website (GPLA-3133).\n",
            "\n",
            "- Other Key Details:\n",
            "  - Release Date: September 1, 2024.\n",
            "\n",
            "- Value:\n",
            "  - Enhances data enrichment capabilities for healthcare applications and improves model support, providing developers with more robust tools for integration and functionality.\n",
            "\n",
            "2025-01-01T20:17:32.799Z: Search entities\n",
            "- Workflow [Semantic search] took 0:00:00.093836, used credits [0.00960000]\n",
            "- Processor name [Azure AI Search], units [48]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}