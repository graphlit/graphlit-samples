{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyO5JCXNjHgNvPqR5ZiEdsWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_11_24_Multi_turn_Analysis_of_Image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to use Graphlit to perform multi-step analysis of provided image."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd97478-0ba3-4193-ff6d-06e10ae164e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphlit-client in /usr/local/lib/python3.10/dist-packages (1.0.20241124001)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.9.2)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Graphlit"
      ],
      "metadata": {
        "id": "abV1114jL-bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "async def create_openai_specification(model: enums.OpenAIModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"OpenAI [{str(model)}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.OPEN_AI,\n",
        "        openAI=input_types.OpenAIModelPropertiesInput(\n",
        "            model=model,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def revise_image(prompt: str, uri: str, specification_id: str, conversation_id: Optional[str] = None):\n",
        "    if graphlit.client is None:\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.revise_image(prompt=prompt, uri=uri, id=conversation_id, specification=input_types.EntityReferenceInput(id=specification_id))\n",
        "\n",
        "        message = response.revise_image.message.message if response.revise_image is not None and response.revise_image.message is not None else None\n",
        "        conversation_id = response.revise_image.conversation.id if response.revise_image is not None and response.revise_image.conversation is not None else None\n",
        "\n",
        "        return message, conversation_id\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None, None\n",
        "\n",
        "async def delete_all_specifications():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_specifications(is_synchronous=True)\n",
        "\n",
        "async def delete_all_conversations():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_conversations(is_synchronous=True)\n"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute Graphlit example"
      ],
      "metadata": {
        "id": "srzhQt4COLVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, HTML\n",
        "import time\n",
        "\n",
        "# Remove any existing conversations and specifications; only needed for notebook example\n",
        "await delete_all_conversations()\n",
        "await delete_all_specifications()\n",
        "\n",
        "print('Deleted all conversations and specifications.')\n",
        "\n",
        "uri = \"https://graphlitplatform.blob.core.windows.net/test/images/2657-Emerging-LLM-App-Stack-R2-1-of-4-2.png\""
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d03b0f6-e1b3-45c7-d42e-5a566c2c57dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all conversations and specifications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create specification, and start multi-turn summarization conversation."
      ],
      "metadata": {
        "id": "kqG9WHXhbld-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_id = None\n",
        "\n",
        "specification_id = await create_openai_specification(enums.OpenAIModels.GPT4O_128K)\n",
        "\n",
        "if specification_id is not None:\n",
        "    print(f'Created specification [{specification_id}].')\n",
        "\n",
        "    prompt = \"Describe this for me.\"\n",
        "\n",
        "    message, conversation_id = await revise_image(prompt, uri, specification_id)\n",
        "\n",
        "    if message is not None:\n",
        "        display(Markdown(f'**Revision:**\\n{message}'))\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "uSwSy3gXq7AN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "29adec9e-0d97-4b5a-e37f-85b93f86db4e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created specification [235343ed-46a8-46a6-8c91-03c7a66da1f2].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Revision:**\nThe Emerging LLM App Stack diagram illustrates the components and data flow involved in building applications with large language models (LLMs).\n\nKey components include data pipelines, embedding models, vector databases, orchestration, APIs/plugins, LLM cache, logging/LLMops, validation, and app hosting.\n\nData pipelines process contextual data, which is then used by embedding models to create vector representations stored in vector databases.\n\nOrchestration tools manage the flow of data and interactions between components, integrating with APIs and plugins to enhance functionality.\n\nLLM APIs and hosting options include proprietary APIs, open APIs, cloud providers, and opinionated cloud services, offering flexibility in deployment.\n\nThe diagram uses arrows to indicate the flow of data: contextual data, prompts, user queries, and outputs, highlighting the interaction between components."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    prompt = \"OK, not bad, but make it more detailed. Infer anything you can from the intent of the image.\"\n",
        "\n",
        "    message, conversation_id = await revise_image(prompt, uri, specification_id, conversation_id)\n",
        "\n",
        "    if message is not None:\n",
        "        display(Markdown(f'**Revision:**\\n{message}'))\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "RitZHrG1TyFg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "c216eff7-b222-4fe2-a831-630c5aa5470b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Revision:**\nThe Emerging LLM App Stack diagram provides a comprehensive overview of the architecture and components involved in developing applications using large language models. It highlights the flow of data from contextual sources through various processing stages to deliver outputs to users. The stack is designed to integrate multiple tools and systems, ensuring flexibility and scalability in application development.\n\nKey components include data pipelines, which handle the ingestion and processing of contextual data, and embedding models that transform this data into vector representations. These vectors are stored in vector databases, facilitating efficient retrieval and manipulation. Orchestration tools manage the interactions between components, ensuring seamless data flow and integration with external APIs and plugins.\n\nThe diagram also outlines the role of LLM APIs and hosting options, which provide the necessary infrastructure for deploying and scaling applications. These options include proprietary APIs, open APIs, and various cloud services, offering developers a range of choices based on their specific needs and preferences. The stack is designed to support robust application development, leveraging the capabilities of large language models."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    prompt = \"Also extract all visible text and format as Markdown, in addition to a Markdown summary of the image.\"\n",
        "\n",
        "    message, conversation_id = await revise_image(prompt, uri, specification_id, conversation_id)\n",
        "\n",
        "    if message is not None:\n",
        "        display(Markdown(f'**Revision:**\\n{message}'))\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "CbmjHmjXTzgg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "33783fcf-9c39-4a6a-f1f5-76989214c188"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Revision:**\n### Emerging LLM App Stack\n\n- **Contextual data**: Data Pipelines (Databricks, Airflow, Unstructured, ...)\n- **Embedding Model**: (OpenAI, Cohere, Hugging Face)\n- **Vector Database**: (Pinecone, Weaviate, Chroma, pgvector)\n- **Playground**: (OpenAI, nat.dev, Humanloop)\n- **Orchestration**: (Python/DIY, LangChain, LlamaIndex, ChatGPT)\n- **APIs/Plugins**: (Serp, Wolfram, Zapier, ...)\n- **LLM Cache**: (Redis, SQLite, GPTCache)\n- **Logging/LLMops**: (Weights & Biases, MLflow, PromptLayer, Helicone)\n- **Validation**: (Guardrails, Rebuff, Guidance, LMQL)\n- **App Hosting**: (Vercel, Steamship, Streamlit, Modal)\n\n### LLM APIs and Hosting\n\n- **Proprietary API**: (OpenAI, Anthropic)\n- **Open API**: (Hugging Face, Replicate)\n- **Cloud Provider**: (AWS, GCP, Azure, Coreweave)\n- **Opinionated Cloud**: (Databricks, Anyscale, Mosaic, Modal, Runpod, ...)\n\n### Legend\n\n- **Gray boxes**: Key components of the stack, with leading tools/systems listed\n- **Arrows**: Show the flow of data through the stack\n  - **Dashed arrows**: Contextual data provided by app developers to condition LLM outputs\n  - **Red arrows**: Prompts and few-shot examples that are sent to the LLM\n  - **Blue arrows**: Queries submitted by users\n  - **Red arrows**: Output returned to users\n\nThe diagram illustrates the architecture of an emerging LLM app stack, detailing the flow of data from contextual sources through various processing stages to deliver outputs. It highlights key components such as data pipelines, embedding models, and vector databases, which work together to process and store data efficiently. Orchestration tools manage these interactions, integrating with APIs and plugins to enhance functionality and scalability."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}