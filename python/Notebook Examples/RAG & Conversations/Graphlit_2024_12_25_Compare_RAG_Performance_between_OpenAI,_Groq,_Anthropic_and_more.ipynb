{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyN0gJ7imm50vyUUIN8RDsqr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_12_25_Compare_RAG_Performance_between_OpenAI%2C_Groq%2C_Anthropic_and_more.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to compare LLM performance between RAG pipelines."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec902421-c80a-445e-b2fb-7a9e4cce6552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphlit-client in /usr/local/lib/python3.10/dist-packages (1.0.20241224002)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.3)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Graphlit"
      ],
      "metadata": {
        "id": "abV1114jL-bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "async def ingest_uri(uri: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        # Using synchronous mode, so the notebook waits for the content to be ingested\n",
        "        response = await graphlit.client.ingest_uri(uri=uri, is_synchronous=True)\n",
        "\n",
        "        return response.ingest_uri.id if response.ingest_uri is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def create_openai_specification(model: enums.OpenAIModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"OpenAI [{str(model)}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.OPEN_AI,\n",
        "        openAI=input_types.OpenAIModelPropertiesInput(\n",
        "            model=model\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_cohere_specification(model: enums.CohereModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"Cohere [{str(model)}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.COHERE,\n",
        "        cohere=input_types.CohereModelPropertiesInput(\n",
        "            model=model\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_google_specification(model: enums.GoogleModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"Google [{str(model)}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.GOOGLE,\n",
        "        google=input_types.GoogleModelPropertiesInput(\n",
        "            model=model\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_groq_specification(model: enums.GroqModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"Groq [{str(model)}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.GROQ,\n",
        "        groq=input_types.GroqModelPropertiesInput(\n",
        "            model=model\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_anthropic_specification(model: enums.AnthropicModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"Anthropic [{str(model)}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.ANTHROPIC,\n",
        "        anthropic=input_types.AnthropicModelPropertiesInput(\n",
        "            model=model\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_deepseek_specification(model: enums.DeepseekModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"Deepseek [{str(model)}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.DEEPSEEK,\n",
        "        deepseek=input_types.DeepseekModelPropertiesInput(\n",
        "            model=model\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_conversation(specification_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.ConversationInput(\n",
        "        name=\"Conversation\",\n",
        "        specification=input_types.EntityReferenceInput(\n",
        "            id=specification_id\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_conversation(input)\n",
        "\n",
        "        return response.create_conversation.id if response.create_conversation is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_conversation(conversation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    if conversation_id is not None:\n",
        "        _ = await graphlit.client.delete_conversation(conversation_id)\n",
        "\n",
        "async def prompt_conversation(conversation_id: str, prompt: str):\n",
        "    if graphlit.client is None:\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.prompt_conversation(prompt, conversation_id, include_details=True)\n",
        "\n",
        "        message = response.prompt_conversation.message if response.prompt_conversation is not None else None\n",
        "        details = response.prompt_conversation.details if response.prompt_conversation is not None else None\n",
        "\n",
        "        return message, details\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None, None\n",
        "\n",
        "async def delete_all_specifications():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_specifications(is_synchronous=True)\n",
        "\n",
        "async def delete_all_conversations():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_conversations(is_synchronous=True)\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)\n"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute Graphlit example"
      ],
      "metadata": {
        "id": "srzhQt4COLVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, HTML\n",
        "import time\n",
        "\n",
        "# Remove any existing contents, conversations and specifications; only needed for notebook example\n",
        "await delete_all_conversations()\n",
        "await delete_all_specifications()\n",
        "await delete_all_contents()\n",
        "\n",
        "print('Deleted all contents, conversations and specifications.')\n",
        "\n",
        "content_id = await ingest_uri(uri=\"https://graphlitplatform.blob.core.windows.net/samples/Unstructured%20Data%20is%20Dark%20Data%20Podcast.mp3\")\n",
        "\n",
        "if content_id is not None:\n",
        "    print(f'Ingested content [{content_id}]:')"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a88fa8e0-0f91-43e1-9976-36bb4e2effac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all contents, conversations and specifications.\n",
            "Ingested content [a2664d78-a881-4f02-b4c1-ed30871c71e8]:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Specify the RAG prompt\n",
        "    prompt = \"In 3-5 detailed paragraphs, explain unstructured data and its usefulness for knowledge capture and retrieval.\""
      ],
      "metadata": {
        "id": "-9kBDUIdbhIy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create OpenAI GPT-4o specification."
      ],
      "metadata": {
        "id": "4snKjK2ycVKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    specification_id = await create_openai_specification(enums.OpenAIModels.GPT4O_128K)\n",
        "\n",
        "    if specification_id is not None:\n",
        "        print(f'Created OpenAI GPT-4o specification [{specification_id}].')\n",
        "\n",
        "        conversation_id = await create_conversation(specification_id)\n",
        "\n",
        "        if conversation_id is not None:\n",
        "            print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "            message, details = await prompt_conversation(conversation_id, prompt)\n",
        "\n",
        "            if message is not None:\n",
        "                display(Markdown(f'### Completion took [{message.completion_time}]:'))\n",
        "                display(Markdown(f'**User:**\\n{prompt}'))\n",
        "                display(Markdown(f'**Assistant:**'))\n",
        "                print(message.message)\n",
        "                print()\n",
        "\n",
        "            if details is not None:\n",
        "                display(Markdown('### Details:'))\n",
        "                display(Markdown(f'**Model**: {details.model_service} {details.model}'))\n",
        "                display(Markdown(f'**Token Limit**: {details.token_limit}'))\n",
        "                display(Markdown(f'**Completion Token Limit**: {details.completion_token_limit}'))\n",
        "\n",
        "            await delete_conversation(conversation_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "L5AHVoKocVmD",
        "outputId": "1b27213c-f0a3-4d85-e227-9643e287afa3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created OpenAI GPT-4o specification [0ccfca3b-4053-49c8-ac90-a7f223cb35d9].\n",
            "Created conversation [4a39d2ad-b0b7-41cc-8e32-201e4266e1e6].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Completion took [PT6.5102745S]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User:**\nIn 3-5 detailed paragraphs, explain unstructured data and its usefulness for knowledge capture and retrieval."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstructured data refers to information that does not have a predefined data model or is not organized in a pre-defined manner. This type of data includes a wide variety of formats such as text, images, audio, video, and more complex data types like 3D models and point clouds. Unlike structured data, which is neatly organized in databases with clear relationships, unstructured data is often stored in its raw form, making it more challenging to process and analyze. Despite this complexity, unstructured data is incredibly valuable because it represents the vast majority of data generated today, encompassing everything from social media posts and emails to satellite imagery and IoT sensor data.\n",
            "\n",
            "The usefulness of unstructured data lies in its potential for knowledge capture and retrieval. By leveraging advanced technologies such as machine learning and natural language processing, organizations can extract meaningful insights from unstructured data. For instance, images can be analyzed to detect objects, identify patterns, or even infer relationships between different elements within the image. Similarly, audio and video files can be transcribed and analyzed to extract sentiment, identify key topics, or recognize entities such as people and places. This process of extracting insights from unstructured data is crucial for building comprehensive knowledge bases that can support decision-making and strategic planning.\n",
            "\n",
            "One of the key challenges in working with unstructured data is the need to contextualize it to make it useful. This involves creating metadata that describes the data and its relationships to other data points. Metadata can be categorized into different orders, such as first-order metadata, which includes basic information like file headers, and second-order metadata, which involves more detailed analysis like object detection in images. Third-order metadata goes even further by linking data to external databases or knowledge graphs, providing a richer context for understanding the data. This layered approach to metadata helps in organizing unstructured data in a way that makes it searchable and retrievable, enabling more effective knowledge management.\n",
            "\n",
            "Knowledge graphs are particularly useful in managing unstructured data as they allow for the dynamic representation of data and its relationships. By creating a network of interconnected data points, knowledge graphs enable users to explore and discover new insights by following the links between related entities. This approach is similar to how search engines like Google organize information, but with a focus on specific domains or industries. For example, in the geospatial field, knowledge graphs can help in understanding the relationships between different geographic entities and their attributes, facilitating more informed decision-making.\n",
            "\n",
            "In conclusion, unstructured data, despite its inherent complexity, offers immense potential for knowledge capture and retrieval. By employing advanced analytical techniques and creating robust metadata frameworks, organizations can unlock the value hidden within unstructured data. This not only enhances their ability to make data-driven decisions but also supports innovation by providing a deeper understanding of the complex relationships and patterns that exist within their data. As the volume of unstructured data continues to grow, the ability to effectively manage and utilize this data will become increasingly important for organizations across all sectors.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Details:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Model**: OPEN_AI GPT-4o 128k (Latest)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token Limit**: 128000"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Completion Token Limit**: 16384"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Groq Llama 3.2 specification."
      ],
      "metadata": {
        "id": "PdBHhNHmk5KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    specification_id = await create_groq_specification(enums.GroqModels.LLAMA_3_2_11B_VISION_PREVIEW)\n",
        "\n",
        "    if specification_id is not None:\n",
        "        print(f'Created Groq Llama 3.2 specification [{specification_id}].')\n",
        "\n",
        "        conversation_id = await create_conversation(specification_id)\n",
        "\n",
        "        if conversation_id is not None:\n",
        "            print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "            message, details = await prompt_conversation(conversation_id, prompt)\n",
        "\n",
        "            if message is not None:\n",
        "                display(Markdown(f'### Completion took [{message.completion_time}]:'))\n",
        "                display(Markdown(f'**User:**\\n{prompt}'))\n",
        "                display(Markdown(f'**Assistant:**'))\n",
        "                print(message.message)\n",
        "                print()\n",
        "\n",
        "            if details is not None:\n",
        "                display(Markdown('### Details:'))\n",
        "                display(Markdown(f'**Model**: {details.model_service} {details.model}'))\n",
        "                display(Markdown(f'**Token Limit**: {details.token_limit}'))\n",
        "                display(Markdown(f'**Completion Token Limit**: {details.completion_token_limit}'))\n",
        "\n",
        "            await delete_conversation(conversation_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "rdS2DoIUk7w0",
        "outputId": "a9bc1b04-5853-421e-ac81-dffb0cc2a003"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Groq Llama 3.2 specification [3c88c956-adcb-420e-be89-8698850c1f22].\n",
            "Created conversation [faeeae2a-22c6-4afd-8c66-88477482cf71].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Completion took [PT1.2487748S]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User:**\nIn 3-5 detailed paragraphs, explain unstructured data and its usefulness for knowledge capture and retrieval."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstructured data refers to a broad set of data that lacks a predefined structure or organization, making it difficult to analyze and process using traditional data management tools. This type of data can include everything from images and audio files to documents and emails, as well as 3D models and geometry point clouds. According to Kirk Marple, the founder of Unstruct Data, unstructured data is a \"broad set of file-based\" data that is often overlooked in favor of more structured data.\n",
            "\n",
            "Despite its challenges, unstructured data holds significant value for knowledge capture and retrieval. By leveraging machine learning and artificial intelligence, it is possible to extract insights and meaning from unstructured data, creating a knowledge graph that can be used to make connections and inferences between different pieces of information. This can be particularly useful in industries such as geospatial, where data is often generated by sensors and other devices, and needs to be analyzed and understood in context. For example, by analyzing images and sensor data, it is possible to create a detailed picture of a physical asset, such as a conveyor belt, and understand its condition and performance over time.\n",
            "\n",
            "The usefulness of unstructured data for knowledge capture and retrieval is also evident in its ability to provide context and meaning to data. By analyzing unstructured data, it is possible to identify patterns and trends that may not be apparent from more structured data. For instance, by analyzing images and sensor data, it is possible to identify changes in a physical asset's condition over time, and understand the factors that contribute to those changes. This can be particularly useful in industries such as manufacturing, where understanding the performance and condition of physical assets is critical to optimizing production and reducing downtime.\n",
            "\n",
            "In addition to its value for knowledge capture and retrieval, unstructured data also holds significant potential for improving data discovery and search. By creating a knowledge graph that can be used to make connections and inferences between different pieces of information, it is possible to create a more intuitive and user-friendly search experience. For example, by analyzing unstructured data, it is possible to identify relationships between different pieces of information, and provide users with a more comprehensive understanding of the data and its context. This can be particularly useful in industries such as healthcare, where understanding the relationships between different pieces of information is critical to making informed decisions.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Details:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Model**: GROQ LLaMA 3.2 11b Vision Preview"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token Limit**: 8192"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Completion Token Limit**: 820"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Groq Llama 3.3 specification."
      ],
      "metadata": {
        "id": "EsYM3Q_eklbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    specification_id = await create_groq_specification(enums.GroqModels.LLAMA_3_3_70B)\n",
        "\n",
        "    if specification_id is not None:\n",
        "        print(f'Created Groq Llama 3.3 specification [{specification_id}].')\n",
        "\n",
        "        conversation_id = await create_conversation(specification_id)\n",
        "\n",
        "        if conversation_id is not None:\n",
        "            print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "            message, details = await prompt_conversation(conversation_id, prompt)\n",
        "\n",
        "            if message is not None:\n",
        "                display(Markdown(f'### Completion took [{message.completion_time}]:'))\n",
        "                display(Markdown(f'**User:**\\n{prompt}'))\n",
        "                display(Markdown(f'**Assistant:**'))\n",
        "                print(message.message)\n",
        "                print()\n",
        "\n",
        "            if details is not None:\n",
        "                display(Markdown('### Details:'))\n",
        "                display(Markdown(f'**Model**: {details.model_service} {details.model}'))\n",
        "                display(Markdown(f'**Token Limit**: {details.token_limit}'))\n",
        "                display(Markdown(f'**Completion Token Limit**: {details.completion_token_limit}'))\n",
        "\n",
        "            await delete_conversation(conversation_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "sW6pnOtrkodS",
        "outputId": "d7cef18a-408a-4971-f7a5-aef33cb9d1ed"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Groq Llama 3.3 specification [100afb5b-3998-47a1-806d-661eb5bcc697].\n",
            "Created conversation [06189990-fe41-4bfe-a9ba-95358fb7aaaf].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Completion took [PT3.3312745S]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User:**\nIn 3-5 detailed paragraphs, explain unstructured data and its usefulness for knowledge capture and retrieval."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstructured data refers to a broad set of data that does not have a predefined format or organization, making it difficult to search and analyze using traditional methods. This type of data can come in various forms, including images, audio, videos, documents, emails, and social media posts. Despite its lack of structure, unstructured data can be incredibly valuable for knowledge capture and retrieval, as it often contains rich and nuanced information that can provide insights and context. For instance, an image of a piece of equipment can reveal details about its condition, location, and surroundings, while an audio recording of a meeting can capture discussions, decisions, and action items.\n",
            "\n",
            "The usefulness of unstructured data lies in its ability to provide a more complete and accurate picture of a particular situation or phenomenon. By analyzing unstructured data, organizations can gain a deeper understanding of their customers, operations, and environments, which can inform decision-making and drive business outcomes. For example, a company that analyzes images of its products can identify trends and patterns in customer behavior, such as how products are being used or what features are most popular. Similarly, a healthcare organization that analyzes medical images can diagnose diseases more accurately and develop more effective treatment plans. Unstructured data can also be used to identify relationships and connections between different pieces of information, which can reveal new insights and patterns that may not be apparent through traditional analysis.\n",
            "\n",
            "To extract value from unstructured data, organizations can use various techniques, such as natural language processing, computer vision, and machine learning. These techniques can help to identify and extract relevant information from unstructured data, such as keywords, entities, and sentiment. For instance, natural language processing can be used to analyze text-based data, such as emails or social media posts, to identify sentiment and tone, while computer vision can be used to analyze images and videos to identify objects, people, and patterns. By applying these techniques to unstructured data, organizations can create structured data that can be easily searched, analyzed, and visualized, providing a more complete and accurate understanding of their operations and environments.\n",
            "\n",
            "The concept of first, second, and third-order metadata is also relevant to unstructured data. First-order metadata refers to the basic information that is embedded in a file, such as the file name, format, and size. Second-order metadata refers to the information that can be extracted from the file itself, such as keywords, entities, and sentiment. Third-order metadata refers to the inferences and relationships that can be derived from the data, such as connections between different pieces of information or patterns and trends. By analyzing unstructured data and extracting metadata, organizations can create a knowledge graph that provides a comprehensive and nuanced understanding of their operations and environments.\n",
            "\n",
            "In addition to its usefulness for knowledge capture and retrieval, unstructured data can also be used to create a knowledge hub that provides a centralized and accessible repository of information. This knowledge hub can be used to support decision-making, improve collaboration, and drive innovation, by providing a single source of truth that can be accessed and shared across the organization. By leveraging unstructured data and creating a knowledge hub, organizations can unlock new insights and opportunities, and gain a competitive advantage in their respective markets. Overall, unstructured data is a rich and valuable resource that can provide a more complete and accurate understanding of a particular situation or phenomenon, and its usefulness for knowledge capture and retrieval cannot be overstated.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Details:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Model**: GROQ LLaMA 3.3 70b"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token Limit**: 8192"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Completion Token Limit**: 820"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Anthropic Sonnet 3.5 specification."
      ],
      "metadata": {
        "id": "icBZWz9-lQVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    specification_id = await create_anthropic_specification(enums.AnthropicModels.CLAUDE_3_5_SONNET_20241022)\n",
        "\n",
        "    if specification_id is not None:\n",
        "        print(f'Created Anthropic Sonnet 3.5 specification [{specification_id}].')\n",
        "\n",
        "        conversation_id = await create_conversation(specification_id)\n",
        "\n",
        "        if conversation_id is not None:\n",
        "            print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "            message, details = await prompt_conversation(conversation_id, prompt)\n",
        "\n",
        "            if message is not None:\n",
        "                display(Markdown(f'### Completion took [{message.completion_time}]:'))\n",
        "                display(Markdown(f'**User:**\\n{prompt}'))\n",
        "                display(Markdown(f'**Assistant:**'))\n",
        "                print(message.message)\n",
        "                print()\n",
        "\n",
        "            if details is not None:\n",
        "                display(Markdown('### Details:'))\n",
        "                display(Markdown(f'**Model**: {details.model_service} {details.model}'))\n",
        "                display(Markdown(f'**Token Limit**: {details.token_limit}'))\n",
        "                display(Markdown(f'**Completion Token Limit**: {details.completion_token_limit}'))\n",
        "\n",
        "            await delete_conversation(conversation_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "XZql4RkJllb9",
        "outputId": "a6032a15-007f-4b6d-dc4b-fe073ae560f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Anthropic Sonnet 3.5 specification [9205cf85-52e2-469c-ae0e-1bbe7aed3965].\n",
            "Created conversation [164b2f67-1222-403e-a027-ddda6e128fb6].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Completion took [PT11.0075589S]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User:**\nIn 3-5 detailed paragraphs, explain unstructured data and its usefulness for knowledge capture and retrieval."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstructured data encompasses a broad range of file-based content including images, audio, 3D geometry, point clouds, documents, and emails. While these files have defined structures and formats, they are considered \"unstructured\" because their valuable content and meaning requires additional processing and analysis to extract. This data often contains rich metadata at multiple levels - from basic file headers and technical details to deeper semantic meaning derived through advanced processing.\n",
            "\n",
            "The real power of unstructured data emerges through the creation of knowledge graphs and relationship networks. By analyzing the content through techniques like computer vision, natural language processing, and audio transcription, organizations can extract meaningful insights and create connections between different pieces of information. This allows for dynamic discovery of relationships between people, places, things and concepts that may not be immediately obvious when looking at individual files in isolation. The ability to spider out and find new connections while maintaining the flexibility to add new relationship types makes knowledge graphs particularly valuable for working with unstructured data.\n",
            "\n",
            "A key challenge many organizations face is that their unstructured data often becomes \"dark data\" over time - valuable information that gets archived but isn't actively used or analyzed. By implementing systems to process and index this historical content alongside new data, companies can uncover trends, patterns and insights across their entire data history. This is especially powerful when combining different types of related content, such as maintenance documentation, inspection photos, and recorded meetings about physical assets or locations. The temporal and spatial relationships between these diverse content types create rich context that enables more intelligent search and analysis capabilities beyond simple keyword matching.\n",
            "\n",
            "The value multiplies when organizations can move beyond basic metadata extraction to what's called \"third-order metadata\" - making intelligent inferences by connecting information across multiple sources and systems. For example, identifying equipment in an inspection photo and automatically linking it to maintenance records and documentation in other enterprise systems. This contextual enrichment process can theoretically continue expanding as new relationships and insights are discovered, though practical considerations around compute costs and diminishing returns typically determine appropriate boundaries for specific use cases.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Details:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Model**: ANTHROPIC Claude 3.5 Sonnet (10-22-2024 version)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token Limit**: 200000"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Completion Token Limit**: 8191"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Anthropic Haiku 3.5 specification."
      ],
      "metadata": {
        "id": "XDvVHelWnPj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    specification_id = await create_anthropic_specification(enums.AnthropicModels.CLAUDE_3_5_HAIKU_20241022)\n",
        "\n",
        "    if specification_id is not None:\n",
        "        print(f'Created Anthropic Haiku 3.5 specification [{specification_id}].')\n",
        "\n",
        "        conversation_id = await create_conversation(specification_id)\n",
        "\n",
        "        if conversation_id is not None:\n",
        "            print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "            message, details = await prompt_conversation(conversation_id, prompt)\n",
        "\n",
        "            if message is not None:\n",
        "                display(Markdown(f'### Completion took [{message.completion_time}]:'))\n",
        "                display(Markdown(f'**User:**\\n{prompt}'))\n",
        "                display(Markdown(f'**Assistant:**'))\n",
        "                print(message.message)\n",
        "                print()\n",
        "\n",
        "            if details is not None:\n",
        "                display(Markdown('### Details:'))\n",
        "                display(Markdown(f'**Model**: {details.model_service} {details.model}'))\n",
        "                display(Markdown(f'**Token Limit**: {details.token_limit}'))\n",
        "                display(Markdown(f'**Completion Token Limit**: {details.completion_token_limit}'))\n",
        "\n",
        "            await delete_conversation(conversation_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "Ubjq7DYFnR26",
        "outputId": "50822745-3b3a-4e27-f7f5-8b506c8725f6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Anthropic Haiku 3.5 specification [fe01f6c9-ce5e-402c-a142-8f3adeac5b85].\n",
            "Created conversation [7a15fea4-89ea-4b52-ada1-35d4b9637c41].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Completion took [PT13.1650699S]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User:**\nIn 3-5 detailed paragraphs, explain unstructured data and its usefulness for knowledge capture and retrieval."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstructured data represents a broad category of digital information that includes imagery, audio, 3D geometry, point clouds, documents, and emails, which traditionally have been challenging to systematically analyze and extract insights from. Unlike structured data stored in traditional databases, unstructured data requires sophisticated processing techniques to transform raw files into meaningful, searchable information. The key challenge is converting these diverse file types into a format where relationships, context, and insights can be discovered and leveraged.\n",
            "\n",
            "The process of working with unstructured data involves multiple layers of metadata extraction, starting with \"first order\" metadata like file headers and embedded information, progressing to \"second order\" metadata through techniques like object detection and computer vision, and ultimately reaching \"third order\" metadata where machine learning can create sophisticated inferences and connections. For example, an image of a conveyor belt can be transformed from a simple file into a rich data point that can be linked to enterprise databases, geospatial coordinates, and historical maintenance records, creating a comprehensive understanding of the asset.\n",
            "\n",
            "Knowledge graphs play a crucial role in making unstructured data valuable, as they allow dynamic connections between different pieces of information. By creating \"edges\" between entities, organizations can build complex networks of understanding that transcend traditional database limitations. This approach enables semantic search, trend analysis, and the ability to discover insights across massive archives of data that would otherwise remain unused or \"go dark\". The ultimate goal is to transform unstructured data from passive storage into an active, discoverable resource that can drive decision-making and provide contextual intelligence.\n",
            "\n",
            "Practical applications of unstructured data processing span multiple industries, from aerial survey companies managing geospatial imagery to property inspection services automatically analyzing maintenance photos. By implementing advanced metadata extraction and knowledge graph technologies, organizations can triage large data collections, automate discovery processes, and generate alerts based on emerging trends or anomalies. The potential is particularly powerful for industries dealing with complex, multi-source data environments where traditional search and indexing methods fall short.\n",
            "\n",
            "The future of unstructured data management looks increasingly sophisticated, with emerging technologies enabling more nuanced, context-aware data processing. Machine learning models are becoming more specialized, allowing for increasingly precise categorization and inference. As computational capabilities expand and metadata extraction techniques improve, the ability to create rich, interconnected knowledge networks from previously unmanageable data sources will continue to evolve, transforming how organizations understand, utilize, and derive value from their digital information assets.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Details:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Model**: ANTHROPIC Claude 3.5 Haiku (10-22-2024 version)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token Limit**: 200000"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Completion Token Limit**: 8191"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Deepseek Chat specification."
      ],
      "metadata": {
        "id": "IxTIcseWlQ2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    specification_id = await create_deepseek_specification(enums.DeepseekModels.CHAT)\n",
        "\n",
        "    if specification_id is not None:\n",
        "        print(f'Created Deepseek Chat specification [{specification_id}].')\n",
        "\n",
        "        conversation_id = await create_conversation(specification_id)\n",
        "\n",
        "        if conversation_id is not None:\n",
        "            print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "            message, details = await prompt_conversation(conversation_id, prompt)\n",
        "\n",
        "            if message is not None:\n",
        "                display(Markdown(f'### Completion took [{message.completion_time}]:'))\n",
        "                display(Markdown(f'**User:**\\n{prompt}'))\n",
        "                display(Markdown(f'**Assistant:**'))\n",
        "                print(message.message)\n",
        "                print()\n",
        "\n",
        "            if details is not None:\n",
        "                display(Markdown('### Details:'))\n",
        "                display(Markdown(f'**Model**: {details.model_service} {details.model}'))\n",
        "                display(Markdown(f'**Token Limit**: {details.token_limit}'))\n",
        "                display(Markdown(f'**Completion Token Limit**: {details.completion_token_limit}'))\n",
        "\n",
        "            await delete_conversation(conversation_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "UJqIL7Uplq7G",
        "outputId": "5714a327-be65-48c0-aaf8-cc9d7d8efbc0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Deepseek Chat specification [47c3307f-5fbc-4b9f-bf68-862c913eb6ad].\n",
            "Created conversation [8157b083-16bf-4919-b321-f58c9f6af036].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Completion took [PT12.656048S]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User:**\nIn 3-5 detailed paragraphs, explain unstructured data and its usefulness for knowledge capture and retrieval."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstructured data refers to information that does not have a predefined data model or is not organized in a structured format like a database. It encompasses a wide range of file types, including images, audio, videos, 3D geometry, point clouds, documents, emails, and more. Unlike structured data, which is neatly organized into rows and columns, unstructured data is often raw and unprocessed, making it challenging to analyze directly. However, it is incredibly valuable because it captures real-world information in its most natural form, such as photos of physical assets, maintenance reports, or even recorded Zoom meetings about inspections. This data is often generated by drones, robots, mobile phones, or other devices that capture real-world assets, making it a critical resource for industries like geospatial, engineering, and infrastructure.\n",
            "\n",
            "The usefulness of unstructured data lies in its ability to provide rich, contextual insights when properly processed and analyzed. For instance, metadata embedded within filessuch as EXIF data in images or XMP data in documentscan serve as a starting point for understanding the content. This \"first-order metadata\" includes basic information like file headers, GPS locations, or timestamps. By leveraging advanced techniques like computer vision, natural language processing, and machine learning, unstructured data can be further enriched to extract \"second-order metadata,\" such as object detection in images or sentiment analysis in documents. This allows organizations to identify specific elements within the data, like recognizing a conveyor belt in a maintenance photo or understanding the tone of a report.\n",
            "\n",
            "The true power of unstructured data emerges when it is contextualized and linked to other data sources, creating what is known as \"third-order metadata.\" For example, an image of a conveyor belt can be linked to an SAP database entry, providing insights into its maintenance history or operational status. This contextualization enables the creation of knowledge graphs, which dynamically connect entities like people, places, and things. Knowledge graphs allow organizations to uncover relationships and trends across vast datasets, such as identifying patterns in drone imagery over time or correlating maintenance reports with asset performance. This capability is particularly valuable for industries that rely on historical data for decision-making, as it transforms \"dark data\"information that is captured but unusedinto actionable insights.\n",
            "\n",
            "Unstructured data also plays a crucial role in enabling semantic search, which goes beyond simple keyword matching to understand the meaning and context of queries. For example, an oil and gas company might use semantic search to find all documents, images, and videos related to a specific piece of equipment, even if the search terms are not explicitly mentioned in the file names or text. This level of searchability is essential for organizations dealing with massive volumes of data, as it allows them to quickly locate relevant information and make informed decisions. Additionally, unstructured data can be integrated with other data formats, such as CAD drawings or PDF documentation, to create a comprehensive view of real-world assets.\n",
            "\n",
            "In summary, unstructured data is a treasure trove of information that, when properly harnessed, can drive innovation and efficiency across industries. By extracting and contextualizing metadata, organizations can unlock the full potential of their data, transforming it from a collection of disparate files into a cohesive knowledge hub. This not only enhances data discovery and retrieval but also enables predictive analytics, trend analysis, and automated alerting, ultimately empowering businesses to make smarter, data-driven decisions.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Details:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Model**: DEEPSEEK Deepseek Chat"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token Limit**: 128000"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Completion Token Limit**: 4095"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Google Gemini 1.5 Flash specification."
      ],
      "metadata": {
        "id": "59Gy3LEvmg58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    specification_id = await create_google_specification(enums.GoogleModels.GEMINI_1_5_FLASH_002)\n",
        "\n",
        "    if specification_id is not None:\n",
        "        print(f'Created Google Gemini 1.5 Flash specification [{specification_id}].')\n",
        "\n",
        "        conversation_id = await create_conversation(specification_id)\n",
        "\n",
        "        if conversation_id is not None:\n",
        "            print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "            message, details = await prompt_conversation(conversation_id, prompt)\n",
        "\n",
        "            if message is not None:\n",
        "                display(Markdown(f'### Completion took [{message.completion_time}]:'))\n",
        "                display(Markdown(f'**User:**\\n{prompt}'))\n",
        "                display(Markdown(f'**Assistant:**'))\n",
        "                print(message.message)\n",
        "                print()\n",
        "\n",
        "            if details is not None:\n",
        "                display(Markdown('### Details:'))\n",
        "                display(Markdown(f'**Model**: {details.model_service} {details.model}'))\n",
        "                display(Markdown(f'**Token Limit**: {details.token_limit}'))\n",
        "                display(Markdown(f'**Completion Token Limit**: {details.completion_token_limit}'))\n",
        "\n",
        "            await delete_conversation(conversation_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "W3rdgPzImjeV",
        "outputId": "9d76a720-9167-415a-a7ea-4366d216239b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Google Gemini 1.5 Flash specification [5410dc05-d3a5-4525-bef3-0e1fe4cf71e5].\n",
            "Created conversation [327e0f1f-a27a-4eb4-b9a4-1d0d6acb16ec].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Completion took [PT4.298679S]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User:**\nIn 3-5 detailed paragraphs, explain unstructured data and its usefulness for knowledge capture and retrieval."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstructured data encompasses a wide range of information formats, including images, audio, 3D models, documents, and emails, lacking the rigid structure of traditional databases.  While often perceived as disorganized, this data is incredibly rich in potential insights.  The challenge lies in effectively processing and extracting meaningful information from these diverse sources.  This is where the concept of metadata becomes crucial, allowing for the organization and contextualization of unstructured data.  Different levels of metadata exist, from basic file headers (first-order) to complex inferences derived from machine learning (third-order), enabling increasingly sophisticated knowledge capture.\n",
            "\n",
            "The process of transforming unstructured data into usable knowledge involves several key steps.  First, initial metadata is extracted from the files themselves.  Then, more detailed information is derived through techniques like object detection in images or sentiment analysis in text.  Finally, this information is linked and contextualized, often using knowledge graphs, to create a network of interconnected data points.  This allows for semantic search, going beyond simple keyword matching to understand the relationships between different pieces of information.  This capability is particularly valuable in domains like geospatial analysis, where connecting images, sensor data, and documents can provide a comprehensive understanding of real-world assets.\n",
            "\n",
            "The usefulness of unstructured data for knowledge capture and retrieval is significant.  By connecting disparate data points, organizations can gain a holistic view of their assets, operations, and environments.  For example, an aerial survey company might use this approach to manage and analyze drone imagery, linking images to maintenance reports and other relevant documents.  This allows for more efficient data discovery, trend analysis, and automated alerting, ultimately improving decision-making and operational efficiency.  The ability to connect historical data with current information provides valuable context, revealing patterns and insights that would be impossible to discern from isolated data sources.\n",
            "\n",
            "Furthermore, the ability to create and update knowledge graphs dynamically offers significant advantages over traditional database systems.  The flexibility of knowledge graphs allows for the easy addition of new relationships and data points without requiring complex schema migrations.  This adaptability is crucial in dealing with the ever-increasing volume and variety of unstructured data generated in today's world.  The development of tools and techniques for processing and analyzing unstructured data is rapidly advancing, unlocking the vast potential of this previously underutilized resource.\n",
            "\n",
            "Finally, the potential for public access to curated unstructured data, through APIs and public catalogs, opens up exciting possibilities for collaboration and data sharing.  Imagine a system where crowdsourced data, such as images of potholes, could be easily integrated and analyzed to improve infrastructure management.  This highlights the transformative potential of unstructured data for knowledge creation and dissemination, driving innovation and efficiency across various sectors.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Details:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Model**: GOOGLE Gemini 1.5 Flash (002 version)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token Limit**: 1048576"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Completion Token Limit**: 8191"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Google Gemini 2.0 Flash specification."
      ],
      "metadata": {
        "id": "7I4ToTS-mss2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    specification_id = await create_google_specification(enums.GoogleModels.GEMINI_2_0_FLASH_EXPERIMENTAL)\n",
        "\n",
        "    if specification_id is not None:\n",
        "        print(f'Created Google Gemini 2.0 Flash specification [{specification_id}].')\n",
        "\n",
        "        conversation_id = await create_conversation(specification_id)\n",
        "\n",
        "        if conversation_id is not None:\n",
        "            print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "            message, details = await prompt_conversation(conversation_id, prompt)\n",
        "\n",
        "            if message is not None:\n",
        "                display(Markdown(f'### Completion took [{message.completion_time}]:'))\n",
        "                display(Markdown(f'**User:**\\n{prompt}'))\n",
        "                display(Markdown(f'**Assistant:**'))\n",
        "                print(message.message)\n",
        "                print()\n",
        "\n",
        "            if details is not None:\n",
        "                display(Markdown('### Details:'))\n",
        "                display(Markdown(f'**Model**: {details.model_service} {details.model}'))\n",
        "                display(Markdown(f'**Token Limit**: {details.token_limit}'))\n",
        "                display(Markdown(f'**Completion Token Limit**: {details.completion_token_limit}'))\n",
        "\n",
        "            await delete_conversation(conversation_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "muueqqIAmosk",
        "outputId": "381ce77e-d7db-4ecf-a6a0-9768253acafb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Google Gemini 2.0 Flash specification [35604323-8d5d-4532-8bfe-f0293b7f6259].\n",
            "Created conversation [b8462250-d561-48de-b8cb-ca4e90d6c95e].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Completion took [PT4.137467S]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User:**\nIn 3-5 detailed paragraphs, explain unstructured data and its usefulness for knowledge capture and retrieval."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstructured data encompasses a wide range of file-based information, including imagery, audio, 3D models, documents, and emails. While these files have a defined structure, the content within them is often considered unstructured because it lacks a predefined format for easy analysis. This is in contrast to structured data, which is organized in databases with clear schemas. The challenge with unstructured data lies in extracting meaningful insights from the raw content, which requires parsing files and identifying relevant information.\n",
            "\n",
            "Metadata plays a crucial role in making unstructured data more accessible and useful. First-order metadata, such as EXIF data in images or file headers, provides basic information about the file itself. Second-order metadata involves analyzing the content of the file, such as identifying objects in an image or extracting terms from a document. Third-order metadata goes further by creating inferences and connections between the data and real-world entities, such as linking a conveyor belt in an image to its corresponding entry in a database. This layered approach to metadata allows for a more comprehensive understanding of the data.\n",
            "\n",
            "The real power of unstructured data lies in its ability to create knowledge graphs. By extracting entities, relationships, and context from the data, it's possible to build dynamic networks that connect various pieces of information. These knowledge graphs enable semantic search, allowing users to find information based on meaning rather than just keywords. For example, a user could search for all maintenance reports related to a specific piece of equipment, even if the reports don't explicitly mention the equipment by name. This capability is particularly valuable for organizations that have large volumes of data that are difficult to navigate using traditional search methods.\n",
            "\n",
            "Furthermore, unstructured data can be used to identify trends and patterns over time. By analyzing historical data, organizations can gain insights into how things have changed and make more informed decisions. For example, an aerial survey company could use its historical data to track changes in land use or identify areas that require maintenance. The ability to connect past and present data is crucial for understanding the full context of a situation. The system can also be used to provide alerts based on trends, such as an increase in observations of a particular issue.\n",
            "\n",
            "In conclusion, unstructured data, when properly analyzed and contextualized, can be a valuable source of knowledge. By leveraging metadata, knowledge graphs, and trend analysis, organizations can unlock the hidden potential of their data and gain a deeper understanding of the world around them. This approach moves beyond simple file storage and enables a more dynamic and insightful way of working with information.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Details:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Model**: GOOGLE Gemini 2.0 Flash (Experimental)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token Limit**: 1048576"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Completion Token Limit**: 8191"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Cohere R7B specification."
      ],
      "metadata": {
        "id": "i1jCTLbBmuV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    specification_id = await create_cohere_specification(enums.CohereModels.COMMAND_R7_B_202412)\n",
        "\n",
        "    if specification_id is not None:\n",
        "        print(f'Created Cohere R7B specification [{specification_id}].')\n",
        "\n",
        "        conversation_id = await create_conversation(specification_id)\n",
        "\n",
        "        if conversation_id is not None:\n",
        "            print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "            message, details = await prompt_conversation(conversation_id, prompt)\n",
        "\n",
        "            if message is not None:\n",
        "                display(Markdown(f'### Completion took [{message.completion_time}]:'))\n",
        "                display(Markdown(f'**User:**\\n{prompt}'))\n",
        "                display(Markdown(f'**Assistant:**'))\n",
        "                print(message.message)\n",
        "                print()\n",
        "\n",
        "            if details is not None:\n",
        "                display(Markdown('### Details:'))\n",
        "                display(Markdown(f'**Model**: {details.model_service} {details.model}'))\n",
        "                display(Markdown(f'**Token Limit**: {details.token_limit}'))\n",
        "                display(Markdown(f'**Completion Token Limit**: {details.completion_token_limit}'))\n",
        "\n",
        "            await delete_conversation(conversation_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "i4n_Kb9hmwum",
        "outputId": "ae3a4a12-dd79-4aad-9d2a-9295bf438d0c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Cohere R7B specification [0004b180-1b7a-4a88-852e-2db2c60c125c].\n",
            "Created conversation [7bbfaf0d-4d88-4007-a291-ccf7a84f57cf].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Completion took [PT6.9068167S]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User:**\nIn 3-5 detailed paragraphs, explain unstructured data and its usefulness for knowledge capture and retrieval."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstructured data encompasses a broad range of information, including imagery, audio, 3D geometry, point clouds, documents, and emails. It is characterized by its lack of a predefined structure, which sets it apart from structured data. While structured data is organized and easily searchable, unstructured data requires additional processing to extract meaningful insights. This is where metadata comes into play, acting as a bridge between the raw data and the information it contains.\n",
            "\n",
            "The value of unstructured data lies in its ability to capture and contextualize real-world entities, such as people, places, and things. By adding metadata, which provides additional context, unstructured data can be transformed into a powerful tool for knowledge capture and retrieval. For example, in the geospatial domain, unstructured data can include satellite imagery, drone footage, and mobile phone data, all of which can be enriched with metadata to create a comprehensive knowledge graph.\n",
            "\n",
            "One of the key advantages of unstructured data is its potential to create a network effect. By connecting various data sources and entities, knowledge graphs can be built, enabling semantic searches and dynamic references. This allows for the creation of new edges and relationships, making it easier to find and analyze information. For instance, in the context of a podcast, unstructured data can be analyzed to extract entities like topics, people, and organizations, and then linked to create a knowledge graph.\n",
            "\n",
            "The process of knowledge capture and retrieval involves several stages. First, unstructured data is ingested and processed to extract initial metadata, known as first-order metadata. This metadata provides basic information about the data, such as file headers or EXIF data. Second-order metadata involves more complex analysis, such as object detection or sentiment analysis, to identify specific entities and relationships. Finally, third-order metadata is created through machine learning and inference, allowing for the creation of dynamic edges and connections between entities.\n",
            "\n",
            "The usefulness of unstructured data extends beyond knowledge capture and retrieval. It can also facilitate data triage, alerting, and trend analysis. By analyzing observations and patterns over time, organizations can identify trends, make predictions, and take proactive measures. Additionally, unstructured data can be exposed as a public catalog, making it searchable and discoverable for users. This enables collaboration and knowledge sharing, as seen in the example of a crowd-sourced app for reporting potholes.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Details:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Model**: COHERE Command R7B (2024-12 version)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token Limit**: 128000"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Completion Token Limit**: 4095"
          },
          "metadata": {}
        }
      ]
    }
  ]
}