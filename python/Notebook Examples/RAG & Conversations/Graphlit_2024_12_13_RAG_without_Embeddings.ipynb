{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyP3x/r2ksSNqNQIAlxdGVo2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_12_13_RAG_without_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to use Graphlit for RAG without doing embedding similarity search."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a43a1b75-677f-433d-d9fc-3b201271ecc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting graphlit-client\n",
            "  Downloading graphlit_client-1.0.20241213002-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.3)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.10.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n",
            "Downloading graphlit_client-1.0.20241213002-py3-none-any.whl (228 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.6/228.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: graphlit-client\n",
            "Successfully installed graphlit-client-1.0.20241213002\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Graphlit"
      ],
      "metadata": {
        "id": "abV1114jL-bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "async def ingest_uri(uri: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        # Using synchronous mode, so the notebook waits for the content to be ingested\n",
        "        response = await graphlit.client.ingest_uri(uri=uri, is_synchronous=True)\n",
        "\n",
        "        return response.ingest_uri.id if response.ingest_uri is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def create_google_specification(model: enums.GoogleModels):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"Google [{str(model)}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.GOOGLE,\n",
        "        google=input_types.GoogleModelPropertiesInput(\n",
        "            model=model,\n",
        "        ),\n",
        "        searchType=enums.ConversationSearchTypes.NONE # bypass semantic search from user prompt\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_conversation(specification_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.ConversationInput(\n",
        "        name=\"Conversation\",\n",
        "        specification=input_types.EntityReferenceInput(\n",
        "            id=specification_id\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_conversation(input)\n",
        "\n",
        "        return response.create_conversation.id if response.create_conversation is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_conversation(conversation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    if conversation_id is not None:\n",
        "        _ = await graphlit.client.delete_conversation(conversation_id)\n",
        "\n",
        "async def prompt_conversation(conversation_id: str, prompt: str):\n",
        "    if graphlit.client is None:\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.prompt_conversation(prompt, conversation_id, include_details=True)\n",
        "\n",
        "        message = response.prompt_conversation.message.message if response.prompt_conversation is not None and response.prompt_conversation.message is not None else None\n",
        "        details = response.prompt_conversation.details if response.prompt_conversation is not None else None\n",
        "\n",
        "        return message, details\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None, None\n",
        "\n",
        "async def delete_all_specifications():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_specifications(is_synchronous=True)\n",
        "\n",
        "async def delete_all_conversations():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_conversations(is_synchronous=True)\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)\n"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute Graphlit example"
      ],
      "metadata": {
        "id": "srzhQt4COLVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, HTML\n",
        "import time\n",
        "\n",
        "# Remove any existing contents, conversations and specifications; only needed for notebook example\n",
        "await delete_all_conversations()\n",
        "await delete_all_specifications()\n",
        "await delete_all_contents()\n",
        "\n",
        "print('Deleted all contents, conversations and specifications.')\n",
        "\n",
        "# Ingest a few papers\n",
        "\n",
        "content_id = await ingest_uri(uri=\"https://graphlitplatform.blob.core.windows.net/test/documents/Attention%20Is%20All%20You%20Need.1706.03762.pdf\")\n",
        "\n",
        "if content_id is not None:\n",
        "    print(f'Ingested content [{content_id}]')\n",
        "\n",
        "content_id = await ingest_uri(uri=\"https://graphlitplatform.blob.core.windows.net/test/documents/Unifying%20Large%20Language%20Models%20and%20Knowledge%20Graphs%20A%20Roadmap-2306.08302.pdf\")\n",
        "\n",
        "if content_id is not None:\n",
        "    print(f'Ingested content [{content_id}]')\n",
        "\n",
        "content_id = await ingest_uri(uri=\"https://graphlitplatform.blob.core.windows.net/test/documents/2404.17723v2.pdf\")\n",
        "\n",
        "if content_id is not None:\n",
        "    print(f'Ingested content [{content_id}]')\n"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38028629-47ef-4707-8a71-01b69e0abf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all contents, conversations and specifications.\n",
            "Ingested content [38b81ac3-b662-46a6-9dcf-2a628e350193]\n",
            "Ingested content [af3c86b0-070a-4711-89bd-c643ef159e4e]\n",
            "Ingested content [1b68039d-20d7-4c2c-8855-2476ae77749d]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Specify the RAG prompt\n",
        "    prompt = \"\"\"\n",
        "    Write me a long, detailed blog post. Explain RAG and its usefulness for knowledge capture and retrieval.\n",
        "    Be specific about the usage of LLMs. Explain how knowledge graphs can be used for RAG, as well. Add quotes from the original papers.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "-9kBDUIdbhIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Google Gemini 1.5 Flash specification, and prompt RAG conversation."
      ],
      "metadata": {
        "id": "4snKjK2ycVKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    specification_id = await create_google_specification(enums.GoogleModels.GEMINI_1_5_FLASH_002)\n",
        "\n",
        "    if specification_id is not None:\n",
        "        print(f'Created specification [{specification_id}].')\n",
        "\n",
        "        conversation_id = await create_conversation(specification_id)\n",
        "\n",
        "        if conversation_id is not None:\n",
        "            print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "            message, details = await prompt_conversation(conversation_id, prompt)\n",
        "\n",
        "            if message is not None:\n",
        "                display(Markdown('### Conversation:'))\n",
        "                display(Markdown(f'**User:**\\n{prompt}'))\n",
        "                display(Markdown(f'**Assistant:**\\n{message}'))\n",
        "                print()\n",
        "\n",
        "            if details is not None:\n",
        "                display(Markdown('### Details:'))\n",
        "                display(Markdown(f'**Model**: {details.model_service} {details.model}'))\n",
        "                display(Markdown(f'**Token Limit**: {details.token_limit}'))\n",
        "                display(Markdown(f'**Completion Token Limit**: {details.completion_token_limit}'))\n",
        "\n",
        "                print()\n",
        "\n",
        "                # NOTE: uncomment to see the formatted sources provided to LLM\n",
        "                #if details.formatted_sources is not None:\n",
        "                    #display(Markdown(f'#### Formatted Sources:'))\n",
        "                    #print(details.formatted_sources)\n",
        "                    #print()\n",
        "\n",
        "            await delete_conversation(conversation_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L5AHVoKocVmD",
        "outputId": "3864e84d-335f-4f5f-84e1-0f94f63dec23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created specification [54770783-df15-4990-98ed-19dd690eb221].\n",
            "Created conversation [be507cee-8c7a-471c-af33-61f7e0b5257d].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Conversation:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**User:**\n\nWrite me a long, detailed blog post. Explain RAG and its usefulness for knowledge capture and retrieval. \nBe specific about the usage of LLMs. Explain how knowledge graphs can be used for RAG, as well. Add quotes from the original papers.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:**\n## Revolutionizing Knowledge Capture and Retrieval with Retrieval-Augmented Generation (RAG)\n\nIn today's data-rich world, efficiently capturing and retrieving relevant knowledge is paramount.  Retrieval-Augmented Generation (RAG) has emerged as a powerful technique, leveraging the strengths of large language models (LLMs) and structured knowledge bases to achieve this goal. This blog post delves into the intricacies of RAG, highlighting its capabilities and exploring its synergy with knowledge graphs.\n\n**Understanding RAG: A Synergistic Approach**\n\nRAG fundamentally alters the traditional approach to question answering. Instead of relying solely on the knowledge embedded within an LLM, RAG augments the LLM with external knowledge retrieved from a relevant knowledge base.  This external knowledge acts as context, enriching the LLM's understanding and enabling it to generate more accurate, informative, and contextually appropriate responses.  As Patrick Lewis et al. state in their seminal paper, \"Retrieval-augmented generation for knowledge-intensive NLP tasks,\" RAG aims to \"combine the strengths of retrieval-based and generation-based methods.\"  This means leveraging the ability of LLMs to generate fluent and coherent text while simultaneously grounding that text in factual information from an external source.\n\n**The Role of LLMs in RAG**\n\nLLMs are the heart of RAG, acting as sophisticated text generators.  They take the retrieved information as input, along with the original query, and generate a response.  The LLM's ability to understand context and generate human-quality text is crucial for creating meaningful and useful answers.  The process isn't simply pasting retrieved text; the LLM synthesizes the information, potentially summarizing, paraphrasing, or combining multiple sources to create a coherent response.  This is a key difference from simple retrieval systems.\n\n**Knowledge Graphs: Enhancing RAG's Precision**\n\nWhile RAG can utilize various knowledge bases, knowledge graphs (KGs) offer a particularly powerful approach. KGs represent information as a network of interconnected entities and relationships, providing a structured and semantically rich representation of knowledge.  This structured nature allows for more precise retrieval.  Instead of searching through unstructured text, RAG with KGs can navigate the graph to find relevant entities and relationships, significantly improving retrieval accuracy.\n\nA recent paper, \"Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering,\" highlights the advantages of using KGs in RAG for customer service. The authors explain how their system \"constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations.\" This approach avoids the limitations of treating a corpus of past issues as plain text, leading to improved retrieval accuracy and answer quality.\n\n**Illustrative Example: Customer Service**\n\nImagine a customer service scenario where a user asks a complex question about a product.  A traditional LLM might struggle to answer accurately if the necessary information isn't directly encoded in its training data.  However, a RAG system with a KG could retrieve relevant past support tickets, product documentation, or other relevant information from the KG.  The LLM would then use this retrieved information to generate a comprehensive and accurate response, potentially even referencing specific sections of the retrieved documents.\n\n**Challenges and Future Directions**\n\nWhile RAG offers significant advantages, challenges remain.  These include:\n\n* **KG Construction:** Creating and maintaining comprehensive and accurate KGs can be a laborious and resource-intensive process.\n* **Retrieval Efficiency:** Efficiently retrieving relevant information from large KGs is crucial for real-time performance.\n* **Hallucination Mitigation:** LLMs can sometimes generate factually incorrect information (\"hallucinations\").  RAG needs mechanisms to mitigate this risk.\n\nFuture research will likely focus on automating KG construction, improving retrieval efficiency, and developing robust methods for detecting and correcting hallucinations.  The integration of multi-modal LLMs, capable of processing information from various sources like images and videos, will further enhance RAG's capabilities.\n\n**Conclusion: A Promising Future**\n\nRAG, particularly when combined with KGs, represents a significant advancement in knowledge capture and retrieval.  By leveraging the strengths of LLMs and structured knowledge, RAG empowers systems to answer complex questions accurately and efficiently.  As research continues to address the existing challenges, RAG's impact across various domains will undoubtedly grow, transforming how we interact with and utilize information.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Details:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Model**: GOOGLE Gemini 1.5 Flash (002 version)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Token Limit**: 1048576"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Completion Token Limit**: 8191"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}