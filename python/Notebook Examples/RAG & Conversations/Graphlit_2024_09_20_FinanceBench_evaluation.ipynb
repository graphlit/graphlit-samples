{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OnDuzB7LElFQaeTTXDxrZgxT_zGUT2CY",
      "authorship_tag": "ABX9TyP711KqatOhwjaebSVlAnfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e383957af84541eba48d5c05dd482343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad92faad568b4e9ca365a20dce351b8b",
              "IPY_MODEL_b808b36f883142c6809d938804f94bd0",
              "IPY_MODEL_83a499e3763440009f802b778a83c876"
            ],
            "layout": "IPY_MODEL_9d4747d4b94141fbbfeb2490754aed40"
          }
        },
        "ad92faad568b4e9ca365a20dce351b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f4484660ef948cfa96618de125a797b",
            "placeholder": "​",
            "style": "IPY_MODEL_e6dfa0b08a1e4101ab6c613615b5b701",
            "value": "  2%"
          }
        },
        "b808b36f883142c6809d938804f94bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1764cb8b97424085a5701517d56362f0",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a296d86470c4f169f68a2dd2350d8ca",
            "value": 3
          }
        },
        "83a499e3763440009f802b778a83c876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0daa8aa69904b16b6e62ebbfb475783",
            "placeholder": "​",
            "style": "IPY_MODEL_fad508adfaf143a2924495a00f54d7d4",
            "value": " 3/150 [03:07&lt;2:43:12, 66.61s/it]"
          }
        },
        "9d4747d4b94141fbbfeb2490754aed40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4484660ef948cfa96618de125a797b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6dfa0b08a1e4101ab6c613615b5b701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1764cb8b97424085a5701517d56362f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a296d86470c4f169f68a2dd2350d8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0daa8aa69904b16b6e62ebbfb475783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fad508adfaf143a2924495a00f54d7d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_09_20_FinanceBench_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to evaluate RAG over the FinanceBench dataset using OpenAI or Anthropic models."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e20cc0e-8aba-4368-a215-f2cbb12e33cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphlit-client in /usr/local/lib/python3.10/dist-packages (1.0.20240914001)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.9.2)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.9.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (13.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize FinanceBench dataset"
      ],
      "metadata": {
        "id": "jYGr9uVrAltI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf financebench\n",
        "!git clone https://github.com/patronus-ai/financebench"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vizX8xM2AklN",
        "outputId": "c5a53233-448c-4532-cce5-149cdbb11b5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'financebench'...\n",
            "remote: Enumerating objects: 423, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 423 (delta 19), reused 38 (delta 16), pack-reused 376 (from 1)\u001b[K\n",
            "Receiving objects: 100% (423/423), 551.70 MiB | 15.86 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "Updating files: 100% (390/390), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize FinanceBench paths"
      ],
      "metadata": {
        "id": "J7rcx1zP9yVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_directory = \"financebench\"\n",
        "\n",
        "PATH_DATASET_JSONL = data_directory + \"/data/financebench_open_source.jsonl\"\n",
        "PATH_DOCUMENT_INFO_JSONL = data_directory + \"/data/financebench_document_information.jsonl\"\n",
        "PATH_RESULTS = data_directory + \"/results/\"\n",
        "PATH_PDFS = data_directory + \"/pdfs/\""
      ],
      "metadata": {
        "id": "jz7VlO8N9k3N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "DATASET_PORTION = \"OPEN_SOURCE\"\n",
        "\n",
        "# Load Full Dataset\n",
        "df_questions = pd.read_json(PATH_DATASET_JSONL, lines=True)\n",
        "df_meta = pd.read_json(PATH_DOCUMENT_INFO_JSONL, lines=True)\n",
        "df_full = pd.merge(df_questions, df_meta, on=\"doc_name\")\n",
        "\n",
        "# Get all docs\n",
        "df_questions = df_questions.sort_values('doc_name')\n",
        "ALL_DOCS = df_questions['doc_name'].unique().tolist()\n",
        "print(f\"Total number of distinct PDF: {len(ALL_DOCS)}\")\n",
        "\n",
        "# Select relevant dataset portion\n",
        "if DATASET_PORTION != \"ALL\":\n",
        "    df_questions = df_questions.loc[df_questions[\"dataset_subset_label\"]==DATASET_PORTION]\n",
        "print(f\"Number of questions: {len(df_questions)}\")\n",
        "\n",
        "# Check relevant documents\n",
        "df_questions = df_questions.sort_values('doc_name')\n",
        "docs = df_questions['doc_name'].unique().tolist()\n",
        "print(f\"Number of distinct PDF: {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEefHKcK-fo6",
        "outputId": "f93a2fef-54f3-462c-c997-ae9ba6ccd256"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of distinct PDF: 84\n",
            "Number of questions: 150\n",
            "Number of distinct PDF: 84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Graphlit"
      ],
      "metadata": {
        "id": "abV1114jL-bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "import base64\n",
        "import mimetypes\n",
        "\n",
        "# NOTE: for local files, load from disk and convert to Base64 data\n",
        "async def ingest_file(file_path: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        content_name, _ = os.path.splitext(file_name)\n",
        "\n",
        "        mime_type = mimetypes.guess_type(file_name)[0]\n",
        "\n",
        "        if mime_type is None:\n",
        "            print('Failed to infer MIME type')\n",
        "            return None\n",
        "\n",
        "        with open(file_path, \"rb\") as file:\n",
        "            file_content = file.read()\n",
        "\n",
        "            base64_content = base64.b64encode(file_content).decode('utf-8')\n",
        "\n",
        "            # Using synchronous mode, so the notebook waits for the content to be ingested\n",
        "            response = await graphlit.client.ingest_encoded_file(content_name, base64_content, mime_type, is_synchronous=True)\n",
        "\n",
        "            return response.ingest_encoded_file.id if response.ingest_encoded_file is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def create_anthropic_specification(model: enums.AnthropicModels, retrievalType: Optional[enums.RetrievalStrategyTypes] = None, enableRerank: bool = False, enableRevision: bool = False, revisionCount: Optional[int] = None,\n",
        "                                         enablePromptStrategy: Optional[bool] = None, promptType: Optional[enums.PromptStrategyTypes] = None, embedCitations: Optional[bool] = False):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"Anthropic [{str(model)}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.ANTHROPIC,\n",
        "        anthropic=input_types.AnthropicModelPropertiesInput(\n",
        "            model=model,\n",
        "            temperature=0.1,\n",
        "            completionTokenLimit=2048\n",
        "        ),\n",
        "        customInstructions='You are a financial analyst with a focused attention to detail. Provide specific details on financial data in your responses, but be concise in your answers. When providing a numeric or dollar value for your answer, only provide the value without any other commentary or explanation. Make sure to add currency symbol in front of values, where appropriate. Only rely on the content sources provided for your response. If you are unable to answer the question, given the content sources provided, answer with \"I was unable to answer given the provided sources.\"',\n",
        "        strategy=input_types.ConversationStrategyInput(\n",
        "            embedCitations=embedCitations\n",
        "        ),\n",
        "        promptStrategy=input_types.PromptStrategyInput(\n",
        "            type=promptType if promptType is not None else enums.PromptStrategyTypes.OPTIMIZE_SEARCH\n",
        "        ) if enablePromptStrategy else None,\n",
        "        retrievalStrategy=input_types.RetrievalStrategyInput(\n",
        "            type=retrievalType if retrievalType is not None else enums.RetrievalStrategyTypes.CHUNK\n",
        "        ),\n",
        "        revisionStrategy=input_types.RevisionStrategyInput(\n",
        "            type=enums.RevisionStrategyTypes.REVISE,\n",
        "            count=revisionCount if revisionCount is not None else 1\n",
        "        ) if enableRevision else None,\n",
        "        rerankingStrategy=input_types.RerankingStrategyInput(\n",
        "            serviceType=enums.RerankingModelServiceTypes.COHERE\n",
        "        ) if enableRerank else None\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_openai_specification(model: enums.OpenAIModels, retrievalType: Optional[enums.RetrievalStrategyTypes] = None, enableRerank: bool = False, enableRevision: bool = False, revisionCount: Optional[int] = None,\n",
        "                                         enablePromptStrategy: Optional[bool] = None, promptType: Optional[enums.PromptStrategyTypes] = None, embedCitations: Optional[bool] = False):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=f\"OpenAI [{str(model)}]\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.OPEN_AI,\n",
        "        openAI=input_types.OpenAIModelPropertiesInput(\n",
        "            model=model,\n",
        "            temperature=0.1,\n",
        "            completionTokenLimit=2048\n",
        "        ),\n",
        "        customInstructions='You are a financial analyst with a focused attention to detail. Provide specific details on financial data in your responses, but be concise in your answers. When providing a numeric or dollar value for your answer, only provide the value without any other commentary or explanation. Make sure to add currency symbol in front of values, where appropriate. Only rely on the content sources provided for your response. If you are unable to answer the question, given the content sources provided, answer with \"I was unable to answer given the provided sources.\"',\n",
        "        strategy=input_types.ConversationStrategyInput(\n",
        "            embedCitations=embedCitations\n",
        "        ),\n",
        "        promptStrategy=input_types.PromptStrategyInput(\n",
        "            type=promptType if promptType is not None else enums.PromptStrategyTypes.OPTIMIZE_SEARCH\n",
        "        ) if enablePromptStrategy else None,\n",
        "        retrievalStrategy=input_types.RetrievalStrategyInput(\n",
        "            type=retrievalType if retrievalType is not None else enums.RetrievalStrategyTypes.CHUNK\n",
        "        ),\n",
        "        revisionStrategy=input_types.RevisionStrategyInput(\n",
        "            type=enums.RevisionStrategyTypes.REVISE,\n",
        "            count=revisionCount if revisionCount is not None else 1\n",
        "        ) if enableRevision else None,\n",
        "        rerankingStrategy=input_types.RerankingStrategyInput(\n",
        "            serviceType=enums.RerankingModelServiceTypes.COHERE\n",
        "        ) if enableRerank else None\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_conversation(specification_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.ConversationInput(\n",
        "        name=\"Conversation\",\n",
        "        specification=input_types.EntityReferenceInput(\n",
        "            id=specification_id\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_conversation(input)\n",
        "\n",
        "        return response.create_conversation.id if response.create_conversation is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_conversation(conversation_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    if conversation_id is not None:\n",
        "        _ = await graphlit.client.delete_conversation(conversation_id)\n",
        "\n",
        "async def prompt_conversation(conversation_id: str, prompt: str):\n",
        "    if graphlit.client is None:\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.prompt_conversation(prompt, conversation_id)\n",
        "\n",
        "        message = response.prompt_conversation.message.message if response.prompt_conversation is not None and response.prompt_conversation.message is not None else None\n",
        "        citations = response.prompt_conversation.message.citations if response.prompt_conversation is not None and response.prompt_conversation.message is not None else None\n",
        "\n",
        "        return message, citations\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None, None\n",
        "\n",
        "async def delete_content(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    if content_id is not None:\n",
        "        _ = await graphlit.client.delete_content(content_id)\n",
        "\n",
        "async def get_content(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.get_content(content_id)\n",
        "\n",
        "        return response.content\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_all_specifications():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_specifications(is_synchronous=True)\n",
        "\n",
        "async def delete_all_conversations():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_conversations(is_synchronous=True)\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)\n"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute Graphlit example"
      ],
      "metadata": {
        "id": "srzhQt4COLVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, HTML\n",
        "import time\n",
        "\n",
        "# Remove any existing contents, conversations and specifications; only needed for notebook example\n",
        "await delete_all_conversations()\n",
        "await delete_all_specifications()\n",
        "await delete_all_contents()\n",
        "\n",
        "print('Deleted all contents, conversations and specifications.')\n",
        "\n",
        "# Set evaluation questions\n",
        "df_eval = df_questions\n",
        "\n",
        "results = []\n",
        "\n",
        "# Configure LLM specification\n",
        "\n",
        "# NOTE: select whether to use OpenAI or Anthropic models\n",
        "modelService = enums.ModelServiceTypes.OPEN_AI\n",
        "#modelService = enums.ModelServiceTypes.ANTHROPIC\n",
        "\n",
        "# NOTE: select your preferred OpenAI and Anthropic model\n",
        "openaiModel = enums.OpenAIModels.O1_MINI_128K\n",
        "anthropicModel = enums.AnthropicModels.CLAUDE_3_5_SONNET\n",
        "\n",
        "# NOTE: select a prompt rewriting strategy\n",
        "#prompt_strategy = None\n",
        "prompt_strategy = enums.PromptStrategyTypes.OPTIMIZE_SEARCH\n",
        "\n",
        "# NOTE: select a retrieval strategy\n",
        "#retrieval_strategy = enums.RetrievalStrategyTypes.CHUNK\n",
        "retrieval_strategy = enums.RetrievalStrategyTypes.SECTION\n",
        "\n",
        "# NOTE: can embed citations in the completed response, and return the cited text sources\n",
        "embedCitations = False\n",
        "\n",
        "# NOTE: using Cohere reranking by default\n",
        "enable_rerank = True\n",
        "\n",
        "# NOTE: can enable re-asking LLM to revise its answer, and assign the number of revisions to make\n",
        "enable_revision = False\n",
        "revision_count = None\n",
        "\n",
        "# Create the LLM specification\n",
        "specification_id = None\n",
        "\n",
        "if modelService == enums.ModelServiceTypes.OPEN_AI:\n",
        "    print(f'Using OpenAI model [{openaiModel}]')\n",
        "\n",
        "    specification_id = await create_openai_specification(openaiModel, retrieval_strategy, enable_rerank, enable_revision, revision_count, prompt_strategy is not None, prompt_strategy, embedCitations)\n",
        "elif modelService == enums.ModelServiceTypes.ANTHROPIC:\n",
        "    print(f'Using Anthropic model [{anthropicModel}]')\n",
        "\n",
        "    specification_id = await create_anthropic_specification(anthropicModel, retrieval_strategy, enable_rerank, enable_revision, revision_count, prompt_strategy is not None, prompt_strategy, embedCitations)\n",
        "\n",
        "if specification_id is not None:\n",
        "    print(f'Created specification [{specification_id}]')\n",
        "\n",
        "    for k, (idx, row) in tqdm(enumerate(df_eval.sort_values(\"doc_name\").iterrows()), total=len(df_eval)):\n",
        "\n",
        "        docs = row[\"doc_name\"]\n",
        "        question = row[\"question\"]\n",
        "        gold_answer = row[\"answer\"]\n",
        "\n",
        "        path_doc = f\"{PATH_PDFS}{docs}.pdf\"\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f'Ingesting [{path_doc}].')\n",
        "\n",
        "        # NOTE: ingestion uses Azure AI Document Intelligent layout model, by default\n",
        "        content_id = await ingest_file(path_doc)\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        if content_id is None:\n",
        "            print(f'Failed to ingest [{path_doc}].')\n",
        "            continue\n",
        "\n",
        "        content = await get_content(content_id)\n",
        "\n",
        "        if content is None:\n",
        "            print(f'Failed to get content [{content_id}].')\n",
        "            continue\n",
        "\n",
        "        print(f'Ingested content [{content_id}] with {content.document.page_count if content.document is not None else 0} pages in {elapsed_time:.2f} seconds.')\n",
        "\n",
        "        # (B) Model Call\n",
        "        if question is not None:\n",
        "            conversation_id = await create_conversation(specification_id=specification_id)\n",
        "\n",
        "            if conversation_id is not None:\n",
        "                print(f'Prompting conversation [{conversation_id}] with question [{question}]')\n",
        "\n",
        "                answer, citations = await prompt_conversation(conversation_id, question)\n",
        "\n",
        "                if answer is not None:\n",
        "                    display(Markdown(f'### Document [{docs}]:'))\n",
        "                    display(Markdown(f'**Question:**\\n{question}'))\n",
        "                    display(Markdown(f'**Answer:**\\n{answer}'))\n",
        "                    display(Markdown(f'**Gold Answer:**\\n{gold_answer}'))\n",
        "                    print()\n",
        "\n",
        "                if citations is not None:\n",
        "                    for citation in citations:\n",
        "                        if citation is not None and citation.content is not None:\n",
        "                            display(Markdown(f'**Citation [{citation.index}]:** {citation.content.name}'))\n",
        "                            display(Markdown(citation.text))\n",
        "                            print()\n",
        "\n",
        "                await delete_conversation(conversation_id)\n",
        "\n",
        "                # (C) Bookkeeping\n",
        "                results.append({\n",
        "                                \"financebench_id\" : row[\"financebench_id\"],\n",
        "                                \"question\" : question,\n",
        "                                \"gold_answer\": gold_answer,\n",
        "                                \"model_answer\": answer,\n",
        "                                })\n",
        "\n",
        "        await delete_content(content_id)\n",
        "\n",
        "        print(f'Deleted content [{content_id}]')\n",
        "        print('-----------------------------------------------------------------------------------')\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results.to_csv(PATH_RESULTS + \"/results.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858,
          "referenced_widgets": [
            "e383957af84541eba48d5c05dd482343",
            "ad92faad568b4e9ca365a20dce351b8b",
            "b808b36f883142c6809d938804f94bd0",
            "83a499e3763440009f802b778a83c876",
            "9d4747d4b94141fbbfeb2490754aed40",
            "6f4484660ef948cfa96618de125a797b",
            "e6dfa0b08a1e4101ab6c613615b5b701",
            "1764cb8b97424085a5701517d56362f0",
            "7a296d86470c4f169f68a2dd2350d8ca",
            "f0daa8aa69904b16b6e62ebbfb475783",
            "fad508adfaf143a2924495a00f54d7d4"
          ]
        },
        "id": "Z5JSHw7nAcjC",
        "outputId": "d1c5becd-b8de-4837-e584-6904c8dda557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all contents, conversations and specifications.\n",
            "Using OpenAI model [O1_MINI_128K]\n",
            "Created specification [2adb69ed-4559-4c46-b62f-2bac9382150b]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/150 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e383957af84541eba48d5c05dd482343"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingesting [financebench/pdfs/3M_2018_10K.pdf].\n",
            "Ingested content [e83808c9-c929-49ff-a77b-c7c6de9b1268] with 160 pages in 37.21 seconds.\n",
            "Prompting conversation [0a021e54-05f0-4c94-8169-1cf8c50ce5fc] with question [What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Document [3M_2018_10K]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Question:**\nWhat is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:**\n$1,577"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Gold Answer:**\n$1577.00"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Deleted content [e83808c9-c929-49ff-a77b-c7c6de9b1268]\n",
            "-----------------------------------------------------------------------------------\n",
            "Ingesting [financebench/pdfs/3M_2018_10K.pdf].\n",
            "Ingested content [685802aa-0b78-4650-ac59-e822fed8592b] with 160 pages in 32.42 seconds.\n",
            "Prompting conversation [c3da54c1-1358-4f4a-bf05-9c92d1eaf4d8] with question [Assume that you are a public equities analyst. Answer the following question by primarily using information that is shown in the balance sheet: what is the year end FY2018 net PPNE for 3M? Answer in USD billions.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Document [3M_2018_10K]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Question:**\nAssume that you are a public equities analyst. Answer the following question by primarily using information that is shown in the balance sheet: what is the year end FY2018 net PPNE for 3M? Answer in USD billions."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:**\n$8.738 billion"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Gold Answer:**\n$8.70"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Deleted content [685802aa-0b78-4650-ac59-e822fed8592b]\n",
            "-----------------------------------------------------------------------------------\n",
            "Ingesting [financebench/pdfs/3M_2022_10K.pdf].\n",
            "Ingested content [497af1c9-a205-4fe7-b9e7-bd69b8cec690] with 252 pages in 57.46 seconds.\n",
            "Prompting conversation [5d62144f-3b0f-407e-8d87-12582665f852] with question [Is 3M a capital-intensive business based on FY2022 data?]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Document [3M_2022_10K]:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Question:**\nIs 3M a capital-intensive business based on FY2022 data?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:**\nYes, 3M is a capital-intensive business based on FY2022 data. The company invested $1.749 billion in property, plant, and equipment, indicating significant capital expenditures to support its operations and growth."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Gold Answer:**\nNo, the company is managing its CAPEX and Fixed Assets pretty efficiently, which is evident from below key metrics:\nCAPEX/Revenue Ratio: 5.1%\nFixed assets/Total Assets: 20%\nReturn on Assets= 12.4%"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Deleted content [497af1c9-a205-4fe7-b9e7-bd69b8cec690]\n",
            "-----------------------------------------------------------------------------------\n",
            "Ingesting [financebench/pdfs/3M_2022_10K.pdf].\n"
          ]
        }
      ]
    }
  ]
}