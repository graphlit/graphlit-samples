{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyOYfm2ARxmmBlu8i9nxi1hJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_09_11_Slack_Research_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to ingest Slack messages, crawl links to files and web pages, and summarize everything related to a Slack message."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "You will also need to create a Slack application (bot) and add it to the channel you wish to ingest from.\n",
        "\n",
        "Assign this property from your Slack bot as Colab secret: SLACK_BOT_TOKEN.\n",
        "\n",
        "More information on creating a Slack bot can be found [here](https://www.graphlit.com/blog/building-a-conversational-slack-bot-with-graphlit).\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60dc6d9c-a7b3-46c6-fed4-f75efbff8515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphlit-client in /usr/local/lib/python3.10/dist-packages (1.0.20240910001)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.8.2)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.9.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (13.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Slack bot"
      ],
      "metadata": {
        "id": "-EJISZXs27h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['SLACK_BOT_TOKEN'] = userdata.get('SLACK_BOT_TOKEN')"
      ],
      "metadata": {
        "id": "wlLGxZky2764"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "# Create specification for Anthropic Sonnet 3.5\n",
        "async def create_specification():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=\"Anthropic Claude Sonnet 3.5\",\n",
        "        type=enums.SpecificationTypes.EXTRACTION,\n",
        "        serviceType=enums.ModelServiceTypes.ANTHROPIC,\n",
        "        anthropic=input_types.AnthropicModelPropertiesInput(\n",
        "            model=enums.AnthropicModels.CLAUDE_3_5_SONNET,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "# Create workflow using LLM specification\n",
        "async def create_workflow(specification_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.WorkflowInput(\n",
        "        name=\"Workflow\",\n",
        "        preparation=input_types.PreparationWorkflowStageInput(\n",
        "            summarizations=[\n",
        "                # Summarize content using Sonnet 3.5 specification\n",
        "                input_types.SummarizationStrategyInput(\n",
        "                    type=enums.SummarizationTypes.SUMMARY,\n",
        "                    specification=input_types.EntityReferenceInput(\n",
        "                        id=specification_id\n",
        "                    )\n",
        "                )\n",
        "            ]\n",
        "        ),\n",
        "        enrichment=input_types.EnrichmentWorkflowStageInput(\n",
        "            link=input_types.LinkStrategyInput(\n",
        "                enableCrawling=True, # opt-in to link crawling\n",
        "                allowedLinks=[enums.LinkTypes.FILE,enums.LinkTypes.WEB], # just ingest web and file links\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_workflow(input)\n",
        "\n",
        "        return response.create_workflow.id if response.create_workflow is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def create_feed(token: str, channel: str, read_limit: int, workflow_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.FeedInput(\n",
        "        name=\"Slack\",\n",
        "        type=enums.FeedTypes.SLACK,\n",
        "        slack=input_types.SlackFeedPropertiesInput(\n",
        "            type=enums.FeedListingTypes.PAST,\n",
        "            token=token,\n",
        "            channel=channel,\n",
        "            includeAttachments=True,\n",
        "            readLimit=read_limit\n",
        "        ),\n",
        "        workflow=input_types.EntityReferenceInput(\n",
        "            id=workflow_id\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_feed(input)\n",
        "\n",
        "        return response.create_feed.id if response.create_feed is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def is_feed_done(feed_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    response = await graphlit.client.is_feed_done(feed_id)\n",
        "\n",
        "    return response.is_feed_done.result if response.is_feed_done is not None else None\n",
        "\n",
        "async def query_contents(feed_id: str, types: List[enums.ContentTypes]):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.query_contents(\n",
        "            filter=input_types.ContentFilter(\n",
        "                types=types,\n",
        "                feeds=[\n",
        "                    input_types.EntityReferenceFilter(\n",
        "                        id=feed_id\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return response.contents.results if response.contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def query_similar_contents(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.query_contents(\n",
        "            filter=input_types.ContentFilter(\n",
        "                searchType=enums.SearchTypes.VECTOR,\n",
        "                contents=[\n",
        "                    input_types.EntityReferenceFilter(\n",
        "                        id=content_id\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return response.contents.results if response.contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def publish_similar_contents(content_id: str, prompt: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.publish_contents(\n",
        "            name=\"Slack Summary\",\n",
        "            connector=input_types.ContentPublishingConnectorInput(\n",
        "               type=enums.ContentPublishingServiceTypes.TEXT,\n",
        "               format=enums.ContentPublishingFormats.MARKDOWN\n",
        "            ),\n",
        "            publish_prompt = prompt,\n",
        "            filter=input_types.ContentFilter(\n",
        "                # Filter on top 5 similar contents\n",
        "                limit=5,\n",
        "                searchType=enums.SearchTypes.VECTOR,\n",
        "                contents=[input_types.EntityReferenceFilter(id=content_id)],\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return response.publish_contents.markdown if response.publish_contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_all_workflows():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_workflows(is_synchronous=True)\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)\n",
        "\n",
        "async def delete_all_feeds():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_feeds(is_synchronous=True)\n"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "import time\n",
        "\n",
        "# Remove any existing feeds, contents and workflows; only needed for notebook example\n",
        "await delete_all_workflows()\n",
        "await delete_all_contents()\n",
        "await delete_all_feeds()\n",
        "\n",
        "print('Deleted all feeds, contents and workflows.')\n",
        "\n",
        "read_limit = 10 # how many Slack messages to ingest from feed\n",
        "\n",
        "# NOTE: specify your Slack channel\n",
        "slack_channel = \"graphlit-demo\"\n",
        "\n",
        "# NOTE: customize prompt to publish summarized cluster of similar content\n",
        "prompt = \"\"\"\n",
        "You are being provided Markdown text from a cluster of similar content.\n",
        "Write 3-5 paragraphs, highlighting any important takeways and common themes found across all the content provided.\n",
        "Remove any irrelevant information, and rewrite as if you were an AI Agent explaining this to a human.\n",
        "Don't mention each specific piece of content, just combine all the information together without listing out where it came from.\n",
        "Be specific when referencing persons, organizations, or any other named entities.\n",
        "\"\"\"\n",
        "\n",
        "specification_id = await create_specification()\n",
        "\n",
        "if specification_id is not None:\n",
        "    print(f'Created specification [{specification_id}].')\n",
        "\n",
        "    workflow_id = await create_workflow(specification_id)\n",
        "\n",
        "    if workflow_id is not None:\n",
        "        print(f'Created workflow [{workflow_id}].')\n",
        "\n",
        "        feed_id = await create_feed(os.environ['SLACK_BOT_TOKEN'], slack_channel, read_limit, workflow_id)\n",
        "\n",
        "        if feed_id is not None:\n",
        "            print(f'Created feed [{feed_id}].')\n",
        "\n",
        "            # Wait for feed to complete, since ingestion happens asychronously\n",
        "            done = False\n",
        "            time.sleep(5)\n",
        "            while not done:\n",
        "                done = await is_feed_done(feed_id)\n",
        "\n",
        "                if not done:\n",
        "                    time.sleep(2)\n",
        "\n",
        "            print(f'Completed feed [{feed_id}].')\n",
        "\n",
        "            # Wait for async crawled content processing to complete\n",
        "            time.sleep(10)\n",
        "\n",
        "            # Query messages by feed\n",
        "            contents = await query_contents(feed_id, [enums.ContentTypes.FILE,enums.ContentTypes.PAGE,enums.ContentTypes.MESSAGE])\n",
        "\n",
        "            if contents is not None and len(contents) > 0:\n",
        "                for content in contents:\n",
        "                    if content is not None:\n",
        "                        display(Markdown(f'### {content.type}: [{content.id}] {content.name}:'))\n",
        "\n",
        "                        if content.type == enums.ContentTypes.MESSAGE:\n",
        "                            display(Markdown(content.markdown))\n",
        "\n",
        "                        if content.uri is not None:\n",
        "                            display(Markdown(f'URI: {content.uri}'))\n",
        "\n",
        "                        if content.parent is not None:\n",
        "                            display(Markdown(f'Parent [{content.parent.id}]: {content.parent.name}'))\n",
        "\n",
        "                        if content.children is not None and len(content.children) > 0:\n",
        "                            display(Markdown('#### Children:'))\n",
        "                            for child in content.children:\n",
        "                                display(Markdown(f'- Child [{child.id}]: {child.name}'))\n",
        "\n",
        "                        if content.links is not None and len(content.links) > 0:\n",
        "                            display(Markdown('#### Links:'))\n",
        "                            for link in content.links[:10]: # just show first 10 links\n",
        "                                display(Markdown(f'- {link.uri}'))\n",
        "                            print()\n",
        "\n",
        "                        print('---------------------------------------------------------------------------')\n",
        "\n",
        "                # Select newest message from feed\n",
        "                content = contents[0]\n",
        "\n",
        "                if content is not None:\n",
        "                    display(Markdown(f'### Finding contents similar to [{content.name}]'))\n",
        "                    print()\n",
        "\n",
        "                    if content.summary is not None:\n",
        "                        display(Markdown(f'Summary:\\n{content.summary}'))\n",
        "                    else:\n",
        "                        print('No summary generated.')\n",
        "\n",
        "                    # Query contents similar to message, via vector embeddings\n",
        "                    similar_contents = await query_similar_contents(content.id)\n",
        "\n",
        "                    if similar_contents is not None:\n",
        "                        display(Markdown(f'### Found [{len(similar_contents)}] contents similar to [{content.name}]'))\n",
        "\n",
        "                        for similar_content in similar_contents:\n",
        "                            if similar_content is not None:\n",
        "                                display(Markdown(f'#### {similar_content.type} [{similar_content.id}]: Relevance {similar_content.relevance}'))\n",
        "\n",
        "                                if similar_content.uri is not None:\n",
        "                                    display(Markdown(f'URI: {similar_content.uri}'))\n",
        "\n",
        "                                if similar_content.summary is not None:\n",
        "                                    display(Markdown(f'Summary:\\n{similar_content.summary}'))\n",
        "                                else:\n",
        "                                    print('No summary generated.')\n",
        "\n",
        "                    print('---------------------------------------------------------------------------')\n",
        "\n",
        "                    # Publish cluster of similar contents as Markdown\n",
        "                    published_markdown = await publish_similar_contents(content.id, prompt)\n",
        "\n",
        "                    if published_markdown is not None:\n",
        "                        display(Markdown(f'### Published content similar to [{content.id}]'))\n",
        "\n",
        "                        display(Markdown(published_markdown))\n",
        "            else:\n",
        "                print('No content was ingested.')"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b95938a-9187-42a0-dcce-2be0a766e625"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all feeds, contents and workflows.\n",
            "Created specification [4f45ae8c-989e-4b58-9d41-d7ffe6a2a2bd].\n",
            "Created workflow [47da485f-3fe5-41bc-9a16-6b1ef83664f6].\n",
            "Created feed [35fed0ca-b5ba-4006-b9f6-9abb0603684f].\n",
            "Completed feed [35fed0ca-b5ba-4006-b9f6-9abb0603684f].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### MESSAGE: [61fbeaca-8fdf-4d60-b0b4-618cefef8d27] Slack message:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Slack Message:\n- From: Kirk Marple\n- Created at 9/9/2024 8:50:55 PM UTC\n\n\nMessage Attachment:\n- Title: Graphlit (@graphlit) on X\n- Description: X (formerly Twitter): Graphlit (@graphlit) on X\n\n:tada: We're starting the Thirty Days of Graphlit.\n\nEach day for the month of September, we'll post a Google Colab notebook which dives into a feature of Graphlit, which you may not know about.\n\nGraphlit supports a lot more than just RAG, and we'll show you examples all month long.\n\n<https://x.com/graphlit/status/1830471107721461959>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Links:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://x.com/graphlit/status/1830471107721461959"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### MESSAGE: [4221f8a0-c3d8-4377-8d1d-a5871399540c] Slack message:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Slack Message:\n- From: Kirk Marple\n- Created at 9/9/2024 8:50:44 PM UTC\n\n<https://docs.graphlit.dev/graphlit-platform/key-concepts>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### MESSAGE: [82166051-5d3c-4aaa-9468-a03ef222c10e] Slack message:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Slack Message:\n- From: Kirk Marple\n- Created at 9/9/2024 8:50:24 PM UTC\n\n\nMessage Attachment:\n- Title: Testing Claude 3.5 Sonnet for document text extraction  - Graphlit\n- Description: Testing Claude 3.5 Sonnet for document text extraction  - Graphlit\n\nGraphlit is an API-first platform for developers building AI-powered applications with unstructured data, which leverage domain knowledge in any vertical market such as legal, sales, entertainment, healthcare or engineering.\n\n<https://www.graphlit.com/blog/testing-claude-3-5-sonnet-for-document-text-extraction>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Links:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://www.graphlit.com/blog/testing-claude-3-5-sonnet-for-document-text-extraction"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### MESSAGE: [c1ae818e-255b-46d8-b5db-e9ec09410c85] Slack message:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Slack Message:\n- From: Kirk Marple\n- Created at 9/9/2024 8:50:09 PM UTC\n\n\nMessage Attachment:\n- Title: GraphRAG: Knowledge Graphs for AI Applications with Kirk Marple | The TWIML AI Podcast\n- Description: TWIML: GraphRAG: Knowledge Graphs for AI Applications with Kirk Marple | The TWIML AI Podcast\n\n<https://twimlai.com/podcast/twimlai/graphrag-knowledge-graphs-for-ai-applications/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Links:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://twimlai.com/podcast/twimlai/graphrag-knowledge-graphs-for-ai-applications/"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### MESSAGE: [85c3ebc5-c216-4396-a7da-9ad031a9e4f0] Slack message:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Slack Message:\n- From: Kirk Marple\n- Created at 9/9/2024 8:49:55 PM UTC\n\n\nMessage Attachment:\n- Title: AIQCON Conference: Kirk Marple discusses GraphRAG and Graphlit\n- Description: YouTube Video: AIQCON Conference: Kirk Marple discusses GraphRAG and Graphlit\n\n<https://www.youtube.com/watch?v=kAj2E_nNcr8>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Links:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://www.youtube.com/watch?v=kAj2E_nNcr8"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### PAGE: [a739d6be-6046-42a9-9d79-c2a5916e8dde] https://twimlai.com/podcast/twimlai/graphrag-knowledge-graphs-for-ai-applications/:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "URI: https://twimlai.com/podcast/twimlai/graphrag-knowledge-graphs-for-ai-applications/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Parent [c1ae818e-255b-46d8-b5db-e9ec09410c85]: Slack message"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Links:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://twimlai.com/podcast/twimlai/graphrag-knowledge-graphs-for-ai-applications/%23content"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://twimlai.com/login/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://twimlai.com/about/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://twimlai.com/contact/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://twimlai.com/newsletter/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://www.facebook.com/twimlai"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://twitter.com/twimlai"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://www.youtube.com/c/twimlai"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://www.linkedin.com/company/twimlai/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://www.instagram.com/twimlai/"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### PAGE: [10cd4c0c-cfab-400f-aa02-bc4f3ff1ff65] https://www.graphlit.com/blog/testing-claude-3-5-sonnet-for-document-text-extraction:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "URI: https://www.graphlit.com/blog/testing-claude-3-5-sonnet-for-document-text-extraction"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Parent [82166051-5d3c-4aaa-9468-a03ef222c10e]: Slack message"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Links:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://www.graphlit.com/blog/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://platform.openai.com/playground/chat?models=gpt-4o"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://www.graphlit.com/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://docs.graphlit.dev/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://discord.gg/ygFmfjy3Qx"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://framerusercontent.com/images/0KCPpSZHooWU24HvBl9VAwOA.png"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://framerusercontent.com/images/l1lTAcGf679u3tJccWjGpvNK9M.png"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://framerusercontent.com/images/YpmUbUwDCCB53jMWOv0LA7xU4.png"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- https://framerusercontent.com/images/gQNjl3ThSL16YI7GDbzne9c0Jc.png"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Finding contents similar to [Slack message]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Summary:\nGraphlit has announced an exciting initiative called 'Thirty Days of Graphlit' starting in September. This program aims to showcase the diverse capabilities of Graphlit beyond its well-known Retrieval-Augmented Generation (RAG) functionality. Each day throughout the month, Graphlit will release a Google Colab notebook that explores a different feature of their platform, providing users with hands-on examples and insights.\n\nThe daily notebooks will serve as an educational resource, allowing users to discover and experiment with lesser-known aspects of Graphlit. This approach demonstrates Graphlit's commitment to user education and highlights the platform's versatility in various applications beyond RAG. By offering practical, interactive content through Google Colab, Graphlit is making it easier for developers and data scientists to explore and implement its features in their projects.\n\nThis initiative reflects a growing trend in the AI and machine learning community of providing accessible, hands-on learning experiences. By breaking down complex features into daily digestible lessons, Graphlit is lowering the barrier to entry for users who may be intimidated by the platform's full range of capabilities. It also suggests that Graphlit has a rich set of functionalities that may not be immediately apparent to casual users.\n\nThe 'Thirty Days of Graphlit' campaign is likely to generate increased interest in the platform and potentially attract new users who are looking for comprehensive tools in the AI and data processing space. By demonstrating its extensive feature set, Graphlit is positioning itself as a versatile solution for a wide range of data-related challenges, potentially expanding its user base and use cases in various industries."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Found [7] contents similar to [Slack message]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### MESSAGE [61fbeaca-8fdf-4d60-b0b4-618cefef8d27]: Relevance 1.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Summary:\nGraphlit has announced an exciting initiative called 'Thirty Days of Graphlit' starting in September. This program aims to showcase the diverse capabilities of Graphlit beyond its well-known Retrieval-Augmented Generation (RAG) functionality. Each day throughout the month, Graphlit will release a Google Colab notebook that explores a different feature of their platform, providing users with hands-on examples and insights.\n\nThe daily notebooks will serve as an educational resource, allowing users to discover and experiment with lesser-known aspects of Graphlit. This approach demonstrates Graphlit's commitment to user education and highlights the platform's versatility in various applications beyond RAG. By offering practical, interactive content through Google Colab, Graphlit is making it easier for developers and data scientists to explore and implement its features in their projects.\n\nThis initiative reflects a growing trend in the AI and machine learning community of providing accessible, hands-on learning experiences. By breaking down complex features into daily digestible lessons, Graphlit is lowering the barrier to entry for users who may be intimidated by the platform's full range of capabilities. It also suggests that Graphlit has a rich set of functionalities that may not be immediately apparent to casual users.\n\nThe 'Thirty Days of Graphlit' campaign is likely to generate increased interest in the platform and potentially attract new users who are looking for comprehensive tools in the AI and data processing space. By demonstrating its extensive feature set, Graphlit is positioning itself as a versatile solution for a wide range of data-related challenges, potentially expanding its user base and use cases in various industries."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### MESSAGE [82166051-5d3c-4aaa-9468-a03ef222c10e]: Relevance 0.90287715"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Summary:\nGraphlit is an innovative API-first platform designed for developers who are creating AI-powered applications that work with unstructured data. This platform is particularly notable for its ability to leverage domain knowledge across a wide range of vertical markets, including but not limited to legal, sales, entertainment, healthcare, and engineering sectors.\n\nThe platform's versatility makes it a powerful tool for developers looking to build sophisticated AI applications that can understand and process complex, industry-specific information. By providing a robust framework for handling unstructured data, Graphlit enables the creation of more intelligent and context-aware applications that can deliver significant value in specialized fields.\n\nOne of the key strengths of Graphlit is its API-first approach, which allows for seamless integration into existing development workflows and systems. This approach provides developers with the flexibility to incorporate Graphlit's capabilities into their projects without having to overhaul their entire tech stack, making it an attractive option for both startups and established enterprises looking to enhance their AI capabilities.\n\nThe platform's ability to work across multiple industries highlights its adaptability and potential for wide-ranging applications. Whether it's analyzing legal documents, enhancing sales processes, optimizing healthcare data, or improving engineering workflows, Graphlit provides the tools necessary to extract meaningful insights from unstructured data, potentially revolutionizing how businesses in these sectors operate and make decisions."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### PAGE [a739d6be-6046-42a9-9d79-c2a5916e8dde]: Relevance 0.8801984"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "URI: https://twimlai.com/podcast/twimlai/graphrag-knowledge-graphs-for-ai-applications/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Summary:\nKirk Marple, CEO and founder of Graphlit, explores the emerging paradigm of 'GraphRAG' or Graph Retrieval Augmented Generation. GraphRAG is an architecture that offers a multi-stage workflow for ingesting, processing, retrieving, and generating content using Large Language Models (LLMs) like GPT-4 and other Generative AI technologies. The system performs entity extraction to build a knowledge graph and integrates graph, vector, and object storage to enhance its capabilities.\n\nOne of the key features of GraphRAG is its use of 'prompt compilation' to improve the results obtained from Large Language Models during the generation phase. This technique allows for more accurate and contextually relevant outputs from the AI models. The architecture's design enables it to handle complex information retrieval and generation tasks by leveraging the power of knowledge graphs in combination with advanced language models.\n\nThe GraphRAG approach supports various use cases, demonstrating its versatility in different applications. These applications range from content generation and information retrieval to more complex tasks that require deep understanding and synthesis of large amounts of data. The system's ability to extract entities and build knowledge graphs makes it particularly useful for domains that involve interconnected information and require contextual understanding.\n\nLooking towards the future, GraphRAG enables agent-based applications, which could revolutionize how AI systems interact with and process information. These agent-based systems could potentially handle more autonomous and complex tasks, leveraging the knowledge graphs and advanced language models to make decisions and generate outputs with minimal human intervention. This opens up new possibilities for AI applications in various industries and research fields."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### MESSAGE [85c3ebc5-c216-4396-a7da-9ad031a9e4f0]: Relevance 0.87524575"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Summary:\nThe AIQCON Conference featured a presentation by Kirk Marple on the topics of GraphRAG and Graphlit. These innovative technologies were discussed in detail during the conference, providing attendees with insights into their applications and potential impact in the field of artificial intelligence and information retrieval.\n\nGraphRAG, which likely stands for Graph Retrieval-Augmented Generation, appears to be a method that combines graph-based data structures with retrieval-augmented generation techniques. This approach potentially enhances the ability of AI systems to access and utilize structured information more effectively, leading to improved performance in various natural language processing tasks.\n\nGraphlit, on the other hand, may refer to a tool or framework that leverages graph-based representations for handling and processing literature or textual data. This technology could offer new ways to analyze and extract insights from large volumes of interconnected textual information, potentially benefiting fields such as research, academia, and knowledge management.\n\nKirk Marple's discussion of these technologies at the AIQCON Conference suggests their growing importance in the AI community. The presentation likely covered the theoretical foundations, practical implementations, and potential applications of GraphRAG and Graphlit, providing valuable knowledge to conference attendees and contributing to the advancement of AI research and development."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### PAGE [10cd4c0c-cfab-400f-aa02-bc4f3ff1ff65]: Relevance 0.85403466"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "URI: https://www.graphlit.com/blog/testing-claude-3-5-sonnet-for-document-text-extraction"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Summary:\nThere are various approaches for performing text extraction from documents like PDFs and Word files. Traditionally, Optical Character Recognition (OCR) has been used to identify text blocks, tables, figures and headings. Cloud services like Azure AI Document Intelligence and Amazon Textract provide robust OCR capabilities. However, with the advent of vision-capable models such as OpenAI GPT-4, there has been increased interest in using Large Multimodal Models (LMMs) for visual text extraction. These multimodal models can potentially offer cost benefits compared to existing cloud services.\n\nAnthropic's Claude 3.5 Sonnet model was tested for document text extraction without requiring OCR. A customized prompt was used to instruct the model on how to analyze the provided image and return a formatted JSON response. The JSON schema was based on Graphlit's internal mezzanine format used for canonical storage of extracted text. The prompt directed Sonnet to carefully analyze all elements in the image including text, tables, radio buttons, checkboxes, figures and charts, with special attention to tables.\n\nThe test results showed that Sonnet 3.5 performed well overall in extracting and structuring the document content. It accurately captured most of the text, headings, tables and form elements. However, it did have some minor issues with radio button extraction, occasionally misidentifying the selected option. Despite this, Sonnet demonstrated strong potential for document text extraction tasks.\n\nIn comparison, initial testing of OpenAI's GPT-4 vision model on the same image and prompt produced significantly worse results. GPT-4 failed to provide an accurate representation of the document structure. This suggests that Sonnet 3.5 may have an advantage for document text extraction use cases. Further investigation into using multimodal models for text extraction is planned, with the goal of potentially adding these capabilities as an option in Graphlit's document preparation workflow."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### MESSAGE [c1ae818e-255b-46d8-b5db-e9ec09410c85]: Relevance 0.8524438"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Summary:\nKirk Marple shared a message on Slack about an episode of The TWIML AI Podcast titled 'GraphRAG: Knowledge Graphs for AI Applications with Kirk Marple'. The podcast episode explores the intersection of knowledge graphs and retrieval-augmented generation (RAG) for artificial intelligence applications.\n\nKnowledge graphs provide a structured way to represent information and relationships, while RAG techniques allow AI models to access and utilize external knowledge sources. The combination of these approaches, referred to as GraphRAG, offers potential benefits for enhancing AI systems' ability to reason over complex information and generate more accurate and contextually relevant outputs.\n\nDuring the podcast, Kirk Marple likely discusses the technical aspects of implementing GraphRAG, including methods for constructing and querying knowledge graphs, integrating them with large language models, and leveraging graph structures to improve information retrieval and reasoning capabilities. The conversation may also cover practical applications and use cases for GraphRAG across various industries and AI tasks.\n\nThe TWIML AI Podcast, known for featuring in-depth discussions on cutting-edge AI topics, provides a platform for exploring the nuances and potential impact of GraphRAG. By combining the structured nature of knowledge graphs with the flexibility of retrieval-augmented generation, this approach represents an important development in the ongoing efforts to create more capable and intelligent AI systems."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### MESSAGE [4221f8a0-c3d8-4377-8d1d-a5871399540c]: Relevance 0.84065014"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Summary:\nThe provided text appears to be a Slack message metadata snippet rather than substantive content to summarize. It contains only basic information about a message sender (Kirk Marple), timestamp (9/9/2024 8:50:44 PM UTC), and a hyperlink to documentation (https://docs.graphlit.dev/graphlit-platform/key-concepts). Without additional context or content from the actual message body, there is insufficient material to generate 4-5 verbose paragraphs as requested."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Published content similar to [61fbeaca-8fdf-4d60-b0b4-618cefef8d27]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Graphlit, an API-first platform, is making significant strides in the field of AI-powered applications, particularly those dealing with unstructured data. The platform is versatile, catering to various vertical markets such as legal, sales, entertainment, healthcare, and engineering. One of the key innovations discussed is GraphRAG (Graph Retrieval Augmented Generation), which combines graph-based data retrieval with generative AI technologies to enhance content generation. This multi-stage workflow involves ingesting, processing, retrieving, and generating content, leveraging Large Language Models (LLMs) like GPT-4 to produce high-quality outputs.\n\nA notable feature of Graphlit's system is its ability to perform entity extraction to build detailed knowledge graphs. This process involves identifying and categorizing key entities within the data, which are then used to create a structured representation of the information. The integration of various storage solutions, including graph, vector, and object storage, ensures efficient data management. Additionally, the system employs \"prompt compilation\" to refine the prompts given to LLMs, thereby improving the quality of the generated content.\n\nGraphlit has also been testing the latest multimodal models, such as Anthropic Claude 3.5 Sonnet, for text extraction accuracy without relying on traditional Optical Character Recognition (OCR). These models have shown potential in accurately extracting text and other elements from images, although some accuracy issues remain, particularly with radio buttons. Despite these challenges, the testing highlights the potential of multimodal models in providing cost-effective and accurate text extraction solutions compared to existing cloud services.\n\nThe TWIML AI Podcast episode featuring Kirk Marple, CEO and founder of Graphlit, delves into the practical applications and future possibilities of GraphRAG. The discussion emphasizes the role of LLMs in modern AI applications and explores various use cases, from content generation to data analysis. The podcast serves as an educational resource, helping AI professionals and enthusiasts stay informed about the latest advancements and trends in AI and machine learning."
          },
          "metadata": {}
        }
      ]
    }
  ]
}