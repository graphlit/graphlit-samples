{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyOle8OWjCMz7bVfh3RizZL0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2025_02_19_Transcribe_Podcast_using_Assembly_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to ingest a podcast MP3 by URL, and configure a workflow to use Assembly.AI for audio transcription."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6eda429-af94-43ce-a574-8c01fce60b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphlit-client in /usr/local/lib/python3.11/dist-packages (1.0.20250216001)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from graphlit-client) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from graphlit-client) (2.10.6)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.11/dist-packages (from graphlit-client) (2.10.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from graphlit-client) (14.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->graphlit-client) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->graphlit-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->graphlit-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->graphlit-client) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->graphlit-client) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Graphlit"
      ],
      "metadata": {
        "id": "abV1114jL-bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "async def create_workflow():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.WorkflowInput(\n",
        "        name=\"Audio Preparation\",\n",
        "        preparation=input_types.PreparationWorkflowStageInput(\n",
        "            jobs=[\n",
        "                input_types.PreparationWorkflowJobInput(\n",
        "                    connector=input_types.FilePreparationConnectorInput(\n",
        "                        type=enums.FilePreparationServiceTypes.ASSEMBLY_AI,\n",
        "                        assemblyAI=input_types.AssemblyAIAudioPreparationPropertiesInput(\n",
        "                            model=enums.AssemblyAIModels.BEST,\n",
        "                            detectLanguage=True\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_workflow(input)\n",
        "\n",
        "        return response.create_workflow.id if response.create_workflow is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def ingest_uri(uri: str, workflow_id: Optional[str] = None):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        # Using synchronous mode, so the notebook waits for the content to be ingested\n",
        "        response = await graphlit.client.ingest_uri(uri=uri, workflow=input_types.EntityReferenceInput(id=workflow_id) if workflow_id is not None else None, is_synchronous=True)\n",
        "\n",
        "        return response.ingest_uri.id if response.ingest_uri is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def get_content(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.get_content(content_id)\n",
        "\n",
        "        return response.content\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_content(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.delete_content(content_id)\n",
        "\n",
        "        return response.delete_content.id if response.delete_content is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_all_workflows():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_workflows(is_synchronous=True)\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)\n"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute Graphlit example"
      ],
      "metadata": {
        "id": "srzhQt4COLVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Remove any existing contents and workflows; only needed for notebook example\n",
        "await delete_all_contents()\n",
        "await delete_all_workflows()\n",
        "\n",
        "print('Deleted all contents and workflows.')\n",
        "\n",
        "uri = \"https://graphlitplatform.blob.core.windows.net/samples/Podcasts/GraphRAG%20Knowledge%20Graphs%20for%20AI%20Applications%20with%20Kirk%20Marple.mp3\"\n",
        "\n",
        "workflow_id = await create_workflow()\n",
        "\n",
        "if workflow_id is not None:\n",
        "    print(f'Created workflow [{workflow_id}]:')\n",
        "\n",
        "    content_id = await ingest_uri(uri=uri, workflow_id=workflow_id)\n",
        "\n",
        "    if content_id is not None:\n",
        "        print(f'Ingested content [{content_id}] with Assembly.AI:')\n",
        "\n",
        "        content = await get_content(content_id)\n",
        "\n",
        "        if content is not None and content.markdown is not None:\n",
        "            display(Markdown(f'## View [Extracted JSON]({content.text_uri})'))\n",
        "            print()\n",
        "\n",
        "            print('-------------------------------------------------------------------')\n",
        "            print(content.markdown)\n",
        "            print('-------------------------------------------------------------------')\n",
        "\n",
        "            await delete_content(content_id)\n"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55b836a1-6bec-47c2-d08b-5d3be7fc4d68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all contents and workflows.\n",
            "Created workflow [4c185f27-807c-406c-b3b3-27e447fdf473]:\n",
            "Ingested content [570413d5-511f-40e2-941d-367ff4ffb06e] with Assembly.AI:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## View [Extracted JSON](None)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------------------------------------------------------\n",
            "[00:00:00] Foreign welcome to another episode of the TWIML AI podcast. I am your host Sam Charrington and today I'm joined by Kirk Marple. Kirk is CEO and founder of graphlit. Before we get going, be sure to take a moment to hit that subscribe button wherever you're listening to today's show. Kirk, welcome to the podcast.\n",
            "\n",
            "[00:00:25] Yeah, thanks so much. I've been a longtime listener and glad to finally be part of this. I'm excited to have you on the show and I'm really looking forward to our topic. We're going to be digging into what you're doing at graphlit, but in particular the broad space of Graph rag. Tell us a little bit about graphlit and kind of how you're approaching RAG as a space.\n",
            "\n",
            "[00:00:48] Yeah, for sure. I mean we've been around for about three years, had started really building an unstructured data platform for I mean everything, I mean multimodal data, documents, audio, video and really started getting interested in the knowledge graph side of this, of pulling all that data into a knowledge graph to make it explorable. And then sort of saw the integration of that with RAG kind of come out over the last year and how we can, how we can benefit from that. We've known each other for a bit now. You attended our first Twimlcon conference in San Francisco.\n",
            "\n",
            "[00:01:22] That was in 2019. You started out doing, really trying to go after applying ML&AI to Media Talk a little bit about how that led you to the way you think about the problem today. Yeah, I mean a lot of the problem in RAG is the R, I mean the retrieval side. But you have to have the data to retrieve in the first place. And so we started focusing on, I mean there wasn't at the time and still kind of isn't a 5 trend for unstructured data.\n",
            "\n",
            "[00:01:58] I mean that's kind of what we were looking at of where is the data pipelines to make the data available to AI models, ML models. And so we started at the ingest side pulling in really all sorts of data. And in my background I had a company in the broadcast video space, so dealt a lot with file based workflows for that. And there's a lot of parallels of, I mean pulling in data, running NLP on them, running computer vision. So we kind of started with the content first of how do you get data into a system like this?\n",
            "\n",
            "[00:02:32] And then search and retrieval obviously is a big step. And we start a lot with the metadata. I mean how do you retrieve data? Well, how do you store the Metadata first and then how do you retrieve it? So, like, metadata filtering is common now in vector databases.\n",
            "\n",
            "[00:02:47] So we were already doing a lot of that a couple years ago. And as RAG kind of became the concept, we realized, I mean, we already have a great retrieval system to plug in to LLMs, and we already had solved the sort of media side of it that plugged into multimodal models very well. So that's when we kind of really focused on Graphlet as a platform that anybody could build an application on. And that's really what we really been pushing for the last year. And so graph RAG is a concept that grew out of a paper that Microsoft published some time ago.\n",
            "\n",
            "[00:03:28] To what degree is your system like trying to implement the specific approaches from that paper, as opposed to the general idea of applying using graphical, you know, relationships in a RAG model? It was really interesting to see that paper. I mean, we had been doing a lot of that already and just hadn't. Hadn't been talking about it as widely. But I mean, the first part of it is just how do you build the knowledge graph?\n",
            "\n",
            "[00:03:58] And that's, that's what we had focused on first of doing entity extraction, people, places, things, and creating that from the content. And as we kind of built up our RAG system, pulling in that data from the knowledge graph, I mean, essentially, graph RAG is really where we're at now. And that's what it was really. I mean, it was good to see it in a paper and kind of see other people kind of looking at that, because it's something I've been thinking about for a couple years. And really, honestly, a lot of this started with a podcast discovery platform that I was trying to build six or seven years ago.\n",
            "\n",
            "[00:04:32] And it was a little too early. I mean, and now I know there's a bunch of great projects out there, and transcription got cheaper and it's just so much easier to build a platform like that. Let's talk a little bit about that. Entity extraction. Like, you know, where did you start with that?\n",
            "\n",
            "[00:04:49] How has it evolved? Is it a solved problem? Like, you know, if someone who's listening wanted to go about doing that, like, what are they going to find? That's difficult. Let's like, dig deep into that part of the problem.\n",
            "\n",
            "[00:05:01] Well, what we found is there's a lot of overlap with the kind of retrieval side of like text extraction, text chunking, pulling those pieces together to do entity extraction, and really any nlp. So you kind of have to solve that part first and then use a model. I mean, we've used like Azure, AI text analytics. I mean we've actually used LLMs for this and I mean instructing the model to identify people, places and things. But what they're also really good at is identifying things like places.\n",
            "\n",
            "[00:05:33] Address extraction we found is that LLMs are especially good at that, which was really difficult with NLP and I mean with, with original algorithms. And so that I think that extraction side happens during the ingestion pipeline. So we look at it. I've heard a little bit of the contrary in particular that large language models aren't great in a lot of cases for kind of traditional named entity extraction and that the traditional models are still better and maybe more controllable. Like how do you think about where to apply what?\n",
            "\n",
            "[00:06:14] We've actually seen that as well. I mean we can use a mix. We've seen LLMs work great for specific cases, like for events. Like we were working with a community website where they were wanting to pull out like okay, when was this dance concert and where was it? We were able to guide the LLM really well, I mean to extract an event.\n",
            "\n",
            "[00:06:39] But I've seen it in other ways where kind of people and companies, the classic sort of AI text analytics, I mean like Azure works way better and so we can support both. So during the same ingestive pipeline we can actually run both models and we can instruct it to say, okay, places use GPT, four people in organizations use Azure, that kind of thing. You would think you can give an LLM a paragraph of text or a document and say, okay, produce a JSON document that has all of the people identified and it just doesn't work as well as you might want it to work. There's cases. I mean it definitely works in some situations, but what we found is you get more noise in some cases.\n",
            "\n",
            "[00:07:25] And so I think it's something you kind of want to try and, and look at. I mean we've definitely seen a mix. I mean there's some cases where, and I mean, honestly the Azure type, I mean, or Amazon type models have similar noise problems where it'll identify a term as a company and it's not really a company. So there's a data quality issue that kind of feeds into both these sides. And I think that's where it does take a bit of testing and evaluation to get it working right.\n",
            "\n",
            "[00:07:56] And have you been able to identify specific patterns that characterize the cases? One thing we've looked at, we haven't released yet is sort of a chaining model using sort of an NLP style to Identify the entities and then go and refine that. We do support data enrichment today where when we identify an entity like a company, we can go call out to like Crunchbase or Wikipedia and enrich metadata for it. But one thing I really want to try is once you've identified the entity, go and re identify and prompt the LLM to say, hey, I think this is Microsoft's in here. Go, I don't know, grab everything you can find about Microsoft.\n",
            "\n",
            "[00:08:41] That's an area that I want to explore. Definitely a bit more. So you've got these, you know, multiple methods to extract the entities. And you mentioned that that is part of the data ingestion. How is that used in the context of ingestion?\n",
            "\n",
            "[00:09:02] So we, what we call our content workflow is kind of a multi stage. So ingestion is really just like downloading it from a Blob storage or a website. And then we have a stage which we call preparation. And that basically includes audio transcription or text extraction, PDF extraction. And what we get out of it is a canonical form of.\n",
            "\n",
            "[00:09:24] I mean essentially we store a JSON file of here's the transcript or here's the text and with semantic chunking or page chunking and then we have an extraction stage and that's where this would happen. And so we kind of have this sort of state machine that the content goes through. And in our platform you can configure each stage of the workflow. And yeah, at extraction, I mean you can tell it which model or which API to use, what you want out of it. And then we basically take the results of that and feed that, connect everything up in the knowledge graph basically.\n",
            "\n",
            "[00:10:01] And then we have an enrichment phase after that, which is optional, where you can kind of just say, oh, I've identified, I mean a company now go get its address, that kind of thing. You recently joined our generative AI meetup. You've been involved in our community and kind of talked a little bit about this and did a demo. And one of the contexts or one of the questions that came up in that conversation was like RDF and these entity relationships from the traditional NLP world. Do you use those kinds of relationships in the graph that you construct or is it more ad hoc?\n",
            "\n",
            "[00:10:47] That's a great question. I mean the way we currently do it is we're basing all of our entities on schema.org so JSON, LD kind of style. So we are, we're not inventing our own data model for that. And I think that's important to have canonical kind of reusable data. And the LLMs actually know JSON LD really well.\n",
            "\n",
            "[00:11:06] So we found. And so that's kind of the first part. I mean today we're not doing, we're leaning more on entity to content relationships rather than entity to entity relationships. That's most of what I mean, we could do both. But I think that's mostly what we're trying to see because with the entity to content relationships with graph rag you can then say, oh, for this piece of content, what are all the entities related to it and what are those entities related to in other pieces of content?\n",
            "\n",
            "[00:11:35] And so that our graph leans a bit more on the entity to content relationships. Okay, okay. And so there's less of a need or desire at this point to kind of traverse entity to entity relationships to pull in this broader graph of content. It's more kind of two step as opposed to a fan out. And that's what I think.\n",
            "\n",
            "[00:11:58] Like Yohei has been working on some graph things you might have seen on Twitter. And there's been different kind of open source projects that are a bit more like, I mean, I work at X that kind of relationship. And I mean we could definitely do that. I mean, but our graph, just because of what we're using it for, is not leading that direction today. So you were talking through your workflow and you kind of got through the ingestion part of the pipeline.\n",
            "\n",
            "[00:12:26] What's next, the preparation stage and the enrichment stage. And all those essentially end up with data in a vector database and in a graph and a document store. And so what we do is have a sort of a hybrid data storage model where. And object storage we can leverage as well. And so I mean if we're, we're caching things essentially there and so we kind of use that all together where the graph is kind of replacing the relational index and that's the relationship between all the content.\n",
            "\n",
            "[00:12:57] I mean we have collection kind of patterns, we have the entity to content patterns. And so this is something I kind of came up with over the last several years and just kind of refined and that's worked really well for us where we can then walk from one piece of content either via similarity in a vector, sort of that vector angle, or through the metadata and the graph. And so we can kind of have that hybrid approach and essentially a searchable knowledge graph. What's been your experience working with the vector databases? Do you kind of support all of them or do you have preferences?\n",
            "\n",
            "[00:13:34] Are there key features that you're using in your system that are not Universally supported or are you just using the basic capability of the vector databases? Yeah, that's a good point. We had looked at all the major players and talked to a lot of folks. There's a lot of great ones out there. We were Azure native today, so we're leveraging a lot of Azure data services like for our Graph database.\n",
            "\n",
            "[00:14:04] And we were actually already using Azure AI search, I mean, then cognitive search for our keyword search. And right at the time when I was evaluating vector databases, they came out with their vector index support and I tried it and it worked. And it's a nice because it supports metadata filtering and keyword vector and hybrid all in the same box. I mean, we're not plug and play with vector databases like some other things. We're kind of more.\n",
            "\n",
            "[00:14:38] We're going to pick different things connected together and have a managed platform on top of it and it works great. I mean, and they even just increased their performance and updated their storage and things like that. And we're actually going to be at Microsoft Build next month kind of showing this off and I mean in that Microsoft domain. But it has worked great for us and we could, I mean, theoretically swap it out if we wanted to, but we've actually had some really good experiences so far with this one. And so we're kind of broadly seeing, you know, as RAG and vector databases become more popular, vector becoming kind of a layer on top of a lot of different data stores.\n",
            "\n",
            "[00:15:26] So I just came back from Google Cloud next and they did a similar thing where BigQuery now has a vector layer, AlloyDB, which is kind of their postgres, now has a vector embeddings layer. My question is, do you see the same thing happening with Graph as well? Is it going to end up. Do you see us moving towards a converged world where you put your data in and you have all these different kind of semantic abstractions on top of it where you're not replicating the data. Or do you think that graph is so distinct that they remain disparate systems?\n",
            "\n",
            "[00:16:09] It's a really interesting point. It's something I've thought a bit about because when I was first looking for the magical database that I really wanted, it didn't exist. And I had to create this Franken database from a couple different things. The searchable side, like keyword search in a graph, I know that a couple vendors have solved in that world. I would envision the vector comes next.\n",
            "\n",
            "[00:16:34] I think there's. It could, I mean you could integrate something like that with the graph. I Mean the big problem we had honestly with graph databases is the payload. You can't store a ton of data in a node. And that was a big reason that we kind of came up with this way that it's sort of a three tier storage model where the graph is the index, the heavy metadata is in a JSON store and then the really heavy content is in the file system in object storage.\n",
            "\n",
            "[00:17:02] And so we kind of use that sort of HSM model of like layered storage. And that's worked great. I mean we only kind of pull it off disk if we really need to. And. But to your point, I think I, I don't think it's a done deal that it'll go there.\n",
            "\n",
            "[00:17:16] I mean I think we'll, we'll have to see. But I wouldn't be surprised really if, if they start, I know they've started editing the keyword search. So I wouldn't be surprised on Vector. And along the same lines, like, I think, you know, vector databases have been around for you know, quite a long time and you know, we were making progress in kind of shifting from keyword based search to more semantic search and embedding based search. And you know, most of the vector databases vendors that we see now kind of started as these tools to support the search use case.\n",
            "\n",
            "[00:17:53] You know, RAG came and kind of popularized that whole space and you know, Vector then started become getting pulled into, you know, every data store. Do you think that Graph RAG will have the same impact on graph. Like graph databases have been around for a very long time, like longer than vector databases I think. Right. You know, Neo4J.\n",
            "\n",
            "[00:18:26] I've known those, those folks for quite a long time. You know, they've always had their place but they've never like, I think had the vector moment. Like vector databases recently have. Do you think that, you know, based on what you've seen with Graph Rag, like do you think Graph Graph Rag is that killer app for graph databases? I think it's definitely possible.\n",
            "\n",
            "[00:18:49] I mean, graph databases, I've, I mean it has been one of those sort of background things where it's really useful for some specific use cases. A lot of times the algorithmic side of like I mean running, I mean algorithms, heavy algorithms on a very large graph. I think the idea of more of a property graph and this sort of entity graph in Graph Rag I think could be a way that it's like what happened with Vector and rag. I mean there are several vendors that made themselves, that created huge companies out of just that plug and play. So I think we'll have to see.\n",
            "\n",
            "[00:19:23] I mean the sort of value of graph rag is still a little tbd. I mean we're still exploring it, but I think that's what makes it interesting. I think there's a lot of interesting paths you can take to see how to get the sort of squeeze of value. And I mean, I'm optimistic. I really think that, I mean more people putting these relationships in the graph rather than just a typical relational database gives them more opportunity to find the sort of explore their data in new ways.\n",
            "\n",
            "[00:19:53] You go through this ingestion process that populates your kind of four data stores, you know, interrelated data stores and is that the end of the pre processing step and then the, you know, then we're to the next half. Which is what happens at the time of a query? Pretty much, yeah. I mean other than enrichment, I mean which is, which is optional. I mean once the data is in the data stores in the vector index, I mean that's like, I mean if you call our API and say, hey, ingest a URL, then the footprint becomes, okay, the extracted text and the data in the graph, then it's all about retrieval and then it's, you can make a query and that query basically can be a mix.\n",
            "\n",
            "[00:20:42] It could be keyword only, it could be vector hybrid. The metadata. We'd already been doing a lot with metadata filtering because one of our key sort of thesis points was index everything in time and space because we were actually doing working in geospatial as well. So a lot of our metadata is not just title, author, keyword, it's, I mean, what lens on a camera are you using? What's the GPS location of the image or the video?\n",
            "\n",
            "[00:21:08] So like we have a really thorough set of metadata that we can extract from any media. And that's when it becomes really interesting I think for, I mean metadata driven rag. I mean asking questions about locations, asking questions about time ranges. And then the graph rag is really pulling out those entities and asking questions about okay, well where are these entities mentioned? And so that everything kind of drives towards that retrieval model.\n",
            "\n",
            "[00:21:36] It strikes me that there is a lot of potential complexity and like the fusion of these different, you know, retrievals. You know it with. Yeah, I think folks who listen to the podcast have heard me talk, you know, previously about like it's easy to get a rag demo up and running, but to get something that's really, really good into production is difficult in part because there's a lot of tuning that goes into the retrieval and that's Just with kind of one, you know, data store, with just your embeddings and, and you know, chunks and all that kind of tuning be difficult. Now you're talking about layering in, you know, metadata and graph and you know, concepts like re ranking. Like what does re ranking mean now across three different, you know, sources of information?\n",
            "\n",
            "[00:22:33] Like how do you approach all that? I mean, we sort of take a layered approach. I mean retrieval. The first is, is this kind of search that is. It definitely does touch.\n",
            "\n",
            "[00:22:41] I mean it touches the graph, touches the search index, the vector. And so the retrieval kind of step happens first, but it applies to kind of that triad of data stores. And then we actually just added support for the new cohere re ranking model recently. And so what we do is the output of retrieval gives essentially a ranked list of sources, but each of those sources has metadata that we pulled from the data stores. And then we provide that to the re ranking model or do it ourselves.\n",
            "\n",
            "[00:23:13] When you say a rank list of sources, meaning source documents or sources like your three different, you know, metadata, vector, graph, data content sources like document chunks or document sections, things like that, or chunks of an audio transcript and things like that. So a source is kind of an abstraction for like, hey, here's a piece of text that we found probably in some content. Got it. So you somehow execute a query across these multiple systems. You get back a bunch of chunks and you treat them from a re ranking perspective.\n",
            "\n",
            "[00:23:52] You treat them equally like you're just doing the best you can to rank them based on the content of the chunks as opposed to the context in a graph or the neighborhood in a, in a embedding space or something like that. At least today we are. I mean, I think that that's something we're exploring. I mean one is time sort of time relevance of sort of doing time clustering. I mean, we're looking at geoclustering, we're looking at some things like that.\n",
            "\n",
            "[00:24:22] The graph can help with that as well. I mean today, I mean, what's, what's in production today is, is essentially just the content sources getting re ranked. And those content sources can come from retrieval. I mean the big thing we're doing today with, we do support retrieval strategies for like expanding the chunk into a semantic chunk, which is like a section of a document or I mean chunks of a transcript and things like that. So we call those strategies that then that's where it's the knobs people could turn during the retrieval step.\n",
            "\n",
            "[00:24:59] And then that's where we just added the re ranking Strategy which initially supports cohere. And that's really helped. I mean, we can definitely see is the relevance you get out of your vector database or even the output of the search. It's not, I mean, that's why these models exist. I mean, it's not exactly in the order you're expecting and LLMs can adapt to misordered data, but it's the filtering that I found is throwing away the irrelevant data lets us kind of have a filter, kind of a low pass filter on.\n",
            "\n",
            "[00:25:30] I mean, okay, let's just get rid of stuff that doesn't make sense that could confuse the LLM. Maybe this is a good time to introduce the topic of evaluation and how you think about relevance and quality of a retrieval. Yeah, I mean a lot of it, I mean there's always. Everybody kind of goes through that ad hoc phase of okay, it looks good. And originally.\n",
            "\n",
            "[00:25:53] Yeah, Vibe check. Exactly. I mean, and we started there as well and we've actually just started working with a vendor in this space that has kind of more automated evals and hopefully by, I mean, we're looking, maybe by next week we'll have some, some details on that. But yeah, it's. We're starting to kind of lean more into that because I mean, for me and like, I, I'm not a data scientist, I came from a more traditional software background.\n",
            "\n",
            "[00:26:18] A lot of this is like unit testing and integration testing. I mean, and you know, you have to have suites of these tests to run when a new model comes out or. And I kind of look at it in that way. And so we are trying to automate that more than just vibecheck. And I think there's some good vendors out there and good projects that really help with that.\n",
            "\n",
            "[00:26:38] Are you thinking about dynamic optimization, DSPY and those kinds of things? Dig into that whole prompting space. Yeah, I mean, I would call it. I mean what we do is more of a prompt compiler and so it is dynamic prompting. We're not just using.\n",
            "\n",
            "[00:26:52] I mean there are, there are static phrases that we use or like, I mean, paragraphs that we use for like some of the instructions and guidance. One of the things I came across, and this is actually when I started implementing Cohere months ago though, how they like XML. I mean the cohere prompt really like XML. And I was actually able to realize that OpenAI likes it as well and pretty much on most of the other models. So we have sort of an XML template that we sort of compile to that has different sections and it has a context section at the top and instructions and guidance.\n",
            "\n",
            "[00:27:28] And so we've kind of broken it out into okay, here's how I'm going to guide. Sort of guidance is like what not to do and instructions is kind of what I want you to do. And then we compile the sources and then the user prompt piece at the bottom. We've had to do some things like dynamic based on model where like, I don't know, like Haiku forgot the schema, the JSON schema. Like you had to remind it of it at the top and the bottom I think it was.\n",
            "\n",
            "[00:27:57] And I'd seen that with a couple different models and those are really some things that it lets us be dynamic on the fly. Basic. So there's a structure, but it is kind of a compiler that we dynamically output based on the query, the incoming query and all the, the retrieval strategies and all that kind of thing. Is it an LLM that's compiling this into a prompt? Is it, you know, a set of rules or heuristics or a combination like yeah, the compiler is just code.\n",
            "\n",
            "[00:28:29] I mean honestly, it's just, it's straightforward. We are using LLMs for things like what is it a prompt rewriting. So we do have a way that we have a couple strategies for optimizing for semantic search versus kind of rewriting the prompt. We actually just have an experimental one for multiquery now that'll kind of break it into multi queries, which is actually works really nicely. And I know that, I mean llamaindex and LangChain, they've seen those kind of results as well.\n",
            "\n",
            "[00:28:59] Those work really nicely. We use LLMs for like summarization of the conversation history. So we support like a windowed conversation as well as a summarized history, so those kind of things. But the compiler itself is kind of, it's just code. I mean it's just kind of looking at all the context that it has and then generating essentially a string, it just gets put into the LLM.\n",
            "\n",
            "[00:29:24] And when you, you said that the LLMs like XML, you know, how different, how much stronger is that statement than they like structure? Like, they like that particular kind of structure apparently like as opposed to, you know, headings and paragraphs, YAML, JSON, that for whatever reason XML is just like some magic pixie dust that makes them work better. I mean, I think it's, it's a bit of both, but I think I learned it from. I mean it's, it's just how cohere documents, they kind of say, hey, here's, here's our preferred prompting format with and using XML to kind of create sections within that. And I found that when I backported that because I tried YAML, I tried JSON, I tried a couple other things and I mean, I don't know if I was a little surprised, but I mean, it's probably trained on a lot of data like that.\n",
            "\n",
            "[00:30:19] But I saw how it. We didn't have to change our method depending on the model as much. It was a common thread that I found that pretty much every model I've tried listens to the prompt very well when it's structured like that. Now could I do that as YAML instead of XML? I think OpenAI is definitely more resilient and you can give it more options for the structure I found.\n",
            "\n",
            "[00:30:46] But cohere would then have a downside, at least with their original models, where it wouldn't listen to the structure. So what we were looking for was something that kind of worked across everything and could just be kind of a standard approach for a given use case. Are you using multiple models and kind of orchestrating them and trying to identify, you know, what's best, cheapest for a given step? Are you defaulting to a single model that you like the best? How do you think about the model space?\n",
            "\n",
            "[00:31:17] Yeah, I mean, we let developers pick. We have what's called a specification. It's kind of a preset that you can pick your model, pick your tokens, like token limit, system prompt, all that kind of stuff. And then depending on what conversation they're having or what, like if they can use a different specification for JSON extraction per se than for having a conversation. So we kind of hand that to the developers that use our platform to choose.\n",
            "\n",
            "[00:31:46] But we, I mean, we're currently using actually GPT 3, 5, 16K as our default. Like if nobody, if they don't pick anything else. But I am actually evaluating moving over to Haiku as our default. I think we'll, I mean, we'll probably do that before the end of the month and just because I've, I've really liked that model just from a price, performance and just quality standpoint. So I'm thinking that that's probably going to become our default.\n",
            "\n",
            "[00:32:12] You've mentioned on a couple of occasions, you know, what has given me the picture of like building blocks that a developer at least, you know, might conceptually think about their application. Are you presenting them as building blocks? Is there a libra of building blocks? Or are these just kind of the natural steps that someone goes through or even like how does the product present itself? Is it, you know, client libraries, APIs, you know, GUI thingy?\n",
            "\n",
            "[00:32:46] It is, it's an API first platform. We have a developer portal, you can sign up for free, get an API key, start using it, same immediately. And we have a GraphQL API that is kind of the native API to it. But we actually just released this week native SDKs for Python and TypeScript for like a node backend. And so they kind of hide the complexity of GraphQL and it just, it's a simpler and it's a type type safe experience which we wanted.\n",
            "\n",
            "[00:33:17] And so yeah, I just updated. We have a bunch of streamlit apps that use the SDKs now and it's been great to see the developer experience that. And on our homepage now it's like two lines of code for ingest a website and prompt, a conversation with a prompt. And so we've abstracted it down to that. So this is an interesting point.\n",
            "\n",
            "[00:33:36] A lot of what we just spent the last 40 minutes talking about, the developer doesn't have to think about this is stuff you're doing under the covers to enable the developer to pass you a URL and then be able to prompt against it. Yeah, and I think this is a really big difference in how we approach this versus a lot of the open source projects that have been around and have really led the way for rag awareness. But we take a more content first approach where it's almost like a cms, like you're just putting data into this content management system and then we're saying anything you want to change is configuration. And so we have this workflow object that you can create that says, here's how I want you to ingest data, here's how I want you to prepare it, enrich it or extract it, and enrich it. And all you do is when you say point me at a website and use this workflow.\n",
            "\n",
            "[00:34:25] And what we're actually pulling from is more the configuration as code model, like GitHub Actions and it's really like, hey, okay, here's your workflow. It's static and as I check in code, it's just going to use that and build it. And that's kind of the approach we took with this. Are those workflows predefined or are they defined in code by the developer? The latter.\n",
            "\n",
            "[00:34:48] Yeah. So the developer can. And we have built in ones that like if you do nothing else, we'll transcribe your audio, we'll, I mean, put it into the vector database. Like you don't have to do anything. But you, a lot of it is just like opt in, do you want to change your transcription model or do you want to use GPT4 instead of GPT 3.5 for summarization?\n",
            "\n",
            "[00:35:09] So kind of hooks or tools or things that you can use to kind of insert into the process and you can just configure all that and they're reusable. And so you really would probably have, a developer would have a set of them for their application, maybe for data extraction or for conversations and they're just reusable across the platform. And then as they ingest data they'd say okay, and use this workflow. And so that was really where, when I was thinking about this last year is really the, I mean the idea of not, it's really not building blocks per se. Like we have the building blocks already connected, but you could turn knobs on each of the building blocks.\n",
            "\n",
            "[00:35:47] And that's an opinionated approach. I mean we were a bit different in that way, but I think for us we feel it makes for a really nice developer experience where it's really simple to get started. And then you could tune and you're not having to put, you're not having to think about cloud infra, you're not having to think about the LLMs. I mean it just works. And we jumped right in and started talking about rag.\n",
            "\n",
            "[00:36:13] And I think for a lot of people the kind of the use case concept when you're talking about RAG is like there's a chatbot. I issue a prompt or question around a document or some sort of content or something and I get a response that's based on some knowledge that an LLM has beyond the kind of pre trained training corpus. Right, but you feel strongly that there are, you know, use cases that are interesting beyond chatbots. You know, talk a little bit about the ones that you're excited about and you know, how you've seen them play out for sure. I mean, I think, I think the chatbot or copilot experience is kind of like the first order.\n",
            "\n",
            "[00:37:03] It's, it's direct. You're seeing a direct RAG put on a page, which is great. I mean, I think that's, there's a lot of value for that. But what we're seeing is RAG as a pattern almost. I mean it's like SQL for unstructured data, I mean, or like a way to format data in a way that you can use it for so many things like content repurposing, which we have something called publishing that you can point at the data, you can filter, create your subset of data, summarize it, and then publish it in a new format like a blog post or a long form article or social media.\n",
            "\n",
            "[00:37:40] And what we really see is that content publishing angle being that kind of second order of really, how can you use RAG as just a function, essentially a piece of functionality to deliver more value? Because it's using RAG in multiple ways. I mean, it's finding all the data, it's using LLMs, doing different things and then creating audio summaries. Like I have one set up that I have a feed on my email and every morning it actually reads through the email, summarizes it, creates an 11 labs audio summary and posts it to Slack. And you can do that with like, I don't know, four API calls or three API calls with us.\n",
            "\n",
            "[00:38:21] And that's the kind of stuff that I really want to see. Developers, what they can build with those kind of, I mean those are sort of building blocks. And that's what excites me is like be able to repurpose that content into new ways. And we're actually even looking at like dynamic image generation using LLMs to create the text, to actually put into an image template and generate marketing copy or marketing graphics and things like that. You know, if you think about the content generation use cases that you know, have played a big role in driving LL and popularity and the image generation popularity, it's like, it's static in the sense that you try to stuff, you know, a bunch of stuff into a prompt or use a prompt to a single prompt to direct the creation of some content assets.\n",
            "\n",
            "[00:39:20] So, you know, if I want to write something, I might give it a bunch of resources and stick it into a prompt. And what you're essentially describing is that, you know, just like we might use RAG to create context dynamically for an interactive system, we can also use RAG to create content or context dynamically for some of these generation use cases. So you've got now a prompt that dynamically drives the generation of a context that results in a document, an audio, a video, a blog post, an image, whatever. Yeah. And looking at like, I mean, integrating with like hey gen for kind of video clips with avatars, like we're, I mean it's, that's the kind of stuff that I think becomes interesting.\n",
            "\n",
            "[00:40:18] And the one thing when you publish that, it becomes a piece of content in the system. Like we re ingest it into the system and so it becomes searchable automatically. And eventually, I mean, we've been Waiting for feedback of like we could auto publish, like put it in your Google Docs for you or put it onto SharePoint or whatever people want. I mean that part's not hard, but I could see that being the connective tissue of dynamic content generation and then the. I'll just bring up agents for a quick sec.\n",
            "\n",
            "[00:40:46] But once you have that piece of content then you could run an editor agent workflow of like, oh hey, go have your editor, I've published this, use this online prompt, edit it, Republic, write that back into the system and you could have all these kind of editor relationships or go generate me a marketing graphic based on this piece of content and then paste it back in. So that's why when I say we're kind of like a CMS with LLMs built in, it's. I think that's the long term value is it really becomes this ecosystem for content generation that may be interactive or maybe offline and it could work both ways. And I'm taking that agent discussion as kind of a future direction thing. Something that's possible based on the foundation and not something that you have gotten very far with today.\n",
            "\n",
            "[00:41:36] I mean we have one thing we call alerts and so that's kind of like a very simplistic one step agent that, that we work with and you can, it's like basically the Slack alert of my email. And so it's a two, it's sort of a two step like summarize, publish and then notify and we can, I mean we're actually looking and kind of seeing where I mean there's Crewai and Autogen and all these different things that are great and do we integrate with them? Do we try and do something our own? I mean I think there's, I think there's so much innovation there. The one thing I will say that I really like is I've worked a lot with like actor models in the past kind of dynamic kind of workers that are interacting.\n",
            "\n",
            "[00:42:19] And I've started to hear this now on some more podcasts of there's a lot of learning that people can take from kind of the distributed systems actor model world and apply to agents. And I think the ones that I like the best are kind of learning from those. It's kind of a solved problem in a lot of ways, how to spin up a kind of like durable entities or that kind of thing. So I just hope that, I mean we don't try and reinvent the wheel on that. And I mean there's.\n",
            "\n",
            "[00:42:48] That part of it should hopefully be the easier part but it's really the memory for them and the workflows and things that'll, that'll be the more innovative part. I mean there's, you know, it's making me think about message queues and message passing. All these, you know, this infrastructure stuff that we've already figured out for you know, distributed systems. We don't necessarily have to reinvent, reinvent all that stuff. And I don't know that a lot of the stuff that I've seen reinvented in a good way like it's all single process, like let's not start there.\n",
            "\n",
            "[00:43:21] Well that's, I mean today we're an event driven system. I mean we're built in that way from scratch. Alerts and feeds are actually kind of agents like they're actually like a daemon process that's running, sitting there doing things on a periodic and running different events. We actually use this thing called durable entities from Azure, from Microsoft that is sort of this Azure model that, sorry the actor model that has state and it sort of like AWS step functions in a way where you can run different steps and define a workflow and to us, I mean that's kind of our agent model. I mean we can construct those dynamically and let Azure run it.\n",
            "\n",
            "[00:44:00] I mean they'll handle all the retries and provisioning and it just works. So if we come out with something that's agent like it's probably just going to be a. Hey, it's a layer on top of that. I mean because I think it's a nice pattern already. Is it open source?\n",
            "\n",
            "[00:44:16] Is it as a service? Is it on prem off prem? Yeah. So we're cloud platform as a service so it's closed source Today the, the SDKs are open source. We just open source the SDKs on top of it.\n",
            "\n",
            "[00:44:29] All the sample apps are open source but we take advantage of a good number of Azure backend services. So we don't do on prem today. We're going to release in the Azure marketplace this quarter that you could essentially have your own sandbox ecosystem of Graphlet running in your own Azure subscription And we do have ingest from all the clouds but currently today it's an Azure native service. Is multi cloud a priority or is it kind of a wait and see? It's a wait and see.\n",
            "\n",
            "[00:45:01] I mean I think it's, I mean we could have gone the kind of kubernetes really abstract multicloud model and there were reasons that I kind of thought okay, if we lean in on one cloud that we can get a lot more benefit. I mean the managed databases especially that we're leveraging solve a lot of problems for us. I mean, and I mean seen other companies that struggle with their kubernetes infrastructure and struggle with just all that the that side of it and we kind of build on. We decided to build on the shoulders of a couple of those things and so there's still, I mean there's a lot we do from the ingest pipeline standpoint. But I mean I'm not worrying about managing a database, I mean right now.\n",
            "\n",
            "[00:45:42] And so I don't know, I mean we're still, I mean somewhat small company and so I think it's just. Where do you want to put your eggs in what basket at this point? It's been great to see the progress you've made and how it's kind of evolved over past two, three years since you first spoke with me about it and looking forward to seeing how it continues to evolve. Thanks so much. No, it's.\n",
            "\n",
            "[00:46:10] I mean there's been so many interesting topics about rag. I think graph. Rag is really. We're still in the early days and that's what's exciting about it. I think we're, we hope we kind of have the underpinnings of it, but I think we're still going to learn so much more, I mean, over the rest of the year.\n",
            "\n",
            "[00:46:23] Well, thanks so much, Kirk, for jumping on and sharing a bit about what you're working on. Yeah, happy to be here. Thanks so much.\n",
            "\n",
            "\n",
            "-------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}