{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyNVwtZPNGqxqZnE0USw9RL6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_09_12_Publish_Audio_Review_of_Paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to ingest a PDF of an academic paper, use Sonnet 3.5 to write a comprehensive review of the paper, and listen to an audio rendition published using an [ElevenLabs](https://elevenlabs.io/) voice."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141ecdc4-67c6-49ee-fe68-009a437554a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting graphlit-client\n",
            "  Downloading graphlit_client-1.0.20240910001-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting httpx (from graphlit-client)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.9.1)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.9.0)\n",
            "Collecting websockets (from graphlit-client)\n",
            "  Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx->graphlit-client)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->graphlit-client)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n",
            "Downloading graphlit_client-1.0.20240910001-py3-none-any.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.8/197.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, h11, httpcore, httpx, graphlit-client\n",
            "Successfully installed graphlit-client-1.0.20240910001 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 websockets-13.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "# Create specification for Anthropic Sonnet 3.5\n",
        "async def create_specification():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=\"Anthropic Claude Sonnet 3.5\",\n",
        "        type=enums.SpecificationTypes.EXTRACTION,\n",
        "        serviceType=enums.ModelServiceTypes.ANTHROPIC,\n",
        "        anthropic=input_types.AnthropicModelPropertiesInput(\n",
        "            model=enums.AnthropicModels.CLAUDE_3_5_SONNET,\n",
        "        ),\n",
        "        # NOTE: Optionally, ask LLM to revise it's response, which guarantees a full length and more detailed response\n",
        "#        revisionStrategy=input_types.RevisionStrategyInput(\n",
        "#            type=enums.RevisionStrategyTypes.CUSTOM,\n",
        "#            customRevision=\"OK, that's not bad, but it needs more technical depth for this audience. You can do better than this. Reread all the context provided, and revise this into a longer, more thorough and compelling version. Don't mention anything about the revision.\",\n",
        "#            count=1\n",
        "#        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def ingest_uri(uri: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        # Using synchronous mode, so the notebook waits for the content to be ingested\n",
        "        response = await graphlit.client.ingest_uri(uri=uri, is_synchronous=True)\n",
        "\n",
        "        return response.ingest_uri.id if response.ingest_uri is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def get_content(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.get_content(content_id)\n",
        "\n",
        "        return response.content\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def publish_content(content_id: str, specification_id: str, prompt: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.publish_contents(\n",
        "            name=\"Published Summary\",\n",
        "            connector=input_types.ContentPublishingConnectorInput(\n",
        "               type=enums.ContentPublishingServiceTypes.ELEVEN_LABS_AUDIO,\n",
        "               format=enums.ContentPublishingFormats.MP3,\n",
        "               elevenLabs=input_types.ElevenLabsPublishingPropertiesInput(\n",
        "                   model=enums.ElevenLabsModels.TURBO_V2_5,\n",
        "                   voice=\"ZF6FPAbjXT4488VcRRnw\" # ElevenLabs Amelia voice\n",
        "               )\n",
        "            ),\n",
        "            summary_specification=input_types.EntityReferenceInput(\n",
        "                id=specification_id\n",
        "            ),\n",
        "            publish_prompt = prompt,\n",
        "            publish_specification=input_types.EntityReferenceInput(\n",
        "                id=specification_id\n",
        "            ),\n",
        "            filter=input_types.ContentFilter(\n",
        "                id=content_id\n",
        "            ),\n",
        "            is_synchronous=True\n",
        "        )\n",
        "\n",
        "        return response.publish_contents if response.publish_contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, HTML\n",
        "import time\n",
        "\n",
        "# Remove any existing contents; only needed for notebook example\n",
        "await delete_all_contents()\n",
        "\n",
        "print('Deleted all contents.')\n",
        "\n",
        "uri = \"https://graphlitplatform.blob.core.windows.net/samples/Attention%20Is%20All%20You%20Need.1706.03762.pdf\"\n",
        "title = \"Attention Is All You Need\"\n",
        "prompt = f\"\"\"\n",
        "Speak as if you are a Ph.D. candidate who is reviewing a paper, and talking to your peers.\n",
        "\n",
        "Follow these steps.\n",
        "\n",
        "Step 1: Think about a structure for 10 minute long, engaging AI-generated paper review, with an welcome and introduction, an in-depth discussion of 4-6 interesting topics from the paper, and a wrap-up.\n",
        "Step 2: For each topic, write 4-6 detailed paragraphs discussing it in-depth. Touch on key points for each topic which would be interesting to listeners. Mention the content metadata, entities and details from the provided summaries, as appropriate in the discussion. Remove any topic or section headings. Remove any references to podcast background music.  Remove any timestamps.\n",
        "Step 3: Combine all topics into a lengthy, single-person script which can be used to record this audio review. Use friendly and compelling conversation to write the scripts.  You can be witty, but don't be cheesy.\n",
        "Step 4: Remove any unnecessary formatting or final notes about being AI generated.\n",
        "\n",
        "Refer to the content as the '{title}' paper.\n",
        "\n",
        "Be specific when referencing persons, organizations, or any other named entities.\n",
        "\"\"\"\n",
        "\n",
        "specification_id = await create_specification()\n",
        "\n",
        "if specification_id is not None:\n",
        "    print(f'Created specification [{specification_id}]:')\n",
        "\n",
        "    content_id = await ingest_uri(uri=uri)\n",
        "\n",
        "    if content_id is not None:\n",
        "        content = await get_content(content_id)\n",
        "\n",
        "        if content is not None:\n",
        "            display(Markdown(f'### Publishing Content [{content.id}]: {content.name}...'))\n",
        "\n",
        "            published_content = await publish_content(content_id, specification_id, prompt)\n",
        "\n",
        "            if published_content is not None:\n",
        "                # Need to reload content to get presigned URL to MP3\n",
        "                published_content = await get_content(published_content.id)\n",
        "\n",
        "                if published_content is not None:\n",
        "                    display(Markdown(f'### Published [{published_content.name}]({published_content.audio_uri})'))\n",
        "\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <audio controls>\n",
        "                    <source src=\"{published_content.audio_uri}\" type=\"audio/mp3\">\n",
        "                    Your browser does not support the audio element.\n",
        "                    </audio>\n",
        "                    \"\"\"))\n",
        "\n",
        "                    display(Markdown('### Transcript'))\n",
        "                    display(Markdown(published_content.markdown))\n"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dcad0f9-21ee-4bd1-e82b-d8857a88127e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all contents.\n",
            "Created specification [728d008c-0e93-40a1-8411-eae2b23070c3]:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Publishing Content [daae0fa9-1131-4983-bcb6-379b2c071e47]: Attention Is All You Need.1706.03762.pdf..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Published [Published Summary.mp3](https://graphlit202409019591444c.blob.core.windows.net/files/e68a12d8-edc1-4ba1-a91c-1e93e2ca8181/Mezzanine/Published%20Summary.mp3?sv=2024-08-04&se=2024-09-13T07%3A14%3A34Z&sr=c&sp=rl&sig=xoTBEDV58x7GXhOdntF9jvRK5507ZuT45VGwqminqPw%3D)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                    <audio controls>\n",
              "                    <source src=\"https://graphlit202409019591444c.blob.core.windows.net/files/e68a12d8-edc1-4ba1-a91c-1e93e2ca8181/Mezzanine/Published%20Summary.mp3?sv=2024-08-04&se=2024-09-13T07%3A14%3A34Z&sr=c&sp=rl&sig=xoTBEDV58x7GXhOdntF9jvRK5507ZuT45VGwqminqPw%3D\" type=\"audio/mp3\">\n",
              "                    Your browser does not support the audio element.\n",
              "                    </audio>\n",
              "                    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Transcript"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[00:00:00] Hello, everyone, and welcome to our review of the groundbreaking\n\n[00:00:04] paper, attention is all you need.\n\n[00:00:08] I'm thrilled to discuss this work with you today as it has truly revolutionized the field of natural language processing\n\n[00:00:16] and machine translation.\n\n[00:00:18] Let's dive into some of the most fascinating aspects of this research.\n\n[00:00:22] To begin, I'd like to highlight the novel architecture introduced in this paper, the transformer.\n\n[00:00:28] What sets the transformer apart is its complete reliance on attention mechanisms,\n\n[00:00:34] eschewing the traditional recurrent and convolutional\n\n[00:00:37] approaches\n\n[00:00:38] we've seen in previous models.\n\n[00:00:40] This innovative design allows for unprecedented\n\n[00:00:44] parallelization,\n\n[00:00:46] significantly\n\n[00:00:47] reducing training time while achieving state of the art results.\n\n[00:00:51] One of the most impressive outcomes of this research\n\n[00:00:54] is the transformers performance on machine translation tasks.\n\n[00:00:58] On the WMT 2014 English to German translation benchmark,\n\n[00:01:03] it achieved a BLEU score of 28.4,\n\n[00:01:06] surpassing previous best results by over 2 BLEU points.\n\n[00:01:10] Even more remarkably,\n\n[00:01:12] on the English to French translation task,\n\n[00:01:15] it reached a blue score of 41.8,\n\n[00:01:18] setting a new single model state of the art.\n\n[00:01:21] These results are not just incremental improvements.\n\n[00:01:24] They represent a significant leap forward in translation quality. The key to the transformer's success\n\n[00:01:30] lies in its innovative use of attention mechanisms,\n\n[00:01:33] particularly the introduction\n\n[00:01:35] of multi head attention.\n\n[00:01:37] This clever approach allows the model to simultaneously\n\n[00:01:40] attend to information\n\n[00:01:42] from different representation\n\n[00:01:43] subspaces\n\n[00:01:45] at various positions.\n\n[00:01:47] In essence, it's as if the model can focus on multiple aspects of the input at once,\n\n[00:01:52] much like how we humans can process various elements of language simultaneously.\n\n[00:01:58] This multi head attention\n\n[00:02:00] is implemented through parallel attention layers or heads with the base model using 8 such heads.\n\n[00:02:07] Another fascinating aspect of the transformer\n\n[00:02:10] is how it handles sequence order\n\n[00:02:12] without relying on recurrence\n\n[00:02:15] or convolution.\n\n[00:02:17] The authors introduced\n\n[00:02:18] positional encodings,\n\n[00:02:20] which are added to the input embeddings.\n\n[00:02:23] Interestingly,\n\n[00:02:24] they opted for sinusoidal\n\n[00:02:26] functions for these encodings\n\n[00:02:28] rather than learned positional embeddings.\n\n[00:02:31] This choice allows the model to potentially extrapolate to sequence lengths\n\n[00:02:36] longer than those seen during training,\n\n[00:02:39] a valuable property for handling diverse inputs.\n\n[00:02:42] The paper also sheds light on the scalability\n\n[00:02:45] of the transformer architecture.\n\n[00:02:48] The authors present results for both a base model with 65,000,000 parameters\n\n[00:02:53] and a larger model with\n\n[00:02:55] 213,000,000 parameters.\n\n[00:02:58] Consistently,\n\n[00:02:59] the larger model demonstrated improved performance,\n\n[00:03:02] suggesting that the architecture can effectively\n\n[00:03:04] leverage increased model capacity.\n\n[00:03:08] This scalability\n\n[00:03:09] is crucial\n\n[00:03:10] for pushing the boundaries of what's possible in natural language processing tasks.\n\n[00:03:16] One of the most intriguing aspects of the transformer\n\n[00:03:19] is its generalization\n\n[00:03:21] capabilities.\n\n[00:03:23] Beyond machine translation,\n\n[00:03:25] the authors demonstrated its effectiveness on English constituency passing without significant task specific modifications.\n\n[00:03:33] This versatility\n\n[00:03:35] suggests that the transformer could serve as a general purpose sequence modeling architecture,\n\n[00:03:41] potentially\n\n[00:03:42] revolutionizing\n\n[00:03:42] a wide range of NLP tasks.\n\n[00:03:45] The visualizations\n\n[00:03:47] of the transformer's attention mechanisms\n\n[00:03:49] provide fascinating insights into how the model operates.\n\n[00:03:53] Some attention heads appear to specialize in specific\n\n[00:03:57] linguistic\n\n[00:03:58] tasks,\n\n[00:03:58] such as anaphora resolution.\n\n[00:04:01] This specialization emerges naturally during training without explicit programming,\n\n[00:04:06] showcasing the model's ability to learn complex language patterns\n\n[00:04:10] autonomously.\n\n[00:04:12] From a practical standpoint, the transformer's efficiency is noteworthy.\n\n[00:04:16] The base model was trained in just 12 hours using 8 NVIDIA per 100 GPUs,\n\n[00:04:22] while the larger model took 3, 5 days on the same hardware.\n\n[00:04:26] This relatively quick training time, combined with the model's strong performance,\n\n[00:04:31] makes it an attractive option for both research and industry applications.\n\n[00:04:36] Looking to the future,\n\n[00:04:37] the authors suggest several exciting research directions\n\n[00:04:41] stemming from their work.\n\n[00:04:43] They propose extending the transformer to other modalities,\n\n[00:04:47] such as images, audio, and video.\n\n[00:04:50] They also mention investigating local restricted attention mechanisms\n\n[00:04:55] to handle extremely large inputs and outputs efficiently.\n\n[00:05:00] Additionally, they aim to make sequence generation less sequential,\n\n[00:05:04] which could lead to even faster processing times.\n\n[00:05:08] In conclusion,\n\n[00:05:09] attention is all you need is a landmark paper that has significantly advanced the field of sequence transduction\n\n[00:05:16] and natural language processing.\n\n[00:05:19] The transformer architecture it introduces\n\n[00:05:22] has not only achieved state of the art results,\n\n[00:05:25] but has also opened up new avenues for research and application.\n\n[00:05:30] As we continue to explore and build upon this work, I'm excited to see how it will shape the future of AI and language understanding.\n\n[00:05:38] Thank you for joining me in this review,\n\n[00:05:41] and I look forward to discussing any questions or thoughts you may have.\n\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}