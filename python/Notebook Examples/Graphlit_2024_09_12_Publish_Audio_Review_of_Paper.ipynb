{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cSnEIDnT7SYyUpfap5KiK6Y_LWLQdk6s",
      "authorship_tag": "ABX9TyNnbZQp5ln0hlgk3fEMFr5Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-samples/blob/main/python/Notebook%20Examples/Graphlit_2024_09_12_Publish_Audio_Review_of_Paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "This example shows how to ingest a PDF of an academic paper, use Sonnet 3.5 to write a comprehensive review of the paper, and listen to an audio rendition published using an [ElevenLabs](https://elevenlabs.io/) voice."
      ],
      "metadata": {
        "id": "pDz1gRPjOtn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**\n",
        "\n",
        "Prior to running this notebook, you will need to [signup](https://docs.graphlit.dev/getting-started/signup) for Graphlit, and [create a project](https://docs.graphlit.dev/getting-started/create-project).\n",
        "\n",
        "You will need the Graphlit organization ID, preview environment ID and JWT secret from your created project.\n",
        "\n",
        "Assign these properties as Colab secrets: GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "laG2MXUIhNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Graphlit Python client SDK"
      ],
      "metadata": {
        "id": "NwRzDHWWienC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fefizrrh4xGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f134d4-feb7-4afd-effc-9f2671e0320e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting graphlit-client\n",
            "  Downloading graphlit_client-1.0.20240910001-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting httpx (from graphlit-client)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.8.2)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from graphlit-client) (2.9.0)\n",
            "Collecting websockets (from graphlit-client)\n",
            "  Downloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->graphlit-client) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx->graphlit-client)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->graphlit-client) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->graphlit-client)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->graphlit-client) (1.2.2)\n",
            "Downloading graphlit_client-1.0.20240910001-py3-none-any.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.8/197.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, h11, httpcore, httpx, graphlit-client\n",
            "Successfully installed graphlit-client-1.0.20240910001 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 websockets-13.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "# Create specification for Anthropic Sonnet 3.5\n",
        "async def create_specification():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=\"Anthropic Claude Sonnet 3.5\",\n",
        "        type=enums.SpecificationTypes.EXTRACTION,\n",
        "        serviceType=enums.ModelServiceTypes.ANTHROPIC,\n",
        "        anthropic=input_types.AnthropicModelPropertiesInput(\n",
        "            model=enums.AnthropicModels.CLAUDE_3_5_SONNET,\n",
        "        ),\n",
        "        # NOTE: Optionally, ask LLM to revise it's response, which guarantees a full length and more detailed response\n",
        "        revisionStrategy=input_types.RevisionStrategyInput(\n",
        "            type=enums.RevisionStrategyTypes.CUSTOM,\n",
        "            customRevision=\"OK, that's not bad, but it needs more technical depth for this audience. You can do better than this. Reread all the context provided, and revise this into a longer, more thorough and compelling version. Don't mention anything about the revision.\",\n",
        "            count=1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def ingest_uri(uri: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        # Using synchronous mode, so the notebook waits for the content to be ingested\n",
        "        response = await graphlit.client.ingest_uri(uri=uri, is_synchronous=True)\n",
        "\n",
        "        return response.ingest_uri.id if response.ingest_uri is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def get_content(content_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.get_content(content_id)\n",
        "\n",
        "        return response.content\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def publish_content(content_id: str, specification_id: str, prompt: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.publish_contents(\n",
        "            name=\"Published Summary\",\n",
        "            connector=input_types.ContentPublishingConnectorInput(\n",
        "               type=enums.ContentPublishingServiceTypes.ELEVEN_LABS_AUDIO,\n",
        "               format=enums.ContentPublishingFormats.MP3,\n",
        "               elevenLabs=input_types.ElevenLabsPublishingPropertiesInput(\n",
        "                   model=enums.ElevenLabsModels.TURBO_V2_5,\n",
        "                   voice=\"ZF6FPAbjXT4488VcRRnw\" # ElevenLabs Amelia voice\n",
        "               )\n",
        "            ),\n",
        "            summary_specification=input_types.EntityReferenceInput(\n",
        "                id=specification_id\n",
        "            ),\n",
        "            publish_prompt = prompt,\n",
        "            publish_specification=input_types.EntityReferenceInput(\n",
        "                id=specification_id\n",
        "            ),\n",
        "            filter=input_types.ContentFilter(\n",
        "                id=content_id\n",
        "            ),\n",
        "            is_synchronous=True\n",
        "        )\n",
        "\n",
        "        return response.publish_contents if response.publish_contents is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents(is_synchronous=True)"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, HTML\n",
        "import time\n",
        "\n",
        "# Remove any existing contents; only needed for notebook example\n",
        "await delete_all_contents()\n",
        "\n",
        "print('Deleted all contents.')\n",
        "\n",
        "uri = \"https://graphlitplatform.blob.core.windows.net/samples/Attention%20Is%20All%20You%20Need.1706.03762.pdf\"\n",
        "title = \"Attention Is All You Need\"\n",
        "prompt = f\"\"\"\n",
        "Speak as if you are a Ph.D. candidate who is reviewing a paper, and talking to your peers.\n",
        "\n",
        "Follow these steps.\n",
        "\n",
        "Step 1: Think about a structure for 10 minute long, engaging AI-generated paper review, with an welcome and introduction, an in-depth discussion of 4-6 interesting topics from the paper, and a wrap-up.\n",
        "Step 2: For each topic, write 4-6 detailed paragraphs discussing it in-depth. Touch on key points for each topic which would be interesting to listeners. Mention the content metadata, entities and details from the provided summaries, as appropriate in the discussion. Remove any topic or section headings. Remove any references to podcast background music.  Remove any timestamps.\n",
        "Step 3: Combine all topics into a lengthy, single-person script which can be used to record this audio review. Use friendly and compelling conversation to write the scripts.  You can be witty, but don't be cheesy.\n",
        "Step 4: Remove any unnecessary formatting or final notes about being AI generated.\n",
        "\n",
        "Refer to the content as the '{title}' paper.\n",
        "\n",
        "Be specific when referencing persons, organizations, or any other named entities.\n",
        "\"\"\"\n",
        "\n",
        "specification_id = await create_specification()\n",
        "\n",
        "if specification_id is not None:\n",
        "    print(f'Created specification [{specification_id}]:')\n",
        "\n",
        "    content_id = await ingest_uri(uri=uri)\n",
        "\n",
        "    if content_id is not None:\n",
        "        content = await get_content(content_id)\n",
        "\n",
        "        if content is not None:\n",
        "            display(Markdown(f'### Publishing Content [{content.id}]: {content.name}...'))\n",
        "\n",
        "            published_content = await publish_content(content_id, specification_id, prompt)\n",
        "\n",
        "            if published_content is not None:\n",
        "                # Need to reload content to get presigned URL to MP3\n",
        "                published_content = await get_content(published_content.id)\n",
        "\n",
        "                if published_content is not None:\n",
        "                    display(Markdown(f'### Published [{published_content.name}]({published_content.audio_uri})'))\n",
        "\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <audio controls>\n",
        "                    <source src=\"{published_content.audio_uri}\" type=\"audio/mp3\">\n",
        "                    Your browser does not support the audio element.\n",
        "                    </audio>\n",
        "                    \"\"\"))\n",
        "\n",
        "                    display(Markdown('### Transcript'))\n",
        "                    display(Markdown(published_content.markdown))\n"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d15cdc0-c2c2-4dcb-d3db-c21cee90c5a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted all contents.\n",
            "Created specification [72075ed5-7b82-4aa1-9d37-aee9bc2f1ed4]:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Publishing Content [b755253b-7e41-40f7-a3cf-f45cfbc64844]: Attention Is All You Need.1706.03762.pdf..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Published [Published Summary.mp3](https://graphlit202409019591444c.blob.core.windows.net/files/aa3b1c6e-fca9-4326-b6cd-134d327fc196/Mezzanine/Published%20Summary.mp3?sv=2024-08-04&se=2024-09-11T11%3A23%3A37Z&sr=c&sp=rl&sig=P4B7BRtpeRVs0XXm1WzevhJO4UHN4rc3nM0uWNi9zBg%3D)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                    <audio controls>\n",
              "                    <source src=\"https://graphlit202409019591444c.blob.core.windows.net/files/aa3b1c6e-fca9-4326-b6cd-134d327fc196/Mezzanine/Published%20Summary.mp3?sv=2024-08-04&se=2024-09-11T11%3A23%3A37Z&sr=c&sp=rl&sig=P4B7BRtpeRVs0XXm1WzevhJO4UHN4rc3nM0uWNi9zBg%3D\" type=\"audio/mp3\">\n",
              "                    Your browser does not support the audio element.\n",
              "                    </audio>\n",
              "                    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Transcript"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[00:00:00] Greetings, colleagues.\n\n[00:00:01] Today, we're delving into the seminal paper, attention is all you need,\n\n[00:00:07] which has fundamentally\n\n[00:00:09] reshaped our understanding of sequence transduction models in natural language processing.\n\n[00:00:15] As we explore this groundbreaking work, I'll be highlighting its technical innovations,\n\n[00:00:21] architectural\n\n[00:00:22] nuances,\n\n[00:00:23] and far reaching implications for our field.\n\n[00:00:26] Let's begin with the core innovation,\n\n[00:00:29] the transformer architecture.\n\n[00:00:32] This model represents a paradigm shift in sequence transduction,\n\n[00:00:36] relying solely on attention mechanisms\n\n[00:00:38] and completely eschewing recurrence and convolutions.\n\n[00:00:42] The implications of this approach are profound,\n\n[00:00:45] both in terms of performance and computational efficiency.\n\n[00:00:49] The transformers architecture is elegantly simple\n\n[00:00:52] yet remarkably powerful.\n\n[00:00:55] It consists of stacked self attention\n\n[00:00:58] and point wise fully connected layers for both the encoder and decoder.\n\n[00:01:04] The base model comprises 6 identical layers in each of these components.\n\n[00:01:10] Each layer contains 2 sublayers,\n\n[00:01:13] a multi head self attention mechanism,\n\n[00:01:16] and a position wise feed forward network.\n\n[00:01:19] The authors employ residual connections around each sublayer\n\n[00:01:22] followed by layer normalization,\n\n[00:01:25] a design choice that facilitates\n\n[00:01:27] gradient flow through the network.\n\n[00:01:30] Now let's dissect the multi head attention mechanism,\n\n[00:01:34] which is arguably the paper's most significant contribution.\n\n[00:01:38] This mechanism allows the model to jointly attend to information from different representation subspaces\n\n[00:01:44] at different positions.\n\n[00:01:46] In practice,\n\n[00:01:47] this means the model can capture various aspects of the input sequence simultaneously,\n\n[00:01:53] be it syntactic, semantic,\n\n[00:01:56] or other linguistic features.\n\n[00:01:58] The multi head attention\n\n[00:02:00] operates by first projecting the queries,\n\n[00:02:03] keys, and values each times with different learned linear projections.\n\n[00:02:08] These projections are then fed into h parallel attention layers or heads.\n\n[00:02:14] The outputs of these heads are concatenated\n\n[00:02:16] and once again projected,\n\n[00:02:18] resulting in the final output.\n\n[00:02:21] This approach allows the model to capture different types of relationships\n\n[00:02:25] within the same attention mechanism.\n\n[00:02:28] The authors use scaled dot product attention,\n\n[00:02:32] defined as\n\n[00:02:33] attention qkv.\n\n[00:02:36] X softmax keys,\n\n[00:02:39] v,\n\n[00:02:42] where q, k, and v are the queries, keys, and values respectively, and d k is the dimension of the keys. The scaling factor of decay\n\n[00:02:52] is crucial here\n\n[00:02:53] as it counteracts the effect of large magnitude dot products pushing the softmax function into regions with extremely small gradients.\n\n[00:03:01] One of the transformers key strengths is its ability to handle long range dependencies in sequences.\n\n[00:03:08] Unlike RNNs, which process sequences step by step, the self attention mechanism creates direct connections between all positions in a sequence.\n\n[00:03:17] This results in a constant\n\n[00:03:19] o one\n\n[00:03:20] maximum path length between any two positions\n\n[00:03:23] regardless\n\n[00:03:24] of sequence length.\n\n[00:03:26] Comparatively,\n\n[00:03:28] RNNs have a path length of o n.\n\n[00:03:31] And even advanced convolutional\n\n[00:03:33] architectures\n\n[00:03:34] like bitet have\n\n[00:03:37] o\n\n[00:03:38] o log n path lengths.\n\n[00:03:41] This constant path length is a significant factor\n\n[00:03:44] in the transformer's superior performance on tasks requiring long range understanding.\n\n[00:03:50] An intriguing aspect of the transformer\n\n[00:03:52] is its approach to encoding positional information.\n\n[00:03:56] Since the model contains no recurrence or convolution,\n\n[00:04:00] it needs a way to inject sequence order information.\n\n[00:04:03] The author's solution is to use sinusoidal\n\n[00:04:06] positional encodings\n\n[00:04:08] defined by sine and cosine functions of different frequencies.\n\n[00:04:12] This approach allows the model to extrapolate to sequence lengths longer than those encountered during training,\n\n[00:04:18] a valuable property for generalization.\n\n[00:04:21] Let's talk about the model's computational characteristics.\n\n[00:04:25] The self attention layer has a complexity of o l two wasp\n\n[00:04:30] d,\n\n[00:04:32] where n is the sequence length and d is the representation\n\n[00:04:36] dimension.\n\n[00:04:38] While this quadratic dependency on sequence length could potentially be problematic\n\n[00:04:43] for very long sequences,\n\n[00:04:45] for the lengths typically encountered in translation tasks,\n\n[00:04:49] n 100,\n\n[00:04:50] This is actually more efficient than the o n touch 2 complexity\n\n[00:04:55] of recurrent layers typically used in sequence transduction models.\n\n[00:04:59] The transformer's performance scales impressively with model size and computational resources.\n\n[00:05:05] The base model with 65,000,000\n\n[00:05:08] parameters achieves state of the art results,\n\n[00:05:11] but the larger model, boasting 213,000,000\n\n[00:05:14] parameters,\n\n[00:05:15] significantly\n\n[00:05:16] outperforms it.\n\n[00:05:17] On the WMT\n\n[00:05:19] 2 1,014\n\n[00:05:20] English to German translation task,\n\n[00:05:22] the big model achieves a BLEU score of 28.4,\n\n[00:05:26] improving over the previous best results\n\n[00:05:29] by more than 2 to BLEU points.\n\n[00:05:32] For English to French, it reaches an impressive 41.8,\n\n[00:05:36] setting\n\n[00:05:37] a new single model state of the art.\n\n[00:05:39] These models were trained on 8 NVIDIA p a 100 GPUs,\n\n[00:05:43] with the base model taking about 12 hours and the big model 3.5 days.\n\n[00:05:49] The authors used the Adam optimizer with a custom learning rate schedule,\n\n[00:05:53] including a warm up period.\n\n[00:05:56] They also employed regularization techniques such as residual dropout with a rate of 0.1\n\n[00:06:02] and label smoothing,\n\n[00:06:04] One of the most fascinating aspects of this work is the model's generalizability.\n\n[00:06:09] The authors demonstrated this by adapting the transformer for English constituency\n\n[00:06:14] passing\n\n[00:06:14] with minimal changes.\n\n[00:06:16] Using a 4 layer model with d model or 10 to 4, trained on approximately\n\n[00:06:22] 40 k WSJ sentences,\n\n[00:06:24] they achieved an f one score of 92.7\n\n[00:06:26] in semi supervised passing,\n\n[00:06:29] outperforming previous state of the art models.\n\n[00:06:32] This success suggests that the transformer is capturing fundamental aspects of sequential data that are applicable across a wide range of tasks.\n\n[00:06:42] The paper\n\n[00:06:43] also provides valuable insights into the model's inner workings through attention visualizations.\n\n[00:06:50] These reveal that different attention heads\n\n[00:06:53] learn to perform different tasks with\n\n[00:06:55] some specializing in syntactic relationships and others in semantic relationships.\n\n[00:07:00] For instance,\n\n[00:07:01] some heads appear to focus on the relationship between verbs and their direct objects,\n\n[00:07:06] while others capture coreference relationships.\n\n[00:07:10] As we consider the implications of this work,\n\n[00:07:13] it's clear that the transformer architecture opens up new avenues for research in attention based models\n\n[00:07:19] and nonrecurrent\n\n[00:07:21] sequence modeling.\n\n[00:07:22] It challenges the long held assumption that recurrence or convolution is necessary for effective sequence modeling,\n\n[00:07:29] potentially\n\n[00:07:30] leading to a paradigm shift in how we approach sequential data problems.\n\n[00:07:36] Looking forward, the authors suggest several promising directions for future work.\n\n[00:07:41] They plan to extend the model to other modalities,\n\n[00:07:44] like images and audio,\n\n[00:07:46] which could lead to exciting developments in multimodal learning.\n\n[00:07:50] They also aim to investigate local restricted attention mechanisms\n\n[00:07:54] to handle very large inputs and outputs more efficiently,\n\n[00:07:58] addressing the quadratic complexity issue\n\n[00:08:01] of the current self attention mechanism.\n\n[00:08:04] In conclusion,\n\n[00:08:06] attention is all you need\n\n[00:08:08] is a landmark paper that has reshaped the landscape of natural language processing.\n\n[00:08:14] Its novel architecture,\n\n[00:08:15] impressive performance,\n\n[00:08:17] and broad applicability\n\n[00:08:19] make it a cornerstone of modern NLP research.\n\n[00:08:23] As we continue to explore the possibilities\n\n[00:08:25] of attention based models,\n\n[00:08:27] we can expect to see its influence extend beyond NLP to other domains,\n\n[00:08:32] like computer vision and reinforcement learning.\n\n[00:08:35] The transformer success has already spawned a new generation of models, including BERT,\n\n[00:08:40] GPT, and their successors,\n\n[00:08:43] which have pushed the boundaries of what's possible in NLP.\n\n[00:08:46] As we stand on the shoulders of this giant, we're poised to make even more exciting discoveries\n\n[00:08:52] in the realm of artificial intelligence\n\n[00:08:54] and machine\n\n[00:08:56] learning.\n\n[00:08:57] Thank you for joining me in this deep dive into the transformer architecture.\n\n[00:09:03] The future of NLP is bright, and papers like this are lighting the way forward.\n\n[00:09:08] I look forward to seeing how our community builds upon this groundbreaking work in the years to come.\n\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}